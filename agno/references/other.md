# Agno - Other

**Pages:** 1373

---

## Now if we pass a new session_state, it will overwrite the stored session_state.

**URL:** llms-txt#now-if-we-pass-a-new-session_state,-it-will-overwrite-the-stored-session_state.

**Contents:**
- Team Member Interactions

agent.print_response(
    "Can you tell me what is in your session_state?",
    session_state={"secret_number": 43},
    stream=True,
)
print(f"Stored session state: {agent.get_session_state()}")
python  theme={null}
from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.team.team import Team

from agno.db.sqlite import SqliteDb
from agno.tools.duckduckgo import DuckDuckGoTools

db = SqliteDb(db_file="tmp/agents.db")

web_research_agent = Agent(
    name="Web Research Agent",
    model=OpenAIChat(id="gpt-5-mini"),
    tools=[DuckDuckGoTools()],
    instructions="You are a web research agent that can answer questions from the web.",
)

report_agent = Agent(
    name="Report Agent",
    model=OpenAIChat(id="gpt-5-mini"),
    instructions="You are a report agent that can write a report from the web research.",
)

team = Team(
    model=OpenAIChat(id="gpt-5-mini"),
    db=db,
    members=[web_research_agent, report_agent],
    share_member_interactions=True,
    instructions=[
        "You are a team of agents that can research the web and write a report.",
        "First, research the web for information about the topic.",
        "Then, use your report agent to write a report from the web research.",
    ],
    show_members_responses=True,
    debug_mode=True,
)

team.print_response("How are LEDs made?")
```

**Examples:**

Example 1 (unknown):
```unknown
## Team Member Interactions

Agent Teams can share interactions between members, allowing agents to learn from each other's outputs:
```

---

## Post-hooks

**URL:** llms-txt#post-hooks

**Contents:**
- Parameters

Source: https://docs.agno.com/reference/hooks/post-hooks

Running a post-hook is handled automatically during the Agent or Team run. These are the parameters that will be injected:

| Parameter       | Type                           | Default  | Description                                                                  |
| --------------- | ------------------------------ | -------- | ---------------------------------------------------------------------------- |
| `agent`         | `Agent`                        | Required | The Agent that is running the post-hook. Only present in Agent runs.         |
| `team`          | `Team`                         | Required | The Team that is running the post-hook. Only present in Team runs.           |
| `run_output`    | `RunOutput` or `TeamRunOutput` | Required | The output of the current Agent or Team run.                                 |
| `session`       | `AgentSession`                 | Required | The `AgentSession` or `TeamSession` object representing the current session. |
| `session_state` | `Optional[Dict[str, Any]]`     | `None`   | The session state of the current session.                                    |
| `dependencies`  | `Optional[Dict[str, Any]]`     | `None`   | The dependencies of the current run.                                         |
| `metadata`      | `Optional[Dict[str, Any]]`     | `None`   | The metadata of the current run.                                             |
| `user_id`       | `Optional[str]`                | `None`   | The contextual user ID, if any.                                              |
| `debug_mode`    | `Optional[bool]`               | `None`   | Whether the debug mode is enabled.                                           |

---

## We will search DDG but limit the site to Politifact

**URL:** llms-txt#we-will-search-ddg-but-limit-the-site-to-politifact

**Contents:**
- Usage

agent = Agent(
    tools=[DuckDuckGoTools(modifier="site:politifact.com")]
)
agent.print_response(
    "Is Taylor Swift promoting energy-saving devices with Elon Musk?", markdown=False
)
bash  theme={null}
    export OPENAI_API_KEY=xxx
    bash  theme={null}
    pip install -U ddgs openai agno
    bash Mac theme={null}
      python cookbook/tools/duckduckgo_tools.py
      bash Windows theme={null}
      python cookbook/tools/duckduckgo_tools.py
      ```
    </CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Set your API key">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Setting Environment Variables

**URL:** llms-txt#setting-environment-variables

**Contents:**
- macOS
  - Setting Environment Variables in Shell
- Windows
  - Setting Environment Variables in PowerShell
  - Setting Environment Variables in Windows Command Prompt

Source: https://docs.agno.com/faq/environment-variables

To configure your environment for applications, you may need to set environment variables. This guide provides instructions for setting environment variables in both macOS (Shell) and Windows (PowerShell and Windows Command Prompt).

### Setting Environment Variables in Shell

#### Temporary Environment Variables

These environment variables will only be available in the current shell session.

To display the environment variable:

#### Permanent Environment Variables

To make environment variables persist across sessions, add them to your shell configuration file (e.g., `.bashrc`, `.bash_profile`, `.zshrc`).

To display the environment variable:

### Setting Environment Variables in PowerShell

#### Temporary Environment Variables

These environment variables will only be available in the current PowerShell session.

To display the environment variable:

#### Permanent Environment Variables

To make environment variables persist across sessions, add them to your PowerShell profile script (e.g., `Microsoft.PowerShell_profile.ps1`).

Add the following line to the profile script:

Save and close the file, then reload the profile:

To display the environment variable:

### Setting Environment Variables in Windows Command Prompt

#### Temporary Environment Variables

These environment variables will only be available in the current Command Prompt session.

To display the environment variable:

#### Permanent Environment Variables

To make environment variables persist across sessions, you can use the `setx` command:

Note: After setting an environment variable using `setx`, you need to restart the Command Prompt or any applications that need to read the new environment variable.

To display the environment variable in a new Command Prompt session:

By following these steps, you can effectively set and display environment variables in macOS Shell, Windows Command Prompt, and PowerShell. This will ensure your environment is properly configured for your applications.

**Examples:**

Example 1 (unknown):
```unknown
To display the environment variable:
```

Example 2 (unknown):
```unknown
#### Permanent Environment Variables

To make environment variables persist across sessions, add them to your shell configuration file (e.g., `.bashrc`, `.bash_profile`, `.zshrc`).

For Zsh:
```

Example 3 (unknown):
```unknown
To display the environment variable:
```

Example 4 (unknown):
```unknown
## Windows

### Setting Environment Variables in PowerShell

#### Temporary Environment Variables

These environment variables will only be available in the current PowerShell session.
```

---

## YouTube Tools

**URL:** llms-txt#youtube-tools

**Contents:**
- Code
- Usage

Source: https://docs.agno.com/examples/concepts/tools/others/youtube

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Set your API key">
    
  </Step>

<Step title="Install libraries">
    
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Set your API key">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Books Recommender

**URL:** llms-txt#books-recommender

**Contents:**
- Code

Source: https://docs.agno.com/examples/use-cases/agents/books-recommender

This example shows how to create an intelligent book recommendation system that provides
comprehensive literary suggestions based on your preferences. The agent combines book databases,
ratings, reviews, and upcoming releases to deliver personalized reading recommendations.

Example prompts to try:

* "I loved 'The Seven Husbands of Evelyn Hugo' and 'Daisy Jones & The Six', what should I read next?"
* "Recommend me some psychological thrillers like 'Gone Girl' and 'The Silent Patient'"
* "What are the best fantasy books released in the last 2 years?"
* "I enjoy historical fiction with strong female leads, any suggestions?"
* "Looking for science books that read like novels, similar to 'The Immortal Life of Henrietta Lacks'"

```python books_recommender.py theme={null}
from textwrap import dedent

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.exa import ExaTools

book_recommendation_agent = Agent(
    name="Shelfie",
    tools=[ExaTools()],
    model=OpenAIChat(id="gpt-5-mini"),
    description=dedent("""\
        You are Shelfie, a passionate and knowledgeable literary curator with expertise in books worldwide! 📚

Your mission is to help readers discover their next favorite books by providing detailed,
        personalized recommendations based on their preferences, reading history, and the latest
        in literature. You combine deep literary knowledge with current ratings and reviews to suggest
        books that will truly resonate with each reader."""),
    instructions=dedent("""\
        Approach each recommendation with these steps:

1. Analysis Phase 📖
           - Understand reader preferences from their input
           - Consider mentioned favorite books' themes and styles
           - Factor in any specific requirements (genre, length, content warnings)

2. Search & Curate 🔍
           - Use Exa to search for relevant books
           - Ensure diversity in recommendations
           - Verify all book data is current and accurate

3. Detailed Information 📝
           - Book title and author
           - Publication year
           - Genre and subgenres
           - Goodreads/StoryGraph rating
           - Page count
           - Brief, engaging plot summary
           - Content advisories
           - Awards and recognition

4. Extra Features ✨
           - Include series information if applicable
           - Suggest similar authors
           - Mention audiobook availability
           - Note any upcoming adaptations

Presentation Style:
        - Use clear markdown formatting
        - Present main recommendations in a structured table
        - Group similar books together
        - Add emoji indicators for genres (📚 🔮 💕 🔪)
        - Minimum 5 recommendations per query
        - Include a brief explanation for each recommendation
        - Highlight diversity in authors and perspectives
        - Note trigger warnings when relevant"""),
    markdown=True,
    add_datetime_to_context=True,
    )

---

## Jina Reader Tools

**URL:** llms-txt#jina-reader-tools

**Contents:**
- Code
- Usage

Source: https://docs.agno.com/examples/concepts/tools/web_scrape/jina_reader

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Set your API key">
    
  </Step>

<Step title="Install libraries">
    
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Set your API key">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Remove Vectors

**URL:** llms-txt#remove-vectors

**Contents:**
- Code

Source: https://docs.agno.com/examples/concepts/knowledge/basic-operations/remove-vectors

```python 10_remove_vectors.py theme={null}
import asyncio
from agno.knowledge.knowledge import Knowledge
from agno.vectordb.pgvector import PgVector

---

## Company Description Workflow

**URL:** llms-txt#company-description-workflow

**Contents:**
- Overview
- Getting Started
  - Prerequisites
  - Quick Setup
- Code Structure
  - 1. Agents (`agents.py`)
  - 2. Prompts (`prompts.py`)
  - 3. Workflow Implementation (`run_workflow.py`)
- Key Features
- Usage Example

Source: https://docs.agno.com/examples/use-cases/workflows/company-description

A workflow that generates comprehensive supplier profiles by gathering information from multiple sources and delivers them via email.

This workflow combines web crawling, search engines, Wikipedia, and competitor analysis to create detailed supplier profiles. It processes company information through 4 specialized agents running in parallel, then generates a structured markdown report and sends it via email.

The workflow uses workflow session state management to cache analysis results. If the same supplier is analyzed again, it returns cached results instead of re-running the expensive analysis pipeline.

* OpenAI API key
* Resend API key for emails \[[https://resend.com/api-keys](https://resend.com/api-keys)]
* Firecrawl API key for web crawling \[[https://www.firecrawl.dev/app/api-keys](https://www.firecrawl.dev/app/api-keys)]

This company description workflow consists of three main files:

### 1. Agents (`agents.py`)

Specialized agents for gathering information from multiple sources:

### 2. Prompts (`prompts.py`)

Detailed instructions for each specialized agent:

### 3. Workflow Implementation (`run_workflow.py`)

Complete workflow with parallel information gathering and email delivery:

* **🔄 Parallel Processing**: Four agents gather information simultaneously for maximum efficiency
* **🌐 Multi-Source Data**: Combines web crawling, search engines, Wikipedia, and competitor analysis
* **📧 Email Integration**: Automatically sends formatted reports via email using Resend
* **📄 Markdown Formatting**: Generates structured, readable reports in HTML format
* **🏗️ Modular Design**: Clean separation of agents, prompts, and workflow logic
* **⚡ Efficient Execution**: Uses parallel steps to minimize execution time
* **🎯 Type Safety**: Pydantic models for structured data validation

```python  theme={null}

**Examples:**

Example 1 (unknown):
```unknown
Install dependencies
```

Example 2 (unknown):
```unknown
## Code Structure

This company description workflow consists of three main files:

### 1. Agents (`agents.py`)

Specialized agents for gathering information from multiple sources:
```

Example 3 (unknown):
```unknown
### 2. Prompts (`prompts.py`)

Detailed instructions for each specialized agent:
```

Example 4 (unknown):
```unknown
### 3. Workflow Implementation (`run_workflow.py`)

Complete workflow with parallel information gathering and email delivery:
```

---

## Cancel Workflow Run

**URL:** llms-txt#cancel-workflow-run

Source: https://docs.agno.com/reference-api/schema/workflows/cancel-workflow-run

post /workflows/{workflow_id}/runs/{run_id}/cancel
Cancel a currently executing workflow run, stopping all active steps and cleanup.
**Note:** Complex workflows with multiple parallel steps may take time to fully cancel.

---

## Brandfetch Tools

**URL:** llms-txt#brandfetch-tools

**Contents:**
- Code
- Usage

Source: https://docs.agno.com/examples/concepts/tools/others/brandfetch

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Set your API keys">
    
  </Step>

<Step title="Install libraries">
    
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Set your API keys">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Reader

**URL:** llms-txt#reader

Source: https://docs.agno.com/reference/knowledge/reader/base

Reader is the base class for all reader classes in Agno.

<Snippet file="base-reader-reference.mdx" />

---

## pprint(memories)

**URL:** llms-txt#pprint(memories)

**Contents:**
- Usage

bash  theme={null}
    ./cookbook/run_pgvector.sh
    bash  theme={null}
    pip install agno openai psycopg sqlalchemy
    bash  theme={null}
    export OPENAI_API_KEY=****
    bash  theme={null}
    python cookbook/examples/teams/memory/01_team_with_memory_manager.py
    ```
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Set up PostgreSQL database">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Install required libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Set environment variables">
```

Example 4 (unknown):
```unknown
</Step>

  <Step title="Run the agent">
```

---

## Overriding Routes

**URL:** llms-txt#overriding-routes

**Contents:**
- Example

Source: https://docs.agno.com/agent-os/customize/os/override_routes

Complete AgentOS setup with custom routes

Override the routes of the AgentOS with your own.

This example demonstrates the `on_route_conflict="preserve_base_app"` functionality which allows your
custom routes to take precedence over conflicting AgentOS routes.

When `on_route_conflict="preserve_base_app"`:

* Your custom routes will be preserved
* Conflicting AgentOS routes will be skipped
* Non-conflicting AgentOS routes will still be added

When `on_route_conflict="preserve_agentos"` (default):

* AgentOS routes will override your custom routes
* Warnings will be logged about the conflicts

<Steps>
  <Step title="Code">
    
  </Step>

<Snippet file="create-venv-step.mdx" />

<Step title="Set Environment Variables">
    
  </Step>

<Step title="Install libraries">
    
  </Step>

<Step title="Setup PostgreSQL Database">
    
  </Step>

<Step title="Run Example with Python">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
</Step>

  <Snippet file="create-venv-step.mdx" />

  <Step title="Set Environment Variables">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Setup PostgreSQL Database">
```

Example 4 (unknown):
```unknown
</Step>

  <Step title="Run Example with Python">
    <CodeGroup>
```

---

## MongoDB Atlas connection string

**URL:** llms-txt#mongodb-atlas-connection-string

**Contents:**
- MongoDB Params
- Developer Resources

"""
Example connection strings:
"mongodb+srv://<username>:<password>@cluster0.mongodb.net/?retryWrites=true&w=majority"
"mongodb://localhost/?directConnection=true"
"""
mdb_connection_string = ""

knowledge_base = Knowledge(
    vector_db=MongoDb(
        collection_name="recipes",
        db_url=mdb_connection_string,
        wait_until_index_ready=60,
        wait_after_insert=300
    ),
)  # adjust wait_after_insert and wait_until_index_ready to your needs

if __name__ == "__main__":
    knowledge_base.add_content(
        url="https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf"
    )

agent = Agent(knowledge=knowledge_base)
    agent.print_response("How to make Thai curry?", markdown=True)
python async_mongodb.py theme={null}
    import asyncio

from agno.agent import Agent
    from agno.knowledge.knowledge import Knowledge
    from agno.vectordb.mongodb import MongoDb

# MongoDB Atlas connection string
    """
    Example connection strings:
    "mongodb+srv://<username>:<password>@cluster0.mongodb.net/?retryWrites=true&w=majority"
    "mongodb://localhost:27017/agno?authSource=admin"
    """
    mdb_connection_string = "mongodb+srv://<username>:<password>@cluster0.mongodb.net/?retryWrites=true&w=majority"

knowledge_base = Knowledge(
        vector_db=MongoDb(
            collection_name="recipes",
            db_url=mdb_connection_string,
        ),
    )

# Create and use the agent
    agent = Agent(knowledge=knowledge_base)

if __name__ == "__main__":
        # Load knowledge base asynchronously
        asyncio.run(knowledge_base.add_content_async(
                url="https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf"
            )
        )

# Create and use the agent asynchronously
        asyncio.run(agent.aprint_response("How to make Thai curry?", markdown=True))
    ```

<Tip className="mt-4">
      Use <code>aload()</code> and <code>aprint\_response()</code> methods with <code>asyncio.run()</code> for non-blocking operations in high-throughput applications.
    </Tip>
  </div>
</Card>

<Snippet file="vectordb_mongodb_params.mdx" />

## Developer Resources

* View [Cookbook (Sync)](https://github.com/agno-agi/agno/blob/main/cookbook/knowledge/vector_db/mongo_db/mongo_db.py)
* View [Cookbook (Async)](https://github.com/agno-agi/agno/blob/main/cookbook/knowledge/vector_db/mongo_db/async_mongo_db.py)

**Examples:**

Example 1 (unknown):
```unknown
<Card title="Async Support ⚡">
  <div className="mt-2">
    <p>
      MongoDB also supports asynchronous operations, enabling concurrency and leading to better performance.
    </p>
```

---

## Initialize Zoom tools with credentials

**URL:** llms-txt#initialize-zoom-tools-with-credentials

zoom_tools = ZoomTools(
    account_id="your_account_id",
    client_id="your_client_id",
    client_secret="your_client_secret"
)

---

## Cal.com

**URL:** llms-txt#cal.com

**Contents:**
- Prerequisites
- Example
- Toolkit Params
- Toolkit Functions
- Developer Resources

Source: https://docs.agno.com/concepts/tools/toolkits/others/calcom

The following example requires the `pytz` and `requests` libraries.

The following agent will use Cal.com to list all events in your Cal.com account for tomorrow.

| Parameter                      | Type   | Default | Description                                |
| ------------------------------ | ------ | ------- | ------------------------------------------ |
| `api_key`                      | `str`  | `None`  | Cal.com API key                            |
| `event_type_id`                | `int`  | `None`  | Event type ID for scheduling               |
| `user_timezone`                | `str`  | `None`  | User's timezone (e.g. "America/New\_York") |
| `enable_get_available_slots`   | `bool` | `True`  | Enable getting available time slots        |
| `enable_create_booking`        | `bool` | `True`  | Enable creating new bookings               |
| `enable_get_upcoming_bookings` | `bool` | `True`  | Enable getting upcoming bookings           |
| `enable_reschedule_booking`    | `bool` | `True`  | Enable rescheduling bookings               |
| `enable_cancel_booking`        | `bool` | `True`  | Enable canceling bookings                  |

| Function                | Description                                      |
| ----------------------- | ------------------------------------------------ |
| `get_available_slots`   | Gets available time slots for a given date range |
| `create_booking`        | Creates a new booking with provided details      |
| `get_upcoming_bookings` | Gets list of upcoming bookings                   |
| `get_booking_details`   | Gets details for a specific booking              |
| `reschedule_booking`    | Reschedules an existing booking                  |
| `cancel_booking`        | Cancels an existing booking                      |

## Developer Resources

* View [Tools](https://github.com/agno-agi/agno/blob/main/libs/agno/agno/tools/calcom.py)
* View [Cookbook](https://github.com/agno-agi/agno/tree/main/cookbook/tools/calcom_tools.py)

**Examples:**

Example 1 (unknown):
```unknown

```

Example 2 (unknown):
```unknown
## Example

The following agent will use Cal.com to list all events in your Cal.com account for tomorrow.
```

---

## No local setup required - just set OLLAMA_API_KEY

**URL:** llms-txt#no-local-setup-required---just-set-ollama_api_key

agent = Agent(model=Ollama(id="gpt-oss:120b-cloud", host="https://ollama.com"))
agent.print_response("Share a 2 sentence horror story", stream=True)
```

---

## Postgres for Team

**URL:** llms-txt#postgres-for-team

**Contents:**
- Usage
  - Run PgVector
- Params
- Developer Resources

Source: https://docs.agno.com/examples/concepts/db/postgres/postgres_for_team

Agno supports using PostgreSQL as a storage backend for Teams using the `PostgresDb` class.

Install [docker desktop](https://docs.docker.com/desktop/install/mac-install/) and run **PgVector** on port **5532** using:

<Snippet file="db-postgres-params.mdx" />

## Developer Resources

* View [Cookbook](https://github.com/agno-agi/agno/tree/main/cookbook/db/postgres/postgres_for_team.py)

**Examples:**

Example 1 (unknown):
```unknown

```

---

## Start the session with user 1 (This means "how are you?" in French)

**URL:** llms-txt#start-the-session-with-user-1-(this-means-"how-are-you?"-in-french)

team.print_response(
    "comment ça va?",
    user_id=user_1_id,
    session_id=user_1_session_id,
)

---

## Milvus

**URL:** llms-txt#milvus

Source: https://docs.agno.com/reference/vector_db/milvus

<Snippet file="vector-db-milvus-reference.mdx" />

---

## Keyword Search

**URL:** llms-txt#keyword-search

**Contents:**
- Code

Source: https://docs.agno.com/examples/concepts/knowledge/search_type/keyword-search

```python keyword_search.py theme={null}
from agno.knowledge.knowledge import Knowledge
from agno.vectordb.pgvector import PgVector, SearchType

db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"

---

## Tools

**URL:** llms-txt#tools

**Contents:**
- Using a Toolkit
- Writing your own Tools
  - Accessing built-in parameters in tools
- MCP Tools
- Developer Resources

Source: https://docs.agno.com/concepts/agents/tools

Learn how to use tools in Agno to build AI agents.

**Agents use tools to take actions and interact with external systems**.

Tools are functions that an Agent can run to achieve tasks. For example: searching the web, running SQL, sending an email or calling APIs. You can use any python function as a tool or use a pre-built Agno **toolkit**.

The general syntax is:

Agno provides many pre-built **toolkits** that you can add to your Agents. For example, let's use the DuckDuckGo toolkit to search the web.

<Tip>
  You can find more toolkits in the [Toolkits](/concepts/tools/toolkits) guide.
</Tip>

<Steps>
  <Step title="Create Web Search Agent">
    Create a file `web_search.py`

<Step title="Run the agent">
    Install libraries

## Writing your own Tools

For more control, write your own python functions and add them as tools to an Agent. For example, here's how to add a `get_top_hackernews_stories` tool to an Agent.

* [Available toolkits](/concepts/tools/toolkits)
* [Creating your own tools](/concepts/tools/custom-tools)

### Accessing built-in parameters in tools

You can access agent attributes like `session_state`, `dependencies`, `agent` and `team` in your tools.

See more in the [Tool Built-in Parameters](/concepts/tools/overview#tool-built-in-parameters) section.

Agno supports [Model Context Protocol (MCP)](/concepts/tools/mcp) tools.

The general syntax is:

Learn more about MCP tools in the [MCP tools](/concepts/tools/mcp) guide.
</Tip>

## Developer Resources

* View the [Agent schema](/reference/agents/agent)
* View the [Knowledge schema](/reference/knowledge/knowledge)
* View [Cookbook](https://github.com/agno-agi/agno/tree/main/cookbook/knowledge/)

**Examples:**

Example 1 (unknown):
```unknown
## Using a Toolkit

Agno provides many pre-built **toolkits** that you can add to your Agents. For example, let's use the DuckDuckGo toolkit to search the web.

<Tip>
  You can find more toolkits in the [Toolkits](/concepts/tools/toolkits) guide.
</Tip>

<Steps>
  <Step title="Create Web Search Agent">
    Create a file `web_search.py`
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Run the agent">
    Install libraries
```

Example 3 (unknown):
```unknown
Run the agent
```

Example 4 (unknown):
```unknown
</Step>
</Steps>

## Writing your own Tools

For more control, write your own python functions and add them as tools to an Agent. For example, here's how to add a `get_top_hackernews_stories` tool to an Agent.
```

---

## - Replace the values below or use environment variables

**URL:** llms-txt#--replace-the-values-below-or-use-environment-variables

vector_db = UpstashVectorDb(
    url=os.getenv("UPSTASH_VECTOR_REST_URL"),
    token=os.getenv("UPSTASH_VECTOR_REST_TOKEN"),
)

---

## Batch processing with async

**URL:** llms-txt#batch-processing-with-async

**Contents:**
- Usage in Knowledge

tasks = [reader.async_read(file) for file in file_list]
all_documents = await asyncio.gather(*tasks)
python  theme={null}
from agno.knowledge.reader.pdf_reader import PDFReader

**Examples:**

Example 1 (unknown):
```unknown
## Usage in Knowledge

Readers integrate seamlessly with Agno Knowledge:
```

---

## Visualization

**URL:** llms-txt#visualization

**Contents:**
- Example
- Toolkit Params
- Toolkit Functions
- Developer Resources

Source: https://docs.agno.com/concepts/tools/toolkits/others/visualization

VisualizationTools enables agents to create various types of charts and plots using matplotlib.

The following agent can create various types of data visualizations:

| Parameter                    | Type   | Default    | Description                         |
| ---------------------------- | ------ | ---------- | ----------------------------------- |
| `output_dir`                 | `str`  | `"charts"` | Directory to save generated charts. |
| `enable_create_bar_chart`    | `bool` | `True`     | Enable bar chart creation.          |
| `enable_create_line_chart`   | `bool` | `True`     | Enable line chart creation.         |
| `enable_create_pie_chart`    | `bool` | `True`     | Enable pie chart creation.          |
| `enable_create_scatter_plot` | `bool` | `True`     | Enable scatter plot creation.       |
| `enable_create_histogram`    | `bool` | `True`     | Enable histogram creation.          |

| Function              | Description                                                 |
| --------------------- | ----------------------------------------------------------- |
| `create_bar_chart`    | Create bar charts for categorical data comparison.          |
| `create_line_chart`   | Create line charts for time series and trend visualization. |
| `create_pie_chart`    | Create pie charts for proportional data representation.     |
| `create_scatter_plot` | Create scatter plots for correlation analysis.              |
| `create_histogram`    | Create histograms for data distribution visualization.      |

## Developer Resources

* View [Tools Source](https://github.com/agno-agi/agno/blob/main/libs/agno/agno/tools/visualization.py)
* [Matplotlib Documentation](https://matplotlib.org/stable/contents.html)

---

## Websearch Builtin Tool

**URL:** llms-txt#websearch-builtin-tool

**Contents:**
- Code
- Usage

Source: https://docs.agno.com/examples/models/openai/responses/websearch_builtin_tool

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Set your API key">
    
  </Step>

<Step title="Install libraries">
    
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Set your API key">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Movie Recommender

**URL:** llms-txt#movie-recommender

**Contents:**
- Code

Source: https://docs.agno.com/examples/use-cases/agents/movie-recommender

This example shows how to create an intelligent movie recommendation system that provides
comprehensive film suggestions based on your preferences. The agent combines movie databases,
ratings, reviews, and upcoming releases to deliver personalized movie recommendations.

Example prompts to try:

* "Suggest thriller movies similar to Inception and Shutter Island"
* "What are the top-rated comedy movies from the last 2 years?"
* "Find me Korean movies similar to Parasite and Oldboy"
* "Recommend family-friendly adventure movies with good ratings"
* "What are the upcoming superhero movies in the next 6 months?"

```python movie_recommender.py theme={null}
from textwrap import dedent

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.exa import ExaTools

movie_recommendation_agent = Agent(
    name="PopcornPal",
    tools=[ExaTools()],
    model=OpenAIChat(id="gpt-5-mini"),
    description=dedent("""\
        You are PopcornPal, a passionate and knowledgeable film curator with expertise in cinema worldwide! 🎥

Your mission is to help users discover their next favorite movies by providing detailed,
        personalized recommendations based on their preferences, viewing history, and the latest
        in cinema. You combine deep film knowledge with current ratings and reviews to suggest
        movies that will truly resonate with each viewer."""),
    instructions=dedent("""\
        Approach each recommendation with these steps:
        1. Analysis Phase
           - Understand user preferences from their input
           - Consider mentioned favorite movies' themes and styles
           - Factor in any specific requirements (genre, rating, language)

2. Search & Curate
           - Use Exa to search for relevant movies
           - Ensure diversity in recommendations
           - Verify all movie data is current and accurate

3. Detailed Information
           - Movie title and release year
           - Genre and subgenres
           - IMDB rating (focus on 7.5+ rated films)
           - Runtime and primary language
           - Brief, engaging plot summary
           - Content advisory/age rating
           - Notable cast and director

4. Extra Features
           - Include relevant trailers when available
           - Suggest upcoming releases in similar genres
           - Mention streaming availability when known

Presentation Style:
        - Use clear markdown formatting
        - Present main recommendations in a structured table
        - Group similar movies together
        - Add emoji indicators for genres (🎭 🎬 🎪)
        - Minimum 5 recommendations per query
        - Include a brief explanation for each recommendation
    """),
    markdown=True,
    add_datetime_to_context=True,
    )

---

## Resend

**URL:** llms-txt#resend

**Contents:**
- Prerequisites
- Example
- Toolkit Params
- Toolkit Functions
- Developer Resources

Source: https://docs.agno.com/concepts/tools/toolkits/others/resend

**ResendTools** enable an Agent to send emails using Resend

The following example requires the `resend` library and an API key from [Resend](https://resend.com/).

The following agent will send an email using Resend

| Parameter           | Type   | Default | Description                                                   |
| ------------------- | ------ | ------- | ------------------------------------------------------------- |
| `api_key`           | `str`  | -       | API key for authentication purposes.                          |
| `from_email`        | `str`  | -       | The email address used as the sender in email communications. |
| `enable_send_email` | `bool` | `True`  | Enable the send\_email functionality.                         |
| `all`               | `bool` | `False` | Enable all functionality.                                     |

| Function     | Description                         |
| ------------ | ----------------------------------- |
| `send_email` | Send an email using the Resend API. |

## Developer Resources

* View [Tools](https://github.com/agno-agi/agno/blob/main/libs/agno/agno/tools/resend.py)
* View [Cookbook](https://github.com/agno-agi/agno/tree/main/cookbook/tools/resend_tools.py)

**Examples:**

Example 1 (unknown):
```unknown

```

Example 2 (unknown):
```unknown
## Example

The following agent will send an email using Resend
```

---

## Get general information and local businesses

**URL:** llms-txt#get-general-information-and-local-businesses

**Contents:**
- Toolkit Params
- Developer Resources
- Resources

agent.print_response(
    """
    I'm traveling to Tokyo next month.
    1. Research the best time to visit and major attractions
    2. Find one good rated sushi restaurants near Shinjuku
    Compile a comprehensive travel guide with this information.
    """,
    markdown=True
)
```

| Parameter         | Type                 | Default | Description                                                         |
| ----------------- | -------------------- | ------- | ------------------------------------------------------------------- |
| `apify_api_token` | `str`                | `None`  | Apify API token (or set via APIFY\_API\_TOKEN environment variable) |
| `actors`          | `str` or `List[str]` | `None`  | Single Actor ID or list of Actor IDs to register                    |

## Developer Resources

* View [Tools](https://github.com/agno-agi/agno/blob/main/libs/agno/agno/tools/apify.py)
* View [Cookbook](https://github.com/agno-agi/agno/tree/main/cookbook/tools/apify_tools.py)

* [Apify Actor Documentation](https://docs.apify.com/Actors)
* [Apify Store - Browse available Actors](https://apify.com/store)
* [How to build and monetize an AI agent on Apify](https://blog.apify.com/how-to-build-an-ai-agent/)

---

## - "What are the trending tech discussions on HN right now?"

**URL:** llms-txt#--"what-are-the-trending-tech-discussions-on-hn-right-now?"

---

## Single Tool Reliability

**URL:** llms-txt#single-tool-reliability

**Contents:**
- Code

Source: https://docs.agno.com/examples/concepts/evals/reliability/reliability_single_tool

Learn how to evaluate reliability of single tool calls.

This example shows how to evaluate the reliability of a single tool call.

---

## Start the session with user 2

**URL:** llms-txt#start-the-session-with-user-2

team.print_response("Tell me about quantum physics.", user_id=user_2_id, session_id=user_2_session_id)

---

## HuggingFace

**URL:** llms-txt#huggingface

**Contents:**
- Parameters

Source: https://docs.agno.com/reference/models/huggingface

The HuggingFace model provides access to models hosted on the HuggingFace Hub.

| Parameter            | Type              | Default                                         | Description                                                    |
| -------------------- | ----------------- | ----------------------------------------------- | -------------------------------------------------------------- |
| `id`                 | `str`             | `"microsoft/DialoGPT-medium"`                   | The id of the Hugging Face model to use                        |
| `name`               | `str`             | `"HuggingFace"`                                 | The name of the model                                          |
| `provider`           | `str`             | `"HuggingFace"`                                 | The provider of the model                                      |
| `api_key`            | `Optional[str]`   | `None`                                          | The API key for Hugging Face (defaults to HF\_TOKEN env var)   |
| `base_url`           | `str`             | `"https://api-inference.huggingface.co/models"` | The base URL for Hugging Face Inference API                    |
| `wait_for_model`     | `bool`            | `True`                                          | Whether to wait for the model to load if it's cold             |
| `use_cache`          | `bool`            | `True`                                          | Whether to use caching for faster inference                    |
| `max_tokens`         | `Optional[int]`   | `None`                                          | Maximum number of tokens to generate                           |
| `temperature`        | `Optional[float]` | `None`                                          | Controls randomness in the model's output                      |
| `top_p`              | `Optional[float]` | `None`                                          | Controls diversity via nucleus sampling                        |
| `repetition_penalty` | `Optional[float]` | `None`                                          | Penalty for repeating tokens (higher values reduce repetition) |

---

## Create a team for collaborative image transformation

**URL:** llms-txt#create-a-team-for-collaborative-image-transformation

**Contents:**
- Usage

transformation_team = Team(
    name="Image Transformation Team",
    model=OpenAIChat(id="gpt-5-mini"),
    members=[style_advisor, image_transformer],
    instructions=[
        "Transform images with artistic style and precision.",
        "Style Advisor: First analyze transformation requirements and recommend styles.",
        "Image Transformer: Apply transformations using AI tools with style guidance.",
    ],
    markdown=True,
)

transformation_team.print_response(
    "a cat dressed as a wizard with a background of a mystic forest. Make it look like 'https://fal.media/files/koala/Chls9L2ZnvuipUTEwlnJC.png'",
    stream=True,
)
bash  theme={null}
    pip install agno
    bash  theme={null}
    export OPENAI_API_KEY=****
    export FAL_KEY=****
    bash  theme={null}
    python cookbook/examples/teams/multimodal/image_to_image_transformation.py
    ```
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Install required libraries">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Set environment variables">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run the agent">
```

---

## Create workflow with media input

**URL:** llms-txt#create-workflow-with-media-input

media_workflow = Workflow(
    name="Image Analysis and Research Workflow",
    description="Analyze an image and research related news",
    steps=[analysis_step, research_step],
    db=SqliteDb(db_file="tmp/workflow.db"),
)

---

## Initialize ChromaDB

**URL:** llms-txt#initialize-chromadb

vector_db = ChromaDb(collection="recipes", path="tmp/chromadb", persistent_client=True)

---

## Step 2: Configure tools

**URL:** llms-txt#step-2:-configure-tools

tools = [
    XTools(
        include_post_metrics=True,
        wait_on_rate_limit=True,
    ),
    ExaTools(
        num_results=10,
        include_domains=["reddit.com", "news.ycombinator.com", "medium.com"],
        exclude_domains=["spam-site.com"],
    )
]

---

## Step 1: Choose the AI model

**URL:** llms-txt#step-1:-choose-the-ai-model

model = OpenAIChat(id="gpt-5-mini")

---

## Process only what you need

**URL:** llms-txt#process-only-what-you-need

**Contents:**
- Advanced Optimizations
  - Use Hybrid Search
  - Add Reranking
  - Optimize Embedder Dimensions

knowledge.add_contents(
    paths=["large_dataset/"],
    include=["*.pdf"],       # Only PDFs
    exclude=["*backup*"],    # Skip backups
    skip_if_exists=True,
    metadata={"batch": "current"}
)
python  theme={null}
from agno.vectordb.pgvector import PgVector, SearchType

vector_db = PgVector(
    table_name="knowledge",
    db_url="postgresql+psycopg://user:pass@localhost:5432/db",
    search_type=SearchType.hybrid  # Vector + keyword search
)
python  theme={null}
from agno.knowledge.reranker.cohere import CohereReranker

vector_db = PgVector(
    table_name="knowledge",
    db_url="postgresql+psycopg://user:pass@localhost:5432/db",
    reranker=CohereReranker(
        model="rerank-multilingual-v3.0",
        top_n=10
    )
)
python  theme={null}
from agno.knowledge.embedder.openai import OpenAIEmbedder

**Examples:**

Example 1 (unknown):
```unknown
## Advanced Optimizations

Once you've applied the quick wins above, consider these for further improvements:

### Use Hybrid Search

Combine vector and keyword search for better results:
```

Example 2 (unknown):
```unknown
### Add Reranking

Improve result quality by reranking with Cohere:
```

Example 3 (unknown):
```unknown
### Optimize Embedder Dimensions

Reduce dimensions for faster search (with slight quality trade-off):
```

---

## OpenAI Embedder

**URL:** llms-txt#openai-embedder

**Contents:**
- Code

Source: https://docs.agno.com/examples/concepts/knowledge/embedders/openai-embedder

```python  theme={null}
from agno.knowledge.embedder.openai import OpenAIEmbedder
from agno.knowledge.knowledge import Knowledge
from agno.vectordb.pgvector import PgVector

embeddings = OpenAIEmbedder().get_embedding(
    "The quick brown fox jumps over the lazy dog."
)

---

## MySQL Workflow Storage

**URL:** llms-txt#mysql-workflow-storage

**Contents:**
- Usage
  - Run MySQL

Source: https://docs.agno.com/examples/concepts/db/mysql/mysql_for_workflow

Agno supports using MySQL as a storage backend for Workflows using the `MysqlDb` class.

Install [docker desktop](https://docs.docker.com/desktop/install/mac-install/) and run **MySQL** on port **3306** using:

```python mysql_for_workflow.py theme={null}
from agno.agent import Agent
from agno.db.mysql import MySQLDb
from agno.models.openai import OpenAIChat
from agno.team import Team
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.hackernews import HackerNewsTools
from agno.workflow.step import Step
from agno.workflow.workflow import Workflow

db_url = "mysql+pymysql://ai:ai@localhost:3306/ai"

db = MySQLDb(db_url=db_url)

**Examples:**

Example 1 (unknown):
```unknown

```

---

## ClickHouse Async

**URL:** llms-txt#clickhouse-async

**Contents:**
- Code
- Usage

Source: https://docs.agno.com/examples/concepts/vectordb/clickhouse-db/async-clickhouse-db

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Install libraries">
    
  </Step>

<Step title="Run ClickHouse">
    
  </Step>

<Step title="Set environment variables">
    
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Install libraries">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Run ClickHouse">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Set environment variables">
```

Example 4 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

---

## Print the response in markdown format

**URL:** llms-txt#print-the-response-in-markdown-format

**Contents:**
- Run Output
- Streaming

pprint_run_response(response, markdown=True)
python  theme={null}
from typing import Iterator
from agno.team import Team
from agno.agent import Agent
from agno.models.openai import OpenAIChat

news_agent = Agent(name="News Agent", role="Get the latest news")
weather_agent = Agent(name="Weather Agent", role="Get the weather for the next 7 days")

team = Team(
    name="News and Weather Team",
    members=[news_agent, weather_agent],
    model=OpenAIChat(id="gpt-4o")
)

**Examples:**

Example 1 (unknown):
```unknown
<Tip>
  You can also run the team asynchronously using `Team.arun()`. This means members will run concurrently if the team leader delegates to multiple members in one request.
</Tip>

<Tip>
  See the [Input & Output](/concepts/teams/input-output) docs for more information, and to see how to use structured input and output with teams.
</Tip>

## Run Output

The `Team.run()` function returns a `TeamRunOutput` object when not streaming. Here are some of the core attributes:

* `run_id`: The id of the run.
* `team_id`: The id of the team.
* `team_name`: The name of the team.
* `session_id`: The id of the session.
* `user_id`: The id of the user.
* `content`: The response content.
* `content_type`: The type of content. In the case of structured output, this will be the class name of the pydantic model.
* `reasoning_content`: The reasoning content.
* `messages`: The list of messages sent to the model.
* `metrics`: The metrics of the run. For more details see [Metrics](/concepts/teams/metrics).
* `model`: The model used for the run.
* `member_responses`: The list of member responses. Optional to add when `store_member_responses=True` on the `Team`.

See detailed documentation in the [TeamRunOutput](/reference/teams/team-response) documentation.

## Streaming

To enable streaming, set `stream=True` when calling `run()`. This will return an iterator of `TeamRunOutputEvent` objects instead of a single response.
```

---

## Load infrastructure secrets

**URL:** llms-txt#load-infrastructure-secrets

load_dotenv(dotenv_path=Path(__file__).resolve().parents[1] / "infra" / "secrets" / ".env")

---

## Wikipedia Reader

**URL:** llms-txt#wikipedia-reader

Source: https://docs.agno.com/reference/knowledge/reader/wikipedia

WikipediaReader is a reader class that allows you to read Wikipedia articles.

<Snippet file="wikipedia-reader-reference.mdx" />

---

## Desi Vocal

**URL:** llms-txt#desi-vocal

**Contents:**
- Example
- Toolkit Params
- Toolkit Functions
- Developer Resources

Source: https://docs.agno.com/concepts/tools/toolkits/others/desi_vocal

DesiVocalTools provides text-to-speech capabilities using Indian voices through the Desi Vocal API.

The following agent can convert text to speech using Indian voices:

| Parameter               | Type            | Default                                  | Description                                                |
| ----------------------- | --------------- | ---------------------------------------- | ---------------------------------------------------------- |
| `api_key`               | `Optional[str]` | `None`                                   | Desi Vocal API key. Uses DESI\_VOCAL\_API\_KEY if not set. |
| `voice_id`              | `Optional[str]` | `"f27d74e5-ea71-4697-be3e-f04bbd80c1a8"` | Default voice ID to use for text-to-speech.                |
| `enable_get_voices`     | `bool`          | `True`                                   | Enable voice listing functionality.                        |
| `enable_text_to_speech` | `bool`          | `True`                                   | Enable text-to-speech conversion functionality.            |

| Function         | Description                                                   |
| ---------------- | ------------------------------------------------------------- |
| `get_voices`     | Retrieve list of available voices with their IDs and details. |
| `text_to_speech` | Convert text to speech using specified or default voice.      |

## Developer Resources

* View [Tools Source](https://github.com/agno-agi/agno/blob/main/libs/agno/agno/tools/desi_vocal.py)
* [Desi Vocal API Documentation](https://desivocal.com/docs)
* [Indian TTS Best Practices](https://desivocal.com/best-practices)

---

## MongoDB for document-based storage

**URL:** llms-txt#mongodb-for-document-based-storage

from agno.db.mongo import MongoDb
contents_db = MongoDb(
    uri="mongodb://localhost:27017",
    database="agno_db"
)

---

## Example usage - sync

**URL:** llms-txt#example-usage---sync

response = agent.run(
    "Please provide me with a personalized summary of today's priorities based on my profile and interests.",
    dependencies={
        "user_profile": get_user_profile,
        "current_context": get_current_context,
    },
    add_dependencies_to_context=True,
    debug_mode=True,
)

print(response.content)

---

## Arxiv Reader

**URL:** llms-txt#arxiv-reader

Source: https://docs.agno.com/reference/knowledge/reader/arxiv

ArxivReader is a reader class that allows you to read papers from the Arxiv API.

<Snippet file="arxiv-reader-reference.mdx" />

---

## Generate a unique bucket name using a base name and a UUID4 suffix.

**URL:** llms-txt#generate-a-unique-bucket-name-using-a-base-name-and-a-uuid4-suffix.

base_bucket_name = "example-gcs-bucket"
unique_bucket_name = f"{base_bucket_name}-{uuid.uuid4().hex[:12]}"
print(f"Using bucket: {unique_bucket_name}")

---

## Building Teams

**URL:** llms-txt#building-teams

Source: https://docs.agno.com/concepts/teams/building-teams

Learn how to build Teams with Agno.

To build effective teams, start simple -- just a model, members, and instructions. Once that works, layer in more functionality as needed.

Here's the simplest possible team with specialized agents:

```python news_weather_team.py lines theme={null}
from agno.team import Team
from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.duckduckgo import DuckDuckGoTools

---

## Weaviate

**URL:** llms-txt#weaviate

Source: https://docs.agno.com/reference/vector_db/weaviate

<Snippet file="vector-db-weaviate-reference.mdx" />

---

## Create an evaluation

**URL:** llms-txt#create-an-evaluation

evaluation = AccuracyEval(
    model=OpenAIChat(id="o4-mini"),
    agent=Agent(model=OpenAIChat(id="gpt-5-mini"), tools=[CalculatorTools()]),
    input="What is 10*5 then to the power of 2? do it step by step",
    expected_output="2500",
    additional_guidelines="Agent output should include the steps and the final answer.",
)

---

## ---------------------------------------------------------

**URL:** llms-txt#---------------------------------------------------------

---

## Perplexity

**URL:** llms-txt#perplexity

**Contents:**
- Parameters

Source: https://docs.agno.com/reference/models/perplexity

The Perplexity model provides access to Perplexity's language models.

| Parameter  | Type            | Default                               | Description                                                           |
| ---------- | --------------- | ------------------------------------- | --------------------------------------------------------------------- |
| `id`       | `str`           | `"llama-3.1-sonar-small-128k-online"` | The id of the Perplexity model to use                                 |
| `name`     | `str`           | `"Perplexity"`                        | The name of the model                                                 |
| `provider` | `str`           | `"Perplexity"`                        | The provider of the model                                             |
| `api_key`  | `Optional[str]` | `None`                                | The API key for Perplexity (defaults to PERPLEXITY\_API\_KEY env var) |
| `base_url` | `str`           | `"https://api.perplexity.ai"`         | The base URL for the Perplexity API                                   |

Perplexity extends the OpenAI-compatible interface and supports most parameters from OpenAI.

---

## from agno.models.groq import Groq

**URL:** llms-txt#from-agno.models.groq-import-groq

**Contents:**
- Usage

from agno.tools.openai import OpenAITools
from agno.utils.media import download_image
from agno.vectordb.pgvector import PgVector

knowledge = Knowledge(
    vector_db=PgVector(
        db_url="postgresql+psycopg://ai:ai@localhost:5532/ai",
        table_name="embed_vision_documents",
        embedder=CohereEmbedder(
            id="embed-v4.0",
        ),
    ),
)

asyncio.run(
    knowledge.add_content_async(
        url="https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf",
    )
)

agent = Agent(
    name="EmbedVisionRAGAgent",
    model=Groq(id="meta-llama/llama-4-scout-17b-16e-instruct"),
    tools=[OpenAITools()],
    knowledge=knowledge,
    instructions=[
        "You are a specialized recipe assistant.",
        "When asked for a recipe:",
        "1. Search the knowledge base to retrieve the relevant recipe details.",
        "2. Analyze the retrieved recipe steps carefully.",
        "3. Use the `generate_image` tool to create a visual, step-by-step image manual for the recipe.",
        "4. Present the recipe text clearly and mention that you have generated an accompanying image manual. Add instructions while generating the image.",
    ],
    markdown=True,
)

agent.print_response(
    "What is the recipe for a Thai curry?",
)
response = agent.get_last_run_output()

if response.images:
    download_image(response.images[0].url, Path("tmp/recipe_image.png"))

bash  theme={null}
    export GROQ_API_KEY=xxx
    bash  theme={null}
    pip install -U agno openai groq cohere
    bash Mac theme={null}
      python cookbook/examples/agents/recipe_rag_image.py
      bash Windows theme={null}
      python cookbook/examples/agents/recipe_rag_image.py
      ```
    </CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Set your API key">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Firestore for Workflows

**URL:** llms-txt#firestore-for-workflows

**Contents:**
- Usage

Source: https://docs.agno.com/examples/concepts/db/firestore/firestore_for_workflow

Agno supports using Firestore as a storage backend for Workflows using the `FirestoreDb` class.

You need to provide a `project_id` parameter to the `FirestoreDb` class. Firestore will connect automatically using your Google Cloud credentials.

```python firestore_for_workflow.py theme={null}
from agno.agent import Agent
from agno.db.firestore import FirestoreDb
from agno.models.openai import OpenAIChat
from agno.team import Team
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.hackernews import HackerNewsTools
from agno.workflow.step import Step
from agno.workflow.workflow import Workflow

PROJECT_ID = "agno-os-test"  # Use your project ID here

---

## Save the generated audio

**URL:** llms-txt#save-the-generated-audio

**Contents:**
- Advanced Example: Translation and Voice Localization
- Toolkit Params
- Toolkit Functions
- Developer Resources

if response.audio:
    write_audio_to_file(audio=response.audio[0].content, filename="tmp/greeting.mp3")

python  theme={null}
from textwrap import dedent
from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.cartesia import CartesiaTools
from agno.utils.audio import write_audio_to_file

agent_instructions = dedent(
    """Follow these steps SEQUENTIALLY to translate text and generate a localized voice note:
    1. Identify the text to translate and the target language from the user request.
    2. Translate the text accurately to the target language.
    3. Analyze the emotion conveyed by the translated text.
    4. Call `list_voices` to retrieve available voices.
    5. Select a base voice matching the language and emotion.
    6. Call `localize_voice` to create a new localized voice.
    7. Call `text_to_speech` to generate the final audio.
    """
)

agent = Agent(
    name="Emotion-Aware Translator Agent",
    description="Translates text, analyzes emotion, selects a suitable voice, creates a localized voice, and generates a voice note (audio file) using Cartesia TTS tools.",
    instructions=agent_instructions,
    model=OpenAIChat(id="gpt-5-mini"),
    tools=[CartesiaTools(enable_localize_voice=True)],  
    )

agent.print_response(
    "Translate 'Hello! How are you? Tell me more about the weather in Paris?' to French and create a voice note."
)
response = agent.run_response

if response.audio:
    write_audio_to_file(
        response.audio[0].base64_audio,
        filename="french_weather.mp3",
    )
```

| Parameter               | Type   | Default                                | Description                                                                                         |
| ----------------------- | ------ | -------------------------------------- | --------------------------------------------------------------------------------------------------- |
| `api_key`               | `str`  | `None`                                 | The Cartesia API key for authentication. If not provided, uses the `CARTESIA_API_KEY` env variable. |
| `model_id`              | `str`  | `sonic-2`                              | The model ID to use for text-to-speech.                                                             |
| `default_voice_id`      | `str`  | `78ab82d5-25be-4f7d-82b3-7ad64e5b85b2` | The default voice ID to use for text-to-speech and localization.                                    |
| `enable_text_to_speech` | `bool` | `True`                                 | Enable text-to-speech functionality.                                                                |
| `enable_list_voices`    | `bool` | `True`                                 | Enable listing available voices functionality.                                                      |
| `enable_localize_voice` | `bool` | `False`                                | Enable voice localization functionality.                                                            |

| Function         | Description                          |
| ---------------- | ------------------------------------ |
| `list_voices`    | List available voices from Cartesia. |
| `text_to_speech` | Converts text to speech.             |
| `localize_voice` | Create a new localized voice.        |

## Developer Resources

* View [Tools](https://github.com/agno-agi/agno/blob/main/libs/agno/agno/tools/cartesia.py)
* View [Cookbook](https://github.com/agno-agi/agno/tree/main/cookbook/tools/cartesia_tools.py)

**Examples:**

Example 1 (unknown):
```unknown
## Advanced Example: Translation and Voice Localization

This example demonstrates how to translate text, analyze emotion, localize a new voice, and generate a voice note using CartesiaTools.
```

---

## Async Performance Evaluation

**URL:** llms-txt#async-performance-evaluation

**Contents:**
- Code

Source: https://docs.agno.com/examples/concepts/evals/performance/performance_async

Learn how to run performance evaluations on async functions.

This example shows how to run a Performance evaluation on an async function.

```python  theme={null}
"""This example shows how to run a Performance evaluation on an async function."""

from agno.agent import Agent
from agno.eval.performance import PerformanceEval
from agno.models.openai import OpenAIChat

---

## Have a conversation with OpenAI

**URL:** llms-txt#have-a-conversation-with-openai

openai_agent.print_response(
    "Explain machine learning basics", session_id=session_id, user_id=user_id
)
openai_agent.print_response(
    "What about deep learning?", session_id=session_id, user_id=user_id
)

---

## Example 1: Generate a basic image with default settings

**URL:** llms-txt#example-1:-generate-a-basic-image-with-default-settings

agent.print_response("Generate an image of a futuristic city with flying cars and tall skyscrapers", markdown=True)

---

## Google Search Tools

**URL:** llms-txt#google-search-tools

**Contents:**
- Code
- Usage

Source: https://docs.agno.com/examples/concepts/tools/search/google_search

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Set your API credentials">
    
  </Step>

<Step title="Install libraries">
    
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Set your API credentials">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Create distributed RAG team

**URL:** llms-txt#create-distributed-rag-team

**Contents:**
- Usage

distributed_rag_team = Team(
    name="Distributed RAG Team",
    model=OpenAIChat(id="gpt-5-mini"),
    members=[
        primary_retriever,
        context_expander,
        answer_synthesizer,
        quality_validator,
    ],
    instructions=[
        "Work together to provide comprehensive, high-quality RAG responses.",
        "Primary Retriever: First retrieve core relevant information.",
        "Context Expander: Then expand with related context and background.",
        "Answer Synthesizer: Synthesize all information into a comprehensive answer.",
        "Quality Validator: Finally validate and suggest any improvements.",
        "Ensure all responses are accurate, complete, and well-structured.",
    ],
    show_members_responses=True,
    markdown=True,
)

async def async_distributed_rag_demo():
    """Demonstrate async distributed RAG processing."""
    print("📚 Async Distributed RAG with LanceDB Demo")
    print("=" * 50)

query = "How do I make chicken and galangal in coconut milk soup? Include cooking tips and variations."

# Add content to knowledge bases
    await primary_knowledge.add_contents_async(
        url="https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf"
    )
    await context_knowledge.add_contents_async(
        url="https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf"
    )

# # Run async distributed RAG
    # await distributed_rag_team.aprint_response(
    #     query, stream=True, stream_events=True
    # )
    await distributed_rag_team.aprint_response(input=query)

def sync_distributed_rag_demo():
    """Demonstrate sync distributed RAG processing."""
    print("📚 Distributed RAG with LanceDB Demo")
    print("=" * 40)

query = "How do I make chicken and galangal in coconut milk soup? Include cooking tips and variations."

# Add content to knowledge bases
    primary_knowledge.add_contents(
        url="https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf"
    )
    context_knowledge.add_contents(
        url="https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf"
    )

# Run distributed RAG
    distributed_rag_team.print_response(input=query)

def multi_course_meal_demo():
    """Demonstrate distributed RAG for complex multi-part queries."""
    print("🍽️ Multi-Course Meal Planning with Distributed RAG")
    print("=" * 55)

query = """Hi, I want to make a 3 course Thai meal. Can you recommend some recipes?
    I'd like to start with a soup, then a thai curry for the main course and finish with a dessert.
    Please include cooking techniques and any special tips."""

# Add content to knowledge bases
    primary_knowledge.add_contents(
        url="https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf"
    )
    context_knowledge.add_contents(
        url="https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf"
    )

distributed_rag_team.print_response(input=query)

if __name__ == "__main__":
    # Choose which demo to run
    asyncio.run(async_distributed_rag_demo())

# multi_course_meal_demo()

# sync_distributed_rag_demo()
bash  theme={null}
    pip install agno openai lancedb tantivy pypdf sqlalchemy
    bash  theme={null}
    export OPENAI_API_KEY=****
    bash  theme={null}
    python cookbook/examples/teams/distributed_rag/02_distributed_rag_lancedb.py
    ```
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Install required libraries">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Set environment variables">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run the agent">
```

---

## What is Chunking?

**URL:** llms-txt#what-is-chunking?

**Contents:**
- Available Chunking Strategies
- Using Chunking Strategies

Source: https://docs.agno.com/concepts/knowledge/chunking/overview

Chunking is the process of breaking down large documents into smaller pieces for effective vector search and retrieval.

Chunking is the process of dividing content into manageable pieces before converting them into embeddings and storing them in vector databases. The chunking strategy you choose directly impacts search quality and retrieval accuracy.

Different chunking strategies serve different purposes. For example, when processing a recipe book, different strategies produce different results:

* **Fixed Size**: Splits text every 500 characters (which may break recipes mid-instruction)
* **Semantic**: Keeps complete recipes together based on meaning
* **Document**: Each page becomes a chunk

The strategy affects whether you get complete, relevant results or fragmented pieces.

## Available Chunking Strategies

<CardGroup cols={2}>
  <Card title="Fixed Size Chunking" icon="ruler" href="/concepts/knowledge/chunking/fixed-size-chunking">
    Split content into uniform chunks with specified size and overlap.
  </Card>

<Card title="Semantic Chunking" icon="brain" href="/concepts/knowledge/chunking/semantic-chunking">
    Use semantic similarity to identify natural breakpoints in content.
  </Card>

<Card title="Recursive Chunking" icon="sitemap" href="/concepts/knowledge/chunking/recursive-chunking">
    Recursively split content using multiple separators for hierarchical processing.
  </Card>

<Card title="Document Chunking" icon="file-lines" href="/concepts/knowledge/chunking/document-chunking">
    Preserve document structure by treating sections as individual chunks.
  </Card>

<Card title="CSV Row Chunking" icon="table" href="/concepts/knowledge/chunking/csv-row-chunking">
    Splits CSV files by treating each row as an individual chunk. Only compatible with CSVs.
  </Card>

<Card title="Markdown Chunking" icon="markdown" href="/concepts/knowledge/chunking/markdown-chunking">
    Split markdown content while preserving heading structure and hierarchy. Only compatible with Markdown files.
  </Card>

<Card title="Agentic Chunking" icon="robot" href="/concepts/knowledge/chunking/agentic-chunking">
    Use AI to intelligently determine optimal chunk boundaries.
  </Card>

<Card title="Custom Chunking" icon="code" href="/concepts/knowledge/chunking/custom-chunking">
    Build your own chunking strategy for specialized use cases.
  </Card>
</CardGroup>

## Using Chunking Strategies

Chunking strategies are configured when setting up readers for your knowledge base:

```python  theme={null}
from agno.knowledge.chunking.semantic import SemanticChunking
from agno.knowledge.reader.pdf_reader import PDFReader
from agno.knowledge.knowledge import Knowledge
from agno.vectordb.pgvector import PgVector
from agno.db.postgres import PostgresDb

---

## Send Message

**URL:** llms-txt#send-message

Source: https://docs.agno.com/reference-api/schema/a2a/send-message

post /a2a/message/send
Send a message to an Agno Agent, Team, or Workflow. The Agent, Team or Workflow is identified via the 'agentId' field in params.message or X-Agent-ID header. Optional: Pass user ID via X-User-ID header (recommended) or 'userId' in params.message.metadata.

---

## Format & Validate

**URL:** llms-txt#format-&-validate

**Contents:**
- Format
- Validate

Source: https://docs.agno.com/templates/infra-management/format-and-validate

Formatting the codebase using a set standard saves us time and mental energy. Agno templates are pre-configured with [ruff](https://docs.astral.sh/ruff/) that you can run using a helper script or directly.

Linting and Type Checking add an extra layer of protection to the codebase. We highly recommending running the validate script before pushing any changes.

Agno templates are pre-configured with [ruff](https://docs.astral.sh/ruff/) and [mypy](https://mypy.readthedocs.io/en/stable/) that you can run using a helper script or directly. Checkout the `pyproject.toml` file for the configuration.

**Examples:**

Example 1 (unknown):
```unknown

```

Example 2 (unknown):
```unknown
</CodeGroup>

## Validate

Linting and Type Checking add an extra layer of protection to the codebase. We highly recommending running the validate script before pushing any changes.

Agno templates are pre-configured with [ruff](https://docs.astral.sh/ruff/) and [mypy](https://mypy.readthedocs.io/en/stable/) that you can run using a helper script or directly. Checkout the `pyproject.toml` file for the configuration.

<CodeGroup>
```

Example 3 (unknown):
```unknown

```

Example 4 (unknown):
```unknown

```

---

## Create analysis steps

**URL:** llms-txt#create-analysis-steps

trend_analysis_step = Step(
    name="Trend Analysis",
    agent=analysis_agent,
    description="Analyze trending patterns in the research",
)

sentiment_analysis_step = Step(
    name="Sentiment Analysis",
    agent=analysis_agent,
    description="Analyze sentiment and opinions from the research",
)

content_step = Step(
    name="Create Content",
    agent=content_agent,
    description="Create content based on research findings",
)

---

## - "Summarize the top 5 stories on Hacker News"

**URL:** llms-txt#--"summarize-the-top-5-stories-on-hacker-news"

---

## More example prompts to explore:

**URL:** llms-txt#more-example-prompts-to-explore:

**Contents:**
- Usage

"""
Tutorial Analysis:
1. "Break down this Python tutorial with focus on code examples"
2. "Create a learning path from this web development course"
3. "Extract all practical exercises from this programming guide"
4. "Identify key concepts and implementation examples"

Educational Content:
1. "Create a study guide with timestamps for this math lecture"
2. "Extract main theories and examples from this science video"
3. "Break down this historical documentary into key events"
4. "Summarize the main arguments in this academic presentation"

Tech Reviews:
1. "List all product features mentioned with timestamps"
2. "Compare pros and cons discussed in this review"
3. "Extract technical specifications and benchmarks"
4. "Identify key comparison points and conclusions"

Creative Content:
1. "Break down the techniques shown in this art tutorial"
2. "Create a timeline of project steps in this DIY video"
3. "List all tools and materials mentioned with timestamps"
4. "Extract tips and tricks with their demonstrations"
"""
bash  theme={null}
    pip install openai youtube_transcript_api agno
    bash  theme={null}
    export OPENAI_API_KEY=xxx
    bash  theme={null}
    python youtube_agent.py
    ```
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Install libraries">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Set environment variables">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run the agent">
```

---

## Create team members

**URL:** llms-txt#create-team-members

data_analyst = Agent(
    model=OpenAIChat(id="gpt-4o"),
    name="Data Analyst",
    description="Specialist in analyzing team metrics and performance data",
    instructions=[
        "You are a data analysis expert focusing on team performance metrics.",
        "Interpret quantitative data and identify trends.",
        "Provide data-driven insights and recommendations.",
    ],
)

team_lead = Agent(
    model=OpenAIChat(id="gpt-4o"),
    name="Team Lead", 
    description="Experienced team leader who provides strategic insights",
    instructions=[
        "You are an experienced team leader and management expert.",
        "Focus on leadership insights and team dynamics.",
        "Provide strategic recommendations for team improvement.",
        "Collaborate with the data analyst to get comprehensive insights.",
    ],
)

---

## Speech-to-Text

**URL:** llms-txt#speech-to-text

**Contents:**
- Using OpenAI Whisper (Cloud)

Source: https://docs.agno.com/concepts/multimodal/audio/speech-to-text

Learn how to transcribe audio with Agno agents.

Agno agents can transcribe audio files using different tools and models. You can use native capabilities of OpenAI or fully multimodal Gemini models.

<Tip>
  Examples of ways to do Audio to Text (Transcribe) are:

* [Using Gemini model](/examples/concepts/multimodal/audio-to-text)
  * [Using OpenAI Model](/examples/concepts/agent/multimodal/audio_input_output)
  * [Using `OpenAI Tool`](/concepts/tools/toolkits/models/openai#1-transcribing-audio)
  * [Using `Groq Tool`](/concepts/tools/toolkits/models/groq#1-transcribing-audio)
</Tip>

## Using OpenAI Whisper (Cloud)

The following agent uses OpenAI Whisper API for audio transcription.

```python cookbook/tools/models/openai_tools.py theme={null}
import base64
from pathlib import Path

from agno.agent import Agent
from agno.run.agent import RunOutput
from agno.tools.openai import OpenAITools
from agno.utils.media import download_file, save_base64_data

---

## Consume the streaming events and get the final response

**URL:** llms-txt#consume-the-streaming-events-and-get-the-final-response

**Contents:**
- Usage

run_response = None
for event_or_response in stream_generator:
    # The last item in the stream is the final TeamRunOutput
    run_response = event_or_response

assert isinstance(run_response.content, StockReport)
print(
    f"✅ Response content is correctly typed as StockReport: {type(run_response.content)}"
)
print(f"✅ Stock Symbol: {run_response.content.symbol}")
print(f"✅ Company Name: {run_response.content.company_name}")
bash  theme={null}
    pip install agno exa_py pydantic
    bash  theme={null}
    export OPENAI_API_KEY=****
    export EXA_API_KEY=****
    bash  theme={null}
    python cookbook/examples/teams/structured_input_output/04_structured_output_streaming.py
    ```
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Install required libraries">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Set environment variables">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run the agent">
```

---

## Check content processing status

**URL:** llms-txt#check-content-processing-status

content_list, total_count = knowledge.get_content()

failed = [c for c in content_list if c.status == "failed"]
if failed:
    print(f"Failed items: {len(failed)}")
    for content in failed:
        status, message = knowledge.get_content_status(content.id)
        print(f"  {content.name}: {message}")

---

## Because we are evaluating an async function, we use the arun method.

**URL:** llms-txt#because-we-are-evaluating-an-async-function,-we-use-the-arun-method.

asyncio.run(performance_eval.arun(print_summary=True, print_results=True))
```

---

## SqliteDb

**URL:** llms-txt#sqlitedb

Source: https://docs.agno.com/reference/storage/sqlite

`SqliteDb` is a class that implements the Db interface using SQLite as the backend storage system. It provides lightweight, file-based storage for agent sessions with support for JSON data types and schema versioning.

<Snippet file="db-sqlite-params.mdx" />

<Snippet file="db-new-bulk-methods.mdx" />

---

## Define two completely different workflows as Steps

**URL:** llms-txt#define-two-completely-different-workflows-as-steps

image_sequence = Steps(
    name="image_generation",
    description="Complete image generation and analysis workflow",
    steps=[
        Step(name="generate_image", agent=image_generator),
        Step(name="describe_image", agent=image_describer),
    ],
)

video_sequence = Steps(
    name="video_generation",
    description="Complete video production and analysis workflow",
    steps=[
        Step(name="generate_video", agent=video_generator),
        Step(name="describe_video", agent=video_describer),
    ],
)

def media_sequence_selector(step_input) -> List[Step]:
    """Route to appropriate media generation pipeline"""
    if not step_input.input:
        return [image_sequence]

message_lower = step_input.input.lower()

if "video" in message_lower:
        return [video_sequence]
    elif "image" in message_lower:
        return [image_sequence]
    else:
        return [image_sequence]  # Default

---

## xAI Grok 3 Mini

**URL:** llms-txt#xai-grok-3-mini

**Contents:**
- Code
- Usage

Source: https://docs.agno.com/examples/concepts/reasoning/models/xai/reasoning-effort

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Set your API key">
    
  </Step>

<Step title="Install libraries">
    
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Set your API key">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## You can use existing session summaries from session storage without creating or updating any new ones.

**URL:** llms-txt#you-can-use-existing-session-summaries-from-session-storage-without-creating-or-updating-any-new-ones.

team = Team(
    model=OpenAIChat(id="gpt-5-mini"),
    db=db,
    session_id="session_summary",
    add_session_summary_to_context=True,
    members=[agent],
)

team.print_response("I also like to play basketball.")

---

## Azure OpenAI Tools

**URL:** llms-txt#azure-openai-tools

**Contents:**
- Code
- Usage

Source: https://docs.agno.com/examples/concepts/tools/models/azure_openai

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Set your credentials">
    
  </Step>

<Step title="Install libraries">
    
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Set your credentials">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## MongoDB connection settings

**URL:** llms-txt#mongodb-connection-settings

**Contents:**
- Params
- Developer Resources

db_url = "mongodb://localhost:27017"
db = MongoDb(db_url=db_url)

class Article(BaseModel):
    title: str
    summary: str
    reference_links: List[str]

hn_researcher = Agent(
    name="HackerNews Researcher",
    model=OpenAIChat("gpt-5-mini"),
    role="Gets top stories from hackernews.",
    tools=[HackerNewsTools()],
)

web_searcher = Agent(
    name="Web Searcher",
    model=OpenAIChat("gpt-5-mini"),
    role="Searches the web for information on a topic",
    tools=[DuckDuckGoTools()],
    add_datetime_to_context=True,
)

hn_team = Team(
    name="HackerNews Team",
    model=OpenAIChat("gpt-5-mini"),
    members=[hn_researcher, web_searcher],
    db=db,
    instructions=[
        "First, search hackernews for what the user is asking about.",
        "Then, ask the web searcher to search for each story to get more information.",
        "Finally, provide a thoughtful and engaging summary.",
    ],
    output_schema=Article,
    markdown=True,
    debug_mode=True,
    show_members_responses=True,
    add_member_tools_to_context=False,
)

hn_team.print_response("Write an article about the top 2 stories on hackernews")

<Snippet file="db-mongo-params.mdx" />

## Developer Resources

* View [Cookbook](https://github.com/agno-agi/agno/blob/main/cookbook/db/mongo/mongodb_storage_for_team.py)

---

## Instantiate with specific configuration

**URL:** llms-txt#instantiate-with-specific-configuration

**Contents:**
- Streaming execution with custom function step on AgentOS:
- Developer Resources

content_planning_step = Step(
    name="Content Planning Step",
    executor=CustomExecutor(max_retries=5, use_cache=False),
)
python  theme={null}
class CustomExecutor:
    async def __call__(self, step_input: StepInput) -> StepOutput:
        # 1. Custom preprocessing
        # 2. Call agents/teams as needed
        # 3. Custom postprocessing
        return StepOutput(content=enhanced_content)

content_planning_step = Step(
    name="Content Planning Step",
    executor=CustomExecutor(),
)
python custom_function_step_async_stream.py theme={null}
content_planner = Agent(
    name="Content Planner",
    model=OpenAIChat(id="gpt-4o"),
    instructions=[
        "Plan a content schedule over 4 weeks for the provided topic and research content",
        "Ensure that I have posts for 3 posts per week",
    ],
    db=InMemoryDb(),
)

async def custom_content_planning_function(
    step_input: StepInput,
) -> AsyncIterator[Union[WorkflowRunOutputEvent, StepOutput]]:
    """
    Custom function that does intelligent content planning with context awareness.

Note: This function calls content_planner.arun() internally, and all events
    from that agent call will automatically get workflow context injected by
    the workflow execution system - no manual intervention required!
    """
    message = step_input.input
    previous_step_content = step_input.previous_step_content

# Create intelligent planning prompt
    planning_prompt = f"""
        STRATEGIC CONTENT PLANNING REQUEST:

Core Topic: {message}

Research Results: {previous_step_content[:500] if previous_step_content else "No research results"}

Planning Requirements:
        1. Create a comprehensive content strategy based on the research
        2. Leverage the research findings effectively
        3. Identify content formats and channels
        4. Provide timeline and priority recommendations
        5. Include engagement and distribution strategies

Please create a detailed, actionable content plan.
    """

try:
        response_iterator = content_planner.arun(
            planning_prompt, stream=True, stream_events=True
        )
        async for event in response_iterator:
            yield event

response = content_planner.get_last_run_output()

enhanced_content = f"""
            ## Strategic Content Plan

**Planning Topic:** {message}

**Research Integration:** {"✓ Research-based" if previous_step_content else "✗ No research foundation"}

**Content Strategy:**
            {response.content}

**Custom Planning Enhancements:**
            - Research Integration: {"High" if previous_step_content else "Baseline"}
            - Strategic Alignment: Optimized for multi-channel distribution
            - Execution Ready: Detailed action items included
        """.strip()

yield StepOutput(content=enhanced_content)

except Exception as e:
        yield StepOutput(
            content=f"Custom content planning failed: {str(e)}",
            success=False,
        )
```

<Note>
  Streaming in case of a class-based executor also works the same way by defining the `__call__` method to yield the events.
</Note>

## Developer Resources

* [Step with a Custom Function](/examples/concepts/workflows/01-basic-workflows/step_with_function)
* [Step with a Custom Function with Streaming on AgentOS](/examples/concepts/workflows/01-basic-workflows/step_with_function_streaming_agentos)
* [Parallel and custom function step streaming on AgentOS](/examples/concepts/workflows/04-workflows-parallel-execution/parallel_and_custom_function_step_streaming_agentos)

**Examples:**

Example 1 (unknown):
```unknown
Also supports async execution by defining the `__call__` method to be an async function.
```

Example 2 (unknown):
```unknown
For a detailed example see [Class-based Executor](/examples/concepts/workflows/01-basic-workflows/class_based_executor).

## Streaming execution with custom function step on AgentOS:

If you are running an agent or team within the custom function step, you can enable streaming on the [AgentOS chat page](/agent-os/introduction#chat-page) by setting `stream=True` and `stream_events=True` when calling `run()` or `arun()` and yielding the events.

<Note>
  Using the AgentOS, runs will be asynchronous and responses will be streamed.
  This means you must keep the custom function step asynchronous, by using `.arun()` instead of `.run()` to run your Agents or Teams.
</Note>
```

---

## LanceDB Hybrid Search

**URL:** llms-txt#lancedb-hybrid-search

**Contents:**
- Code
- Usage

Source: https://docs.agno.com/examples/concepts/vectordb/lance-db/lance-db-hybrid-search

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Install libraries">
    
  </Step>

<Step title="Set environment variables">
    
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Install libraries">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Set environment variables">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Guardrails

**URL:** llms-txt#guardrails

**Contents:**
- Agno built-in Guardrails
- Custom Guardrails

Source: https://docs.agno.com/concepts/teams/guardrails

Learn about securing the input of your Teams using guardrails.

Guardrails are built-in safeguards for your Teams. You can use them to make sure the input you send to the LLM is safe and doesn't contain anything undesired.

Some of the most popular usages are:

* PII detection and redaction
* Prompt injection defense
* Jailbreak defense
* Data leakage prevention
* NSFW content filtering

## Agno built-in Guardrails

To simplify the usage of guardrails, Agno provides some built-in guardrails you can use out of the box:

* PIIDetectionGuardrail: detect PII (Personally Identifiable Information). See the [PII Detection Guardrail](/concepts/agents/guardrails/pii) for agents page for more information.
* PromptInjectionGuardrail: detect and stop prompt injection attemps. See the [Prompt Injection Guardrail](/concepts/agents/guardrails/prompt-injection) for agents page for more information.
* OpenAIModerationGuardrail: detect content that violates OpenAI's content policy. See the [OpenAI Moderation Guardrail](/concepts/agents/guardrails/openai-moderation) for agents page for more information.

To use the Agno built-in guardrails, you just need to import them and pass them to the Team with the `pre_hooks` parameter:

You can some complete examples using the Agno Guardrails in the [examples](/examples/concepts/teams/guardrails) section.

You can create custom guardrails by extending the BaseGuardrail class.

This is useful if you need to perform any check or transformation not handled by the built-in guardrails, or just to implement your own validation logic.

You will need to implement the `check` and `async_check` methods to perform your validation and raise exceptions when detecting undesired content.

<Check>
  Agno automatically uses the sync or async version of the guardrail based on whether you are running the team with `.run()` or `.arun()`.
</Check>

For example, let's create a simple custom guardrail that checks if the input contains any URLs:

Now you can use your custom guardrail in your Team:

```python  theme={null}
from agno.team import Team
from agno.models.openai import OpenAIChat

**Examples:**

Example 1 (unknown):
```unknown
You can some complete examples using the Agno Guardrails in the [examples](/examples/concepts/teams/guardrails) section.

## Custom Guardrails

You can create custom guardrails by extending the BaseGuardrail class.

This is useful if you need to perform any check or transformation not handled by the built-in guardrails, or just to implement your own validation logic.

You will need to implement the `check` and `async_check` methods to perform your validation and raise exceptions when detecting undesired content.

<Check>
  Agno automatically uses the sync or async version of the guardrail based on whether you are running the team with `.run()` or `.arun()`.
</Check>

For example, let's create a simple custom guardrail that checks if the input contains any URLs:
```

Example 2 (unknown):
```unknown
Now you can use your custom guardrail in your Team:
```

---

## session_summary_manager = SessionSummaryManager(model=OpenAIChat(id="gpt-5-mini"))

**URL:** llms-txt#session_summary_manager-=-sessionsummarymanager(model=openaichat(id="gpt-5-mini"))

---

## Create workflow

**URL:** llms-txt#create-workflow

structured_workflow = Workflow(
    name="Structured Content Creation Pipeline",
    description="AI-powered content creation with structured data flow",
    steps=[research_step, strategy_step, planning_step],
)

if __name__ == "__main__":
    print("=== Testing Structured Output Flow Between Steps ===")

# Test with simple string input
    structured_workflow.print_response(
        input="Latest developments in artificial intelligence and machine learning"
    )
```

Examples for some more scenarios where you can use this pattern:

* [Structured IO at each Step level Team](https://github.com/agno-agi/agno/blob/main/cookbook/workflows/_06_advanced_concepts/_01_structured_io_at_each_level/structured_io_at_each_level_team_stream.py)
* [Structured IO at each Step level Custom Function-1](https://github.com/agno-agi/agno/blob/main/cookbook/workflows/_06_advanced_concepts/_01_structured_io_at_each_level/structured_io_at_each_level_function_1.py)
* [Structured IO at each Step level Custom Function-2](https://github.com/agno-agi/agno/blob/main/cookbook/workflows/_06_advanced_concepts/_01_structured_io_at_each_level/structured_io_at_each_level_function_2.py)

---

## No system message should be provided

**URL:** llms-txt#no-system-message-should-be-provided

agent = Agent(
    model=Gemini(
        id="gemini-2.0-flash-exp-image-generation",
        response_modalities=["Text", "Image"],
    )
)

---

## Discord Bot

**URL:** llms-txt#discord-bot

**Contents:**
- Setup Steps
  - Example Usage
- Core Components
- `DiscordClient` Class
  - Initialization Parameters
- Event Handling
  - Message Events
  - Supported Media Types
- Environment Variables
- Message Processing

Source: https://docs.agno.com/integrations/discord/overview

Host agents as Discord Bots.

The Discord Bot integration allows you to serve Agents or Teams via Discord, using the discord.py library to handle Discord events and send messages.

<Snippet file="setup-discord-app.mdx" />

Create an agent, wrap it with `DiscordClient`, and run it:

* `DiscordClient`: Wraps Agno agents/teams for Discord integration using discord.py.
* `DiscordClient.serve`: Starts the Discord bot client with the provided token.

## `DiscordClient` Class

Main entry point for Agno Discord bot applications.

### Initialization Parameters

| Parameter | Type              | Default | Description            |
| --------- | ----------------- | ------- | ---------------------- |
| `agent`   | `Optional[Agent]` | `None`  | Agno `Agent` instance. |
| `team`    | `Optional[Team]`  | `None`  | Agno `Team` instance.  |

*Provide `agent` or `team`, not both.*

The Discord bot automatically handles various Discord events:

* **Description**: Processes all incoming messages from users
* **Media Support**: Handles images, videos, audio files, and documents
* **Threading**: Automatically creates threads for conversations
* **Features**:
  * Automatic thread creation for each conversation
  * Media processing and forwarding to agents
  * Message splitting for responses longer than 1500 characters
  * Support for reasoning content display
  * Context enrichment with username and message URL

### Supported Media Types

* **Images**: Direct URL processing for image analysis
* **Videos**: Downloads and processes video content
* **Audio**: URL-based audio processing
* **Files**: Downloads and processes document attachments

## Environment Variables

Ensure the following environment variable is set:

## Message Processing

The bot processes messages with the following workflow:

1. **Message Reception**: Receives messages from Discord channels
2. **Media Processing**: Downloads and processes any attached media
3. **Thread Management**: Creates or uses existing threads for conversations
4. **Agent/Team Execution**: Forwards the message and media to the configured agent or team
5. **Response Handling**: Sends the response back to Discord, splitting long messages if necessary
6. **Reasoning Display**: Shows reasoning content in italics if available

### Automatic Thread Creation

* Creates a new thread for each user's first message
* Maintains conversation context within threads
* Uses the format: `{username}'s thread`

* **Images**: Passed as `Image` objects with URLs
* **Videos**: Downloaded and passed as `Video` objects with content
* **Audio**: Passed as `Audio` objects with URLs
* **Files**: Downloaded and passed as `File` objects with content

### Message Formatting

* Long messages (>1500 characters) are automatically split
* Reasoning content is displayed in italics
* Batch numbering for split messages: `[1/3] message content`

## Testing the Integration

1. Set up your Discord bot token: `export DISCORD_BOT_TOKEN="your-token"`
2. Run your application: `python your_discord_bot.py`
3. Invite the bot to your Discord server
4. Send a message in any channel where the bot has access
5. The bot will automatically create a thread and respond

**Examples:**

Example 1 (unknown):
```unknown
## Core Components

* `DiscordClient`: Wraps Agno agents/teams for Discord integration using discord.py.
* `DiscordClient.serve`: Starts the Discord bot client with the provided token.

## `DiscordClient` Class

Main entry point for Agno Discord bot applications.

### Initialization Parameters

| Parameter | Type              | Default | Description            |
| --------- | ----------------- | ------- | ---------------------- |
| `agent`   | `Optional[Agent]` | `None`  | Agno `Agent` instance. |
| `team`    | `Optional[Team]`  | `None`  | Agno `Team` instance.  |

*Provide `agent` or `team`, not both.*

## Event Handling

The Discord bot automatically handles various Discord events:

### Message Events

* **Description**: Processes all incoming messages from users
* **Media Support**: Handles images, videos, audio files, and documents
* **Threading**: Automatically creates threads for conversations
* **Features**:
  * Automatic thread creation for each conversation
  * Media processing and forwarding to agents
  * Message splitting for responses longer than 1500 characters
  * Support for reasoning content display
  * Context enrichment with username and message URL

### Supported Media Types

* **Images**: Direct URL processing for image analysis
* **Videos**: Downloads and processes video content
* **Audio**: URL-based audio processing
* **Files**: Downloads and processes document attachments

## Environment Variables

Ensure the following environment variable is set:
```

---

## Test the counter functionality

**URL:** llms-txt#test-the-counter-functionality

**Contents:**
- Usage

print("Testing counter functionality...")
agent.print_response(
    "Let's increment the counter 3 times and observe the state changes!", stream=True
)
print(f"Final session state: {agent.get_session_state()}")
bash  theme={null}
    pip install openai agno
    bash  theme={null}
    python agent_state.py
    ```
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Install libraries">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Run the agent">
```

---

## Discord Tools

**URL:** llms-txt#discord-tools

**Contents:**
- Code
- Usage

Source: https://docs.agno.com/examples/concepts/tools/social/discord

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Set your Discord token">
    
  </Step>

<Step title="Install libraries">
    
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Set your Discord token">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Conditional Workflow

**URL:** llms-txt#conditional-workflow

**Contents:**
- Example
- Developer Resources
- Reference

Source: https://docs.agno.com/concepts/workflows/workflow-patterns/conditional-workflow

Deterministic branching based on input analysis or business rules

**Example Use-Cases**: Content type routing, topic-specific processing, quality-based decisions

Conditional workflows provide predictable branching logic while maintaining deterministic execution paths.

<img className="block dark:hidden" src="https://mintcdn.com/agno-v2/JYIBgMrzFEujZh3_/images/workflows-condition-steps-light.png?fit=max&auto=format&n=JYIBgMrzFEujZh3_&q=85&s=7bc060741f060c43747d9866246d0587" alt="Workflows condition steps diagram" data-og-width="3441" width="3441" data-og-height="756" height="756" data-path="images/workflows-condition-steps-light.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/agno-v2/JYIBgMrzFEujZh3_/images/workflows-condition-steps-light.png?w=280&fit=max&auto=format&n=JYIBgMrzFEujZh3_&q=85&s=051009cf50418538acbc49a9c690cdf8 280w, https://mintcdn.com/agno-v2/JYIBgMrzFEujZh3_/images/workflows-condition-steps-light.png?w=560&fit=max&auto=format&n=JYIBgMrzFEujZh3_&q=85&s=5f8e6a2ed1301cf1d4edda7804c5ec08 560w, https://mintcdn.com/agno-v2/JYIBgMrzFEujZh3_/images/workflows-condition-steps-light.png?w=840&fit=max&auto=format&n=JYIBgMrzFEujZh3_&q=85&s=65a6ba82ef0a22a0927439644a6c7912 840w, https://mintcdn.com/agno-v2/JYIBgMrzFEujZh3_/images/workflows-condition-steps-light.png?w=1100&fit=max&auto=format&n=JYIBgMrzFEujZh3_&q=85&s=f5d676c0bf82f2045e61f31126b66a42 1100w, https://mintcdn.com/agno-v2/JYIBgMrzFEujZh3_/images/workflows-condition-steps-light.png?w=1650&fit=max&auto=format&n=JYIBgMrzFEujZh3_&q=85&s=408f8ed2a78755b289c7a0b4e07d6f0e 1650w, https://mintcdn.com/agno-v2/JYIBgMrzFEujZh3_/images/workflows-condition-steps-light.png?w=2500&fit=max&auto=format&n=JYIBgMrzFEujZh3_&q=85&s=548ca991ffb6e9e5d7669bf37e98fed2 2500w" />

<img className="hidden dark:block" src="https://mintcdn.com/agno-v2/JYIBgMrzFEujZh3_/images/workflows-condition-steps.png?fit=max&auto=format&n=JYIBgMrzFEujZh3_&q=85&s=de3fa0bc3fc9b4079e7dd3596d6e589a" alt="Workflows condition steps diagram" data-og-width="3441" width="3441" data-og-height="756" height="756" data-path="images/workflows-condition-steps.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/agno-v2/JYIBgMrzFEujZh3_/images/workflows-condition-steps.png?w=280&fit=max&auto=format&n=JYIBgMrzFEujZh3_&q=85&s=3b0eb6ed78b037dd85346647665f373e 280w, https://mintcdn.com/agno-v2/JYIBgMrzFEujZh3_/images/workflows-condition-steps.png?w=560&fit=max&auto=format&n=JYIBgMrzFEujZh3_&q=85&s=9c5e9785043d807c36b6ee130fde63ef 560w, https://mintcdn.com/agno-v2/JYIBgMrzFEujZh3_/images/workflows-condition-steps.png?w=840&fit=max&auto=format&n=JYIBgMrzFEujZh3_&q=85&s=5044466baae4103aadf462fb81b9be60 840w, https://mintcdn.com/agno-v2/JYIBgMrzFEujZh3_/images/workflows-condition-steps.png?w=1100&fit=max&auto=format&n=JYIBgMrzFEujZh3_&q=85&s=d67a6cb9bb25245259a4001be56f7d91 1100w, https://mintcdn.com/agno-v2/JYIBgMrzFEujZh3_/images/workflows-condition-steps.png?w=1650&fit=max&auto=format&n=JYIBgMrzFEujZh3_&q=85&s=ff169ab64d750cfb21073305fdedd213 1650w, https://mintcdn.com/agno-v2/JYIBgMrzFEujZh3_/images/workflows-condition-steps.png?w=2500&fit=max&auto=format&n=JYIBgMrzFEujZh3_&q=85&s=0f44b53b13a00c51b26314825d605013 2500w" />

## Developer Resources

* [Condition Steps Workflow](/examples/concepts/workflows/02-workflows-conditional-execution/condition_steps_workflow_stream)
* [Condition with List of Steps](/examples/concepts/workflows/02-workflows-conditional-execution/condition_with_list_of_steps)

For complete API documentation, see [Condition Steps Reference](/reference/workflows/conditional-steps).

---

## Example 2: Parallel Primitive with Event Storage

**URL:** llms-txt#example-2:-parallel-primitive-with-event-storage

print("=== 2. Parallel Example ===")
parallel_workflow = Workflow(
    name="Parallel Research Workflow",
    steps=[
        Parallel(
            Step(name="News Research", agent=news_agent),
            Step(name="Web Search", agent=search_agent),
            name="Parallel Research",
        ),
        Step(name="Combine Results", agent=analysis_agent),
    ],
    db=SqliteDb(
        session_table="workflow_parallel",
        db_file="tmp/workflow_parallel.db",
    ),
    store_events=True,
    events_to_skip=[
        WorkflowRunEvent.parallel_execution_started,
        WorkflowRunEvent.parallel_execution_completed,
    ],
)

print("Running Parallel workflow...")
for event in parallel_workflow.run(
    input="Research machine learning developments",
    stream=True,
    stream_events=True,
):
    # Filter out RunContentEvent from printing
    if not isinstance(event, RunContentEvent):
        print(
            f"Event: {event.event if hasattr(event, 'event') else type(event).__name__}"
        )

run_response = parallel_workflow.get_last_run_output()
print(f"Parallel workflow stored {len(run_response.events)} events")
print_stored_events(run_response, "Parallel Workflow")
print()
```

---

## DashScope

**URL:** llms-txt#dashscope

**Contents:**
- Authentication
- Example
- Parameters
- Thinking Models

Source: https://docs.agno.com/concepts/models/dashscope

Learn how to use DashScope models in Agno.

Leverage DashScope's powerful command models and more.

[DashScope](https://dashscope.aliyun.com/) supports a wide range of models

We recommend experimenting to find the best-suited model for your use-case. Here are some general recommendations:

* `qwen-plus` model is good for most use-cases.

Set your `DASHSCOPE_API_KEY` environment variable. Get your key from [here](https://dashscope.aliyun.com/api-keys).

Use `DashScope` with your `Agent`:

<CodeGroup>
  
</CodeGroup>

<Note> View more examples [here](/examples/models/dashscope/basic). </Note>

| Parameter          | Type             | Default                                                    | Description                                                         |
| ------------------ | ---------------- | ---------------------------------------------------------- | ------------------------------------------------------------------- |
| `id`               | `str`            | `"qwen-plus"`                                              | The id of the Qwen model to use                                     |
| `name`             | `str`            | `"Qwen"`                                                   | The name of the model                                               |
| `provider`         | `str`            | `"Dashscope"`                                              | The provider of the model                                           |
| `api_key`          | `Optional[str]`  | `None`                                                     | The API key for DashScope (defaults to DASHSCOPE\_API\_KEY env var) |
| `base_url`         | `str`            | `"https://dashscope-intl.aliyuncs.com/compatible-mode/v1"` | The base URL for the DashScope API                                  |
| `enable_thinking`  | `bool`           | `False`                                                    | Enable thinking process for reasoning models                        |
| `include_thoughts` | `Optional[bool]` | `None`                                                     | Include thinking process in response (alternative parameter)        |
| `thinking_budget`  | `Optional[int]`  | `None`                                                     | Budget for thinking tokens in reasoning models                      |

`DashScope` extends the OpenAI-compatible interface and supports most parameters from the [OpenAI model](/concepts/models/openai).

DashScope supports reasoning models with thinking capabilities:

**Examples:**

Example 1 (unknown):
```unknown

```

Example 2 (unknown):
```unknown
</CodeGroup>

## Example

Use `DashScope` with your `Agent`:

<CodeGroup>
```

Example 3 (unknown):
```unknown
</CodeGroup>

<Note> View more examples [here](/examples/models/dashscope/basic). </Note>

## Parameters

| Parameter          | Type             | Default                                                    | Description                                                         |
| ------------------ | ---------------- | ---------------------------------------------------------- | ------------------------------------------------------------------- |
| `id`               | `str`            | `"qwen-plus"`                                              | The id of the Qwen model to use                                     |
| `name`             | `str`            | `"Qwen"`                                                   | The name of the model                                               |
| `provider`         | `str`            | `"Dashscope"`                                              | The provider of the model                                           |
| `api_key`          | `Optional[str]`  | `None`                                                     | The API key for DashScope (defaults to DASHSCOPE\_API\_KEY env var) |
| `base_url`         | `str`            | `"https://dashscope-intl.aliyuncs.com/compatible-mode/v1"` | The base URL for the DashScope API                                  |
| `enable_thinking`  | `bool`           | `False`                                                    | Enable thinking process for reasoning models                        |
| `include_thoughts` | `Optional[bool]` | `None`                                                     | Include thinking process in response (alternative parameter)        |
| `thinking_budget`  | `Optional[int]`  | `None`                                                     | Budget for thinking tokens in reasoning models                      |

`DashScope` extends the OpenAI-compatible interface and supports most parameters from the [OpenAI model](/concepts/models/openai).

## Thinking Models

DashScope supports reasoning models with thinking capabilities:
```

---

## Enable scrape method for raw HTML content

**URL:** llms-txt#enable-scrape-method-for-raw-html-content

**Contents:**
  - All Functions with JavaScript Rendering

scrapegraph_scrape = ScrapeGraphTools(enable_scrape=True, enable_smartscraper=False)

scrape_agent = Agent(
    tools=[scrapegraph_scrape],
    model=agent_model,
    markdown=True,
    stream=True,
)

scrape_agent.print_response(
    "Use the scrape tool to get the complete raw HTML content from https://en.wikipedia.org/wiki/2025_FIFA_Club_World_Cup"
)
python cookbook/tools/scrapegraph_tools.py theme={null}

**Examples:**

Example 1 (unknown):
```unknown
### All Functions with JavaScript Rendering

Enable all ScrapeGraph functions with heavy JavaScript support:
```

---

## GcsJsonDb

**URL:** llms-txt#gcsjsondb

Source: https://docs.agno.com/reference/storage/gcs

`GcsJsonDb` is a class that implements the Db interface using Google Cloud Storage as a database using JSON files. It provides high-performance, distributed storage for agent sessions with support for JSON data types and schema versioning.

<Snippet file="db-gcs-params.mdx" />

<Snippet file="db-new-bulk-methods.mdx" />

---

## Simple function we will use as a pre-hook

**URL:** llms-txt#simple-function-we-will-use-as-a-pre-hook

**Contents:**
- Pre-hooks Parameters
- Post-hooks
  - Common Use Cases
  - Basic Example

def validate_input_length(
    run_input: RunInput,
    session: TeamSession,
    user_id: Optional[str] = None,
    debug_mode: Optional[bool] = None,
) -> None:
    """Pre-hook to validate input length."""
    max_length = 1000
    if len(run_input.input_content) > max_length:
        raise InputCheckError(
            f"Input too long. Max {max_length} characters allowed",
            check_trigger=CheckTrigger.INPUT_NOT_ALLOWED,
        )

team = Team(
    name="My Team",
    model=OpenAIChat(id="gpt-5-mini"),
    # Provide the pre-hook to the Team using the pre_hooks parameter
    pre_hooks=[validate_input_length],
)

You can see some complete examples of pre-hooks in the [Examples](/examples/concepts/teams/pre-hooks-and-post-hooks) section.

## Pre-hooks Parameters

Pre-hooks run automatically during the Team run. Some parameters will be injected automatically.

You can learn more about the parameters in the [Pre-hooks](/reference/hooks/pre-hooks) reference.

Post-hooks execute **after** your Team generates a response, allowing you to validate, transform, or enrich the output before it reaches the user.

They're perfect to handle output filtering, compliance checks, or any other output transformation you need.

**Output Validation**
- Validate response format, length, and content quality.
- Remove sensitive or inappropriate information from responses.
- Ensure compliance with business rules and regulations.

**Output Transformation**
- Add metadata or additional context to responses.
- Transform output format for different clients or use cases.
- Implement any other output transformation you need.

Let's create a simple post-hook that validates the output length and raises an error if it's too long:

```python
from agno.exceptions import CheckTrigger, OutputCheckError
from agno.run.team import RunOutput

---

## All logging will use our custom logger.

**URL:** llms-txt#all-logging-will-use-our-custom-logger.

**Contents:**
- Multiple Loggers
- Using Named Loggers

team = Team(members=[agent])
team.print_response("What can I do to improve my sleep?")
python  theme={null}
configure_agno_logging(
    custom_default_logger=custom_agent_logger,
    custom_agent_logger=custom_agent_logger,
    custom_team_logger=custom_team_logger,
    custom_workflow_logger=custom_workflow_logger,
)
```

## Using Named Loggers

As it's conventional in Python, you can also provide custom loggers just by setting loggers with specific names. This is useful if you want to set them up using configuration files.

* `agno.agent` will be used for all Agent logs
* `agno.team` will be used for all Team logs
* `agno.workflow` will be used for all Workflow logs

These loggers will be automatically picked up if they are set.

**Examples:**

Example 1 (unknown):
```unknown
## Multiple Loggers

Notice that you can also configure different loggers for your Agents, Teams and Workflows:
```

---

## Example usage with different locations

**URL:** llms-txt#example-usage-with-different-locations

json_mode_agent.print_response("Tokyo", stream=True)
structured_output_agent.print_response("Ancient Rome", stream=True)

---

## Web Search Reader

**URL:** llms-txt#web-search-reader

Source: https://docs.agno.com/reference/knowledge/reader/web-search

WebSearchReader is a reader class that allows you to read data from web search results.

<Snippet file="web-search-reader-reference.mdx" />

---

## RunOutput

**URL:** llms-txt#runoutput

**Contents:**
- RunOutput Attributes
- RunOutputEvent Types and Attributes
  - Base RunOutputEvent Attributes
  - RunStartedEvent
  - RunContentEvent
  - RunContentCompletedEvent
  - IntermediateRunContentEvent
  - RunCompletedEvent
  - RunPausedEvent
  - RunContinuedEvent

Source: https://docs.agno.com/reference/agents/run-response

## RunOutput Attributes

| Attribute             | Type                                | Default             | Description                                                      |
| --------------------- | ----------------------------------- | ------------------- | ---------------------------------------------------------------- |
| `run_id`              | `Optional[str]`                     | `None`              | Run ID                                                           |
| `agent_id`            | `Optional[str]`                     | `None`              | Agent ID for the run                                             |
| `agent_name`          | `Optional[str]`                     | `None`              | Agent name for the run                                           |
| `session_id`          | `Optional[str]`                     | `None`              | Session ID for the run                                           |
| `parent_run_id`       | `Optional[str]`                     | `None`              | Parent run ID                                                    |
| `workflow_id`         | `Optional[str]`                     | `None`              | Workflow ID if this run is part of a workflow                    |
| `user_id`             | `Optional[str]`                     | `None`              | User ID associated with the run                                  |
| `content`             | `Optional[Any]`                     | `None`              | Content of the response                                          |
| `content_type`        | `str`                               | `"str"`             | Specifies the data type of the content                           |
| `reasoning_content`   | `Optional[str]`                     | `None`              | Any reasoning content the model produced                         |
| `reasoning_steps`     | `Optional[List[ReasoningStep]]`     | `None`              | List of reasoning steps                                          |
| `reasoning_messages`  | `Optional[List[Message]]`           | `None`              | List of reasoning messages                                       |
| `model`               | `Optional[str]`                     | `None`              | The model used in the run                                        |
| `model_provider`      | `Optional[str]`                     | `None`              | The model provider used in the run                               |
| `messages`            | `Optional[List[Message]]`           | `None`              | A list of messages included in the response                      |
| `metrics`             | `Optional[Metrics]`                 | `None`              | Usage metrics of the run                                         |
| `additional_input`    | `Optional[List[Message]]`           | `None`              | Additional input messages                                        |
| `tools`               | `Optional[List[ToolExecution]]`     | `None`              | List of tool executions                                          |
| `images`              | `Optional[List[Image]]`             | `None`              | List of images attached to the response                          |
| `videos`              | `Optional[List[Video]]`             | `None`              | List of videos attached to the response                          |
| `audio`               | `Optional[List[Audio]]`             | `None`              | List of audio snippets attached to the response                  |
| `files`               | `Optional[List[File]]`              | `None`              | List of files attached to the response                           |
| `response_audio`      | `Optional[Audio]`                   | `None`              | The model's raw response in audio                                |
| `input`               | `Optional[RunInput]`                | `None`              | Input media and messages from user                               |
| `citations`           | `Optional[Citations]`               | `None`              | Any citations used in the response                               |
| `model_provider_data` | `Optional[Any]`                     | `None`              | Model provider specific metadata                                 |
| `references`          | `Optional[List[MessageReferences]]` | `None`              | References used in the response                                  |
| `metadata`            | `Optional[Dict[str, Any]]`          | `None`              | Metadata associated with the run                                 |
| `created_at`          | `int`                               | Current timestamp   | Unix timestamp of the response creation                          |
| `events`              | `Optional[List[RunOutputEvent]]`    | `None`              | List of events that occurred during the run                      |
| `status`              | `RunStatus`                         | `RunStatus.running` | Status of the run (running, completed, paused, cancelled, error) |
| `workflow_step_id`    | `Optional[str]`                     | `None`              | Workflow step ID (foreign key relationship)                      |

## RunOutputEvent Types and Attributes

### Base RunOutputEvent Attributes

All events inherit from `BaseAgentRunEvent` which provides these common attributes:

| Attribute         | Type                            | Default           | Description                                      |
| ----------------- | ------------------------------- | ----------------- | ------------------------------------------------ |
| `created_at`      | `int`                           | Current timestamp | Unix timestamp of the event creation             |
| `event`           | `str`                           | Event type value  | The type of event                                |
| `agent_id`        | `str`                           | `""`              | ID of the agent generating the event             |
| `agent_name`      | `str`                           | `""`              | Name of the agent generating the event           |
| `run_id`          | `Optional[str]`                 | `None`            | ID of the current run                            |
| `session_id`      | `Optional[str]`                 | `None`            | ID of the current session                        |
| `workflow_id`     | `Optional[str]`                 | `None`            | ID of the workflow if part of workflow execution |
| `workflow_run_id` | `Optional[str]`                 | `None`            | ID of the workflow run                           |
| `step_id`         | `Optional[str]`                 | `None`            | ID of the workflow step                          |
| `step_name`       | `Optional[str]`                 | `None`            | Name of the workflow step                        |
| `step_index`      | `Optional[int]`                 | `None`            | Index of the workflow step                       |
| `tools`           | `Optional[List[ToolExecution]]` | `None`            | Tools associated with this event                 |
| `content`         | `Optional[Any]`                 | `None`            | For backwards compatibility                      |

| Attribute        | Type  | Default        | Description               |
| ---------------- | ----- | -------------- | ------------------------- |
| `event`          | `str` | `"RunStarted"` | Event type                |
| `model`          | `str` | `""`           | The model being used      |
| `model_provider` | `str` | `""`           | The provider of the model |

| Attribute             | Type                                | Default        | Description                      |
| --------------------- | ----------------------------------- | -------------- | -------------------------------- |
| `event`               | `str`                               | `"RunContent"` | Event type                       |
| `content`             | `Optional[Any]`                     | `None`         | The content of the response      |
| `content_type`        | `str`                               | `"str"`        | Type of the content              |
| `reasoning_content`   | `Optional[str]`                     | `None`         | Reasoning content produced       |
| `citations`           | `Optional[Citations]`               | `None`         | Citations used in the response   |
| `model_provider_data` | `Optional[Any]`                     | `None`         | Model provider specific metadata |
| `response_audio`      | `Optional[Audio]`                   | `None`         | Model's audio response           |
| `image`               | `Optional[Image]`                   | `None`         | Image attached to the response   |
| `references`          | `Optional[List[MessageReferences]]` | `None`         | References used in the response  |
| `additional_input`    | `Optional[List[Message]]`           | `None`         | Additional input messages        |
| `reasoning_steps`     | `Optional[List[ReasoningStep]]`     | `None`         | Reasoning steps                  |
| `reasoning_messages`  | `Optional[List[Message]]`           | `None`         | Reasoning messages               |

### RunContentCompletedEvent

| Attribute | Type  | Default                 | Description |
| --------- | ----- | ----------------------- | ----------- |
| `event`   | `str` | `"RunContentCompleted"` | Event type  |

### IntermediateRunContentEvent

| Attribute      | Type            | Default                    | Description                          |
| -------------- | --------------- | -------------------------- | ------------------------------------ |
| `event`        | `str`           | `"RunIntermediateContent"` | Event type                           |
| `content`      | `Optional[Any]` | `None`                     | Intermediate content of the response |
| `content_type` | `str`           | `"str"`                    | Type of the content                  |

### RunCompletedEvent

| Attribute             | Type                                | Default          | Description                             |
| --------------------- | ----------------------------------- | ---------------- | --------------------------------------- |
| `event`               | `str`                               | `"RunCompleted"` | Event type                              |
| `content`             | `Optional[Any]`                     | `None`           | Final content of the response           |
| `content_type`        | `str`                               | `"str"`          | Type of the content                     |
| `reasoning_content`   | `Optional[str]`                     | `None`           | Reasoning content produced              |
| `citations`           | `Optional[Citations]`               | `None`           | Citations used in the response          |
| `model_provider_data` | `Optional[Any]`                     | `None`           | Model provider specific metadata        |
| `images`              | `Optional[List[Image]]`             | `None`           | Images attached to the response         |
| `videos`              | `Optional[List[Video]]`             | `None`           | Videos attached to the response         |
| `audio`               | `Optional[List[Audio]]`             | `None`           | Audio snippets attached to the response |
| `response_audio`      | `Optional[Audio]`                   | `None`           | Model's audio response                  |
| `references`          | `Optional[List[MessageReferences]]` | `None`           | References used in the response         |
| `additional_input`    | `Optional[List[Message]]`           | `None`           | Additional input messages               |
| `reasoning_steps`     | `Optional[List[ReasoningStep]]`     | `None`           | Reasoning steps                         |
| `reasoning_messages`  | `Optional[List[Message]]`           | `None`           | Reasoning messages                      |
| `metadata`            | `Optional[Dict[str, Any]]`          | `None`           | Additional metadata                     |
| `metrics`             | `Optional[Metrics]`                 | `None`           | Usage metrics                           |

| Attribute | Type                            | Default       | Description                     |
| --------- | ------------------------------- | ------------- | ------------------------------- |
| `event`   | `str`                           | `"RunPaused"` | Event type                      |
| `tools`   | `Optional[List[ToolExecution]]` | `None`        | Tools that require confirmation |

### RunContinuedEvent

| Attribute | Type  | Default          | Description |
| --------- | ----- | ---------------- | ----------- |
| `event`   | `str` | `"RunContinued"` | Event type  |

| Attribute | Type            | Default      | Description   |
| --------- | --------------- | ------------ | ------------- |
| `event`   | `str`           | `"RunError"` | Event type    |
| `content` | `Optional[str]` | `None`       | Error message |

### RunCancelledEvent

| Attribute | Type            | Default          | Description             |
| --------- | --------------- | ---------------- | ----------------------- |
| `event`   | `str`           | `"RunCancelled"` | Event type              |
| `reason`  | `Optional[str]` | `None`           | Reason for cancellation |

### PreHookStartedEvent

| Attribute       | Type                 | Default            | Description                         |
| --------------- | -------------------- | ------------------ | ----------------------------------- |
| `event`         | `str`                | `"PreHookStarted"` | Event type                          |
| `pre_hook_name` | `Optional[str]`      | `None`             | Name of the pre-hook being executed |
| `run_input`     | `Optional[RunInput]` | `None`             | The run input passed to the hook    |

### PreHookCompletedEvent

| Attribute       | Type                 | Default              | Description                         |
| --------------- | -------------------- | -------------------- | ----------------------------------- |
| `event`         | `str`                | `"PreHookCompleted"` | Event type                          |
| `pre_hook_name` | `Optional[str]`      | `None`               | Name of the pre-hook that completed |
| `run_input`     | `Optional[RunInput]` | `None`               | The run input passed to the hook    |

### PostHookStartedEvent

| Attribute        | Type            | Default             | Description                          |
| ---------------- | --------------- | ------------------- | ------------------------------------ |
| `event`          | `str`           | `"PostHookStarted"` | Event type                           |
| `post_hook_name` | `Optional[str]` | `None`              | Name of the post-hook being executed |

### PostHookCompletedEvent

| Attribute        | Type            | Default               | Description                          |
| ---------------- | --------------- | --------------------- | ------------------------------------ |
| `event`          | `str`           | `"PostHookCompleted"` | Event type                           |
| `post_hook_name` | `Optional[str]` | `None`                | Name of the post-hook that completed |

### ReasoningStartedEvent

| Attribute | Type  | Default              | Description |
| --------- | ----- | -------------------- | ----------- |
| `event`   | `str` | `"ReasoningStarted"` | Event type  |

### ReasoningStepEvent

| Attribute           | Type            | Default           | Description                   |
| ------------------- | --------------- | ----------------- | ----------------------------- |
| `event`             | `str`           | `"ReasoningStep"` | Event type                    |
| `content`           | `Optional[Any]` | `None`            | Content of the reasoning step |
| `content_type`      | `str`           | `"str"`           | Type of the content           |
| `reasoning_content` | `str`           | `""`              | Detailed reasoning content    |

### ReasoningCompletedEvent

| Attribute      | Type            | Default                | Description                   |
| -------------- | --------------- | ---------------------- | ----------------------------- |
| `event`        | `str`           | `"ReasoningCompleted"` | Event type                    |
| `content`      | `Optional[Any]` | `None`                 | Content of the reasoning step |
| `content_type` | `str`           | `"str"`                | Type of the content           |

### ToolCallStartedEvent

| Attribute | Type                      | Default             | Description           |
| --------- | ------------------------- | ------------------- | --------------------- |
| `event`   | `str`                     | `"ToolCallStarted"` | Event type            |
| `tool`    | `Optional[ToolExecution]` | `None`              | The tool being called |

### ToolCallCompletedEvent

| Attribute | Type                      | Default               | Description                 |
| --------- | ------------------------- | --------------------- | --------------------------- |
| `event`   | `str`                     | `"ToolCallCompleted"` | Event type                  |
| `tool`    | `Optional[ToolExecution]` | `None`                | The tool that was called    |
| `content` | `Optional[Any]`           | `None`                | Result of the tool call     |
| `images`  | `Optional[List[Image]]`   | `None`                | Images produced by the tool |
| `videos`  | `Optional[List[Video]]`   | `None`                | Videos produced by the tool |
| `audio`   | `Optional[List[Audio]]`   | `None`                | Audio produced by the tool  |

### MemoryUpdateStartedEvent

| Attribute | Type  | Default                 | Description |
| --------- | ----- | ----------------------- | ----------- |
| `event`   | `str` | `"MemoryUpdateStarted"` | Event type  |

### MemoryUpdateCompletedEvent

| Attribute | Type  | Default                   | Description |
| --------- | ----- | ------------------------- | ----------- |
| `event`   | `str` | `"MemoryUpdateCompleted"` | Event type  |

### SessionSummaryStartedEvent

| Attribute | Type  | Default                   | Description |
| --------- | ----- | ------------------------- | ----------- |
| `event`   | `str` | `"SessionSummaryStarted"` | Event type  |

### SessionSummaryCompletedEvent

| Attribute         | Type                       | Default                     | Description                   |
| ----------------- | -------------------------- | --------------------------- | ----------------------------- |
| `event`           | `str`                      | `"SessionSummaryCompleted"` | Event type                    |
| `session_summary` | `Optional[SessionSummary]` | `None`                      | The generated session summary |

### ParserModelResponseStartedEvent

| Attribute | Type  | Default                        | Description |
| --------- | ----- | ------------------------------ | ----------- |
| `event`   | `str` | `"ParserModelResponseStarted"` | Event type  |

### ParserModelResponseCompletedEvent

| Attribute | Type  | Default                          | Description |
| --------- | ----- | -------------------------------- | ----------- |
| `event`   | `str` | `"ParserModelResponseCompleted"` | Event type  |

### OutputModelResponseStartedEvent

| Attribute | Type  | Default                        | Description |
| --------- | ----- | ------------------------------ | ----------- |
| `event`   | `str` | `"OutputModelResponseStarted"` | Event type  |

### OutputModelResponseCompletedEvent

| Attribute | Type  | Default                          | Description |
| --------- | ----- | -------------------------------- | ----------- |
| `event`   | `str` | `"OutputModelResponseCompleted"` | Event type  |

| Attribute | Type  | Default         | Description |
| --------- | ----- | --------------- | ----------- |
| `event`   | `str` | `"CustomEvent"` | Event type  |

---

## Define tools to manage our shopping list

**URL:** llms-txt#define-tools-to-manage-our-shopping-list

def add_item(session_state, item: str) -> str:
    """Add an item to the shopping list and return confirmation.

Args:
        item (str): The item to add to the shopping list.
    """
    # Add the item if it's not already in the list
    if item.lower() not in [i.lower() for i in session_state["shopping_list"]]:
        session_state["shopping_list"].append(item)
        return f"Added '{item}' to the shopping list"
    else:
        return f"'{item}' is already in the shopping list"

def remove_item(session_state, item: str) -> str:
    """Remove an item from the shopping list by name.

Args:
        item (str): The item to remove from the shopping list.
    """
    # Case-insensitive search
    for i, list_item in enumerate(session_state["shopping_list"]):
        if list_item.lower() == item.lower():
            session_state["shopping_list"].pop(i)
            return f"Removed '{list_item}' from the shopping list"

return f"'{item}' was not found in the shopping list. Current shopping list: {session_state['shopping_list']}"

def remove_all_items(session_state) -> str:
    """Remove all items from the shopping list."""
    session_state["shopping_list"] = []
    return "All items removed from the shopping list"

shopping_list_agent = Agent(
    name="Shopping List Agent",
    role="Manage the shopping list",
    id="shopping_list_manager",
    model=OpenAIChat(id="gpt-5-mini"),
    tools=[add_item, remove_item, remove_all_items],
    instructions=[
        "Manage the shopping list by adding and removing items",
        "Always confirm when items are added or removed",
        "If the task is done, update the session state to log the changes & chores you've performed",
    ],
)

---

## Exa

**URL:** llms-txt#exa

**Contents:**
- Prerequisites
- Example
- Toolkit Functions
- Toolkit Params
  - Categories
- Developer Resources

Source: https://docs.agno.com/concepts/tools/toolkits/search/exa

**ExaTools** enable an Agent to search the web using Exa, retrieve content from URLs, find similar content, and get AI-powered answers.

The following examples require the `exa-py` library and an API key which can be obtained from [Exa](https://exa.ai).

The following agent will search Exa for AAPL news and print the response.

| Function       | Description                                                      |
| -------------- | ---------------------------------------------------------------- |
| `search_exa`   | Searches Exa for a query with optional category filtering        |
| `get_contents` | Retrieves detailed content from specific URLs                    |
| `find_similar` | Finds similar content to a given URL                             |
| `exa_answer`   | Gets an AI-powered answer to a question using Exa search results |

| Parameter              | Type                  | Default    | Description                                        |
| ---------------------- | --------------------- | ---------- | -------------------------------------------------- |
| `enable_search`        | `bool`                | `True`     | Enable search functionality                        |
| `enable_get_contents`  | `bool`                | `True`     | Enable content retrieval                           |
| `enable_find_similar`  | `bool`                | `True`     | Enable finding similar content                     |
| `enable_answer`        | `bool`                | `True`     | Enable AI-powered answers                          |
| `enable_research`      | `bool`                | `True`     | Enable research functionality                      |
| `all`                  | `bool`                | `False`    | Enable all functionality                           |
| `text`                 | `bool`                | `True`     | Include text content in results                    |
| `text_length_limit`    | `int`                 | `1000`     | Maximum length of text content per result          |
| `highlights`           | `bool`                | `True`     | Include highlighted snippets                       |
| `summary`              | `bool`                | `False`    | Include result summaries                           |
| `num_results`          | `Optional[int]`       | `None`     | Default number of results                          |
| `livecrawl`            | `str`                 | `"always"` | Livecrawl behavior                                 |
| `start_crawl_date`     | `Optional[str]`       | `None`     | Include results crawled after date (YYYY-MM-DD)    |
| `end_crawl_date`       | `Optional[str]`       | `None`     | Include results crawled before date (YYYY-MM-DD)   |
| `start_published_date` | `Optional[str]`       | `None`     | Include results published after date (YYYY-MM-DD)  |
| `end_published_date`   | `Optional[str]`       | `None`     | Include results published before date (YYYY-MM-DD) |
| `use_autoprompt`       | `Optional[bool]`      | `None`     | Enable autoprompt features                         |
| `type`                 | `Optional[str]`       | `None`     | Content type filter (e.g., article, blog, video)   |
| `category`             | `Optional[str]`       | `None`     | Category filter (e.g., news, research paper)       |
| `include_domains`      | `Optional[List[str]]` | `None`     | Restrict results to these domains                  |
| `exclude_domains`      | `Optional[List[str]]` | `None`     | Exclude results from these domains                 |
| `show_results`         | `bool`                | `False`    | Log search results for debugging                   |
| `model`                | `Optional[str]`       | `None`     | Search model to use ('exa' or 'exa-pro')           |

Available categories for filtering:

* company
* research paper
* news
* pdf
* github
* tweet
* personal site
* linkedin profile
* financial report

## Developer Resources

* View [Tools](https://github.com/agno-agi/agno/blob/main/libs/agno/agno/tools/exa.py)
* View [Cookbook](https://github.com/agno-agi/agno/tree/main/cookbook/tools/exa_tools.py)

**Examples:**

Example 1 (unknown):
```unknown

```

Example 2 (unknown):
```unknown
## Example

The following agent will search Exa for AAPL news and print the response.
```

---

## the `step_input`.

**URL:** llms-txt#the-`step_input`.

class CustomContentPlanning:
    def __call__(self, step_input: StepInput) -> StepOutput:
        """
        Custom function that does intelligent content planning with context awareness
        """
        message = step_input.input
        previous_step_content = step_input.previous_step_content

# Create intelligent planning prompt
        planning_prompt = f"""
            STRATEGIC CONTENT PLANNING REQUEST:
            Core Topic: {message}
            Research Results: {previous_step_content[:500] if previous_step_content else "No research results"}
            Planning Requirements:
            1. Create a comprehensive content strategy based on the research
            2. Leverage the research findings effectively
            3. Identify content formats and channels
            4. Provide timeline and priority recommendations
            5. Include engagement and distribution strategies
            Please create a detailed, actionable content plan.
        """

try:
            response = content_planner.run(planning_prompt)

enhanced_content = f"""
                ## Strategic Content Plan
                **Planning Topic:** {message}
                **Research Integration:** {"✓ Research-based" if previous_step_content else "✗ No research foundation"}
                **Content Strategy:**
                {response.content}
                **Custom Planning Enhancements:**
                - Research Integration: {"High" if previous_step_content else "Baseline"}
                - Strategic Alignment: Optimized for multi-channel distribution
                - Execution Ready: Detailed action items included
            """.strip()

return StepOutput(content=enhanced_content)

except Exception as e:
            return StepOutput(
                content=f"Custom content planning failed: {str(e)}",
                success=False,
            )

---

## Check reasoning_content from the response

**URL:** llms-txt#check-reasoning_content-from-the-response

print("\n--- reasoning_content from response ---")
if hasattr(response, "reasoning_content") and response.reasoning_content:
    print("✅ reasoning_content FOUND in non-streaming response")
    print(f"   Length: {len(response.reasoning_content)} characters")
    print("\n=== reasoning_content preview (non-streaming) ===")
    preview = response.reasoning_content[:1000]
    if len(response.reasoning_content) > 1000:
        preview += "..."
    print(preview)
else:
    print("❌ reasoning_content NOT FOUND in non-streaming response")

print("\n\n=== Example 2: Using KnowledgeTools in streaming mode ===\n")

---

## WhatsApp Tools

**URL:** llms-txt#whatsapp-tools

**Contents:**
- Code

Source: https://docs.agno.com/examples/concepts/tools/social/whatsapp

```python cookbook/tools/whatsapp_tools.py theme={null}
from agno.agent import Agent
from agno.models.google import Gemini
from agno.tools.whatsapp import WhatsAppTools

agent = Agent(
    name="whatsapp",
    model=Gemini(id="gemini-2.0-flash"),
    tools=[WhatsAppTools()],
)

---

## GitHub Tools

**URL:** llms-txt#github-tools

**Contents:**
- Code
- Usage

Source: https://docs.agno.com/examples/concepts/tools/others/github

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Set your GitHub token">
    
  </Step>

<Step title="Install libraries">
    
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Set your GitHub token">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## How team execution works

**URL:** llms-txt#how-team-execution-works

**Contents:**
- Members respond directly

Source: https://docs.agno.com/concepts/teams/delegation

How tasks are delegated to team members.

A `Team` internally has a team-leader "agent" that delegates tasks to the members.  When you call `run` or `arun` on a team, the team leader agent uses a model to determine which member to delegate the task to.

1. The team receives user input
2. A Team Leader analyzes the input and decides how to break it down into subtasks
3. The Team Leader delegates specific tasks to appropriate team members
4. Team members complete their assigned tasks and return their results
5. The Team Leader then either delegates to more team members, or synthesizes all outputs into a final, cohesive response to return to the user

<Note>
  Delegating to members is done by the team leader doing deciding to use a **tool**, namely the `delegate_task_to_members` tool.

This also means that when running the team asynchronously, i.e. when using `arun`, and the team leader decides to delegate to multiple members at once, these members will run concurrently.
</Note>

There are various ways to manipulate the execution of a team.
Some common questions are:

* **How do I return the response of members directly?** -> See the [Members respond directly](#members-respond-directly) section.
* **How do I send my user input directly to the members?** -> See the [Determine input for members](#determine-input-for-members) section.
* **How do I make sure the team leader delegates the task to all members?** -> See the [Delegate task to all members](#delegate-task-to-all-members) section.

Below are some examples of how to change the behaviour of how tasks are delegated to the members.

## Members respond directly

During normal team execution, the team leader will process the responses from the members and return a single response to the user.  This is the default behaviour.

If instead you want to return the response of members directly, you can set `respond_directly` to `True`.

<Tip>
  It can make sense to use this feature in combination with `determine_input_for_members=False`. This would effectively turn the team into a router that simply routes requests to the appropriate member, without the team leader processing the responses.
</Tip>

Below is an example of how to create a team that routes requests to the appropriate member based on the language of the user's input.

```python  theme={null}
from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.team.team import Team

english_agent = Agent(
    name="English Agent",
    role="You can only answer in English",
    model=OpenAIChat(id="gpt-5-mini"),
    instructions=[
        "You must only respond in English",
    ],
)

japanese_agent = Agent(
    name="Japanese Agent",
    role="You can only answer in Japanese",
    model=OpenAIChat(id="gpt-5-mini"),
    instructions=[
        "You must only respond in Japanese",
    ],
)
multi_language_team = Team(
    name="Multi Language Team",
    model=OpenAIChat("gpt-4.5-preview"),
    respond_directly=True,
    members=[
        english_agent,
        japanese_agent,
    ],
    markdown=True,
    instructions=[
        "You are a language router that directs questions to the appropriate language agent.",
        "If the user asks in a language whose agent is not a team member, respond in English with:",
        "'I can only answer in the following languages: English and Japanese. Please ask your question in one of these languages.'",
        "Always check the language of the user's input before routing to an agent.",
        "For unsupported languages like Italian, respond in English with the above message.",
    ],
    show_members_responses=True,
)

---

## AWS Bedrock Claude

**URL:** llms-txt#aws-bedrock-claude

**Contents:**
- Parameters

Source: https://docs.agno.com/reference/models/bedrock_claude

The AWS Bedrock Claude model provides access to Anthropic's Claude models hosted on AWS Bedrock.

| Parameter               | Type                       | Default                                       | Description                                                          |
| ----------------------- | -------------------------- | --------------------------------------------- | -------------------------------------------------------------------- |
| `id`                    | `str`                      | `"anthropic.claude-3-5-sonnet-20241022-v2:0"` | The id of the AWS Bedrock Claude model to use                        |
| `name`                  | `str`                      | `"BedrockClaude"`                             | The name of the model                                                |
| `provider`              | `str`                      | `"AWS"`                                       | The provider of the model                                            |
| `max_tokens`            | `Optional[int]`            | `4096`                                        | Maximum number of tokens to generate in the chat completion          |
| `thinking`              | `Optional[Dict[str, Any]]` | `None`                                        | Configuration for the thinking (reasoning) process                   |
| `temperature`           | `Optional[float]`          | `None`                                        | Controls randomness in the model's output                            |
| `stop_sequences`        | `Optional[List[str]]`      | `None`                                        | A list of strings that the model should stop generating text at      |
| `top_p`                 | `Optional[float]`          | `None`                                        | Controls diversity via nucleus sampling                              |
| `top_k`                 | `Optional[int]`            | `None`                                        | Controls diversity via top-k sampling                                |
| `cache_system_prompt`   | `Optional[bool]`           | `False`                                       | Whether to cache the system prompt for improved performance          |
| `extended_cache_time`   | `Optional[bool]`           | `False`                                       | Whether to use extended cache time (1 hour instead of default)       |
| `request_params`        | `Optional[Dict[str, Any]]` | `None`                                        | Additional parameters to include in the request                      |
| `aws_region`            | `Optional[str]`            | `None`                                        | The AWS region to use (defaults to AWS\_REGION env var)              |
| `aws_access_key_id`     | `Optional[str]`            | `None`                                        | AWS access key ID (defaults to AWS\_ACCESS\_KEY\_ID env var)         |
| `aws_secret_access_key` | `Optional[str]`            | `None`                                        | AWS secret access key (defaults to AWS\_SECRET\_ACCESS\_KEY env var) |
| `aws_session_token`     | `Optional[str]`            | `None`                                        | AWS session token (defaults to AWS\_SESSION\_TOKEN env var)          |
| `aws_profile`           | `Optional[str]`            | `None`                                        | AWS profile to use (defaults to AWS\_PROFILE env var)                |
| `client_params`         | `Optional[Dict[str, Any]]` | `None`                                        | Additional parameters for client configuration                       |

---

## Install & Setup

**URL:** llms-txt#install-&-setup

**Contents:**
- Install Agno
- Upgrade Agno

Source: https://docs.agno.com/templates/infra-management/install

* Installing `agno` using `pip` in a python virtual environment.
* Creating an `ai` directory for your ai infra

<Steps>
  <Step title="Create a virtual environment">
    Open the `Terminal` and create an `ai` directory with a python virtual environment.

</CodeGroup>
  </Step>

<Step title="Install Agno">
    Install `agno` using pip

</CodeGroup>
  </Step>

<Step title="Install Docker">
    Install [docker desktop](https://docs.docker.com/desktop/install/mac-install/) to run apps locally
  </Step>
</Steps>

<Note>
  If you encounter errors, try updating pip using `python -m pip install --upgrade pip`
</Note>

To upgrade `agno`, run this in your virtual environment

**Examples:**

Example 1 (unknown):
```unknown

```

Example 2 (unknown):
```unknown
</CodeGroup>
  </Step>

  <Step title="Install Agno">
    Install `agno` using pip

    <CodeGroup>
```

Example 3 (unknown):
```unknown

```

Example 4 (unknown):
```unknown
</CodeGroup>
  </Step>

  <Step title="Install Docker">
    Install [docker desktop](https://docs.docker.com/desktop/install/mac-install/) to run apps locally
  </Step>
</Steps>

<br />

<Note>
  If you encounter errors, try updating pip using `python -m pip install --upgrade pip`
</Note>

***

## Upgrade Agno

To upgrade `agno`, run this in your virtual environment
```

---

## Fireworks

**URL:** llms-txt#fireworks

**Contents:**
- Parameters

Source: https://docs.agno.com/reference/models/fireworks

The Fireworks model provides access to Fireworks' language models.

| Parameter  | Type            | Default                                                | Description                                                         |
| ---------- | --------------- | ------------------------------------------------------ | ------------------------------------------------------------------- |
| `id`       | `str`           | `"accounts/fireworks/models/llama-v3p1-405b-instruct"` | The id of the Fireworks model to use                                |
| `name`     | `str`           | `"Fireworks"`                                          | The name of the model                                               |
| `provider` | `str`           | `"Fireworks"`                                          | The provider of the model                                           |
| `api_key`  | `Optional[str]` | `None`                                                 | The API key for Fireworks (defaults to FIREWORKS\_API\_KEY env var) |
| `base_url` | `str`           | `"https://api.fireworks.ai/inference/v1"`              | The base URL for the Fireworks API                                  |

Fireworks extends the OpenAI-compatible interface and supports most parameters from OpenAI.

---

## Qdrant Async

**URL:** llms-txt#qdrant-async

**Contents:**
- Code
- Usage

Source: https://docs.agno.com/examples/concepts/vectordb/qdrant-db/async-qdrant-db

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Install libraries">
    
  </Step>

<Step title="Run Qdrant">
    
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Install libraries">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Run Qdrant">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## "Is it better to rent or buy a home given current interest rates, inflation, and market trends? "

**URL:** llms-txt#"is-it-better-to-rent-or-buy-a-home-given-current-interest-rates,-inflation,-and-market-trends?-"

---

## Import the workflows

**URL:** llms-txt#import-the-workflows

from agno.db.sqlite import SqliteDb
from agno.models.openai.chat import OpenAIChat
from agno.os import AgentOS
from agno.team import Team
from agno.tools.googlesearch import GoogleSearchTools
from agno.tools.hackernews import HackerNewsTools
from agno.workflow.step import Step, StepInput, StepOutput, WorkflowRunOutputEvent
from agno.workflow.workflow import Workflow

---

## Setting up a custom logger

**URL:** llms-txt#setting-up-a-custom-logger

custom_logger = logging.getLogger("custom_logger")
handler = logging.StreamHandler()
formatter = logging.Formatter("[CUSTOM_LOGGER] %(levelname)s: %(message)s")
handler.setFormatter(formatter)
custom_logger.addHandler(handler)
custom_logger.setLevel(logging.INFO)  # Set level to INFO to show info messages
custom_logger.propagate = False

---

## This will raise an error - unexpected field

**URL:** llms-txt#this-will-raise-an-error---unexpected-field

**Contents:**
- Usage

try:
    hackernews_agent.print_response(
        input={
            "topic": "AI",
            "focus_areas": ["Machine Learning"],
            "target_audience": "Developers",
            "sources_required": 5,
            "unexpected_field": "value",  # This field is not in the TypedDict
        }
    )
except ValueError as e:
    print("\n=== Expected Error for Unexpected Field ===")
    print(f"Error: {e}")
bash  theme={null}
    pip install -U agno openai
    bash Mac/Linux theme={null}
        export OPENAI_API_KEY="your_openai_api_key_here"
      bash Windows theme={null}
        $Env:OPENAI_API_KEY="your_openai_api_key_here"
      bash  theme={null}
    touch input_schema_on_agent_as_typed_dict.py
    bash Mac theme={null}
      python input_schema_on_agent_as_typed_dict.py
      bash Windows   theme={null}
      python input_schema_on_agent_as_typed_dict.py
      ```
    </CodeGroup>
  </Step>

<Step title="Find All Cookbooks">
    Explore all the available cookbooks in the Agno repository. Click the link below to view the code on GitHub:

<Link href="https://github.com/agno-agi/agno/tree/main/cookbook/agents/input_and_output" target="_blank">
      Agno Cookbooks on GitHub
    </Link>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Install libraries">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Export your OpenAI API key">
    <CodeGroup>
```

Example 3 (unknown):
```unknown

```

Example 4 (unknown):
```unknown
</CodeGroup>
  </Step>

  <Step title="Create a Python file">
    Create a Python file and add the above code.
```

---

## Named steps for better tracking

**URL:** llms-txt#named-steps-for-better-tracking

**Contents:**
- Developer Resources
- Reference

workflow = Workflow(
    name="Content Creation Pipeline",
    steps=[
        Step(name="Research Phase", team=researcher),
        Step(name="Analysis Phase", executor=custom_function),
        Step(name="Writing Phase", agent=writer),
    ]
)

workflow.print_response(
    "AI trends in 2024",
    markdown=True,
)
```

## Developer Resources

* [Sequence of Steps](/examples/concepts/workflows/01-basic-workflows/sequence_of_steps)
* [Step with a Custom Function](/examples/concepts/workflows/01-basic-workflows/step_with_function)

For complete API documentation, see [Step Reference](/reference/workflows/step).

---

## Define content preparation step

**URL:** llms-txt#define-content-preparation-step

content_prep_step = Step(
    name="ContentPreparation",
    agent=content_agent,
    description="Prepare and organize all research for writing",
)

writing_step = Step(
    name="Writing",
    agent=writer,
    description="Write an article based on the research",
)

editing_step = Step(
    name="Editing",
    agent=editor,
    description="Edit and polish the article",
)

---

## Use custom reader

**URL:** llms-txt#use-custom-reader

**Contents:**
- Best Practices
  - Choose the Right Reader
  - Configure Chunking Appropriately
  - Optimize for Performance
  - Handle Errors Gracefully
- Next Steps

knowledge_base.add_content(
    path="data/documents",
    reader=reader  # Override default reader
)
```

### Choose the Right Reader

* Use specialized readers for better extraction quality
* Consider format-specific features (PDF encryption, CSV delimiters, etc.)

### Configure Chunking Appropriately

* Smaller chunks for precise retrieval
* Larger chunks for maintaining context
* Use semantic chunking for structured documents

### Optimize for Performance

* Use async readers for I/O-heavy operations
* Batch process multiple files when possible
* Cache readers through ReaderFactory when processing many files

### Handle Errors Gracefully

* Readers return empty lists for failed processing
* Check reader logs for debugging information
* Provide fallback readers for unknown formats

<CardGroup cols={2}>
  <Card title="Chunking Strategies" icon="scissors" href="/concepts/knowledge/chunking/overview">
    Learn how to optimize content chunking for better search results
  </Card>

<Card title="Content Types" icon="file-lines" href="/concepts/knowledge/content_types">
    Understand different ways to add information to your knowledge base
  </Card>

<Card title="Vector Databases" icon="database" href="/concepts/vectordb/overview">
    Choose the right storage solution for your processed content
  </Card>

<Card title="Examples" icon="code" href="/examples/introduction">
    See readers in action with practical examples
  </Card>
</CardGroup>

---

## Workflow

**URL:** llms-txt#workflow

**Contents:**
- Parameters
- Functions
  - `run`
  - `arun`
  - `print_response`
  - `aprint_response`
  - `cancel_run`
  - `get_run`
  - `get_run_output`
  - `get_last_run_output`

Source: https://docs.agno.com/reference/workflows/workflow

| Parameter                       | Type                                                              | Default | Description                                                                                              |
| ------------------------------- | ----------------------------------------------------------------- | ------- | -------------------------------------------------------------------------------------------------------- |
| `name`                          | `Optional[str]`                                                   | `None`  | Workflow name                                                                                            |
| `id`                            | `Optional[str]`                                                   | `None`  | Workflow ID (autogenerated if not set)                                                                   |
| `description`                   | `Optional[str]`                                                   | `None`  | Workflow description                                                                                     |
| `steps`                         | `Optional[WorkflowSteps]`                                         | `None`  | Workflow steps - can be a callable function, Steps object, or list of steps                              |
| `db`                            | `Optional[BaseDb]`                                                | `None`  | Database to use for this workflow                                                                        |
| `session_id`                    | `Optional[str]`                                                   | `None`  | Default session\_id to use for this workflow (autogenerated if not set)                                  |
| `user_id`                       | `Optional[str]`                                                   | `None`  | Default user\_id to use for this workflow                                                                |
| `session_state`                 | `Optional[Dict[str, Any]]`                                        | `None`  | Default session state (stored in the database to persist across runs)                                    |
| `debug_mode`                    | `Optional[bool]`                                                  | `False` | If True, the workflow runs in debug mode                                                                 |
| `stream`                        | `Optional[bool]`                                                  | `None`  | Stream the response from the Workflow                                                                    |
| `stream_events`                 | `bool`                                                            | `False` | Stream the intermediate steps from the Workflow                                                          |
| `stream_executor_events`        | `bool`                                                            | `True`  | Stream the events emitted by the Step executor (the agent/team events) together with the Workflow events |
| `store_events`                  | `bool`                                                            | `False` | Persist the events on the run response                                                                   |
| `events_to_skip`                | `Optional[List[Union[WorkflowRunEvent, RunEvent, TeamRunEvent]]]` | `None`  | Events to skip when persisting the events on the run response                                            |
| `store_executor_outputs`        | `bool`                                                            | `True`  | Control whether to store executor responses (agent/team responses) in flattened runs                     |
| `websocket_handler`             | `Optional[WebSocketHandler]`                                      | `None`  | WebSocket handler for real-time communication                                                            |
| `input_schema`                  | `Optional[Type[BaseModel]]`                                       | `None`  | Input schema to validate the input to the workflow                                                       |
| `metadata`                      | `Optional[Dict[str, Any]]`                                        | `None`  | Metadata stored with this workflow                                                                       |
| `add_workflow_history_to_steps` | `bool`                                                            | `False` | If True, add the workflow history to the steps                                                           |
| `num_history_runs`              | `int`                                                             | `None`  | Number of runs to include in the workflow history, if not provided, all history runs are included        |
| `cache_session`                 | `bool`                                                            | `False` | If True, cache the current workflow session in memory for faster access                                  |
| `telemetry`                     | `bool`                                                            | `True`  | Log minimal telemetry for analytics                                                                      |

Execute the workflow synchronously with optional streaming.

* `input` (Optional\[Union\[str, Dict\[str, Any], List\[Any], BaseModel]]): The input to send to the workflow
* `additional_data` (Optional\[Dict\[str, Any]]): Additional data to include with the input
* `user_id` (Optional\[str]): User ID to use
* `session_id` (Optional\[str]): Session ID to use
* `session_state` (Optional\[Dict\[str, Any]]): Session state to use
* `audio` (Optional\[List\[Audio]]): Audio files to include
* `images` (Optional\[List\[Image]]): Image files to include
* `videos` (Optional\[List\[Video]]): Video files to include
* `files` (Optional\[List\[File]]): Files to include
* `stream` (bool): Whether to stream the response
* `stream_events` (Optional\[bool]): Whether to stream intermediate steps

* `Union[WorkflowRunOutput, Iterator[WorkflowRunOutputEvent]]`: Either a WorkflowRunOutput or an iterator of WorkflowRunOutputEvents, depending on the `stream` parameter

Execute the workflow asynchronously with optional streaming.

* `input` (Optional\[Union\[str, Dict\[str, Any], List\[Any], BaseModel, List\[Message]]]): The input to send to the workflow
* `additional_data` (Optional\[Dict\[str, Any]]): Additional data to include with the input
* `user_id` (Optional\[str]): User ID to use
* `session_id` (Optional\[str]): Session ID to use
* `session_state` (Optional\[Dict\[str, Any]]): Session state to use
* `audio` (Optional\[List\[Audio]]): Audio files to include
* `images` (Optional\[List\[Image]]): Image files to include
* `videos` (Optional\[List\[Video]]): Video files to include
* `files` (Optional\[List\[File]]): Files to include
* `stream` (bool): Whether to stream the response
* `stream_events` (Optional\[bool]): Whether to stream intermediate steps
* `background` (Optional\[bool]): Whether to run in background
* `websocket` (Optional\[WebSocket]): WebSocket for real-time communication

* `Union[WorkflowRunOutput, AsyncIterator[WorkflowRunOutputEvent]]`: Either a WorkflowRunOutput or an iterator of WorkflowRunOutputEvents, depending on the `stream` parameter

Print workflow execution with rich formatting and optional streaming.

* `input` (Union\[str, Dict\[str, Any], List\[Any], BaseModel, List\[Message]]): The input to send to the workflow
* `additional_data` (Optional\[Dict\[str, Any]]): Additional data to include with the input
* `user_id` (Optional\[str]): User ID to use
* `session_id` (Optional\[str]): Session ID to use
* `audio` (Optional\[List\[Audio]]): Audio files to include
* `images` (Optional\[List\[Image]]): Image files to include
* `videos` (Optional\[List\[Video]]): Video files to include
* `files` (Optional\[List\[File]]): Files to include
* `stream` (Optional\[bool]): Whether to stream the response content
* `stream_events` (Optional\[bool]): Whether to stream intermediate steps
* `markdown` (bool): Whether to render content as markdown
* `show_time` (bool): Whether to show execution time
* `show_step_details` (bool): Whether to show individual step outputs
* `console` (Optional\[Any]): Rich console instance (optional)

### `aprint_response`

Print workflow execution with rich formatting and optional streaming asynchronously.

* `input` (Union\[str, Dict\[str, Any], List\[Any], BaseModel, List\[Message]]): The input to send to the workflow
* `additional_data` (Optional\[Dict\[str, Any]]): Additional data to include with the input
* `user_id` (Optional\[str]): User ID to use
* `session_id` (Optional\[str]): Session ID to use
* `audio` (Optional\[List\[Audio]]): Audio files to include
* `images` (Optional\[List\[Image]]): Image files to include
* `videos` (Optional\[List\[Video]]): Video files to include
* `files` (Optional\[List\[File]]): Files to include
* `stream` (Optional\[bool]): Whether to stream the response content
* `stream_events` (Optional\[bool]): Whether to stream intermediate steps
* `markdown` (bool): Whether to render content as markdown
* `show_time` (bool): Whether to show execution time
* `show_step_details` (bool): Whether to show individual step outputs
* `console` (Optional\[Any]): Rich console instance (optional)

Cancel a running workflow execution.

* `run_id` (str): The run\_id to cancel

* `bool`: True if the run was found and marked for cancellation, False otherwise

Get the status and details of a background workflow run.

* `run_id` (str): The run ID to get

* `Optional[WorkflowRunOutput]`: The workflow run output if found

Get a WorkflowRunOutput from the database.

* `run_id` (str): The run ID
* `session_id` (Optional\[str]): Session ID to use

* `Optional[WorkflowRunOutput]`: The run output

### `get_last_run_output`

Get the last run response from the database for the given session ID.

* `session_id` (Optional\[str]): Session ID to use

* `Optional[WorkflowRunOutput]`: The last run output

Get the session for the given session ID.

* `session_id` (Optional\[str]): Session ID to use

* `Optional[WorkflowSession]`: The workflow session

### `get_session_state`

Get the session state for the given session ID.

* `session_id` (Optional\[str]): Session ID to use

* `Dict[str, Any]`: The session state

### `get_session_name`

Get the session name for the given session ID.

* `session_id` (Optional\[str]): Session ID to use

* `str`: The session name

### `set_session_name`

Set the session name and save to storage.

* `session_id` (Optional\[str]): Session ID to use
* `autogenerate` (bool): Whether to autogenerate the name
* `session_name` (Optional\[str]): The name to set

* `WorkflowSession`: The updated session

### `get_session_metrics`

Get the session metrics for the given session ID.

* `session_id` (Optional\[str]): Session ID to use

* `Optional[Metrics]`: The session metrics

* `session_id` (str): Session ID to delete

Save the WorkflowSession to storage.

* `session` (WorkflowSession): The session to save

Convert workflow to dictionary representation.

* `Dict[str, Any]`: Dictionary representation of the workflow

---

## Hybrid Search

**URL:** llms-txt#hybrid-search

**Contents:**
- Code

Source: https://docs.agno.com/examples/concepts/knowledge/search_type/hybrid-search

```python hybrid_search.py theme={null}
from agno.knowledge.knowledge import Knowledge
from agno.vectordb.pgvector import PgVector, SearchType

db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"

---

## Example usage with different types of recipe queries

**URL:** llms-txt#example-usage-with-different-types-of-recipe-queries

recipe_agent.print_response(
    "I have chicken breast, broccoli, garlic, and rice. Need a healthy dinner recipe that takes less than 45 minutes.",
    stream=True,
)

---

## Verify Webhook

**URL:** llms-txt#verify-webhook

Source: https://docs.agno.com/reference-api/schema/whatsapp/verify-webhook

get /whatsapp/webhook
Handle WhatsApp webhook verification

---

## Azure OpenAI GPT 4.1

**URL:** llms-txt#azure-openai-gpt-4.1

**Contents:**
- Code
- Usage

Source: https://docs.agno.com/examples/concepts/reasoning/models/azure-openai/reasoning-model-gpt4-1

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Set your API key">
    
  </Step>

<Step title="Install libraries">
    
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Set your API key">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Params

**URL:** llms-txt#params

| Parameter               | Type                       | Default                          | Description                                                                                                                   |
| ----------------------- | -------------------------- | -------------------------------- | ----------------------------------------------------------------------------------------------------------------------------- |
| `id`                    | `str`                      | `"cohere.embed-multilingual-v3"` | The model ID to use. You need to enable this model in your AWS Bedrock model catalog.                                         |
| `dimensions`            | `int`                      | `1024`                           | The dimensionality of the embeddings generated by the model(1024 for Cohere models).                                          |
| `input_type`            | `str`                      | `"search_query"`                 | Prepends special tokens to differentiate types. Options: 'search\_document', 'search\_query', 'classification', 'clustering'. |
| `truncate`              | `Optional[str]`            | `None`                           | How to handle inputs longer than the maximum token length. Options: 'NONE', 'START', 'END'.                                   |
| `embedding_types`       | `Optional[List[str]]`      | `None`                           | Types of embeddings to return . Options: 'float', 'int8', 'uint8', 'binary', 'ubinary'.                                       |
| `aws_region`            | `Optional[str]`            | `None`                           | The AWS region to use. If not provided, falls back to AWS\_REGION env variable.                                               |
| `aws_access_key_id`     | `Optional[str]`            | `None`                           | The AWS access key ID. If not provided, falls back to AWS\_ACCESS\_KEY\_ID env variable.                                      |
| `aws_secret_access_key` | `Optional[str]`            | `None`                           | The AWS secret access key. If not provided, falls back to AWS\_SECRET\_ACCESS\_KEY env variable.                              |
| `session`               | `Optional[Session]`        | `None`                           | A boto3 Session object to use for authentication.                                                                             |
| `request_params`        | `Optional[Dict[str, Any]]` | `None`                           | Additional parameters to pass to the API requests.                                                                            |
| `client_params`         | `Optional[Dict[str, Any]]` | `None`                           | Additional parameters to pass to the boto3 client.                                                                            |
| `client`                | `Optional[AwsClient]`      | `None`                           | An instance of the AWS Bedrock client to use for making API requests.                                                         |

---

## -*- Print a response to the cli

**URL:** llms-txt#-*--print-a-response-to-the-cli

**Contents:**
- Usage

asyncio.run(agent.aprint_response("Share a breakfast recipe.", markdown=True))

bash  theme={null}
    ollama pull llama3.1:8b
    bash  theme={null}
    pip install -U ollama agno
    bash Mac theme={null}
      python cookbook/models/ollama/async_basic.py
      bash Windows theme={null}
      python cookbook/models/ollama/async_basic.py
      ```
    </CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Install Ollama">
    Follow the [Ollama installation guide](https://github.com/ollama/ollama?tab=readme-ov-file#macos) and run:
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Example: Send a template message

**URL:** llms-txt#example:-send-a-template-message

---

## Create Team Run

**URL:** llms-txt#create-team-run

Source: https://docs.agno.com/reference-api/schema/teams/create-team-run

post /teams/{team_id}/runs
Execute a team collaboration with multiple agents working together on a task.

**Features:**
- Text message input with optional session management
- Multi-media support: images (PNG, JPEG, WebP), audio (WAV, MP3), video (MP4, WebM, etc.)
- Document processing: PDF, CSV, DOCX, TXT, JSON
- Real-time streaming responses with Server-Sent Events (SSE)
- User and session context preservation

**Streaming Response:**
When `stream=true`, returns SSE events with `event` and `data` fields.

---

## stream=True

**URL:** llms-txt#stream=true

---

## Handle confirmation

**URL:** llms-txt#handle-confirmation

**Contents:**
- User Input

if run_response.is_paused:
    for tool in run_response.tools_requiring_confirmation:
        # Get user confirmation
        print(f"Tool {tool.tool_name}({tool.tool_args}) requires confirmation")
        confirmed = input(f"Confirm? (y/n): ").lower() == "y"
        tool.confirmed = confirmed

# Continue execution
    response = agent.continue_run(run_response=run_response)
python  theme={null}
from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools import tool
from agno.tools.yfinance import YFinanceTools
from agno.utils import pprint
from rich.console import Console
from rich.prompt import Prompt

agent = Agent(
    model=OpenAIChat(id="gpt-4o-mini"),
    tools=[YFinanceTools(requires_confirmation_tools=["get_current_stock_price"])],
    markdown=True,
)

run_response = agent.run("What is the current stock price of Apple?")
if run_response.is_paused:  # Or agent.run_response.is_paused
    for tool in run_response.tools_requiring_confirmation:  # type: ignore
        # Ask for confirmation
        console.print(
            f"Tool name [bold blue]{tool.tool_name}({tool.tool_args})[/] requires confirmation."
        )
        message = (
            Prompt.ask("Do you want to continue?", choices=["y", "n"], default="y")
            .strip()
            .lower()
        )

if message == "n":
            tool.confirmed = False
        else:
            # We update the tools in place
            tool.confirmed = True

run_response = agent.continue_run(run_response=run_response)
    pprint.pprint_run_response(run_response)
python  theme={null}
from typing import List

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools import tool
from agno.tools.function import UserInputField
from agno.utils import pprint

**Examples:**

Example 1 (unknown):
```unknown
You can also specify which tools in a toolkit require confirmation.
```

Example 2 (unknown):
```unknown
## User Input

User input flows allow you to gather specific information from users during execution. This is useful for:

* Collecting required parameters
* Getting user preferences
* Gathering missing information

In the example below, we require all the input for the `send_email` tool from the user.
```

---

## Pass a dict that matches the input schema

**URL:** llms-txt#pass-a-dict-that-matches-the-input-schema

hackernews_agent.print_response(
    input={
        "topic": "AI",
        "focus_areas": ["AI", "Machine Learning"],
        "target_audience": "Developers",
        "sources_required": "5",
    }
)

---

## Hybrid Search- Combining Keyword and Vector Search

**URL:** llms-txt#hybrid-search--combining-keyword-and-vector-search

**Contents:**
- What exactly is Hybrid Search?
- Keyword Search vs Vector Search vs Hybrid Search
- Vector DBs in Agno that Support Hybrid Search
- Example: Hybrid Search using `pgvector`

Source: https://docs.agno.com/concepts/knowledge/advanced/hybrid-search

Understanding Hybrid Search and its benefits in combining keyword and vector search for better results.

With Hybrid search, you can get the precision of exact matching with the intelligence of semantic understanding. Combining both approaches will deliver more comprehensive and relevant results in many cases.

## What exactly is Hybrid Search?

**Hybrid search** is a retrieval technique that combines the strengths of both **vector search** (semantic search) and **keyword search** (lexical search) to find the most relevant results for a query.

* Vector search uses embeddings (dense vectors) to capture the semantic meaning of text, enabling the system to find results that are similar in meaning, even if the exact words don't match.
* Keyword search (BM25, TF-IDF, etc.) matches documents based on the presence and frequency of exact words or phrases in the query.

Hybrid search blends these approaches, typically by scoring and/or ranking results from both methods, to maximize both precision and recall.

## Keyword Search vs Vector Search vs Hybrid Search

| Feature       | Keyword Search                  | Vector Search                             | Hybrid Search                             |
| ------------- | ------------------------------- | ----------------------------------------- | ----------------------------------------- |
| Based On      | Lexical matching (BM25, TF-IDF) | Embedding similarity (cosine, dot)        | Both                                      |
| Strength      | Exact matches, relevance        | Contextual meaning                        | Balanced relevance + meaning              |
| Weakness      | No semantic understanding       | Misses exact keywords                     | Slightly heavier in compute               |
| Example Match | "chicken soup" = *chicken soup* | "chicken soup" = *hot broth with chicken* | Both literal and related concepts         |
| Best Use Case | Legal docs, structured data     | Chatbots, Q\&A, semantic search           | Multimodal, real-world messy user queries |

<Note>
  Why Hybrid Search might be better for your application-

* **Improved Recall**: Captures more relevant results missed by pure keyword or vector search.
  * **Balanced Precision**: Exact matches get priority while also including semantically relevant results.
  * **Robust to Ambiguity**: Handles spelling variations, synonyms, and fuzzy user intent.
  * **Best of Both Worlds**: Keywords matter when they should, and meaning matters when needed.

**Perfect for **real-world apps** like recipe search, customer support, legal discovery, etc.**
</Note>

## Vector DBs in Agno that Support Hybrid Search

The following vector databases support hybrid search natively or via configurations:

| Database   | Hybrid Search Support       |
| ---------- | --------------------------- |
| `pgvector` | ✅ Yes                       |
| `milvus`   | ✅ Yes                       |
| `lancedb`  | ✅ Yes                       |
| `qdrantdb` | ✅ Yes                       |
| `weaviate` | ✅ Yes                       |
| `mongodb`  | ✅ Yes (Atlas Vector Search) |

## Example: Hybrid Search using `pgvector`

```python  theme={null}
from agno.knowledge.knowledge import Knowledge
from agno.vectordb.pgvector import PgVector, SearchType

---

## dependencies={"user_profile": get_user_profile},

**URL:** llms-txt#dependencies={"user_profile":-get_user_profile},

---

## 1. CUSTOMER SUPPORT WORKFLOW

**URL:** llms-txt#1.-customer-support-workflow

---

## OpenAI GPT-4.1

**URL:** llms-txt#openai-gpt-4.1

**Contents:**
- Code
- Usage

Source: https://docs.agno.com/examples/concepts/reasoning/models/openai/reasoning-model-gpt-4-1

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Set your API key">
    
  </Step>

<Step title="Install libraries">
    
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Set your API key">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## SurrealDB connection parameters

**URL:** llms-txt#surrealdb-connection-parameters

SURREALDB_URL = "ws://localhost:8000"
SURREALDB_USER = "root"
SURREALDB_PASSWORD = "root"
SURREALDB_NAMESPACE = "test"
SURREALDB_DATABASE = "test"

---

## Google Search

**URL:** llms-txt#google-search

**Contents:**
- Prerequisites
- Example
- Toolkit Params
- Toolkit Functions
- Developer Resources

Source: https://docs.agno.com/concepts/tools/toolkits/search/googlesearch

**GoogleSearch** enables an Agent to perform web crawling and scraping tasks.

The following examples requires the `googlesearch` and `pycountry` libraries.

The following agent will search Google for the latest news about "Mistral AI":

| Parameter              | Type   | Default | Description                                        |
| ---------------------- | ------ | ------- | -------------------------------------------------- |
| `fixed_max_results`    | `int`  | `None`  | Optional fixed maximum number of results to return |
| `fixed_language`       | `str`  | `None`  | Optional fixed language for the requests           |
| `headers`              | `Any`  | `None`  | Optional headers to include in the requests        |
| `proxy`                | `str`  | `None`  | Optional proxy to be used for the requests         |
| `timeout`              | `int`  | `10`    | Timeout for the requests in seconds                |
| `enable_google_search` | `bool` | `True`  | Enables Google search functionality                |
| `all`                  | `bool` | `False` | Enables all functionality when set to True         |

| Function        | Description                                                                                                                                                                                                                                                                            |
| --------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| `google_search` | Searches Google for a specified query. Parameters include `query` for the search term, `max_results` for the maximum number of results (default is 5), and `language` for the language of the search results (default is "en"). Returns the search results as a JSON formatted string. |

## Developer Resources

* View [Tools](https://github.com/agno-agi/agno/blob/main/libs/agno/agno/tools/googlesearch.py)
* View [Cookbook](https://github.com/agno-agi/agno/tree/main/cookbook/tools/googlesearch_tools.py)

**Examples:**

Example 1 (unknown):
```unknown
## Example

The following agent will search Google for the latest news about "Mistral AI":
```

---

## === CONDITION EVALUATORS ===

**URL:** llms-txt#===-condition-evaluators-===

def check_if_we_should_search_hn(step_input: StepInput) -> bool:
    """Check if we should search Hacker News"""
    topic = step_input.input or step_input.previous_step_content or ""
    tech_keywords = [
        "ai",
        "machine learning",
        "programming",
        "software",
        "tech",
        "startup",
        "coding",
    ]
    return any(keyword in topic.lower() for keyword in tech_keywords)

def check_if_comprehensive_research_needed(step_input: StepInput) -> bool:
    """Check if comprehensive multi-step research is needed"""
    topic = step_input.input or step_input.previous_step_content or ""
    comprehensive_keywords = [
        "comprehensive",
        "detailed",
        "thorough",
        "in-depth",
        "complete analysis",
        "full report",
        "extensive research",
    ]
    return any(keyword in topic.lower() for keyword in comprehensive_keywords)

if __name__ == "__main__":
    workflow = Workflow(
        name="Conditional Workflow with Multi-Step Condition",
        steps=[
            Parallel(
                Condition(
                    name="HackerNewsCondition",
                    description="Check if we should search Hacker News for tech topics",
                    evaluator=check_if_we_should_search_hn,
                    steps=[research_hackernews_step],  # Single step
                ),
                Condition(
                    name="ComprehensiveResearchCondition",
                    description="Check if comprehensive multi-step research is needed",
                    evaluator=check_if_comprehensive_research_needed,
                    steps=[  # Multiple steps
                        deep_exa_analysis_step,
                        trend_analysis_step,
                        fact_verification_step,
                    ],
                ),
                name="ConditionalResearch",
                description="Run conditional research steps in parallel",
            ),
            write_step,
        ],
    )

try:
        workflow.print_response(
            input="Comprehensive analysis of climate change research",
            stream=True,
            stream_events=True,
        )
    except Exception as e:
        print(f"❌ Error: {e}")
    print()
```

To see the async example, see the cookbook-

* [Condition with list of steps (async)](https://github.com/agno-agi/agno/blob/main/cookbook/workflows/_02_workflows_conditional_execution/async/condition_with_list_of_steps.py)

---

## --- Response Models ---

**URL:** llms-txt#----response-models----

class NewsArticle(BaseModel):
    title: str = Field(..., description="Title of the article.")
    url: str = Field(..., description="Link to the article.")
    summary: Optional[str] = Field(
        ..., description="Summary of the article if available."
    )

class SearchResults(BaseModel):
    articles: list[NewsArticle]

class ScrapedArticle(BaseModel):
    title: str = Field(..., description="Title of the article.")
    url: str = Field(..., description="Link to the article.")
    summary: Optional[str] = Field(
        ..., description="Summary of the article if available."
    )
    content: Optional[str] = Field(
        ...,
        description="Full article content in markdown format. None if content is unavailable.",
    )

---

## Generate Video Analysis

**URL:** llms-txt#generate-video-analysis

response = agent.run(query, videos=[Video(content=video_file)])

---

## File Generation

**URL:** llms-txt#file-generation

**Contents:**
- Prerequisites
- Example
- Toolkit Params
- Toolkit Functions
- Developer Resources

Source: https://docs.agno.com/concepts/tools/toolkits/file-generation/file-generation

**FileGenerationTools** enable an Agent or Team to generate files in multiple formats.

<Tip>
  Supported file types:

* JSON
  * CSV
  * PDF
  * TXT
</Tip>

1. **Install the libraries:**

2. **Set your credentials:**
   For OpenAI API:

The following agent will generate files in different formats based on user requests.

<Note>
  You can use the `output_directory` parameter to specify a custom output directory for the generated files.
  If not specified, the files will be available in the `RunOutput` object.
</Note>

| Parameter                | Type   | Default | Description                                      |
| ------------------------ | ------ | ------- | ------------------------------------------------ |
| `enable_json_generation` | `bool` | `True`  | Enables JSON file generation                     |
| `enable_csv_generation`  | `bool` | `True`  | Enables CSV file generation                      |
| `enable_pdf_generation`  | `bool` | `True`  | Enables PDF file generation (requires reportlab) |
| `enable_txt_generation`  | `bool` | `True`  | Enables text file generation                     |
| `output_directory`       | `str`  | `None`  | Custom output directory path                     |
| `all`                    | `bool` | `False` | Enables all file generation types when True      |

| Name                 | Description                                                  |
| -------------------- | ------------------------------------------------------------ |
| `generate_json_file` | Generates a JSON file from data (dict, list, or JSON string) |
| `generate_csv_file`  | Generates a CSV file from tabular data                       |
| `generate_pdf_file`  | Generates a PDF document from text content                   |
| `generate_text_file` | Generates a plain text file from string content              |

## Developer Resources

* View [Tools](https://github.com/agno-agi/agno/blob/main/libs/agno/agno/tools/file_generation.py)
* View [Cookbook](https://github.com/agno-agi/agno/tree/main/cookbook/tools/file_generation_tools.py)

**Examples:**

Example 1 (unknown):
```unknown
2. **Set your credentials:**
   For OpenAI API:
```

Example 2 (unknown):
```unknown
## Example

The following agent will generate files in different formats based on user requests.
```

---

## Reddit Tools

**URL:** llms-txt#reddit-tools

**Contents:**
- Code
- Usage

Source: https://docs.agno.com/examples/concepts/tools/social/reddit

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Set your credentials">
    
  </Step>

<Step title="Install libraries">
    
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Set your credentials">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Showing cached tokens metrics

**URL:** llms-txt#showing-cached-tokens-metrics

**Contents:**
- Usage

print(f"Cached tokens: {run_response.metrics.cache_read_tokens}")
bash  theme={null}
    pip install -U agno openai requests
    bash Mac/Linux theme={null}
        export OPENAI_API_KEY="your_openai_api_key_here"
      bash Windows theme={null}
        $Env:OPENAI_API_KEY="your_openai_api_key_here"
      bash  theme={null}
    touch agent_extra_metrics.py
    bash Mac theme={null}
      python agent_extra_metrics.py
      bash Windows   theme={null}
      python agent_extra_metrics.py
      ```
    </CodeGroup>
  </Step>

<Step title="Find All Cookbooks">
    Explore all the available cookbooks in the Agno repository. Click the link below to view the code on GitHub:

<Link href="https://github.com/agno-agi/agno/tree/main/cookbook/agents/other" target="_blank">
      Agno Cookbooks on GitHub
    </Link>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Install libraries">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Export your OpenAI API key">
    <CodeGroup>
```

Example 3 (unknown):
```unknown

```

Example 4 (unknown):
```unknown
</CodeGroup>
  </Step>

  <Step title="Create a Python file">
    Create a Python file and add the above code.
```

---

## - "Show me the most recent story from Hacker News"

**URL:** llms-txt#--"show-me-the-most-recent-story-from-hacker-news"

---

## Create research steps

**URL:** llms-txt#create-research-steps

research_hackernews_step = Step(
    name="Research HackerNews",
    agent=research_agent,
    description="Research trending topics on HackerNews",
)

research_web_step = Step(
    name="Research Web",
    agent=research_agent,
    description="Research additional information from web sources",
)

---

## Example: Generate Fibonacci numbers

**URL:** llms-txt#example:-generate-fibonacci-numbers

agent.print_response(
    "Write Python code to generate the first 10 Fibonacci numbers and calculate their sum and average"
)

---

## Sets the session state for the session with the id "user_2_session_1"

**URL:** llms-txt#sets-the-session-state-for-the-session-with-the-id-"user_2_session_1"

team.print_response(
    "What is my name?",
    session_id="user_2_session_1",
    user_id="user_2",
    session_state={"user_name": "Jane", "age": 25},
)

---

## URL-based reader selection

**URL:** llms-txt#url-based-reader-selection

**Contents:**
- Supported Readers
- Async Processing

reader = ReaderFactory.get_reader_for_url("https://youtube.com/watch?v=...")  # YouTubeReader
reader = ReaderFactory.get_reader_for_url("https://example.com/doc.pdf")     # PDFReader
python  theme={null}

**Examples:**

Example 1 (unknown):
```unknown
## Supported Readers

The following readers are currently supported:

| Reader Name           | Description                                           |
| --------------------- | ----------------------------------------------------- |
| ArxivReader           | Fetches and processes academic papers from arXiv      |
| CSVReader             | Parses CSV files and converts rows to documents       |
| FieldLabeledCSVReader | Converts CSV rows to field-labeled text documents     |
| FirecrawlReader       | Uses Firecrawl API to scrape and crawl web content    |
| JSONReader            | Processes JSON files and converts them into documents |
| MarkdownReader        | Reads and parses Markdown files                       |
| PDFReader             | Reads and extracts text from PDF files                |
| PPTXReader            | Reads and extracts text from PowerPoint (.pptx) files |
| TextReader            | Handles plain text files                              |
| WebsiteReader         | Crawls entire websites following links recursively    |
| WebSearchReader       | Searches and reads web search results                 |
| WikipediaReader       | Searches and reads Wikipedia articles                 |
| YouTubeReader         | Extracts transcripts and metadata from YouTube videos |

## Async Processing

All readers support asynchronous processing for better performance:
```

---

## SQL Tools

**URL:** llms-txt#sql-tools

**Contents:**
- Code
- Usage

Source: https://docs.agno.com/examples/concepts/tools/database/sql

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Set your API key">
    
  </Step>

<Step title="Install libraries">
    
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Set your API key">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Add documents with metadata for filtering

**URL:** llms-txt#add-documents-with-metadata-for-filtering

knowledge_base.add_contents(
    [
        {
            "path": downloaded_cv_paths[0],
            "metadata": {
                "user_id": "jordan_mitchell",
                "document_type": "cv",
                "year": 2025,
            },
        },
        {
            "path": downloaded_cv_paths[1],
            "metadata": {
                "user_id": "taylor_brooks",
                "document_type": "cv",
                "year": 2025,
            },
        },
        {
            "path": downloaded_cv_paths[2],
            "metadata": {
                "user_id": "morgan_lee",
                "document_type": "cv",
                "year": 2025,
            },
        },
        {
            "path": downloaded_cv_paths[3],
            "metadata": {
                "user_id": "casey_jordan",
                "document_type": "cv",
                "year": 2025,
            },
        },
        {
            "path": downloaded_cv_paths[4],
            "metadata": {
                "user_id": "alex_rivera",
                "document_type": "cv",
                "year": 2025,
            },
        },
    ],
    reader=PDFReader(chunk=True),
)

---

## Ask a question that would likely trigger tool use

**URL:** llms-txt#ask-a-question-that-would-likely-trigger-tool-use

**Contents:**
- Usage

openai_agent.print_response("What is happening in France?")
bash  theme={null}
    export LITELLM_API_KEY=xxx
    bash  theme={null}
    pip install -U litellm openai agno ddgs
    bash Mac theme={null}
      python cookbook/models/litellm/tool_use.py
      bash Windows theme={null}
      python cookbook/models/litellm/tool_use.py
      ```
    </CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Set your API key">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Print the response

**URL:** llms-txt#print-the-response

**Contents:**
- Notes

print(response.content)
```

* **Environment Variables**: Ensure your environment variable is correctly set for the AgentOps API key.
* **Initialization**: Call `agentops.init()` to initialize AgentOps.
* **AgentOps Docs**: [AgentOps Docs](https://docs.agentops.ai/v2/integrations/agno)

Following these steps will integrate Agno with AgentOps, providing comprehensive logging and visualization for your AI agents’ model calls.

---

## OpenAI o4-mini with reasoning summary

**URL:** llms-txt#openai-o4-mini-with-reasoning-summary

**Contents:**
- Code

Source: https://docs.agno.com/examples/concepts/reasoning/models/openai/reasoning-summary

```python cookbook/reasoning/models/openai/reasoning_summary.py theme={null}
"""Run `pip install openai ddgs` to install dependencies."""

from agno.agent import Agent
from agno.models.openai import OpenAIResponses
from agno.tools.duckduckgo import DuckDuckGoTools

---

## --- Helper Functions ---

**URL:** llms-txt#----helper-functions----

def get_cached_blog_post(session_state, topic: str) -> Optional[str]:
    """Get cached blog post from workflow session state"""
    logger.info("Checking if cached blog post exists")
    return session_state.get("blog_posts", {}).get(topic)

def cache_blog_post(session_state, topic: str, blog_post: str):
    """Cache blog post in workflow session state"""
    logger.info(f"Saving blog post for topic: {topic}")
    if "blog_posts" not in session_state:
        session_state["blog_posts"] = {}
    session_state["blog_posts"][topic] = blog_post

def get_cached_search_results(session_state, topic: str) -> Optional[SearchResults]:
    """Get cached search results from workflow session state"""
    logger.info("Checking if cached search results exist")
    search_results = session_state.get("search_results", {}).get(topic)
    if search_results and isinstance(search_results, dict):
        try:
            return SearchResults.model_validate(search_results)
        except Exception as e:
            logger.warning(f"Could not validate cached search results: {e}")
    return search_results if isinstance(search_results, SearchResults) else None

def cache_search_results(session_state, topic: str, search_results: SearchResults):
    """Cache search results in workflow session state"""
    logger.info(f"Saving search results for topic: {topic}")
    if "search_results" not in session_state:
        session_state["search_results"] = {}
    session_state["search_results"][topic] = search_results.model_dump()

def get_cached_scraped_articles(
    session_state, topic: str
) -> Optional[Dict[str, ScrapedArticle]]:
    """Get cached scraped articles from workflow session state"""
    logger.info("Checking if cached scraped articles exist")
    scraped_articles = session_state.get("scraped_articles", {}).get(topic)
    if scraped_articles and isinstance(scraped_articles, dict):
        try:
            return {
                url: ScrapedArticle.model_validate(article)
                for url, article in scraped_articles.items()
            }
        except Exception as e:
            logger.warning(f"Could not validate cached scraped articles: {e}")
    return scraped_articles if isinstance(scraped_articles, dict) else None

def cache_scraped_articles(
    session_state, topic: str, scraped_articles: Dict[str, ScrapedArticle]
):
    """Cache scraped articles in workflow session state"""
    logger.info(f"Saving scraped articles for topic: {topic}")
    if "scraped_articles" not in session_state:
        session_state["scraped_articles"] = {}
    session_state["scraped_articles"][topic] = {
        url: article.model_dump() for url, article in scraped_articles.items()
    }

async def get_search_results(
    session_state, topic: str, use_cache: bool = True, num_attempts: int = 3
) -> Optional[SearchResults]:
    """Get search results with caching support"""

# Check cache first
    if use_cache:
        cached_results = get_cached_search_results(session_state, topic)
        if cached_results:
            logger.info(f"Found {len(cached_results.articles)} articles in cache.")
            return cached_results

# Search for new results
    for attempt in range(num_attempts):
        try:
            print(
                f"🔍 Searching for articles about: {topic} (attempt {attempt + 1}/{num_attempts})"
            )
            response = await research_agent.arun(topic)

if (
                response
                and response.content
                and isinstance(response.content, SearchResults)
            ):
                article_count = len(response.content.articles)
                logger.info(f"Found {article_count} articles on attempt {attempt + 1}")
                print(f"✅ Found {article_count} relevant articles")

# Cache the results
                cache_search_results(session_state, topic, response.content)
                return response.content
            else:
                logger.warning(
                    f"Attempt {attempt + 1}/{num_attempts} failed: Invalid response type"
                )

except Exception as e:
            logger.warning(f"Attempt {attempt + 1}/{num_attempts} failed: {str(e)}")

logger.error(f"Failed to get search results after {num_attempts} attempts")
    return None

async def scrape_articles(
    session_state,
    topic: str,
    search_results: SearchResults,
    use_cache: bool = True,
) -> Dict[str, ScrapedArticle]:
    """Scrape articles with caching support"""

# Check cache first
    if use_cache:
        cached_articles = get_cached_scraped_articles(session_state, topic)
        if cached_articles:
            logger.info(f"Found {len(cached_articles)} scraped articles in cache.")
            return cached_articles

scraped_articles: Dict[str, ScrapedArticle] = {}

print(f"📄 Scraping {len(search_results.articles)} articles...")

for i, article in enumerate(search_results.articles, 1):
        try:
            print(
                f"📖 Scraping article {i}/{len(search_results.articles)}: {article.title[:50]}..."
            )
            response = await content_scraper_agent.arun(article.url)

if (
                response
                and response.content
                and isinstance(response.content, ScrapedArticle)
            ):
                scraped_articles[response.content.url] = response.content
                logger.info(f"Scraped article: {response.content.url}")
                print(f"✅ Successfully scraped: {response.content.title[:50]}...")
            else:
                print(f"❌ Failed to scrape: {article.title[:50]}...")

except Exception as e:
            logger.warning(f"Failed to scrape {article.url}: {str(e)}")
            print(f"❌ Error scraping: {article.title[:50]}...")

# Cache the scraped articles
    cache_scraped_articles(session_state, topic, scraped_articles)
    return scraped_articles

---

## OpenAI o4-mini

**URL:** llms-txt#openai-o4-mini

**Contents:**
- Code
- Usage

Source: https://docs.agno.com/examples/concepts/reasoning/models/openai/o4-mini

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Set your API key">
    
  </Step>

<Step title="Install libraries">
    
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Set your API key">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Refresh Metrics

**URL:** llms-txt#refresh-metrics

Source: https://docs.agno.com/reference-api/schema/metrics/refresh-metrics

post /metrics/refresh
Manually trigger recalculation of system metrics from raw data. This operation analyzes system activity logs and regenerates aggregated metrics. Useful for ensuring metrics are up-to-date or after system maintenance.

---

## Delete Content by ID

**URL:** llms-txt#delete-content-by-id

Source: https://docs.agno.com/reference-api/schema/knowledge/delete-content-by-id

delete /knowledge/content/{content_id}
Permanently remove a specific content item from the knowledge base. This action cannot be undone.

---

## More example interactions to try:

**URL:** llms-txt#more-example-interactions-to-try:

**Contents:**
- Usage

"""
Try these voice interaction scenarios:
1. "Can you summarize the main points discussed in this recording?"
2. "What emotions or tone do you detect in the speaker's voice?"
3. "Please provide a detailed analysis of the speech patterns and clarity"
4. "Can you identify any background noises or audio quality issues?"
5. "What is the overall context and purpose of this recording?"

Note: You can use your own audio files by converting them to base64 format.
Example for using your own audio file:

with open('your_audio.wav', 'rb') as audio_file:
    audio_data = audio_file.read()
    agent.run("Analyze this audio", audio=[Audio(content=audio_data, format="wav")])
"""
bash  theme={null}
    pip install openai requests agno
    bash  theme={null}
    python audio_input_output.py
    ```
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Install libraries">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Run the agent">
```

---

## Composio Tools

**URL:** llms-txt#composio-tools

**Contents:**
- Code
- Usage

Source: https://docs.agno.com/examples/concepts/tools/others/composio

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Set your API key">
    
  </Step>

<Step title="Install libraries">
    
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Set your API key">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Run with streaming and consume the generator to get the final response

**URL:** llms-txt#run-with-streaming-and-consume-the-generator-to-get-the-final-response

stream_generator = team.run(
    "Give me a stock report for NVDA",
    stream=True,
    stream_events=True,
)

---

## === Rate Limiting Middleware ===

**URL:** llms-txt#===-rate-limiting-middleware-===

class RateLimitMiddleware(BaseHTTPMiddleware):
    """
    Rate limiting middleware that limits requests per IP address.
    """

def __init__(self, app, requests_per_minute: int = 60, window_size: int = 60):
        super().__init__(app)
        self.requests_per_minute = requests_per_minute
        self.window_size = window_size
        # Store request timestamps per IP
        self.request_history: Dict[str, deque] = defaultdict(lambda: deque())

async def dispatch(self, request: Request, call_next) -> Response:
        # Get client IP
        client_ip = request.client.host if request.client else "unknown"
        current_time = time.time()

# Clean old requests outside the window
        history = self.request_history[client_ip]
        while history and current_time - history[0] > self.window_size:
            history.popleft()

# Check if rate limit exceeded
        if len(history) >= self.requests_per_minute:
            return JSONResponse(
                status_code=429,
                content={
                    "detail": f"Rate limit exceeded. Max {self.requests_per_minute} requests per minute."
                },
            )

# Add current request to history
        history.append(current_time)

# Add rate limit headers
        response = await call_next(request)
        response.headers["X-RateLimit-Limit"] = str(self.requests_per_minute)
        response.headers["X-RateLimit-Remaining"] = str(
            self.requests_per_minute - len(history)
        )
        response.headers["X-RateLimit-Reset"] = str(
            int(current_time + self.window_size)
        )

---

## Showing reasoning tokens metrics

**URL:** llms-txt#showing-reasoning-tokens-metrics

print(f"Reasoning tokens: {run_response.metrics.reasoning_tokens}")

agent = Agent(model=OpenAIChat(id="gpt-5-mini"), markdown=True, telemetry=False)
agent.run("Share a 2 sentence horror story" * 150)
run_response = agent.run("Share a 2 sentence horror story" * 150)

---

## Human in the Loop

**URL:** llms-txt#human-in-the-loop

**Contents:**
- Code

Source: https://docs.agno.com/examples/getting-started/12-human-in-the-loop

This example shows how to implement human-in-the-loop functionality in your Agno tools.
It shows how to:

* Add pre-hooks to tools for user confirmation
* Handle user input during tool execution
* Gracefully cancel operations based on user choice

Some practical applications:

* Confirming sensitive operations before execution
* Reviewing API calls before they're made
* Validating data transformations
* Approving automated actions in critical systems

```python human_in_the_loop.py theme={null}
import json
from textwrap import dedent

import httpx
from agno.agent import Agent
from agno.tools import tool
from agno.utils import pprint
from rich.console import Console
from rich.prompt import Prompt

@tool(requires_confirmation=True)
def get_top_hackernews_stories(num_stories: int) -> str:
    """Fetch top stories from Hacker News.

Args:
        num_stories (int): Number of stories to retrieve

Returns:
        str: JSON string containing story details
    """
    # Fetch top story IDs
    response = httpx.get("https://hacker-news.firebaseio.com/v0/topstories.json")
    story_ids = response.json()

# Yield story details
    all_stories = []
    for story_id in story_ids[:num_stories]:
        story_response = httpx.get(
            f"https://hacker-news.firebaseio.com/v0/item/{story_id}.json"
        )
        story = story_response.json()
        if "text" in story:
            story.pop("text", None)
        all_stories.append(story)
    return json.dumps(all_stories)

---

## Step 3: Instrument and execute

**URL:** llms-txt#step-3:-instrument-and-execute

with instrument_agno("openai"):
    response = agent.run("Retrieve the latest news about the stock market.")
    print(response.content)
```

Now go to the [Atla dashboard](https://app.atla-ai.com/app/) and view the traces created by your agent. You can visualize the execution flow, monitor performance, and debug issues directly from the Atla dashboard.

<Frame caption="Atla Agent run trace">
  <img src="https://mintcdn.com/agno-v2/Y7twezR0wF2re1xh/images/atla-trace-summary.png?fit=max&auto=format&n=Y7twezR0wF2re1xh&q=85&s=83334fabbdbc6d69fd6c322568a79910" style={{ borderRadius: '10px', width: '100%', maxWidth: '800px' }} alt="atla-trace" data-og-width="1482" width="1482" data-og-height="853" height="853" data-path="images/atla-trace-summary.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/agno-v2/Y7twezR0wF2re1xh/images/atla-trace-summary.png?w=280&fit=max&auto=format&n=Y7twezR0wF2re1xh&q=85&s=c16251e3e7934ba377fcc96c87fb9c94 280w, https://mintcdn.com/agno-v2/Y7twezR0wF2re1xh/images/atla-trace-summary.png?w=560&fit=max&auto=format&n=Y7twezR0wF2re1xh&q=85&s=30b4c98898ac04641de69533b0e9b2b3 560w, https://mintcdn.com/agno-v2/Y7twezR0wF2re1xh/images/atla-trace-summary.png?w=840&fit=max&auto=format&n=Y7twezR0wF2re1xh&q=85&s=846aa70151fd7874c52b08db17fb25ac 840w, https://mintcdn.com/agno-v2/Y7twezR0wF2re1xh/images/atla-trace-summary.png?w=1100&fit=max&auto=format&n=Y7twezR0wF2re1xh&q=85&s=0ddcdd1f24323b0cfe1f38af5bc56b03 1100w, https://mintcdn.com/agno-v2/Y7twezR0wF2re1xh/images/atla-trace-summary.png?w=1650&fit=max&auto=format&n=Y7twezR0wF2re1xh&q=85&s=6f8a77a8d026dfc3533084b788e3caf0 1650w, https://mintcdn.com/agno-v2/Y7twezR0wF2re1xh/images/atla-trace-summary.png?w=2500&fit=max&auto=format&n=Y7twezR0wF2re1xh&q=85&s=e9e30bd974ffc954415f1a9e0a5931d3 2500w" />
</Frame>

---

## AWS Claude

**URL:** llms-txt#aws-claude

**Contents:**
- Authentication
- Example
- Parameters

Source: https://docs.agno.com/concepts/models/aws-claude

Learn how to use AWS Claude models in Agno.

Use Claude models through AWS Bedrock. This provides a native Claude integration optimized for AWS infrastructure.

We recommend experimenting to find the best-suited model for your use-case. Here are some general recommendations:

* `anthropic.claude-3-5-sonnet-20241022-v2:0` model is good for most use-cases and supports image input.
* `anthropic.claude-3-5-haiku-20241022-v2:0` model is their fastest model.

Set your `AWS_ACCESS_KEY_ID`, `AWS_SECRET_ACCESS_KEY` and `AWS_REGION` environment variables.

Get your keys from [here](https://us-east-1.console.aws.amazon.com/iam/home?region=us-east-1#/home).

Use `Claude` with your `Agent`:

<CodeGroup>
  
</CodeGroup>

<Note> View more examples [here](/examples/models/aws/claude/basic). </Note>

| Parameter        | Type                       | Default                                       | Description                                                      |
| ---------------- | -------------------------- | --------------------------------------------- | ---------------------------------------------------------------- |
| `id`             | `str`                      | `"anthropic.claude-3-5-sonnet-20240620-v1:0"` | The specific AWS Bedrock Claude model ID to use                  |
| `name`           | `str`                      | `"AwsBedrockAnthropicClaude"`                 | The name identifier for the AWS Bedrock Claude model             |
| `provider`       | `str`                      | `"AwsBedrock"`                                | The provider of the model                                        |
| `aws_access_key` | `Optional[str]`            | `None`                                        | The AWS access key to use (defaults to AWS\_ACCESS\_KEY env var) |
| `aws_secret_key` | `Optional[str]`            | `None`                                        | The AWS secret key to use (defaults to AWS\_SECRET\_KEY env var) |
| `aws_region`     | `Optional[str]`            | `None`                                        | The AWS region to use (defaults to AWS\_REGION env var)          |
| `session`        | `Optional[Session]`        | `None`                                        | A boto3 Session object to use for authentication                 |
| `max_tokens`     | `int`                      | `4096`                                        | Maximum number of tokens to generate in the chat completion      |
| `temperature`    | `Optional[float]`          | `None`                                        | Controls randomness in the model's output                        |
| `top_p`          | `Optional[float]`          | `None`                                        | Controls diversity via nucleus sampling                          |
| `top_k`          | `Optional[int]`            | `None`                                        | Controls diversity via top-k sampling                            |
| `stop_sequences` | `Optional[List[str]]`      | `None`                                        | A list of strings that the model should stop generating text at  |
| `request_params` | `Optional[Dict[str, Any]]` | `None`                                        | Additional parameters to include in the request                  |
| `client_params`  | `Optional[Dict[str, Any]]` | `None`                                        | Additional parameters for client configuration                   |

`Claude` (AWS) extends the [Anthropic Claude](/concepts/models/anthropic) model with AWS Bedrock integration and has access to most of the same parameters.

**Examples:**

Example 1 (unknown):
```unknown

```

Example 2 (unknown):
```unknown
</CodeGroup>

## Example

Use `Claude` with your `Agent`:

<CodeGroup>
```

---

## Process streaming responses and look for the final RunOutput

**URL:** llms-txt#process-streaming-responses-and-look-for-the-final-runoutput

**Contents:**
- Usage

final_response = None
for event in streaming_agent_alt.run(
    "What is the value of 3! (factorial)?",
    stream=True,
    stream_events=True,
):
    # The final event in the stream should be a RunOutput object
    if hasattr(event, "reasoning_content"):
        final_response = event

print("--- Checking reasoning_content from final stream event ---")
if (
    final_response
    and hasattr(final_response, "reasoning_content")
    and final_response.reasoning_content
):
    print("✅ reasoning_content FOUND in final stream event")
    print(f"   Length: {len(final_response.reasoning_content)} characters")
    print("\n=== reasoning_content preview (final stream event) ===")
    preview = final_response.reasoning_content[:1000]
    if len(final_response.reasoning_content) > 1000:
        preview += "..."
    print(preview)
else:
    print("❌ reasoning_content NOT FOUND in final stream event")
bash  theme={null}
    export OPENAI_API_KEY=xxx
    export ANTHROPIC_API_KEY=xxx
    bash  theme={null}
    pip install -U openai anthropic agno
    bash Mac theme={null}
      python cookbook/reasoning/tools/capture_reasoning_content_reasoning_tools.py
      bash Windows theme={null}
      python cookbook/reasoning/tools/capture_reasoning_content_reasoning_tools.py
      ```
    </CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Set your API key">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Example">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Structured Output

**URL:** llms-txt#structured-output

**Contents:**
- Code

Source: https://docs.agno.com/examples/models/xai/structured_output

```python cookbook/models/xai/structured_output.py theme={null}
from typing import List

from agno.agent import Agent
from agno.models.xai.xai import xAI
from agno.run.agent import RunOutput
from pydantic import BaseModel, Field
from rich.pretty import pprint  # noqa

class MovieScript(BaseModel):
    name: str = Field(..., description="Give a name to this movie")
    setting: str = Field(
        ..., description="Provide a nice setting for a blockbuster movie."
    )
    ending: str = Field(
        ...,
        description="Ending of the movie. If not available, provide a happy ending.",
    )
    genre: str = Field(
        ...,
        description="Genre of the movie. If not available, select action, thriller or romantic comedy.",
    )
    characters: List[str] = Field(..., description="Name of characters for this movie.")
    storyline: str = Field(
        ..., description="3 sentence storyline for the movie. Make it exciting!"
    )

---

## Create a research team

**URL:** llms-txt#create-a-research-team

**Contents:**
- Usage

team = Team(
    members=[
        Agent(
            name="Sarah",
            role="Data Researcher",
            tools=[DuckDuckGoTools()],
            instructions="Focus on gathering and analyzing data",
        ),
        Agent(
            name="Mike",
            role="Technical Writer",
            instructions="Create clear, concise summaries",
        ),
    ],
    retries=3,
    exponential_backoff=True,
)

team.print_response(
    "Search for latest news about the latest AI models",
    stream=True,
)
bash  theme={null}
    pip install agno ddgs
    bash  theme={null}
    export OPENAI_API_KEY=****
    bash  theme={null}
    python cookbook/examples/teams/basic/team_exponential_backoff.py
    ```
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Install required libraries">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Set environment variables">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run the agent">
```

---

## Create workflow with workflow_session_state

**URL:** llms-txt#create-workflow-with-workflow_session_state

**Contents:**
  - 3. `session_state` as a parameter for custom python functions step in workflow
- Key Benefits
- Useful Links

shopping_workflow = Workflow(
    name="Shopping List Workflow",
    db=db,
    steps=[manage_items_step, view_list_step],
    session_state={"shopping_list": []},
)

if __name__ == "__main__":
    # Example 1: Add items to the shopping list
    print("=== Example 1: Adding Items ===")
    shopping_workflow.print_response(
        input="Please add milk, bread, and eggs to my shopping list."
    )
    print("Workflow session state:", shopping_workflow.get_session_state())

# Example 2: Add more items and view list
    print("\n=== Example 2: Adding More Items ===")
    shopping_workflow.print_response(
        input="Add apples and bananas to the list, then show me the complete list."
    )
    print("Workflow session state:", shopping_workflow.get_session_state())

# Example 3: Remove items
    print("\n=== Example 3: Removing Items ===")
    shopping_workflow.print_response(
        input="Remove bread from the list and show me what's left."
    )
    print("Workflow session state:", shopping_workflow.get_session_state())

# Example 4: Clear the entire list
    print("\n=== Example 4: Clearing List ===")
    shopping_workflow.print_response(
        input="Clear the entire shopping list and confirm it's empty."
    )
    print("Final workflow session state:", shopping_workflow.get_session_state())
python  theme={null}
  def custom_function_step(step_input: StepInput, session_state):
      session_state["test"] = test_1 # updates the workflow session state
  python  theme={null}
def evaluator_function(step_input: StepInput, session_state):
    return session_state["test"] == "test_1"

condition_step = Condition(
    name="condition_step",
    evaluator=evaluator_function,
    steps=[step_1, step_2],
)
python  theme={null}
def selector_function(step_input: StepInput, session_state):
    return session_state["test"] == "test_1"

router_step = Router(
    name="router_step",
    selector=selector_function,
    choices=[step_1, step_2],
)
```

See example of [Session State in Condition Evaluator Function](/examples/concepts/workflows/06_workflows_advanced_concepts/access_session_state_in_condition_evaluator_function)
and [Session State in Router Selector Function](/examples/concepts/workflows/06_workflows_advanced_concepts/access_session_state_in_router_selector_function) for more details.

**Persistent State Management**

* Data persists across all workflow steps and components
* Enables complex, stateful workflows with memory
* Supports deterministic execution with consistent state

**Cross-Component Coordination**

* Agents, teams, and functions share the same state object
* Enables sophisticated collaboration patterns
* Maintains data consistency across workflow execution

**Flexible Data Structure**

* Use any Python data structure (dictionaries, lists, objects)
* Structure data to match your workflow requirements
* Access and modify state through standard Python operations

<Note>
  The `session_state` is automatically passed to all agents and teams within a
  workflow, enabling seamless collaboration and data sharing between different
  components without manual state management.
</Note>

<CardGroup cols={2}>
  <Card title="Agent Examples" icon="user" href="https://github.com/agno-agi/agno/tree/main/cookbook/workflows/_06_advanced_concepts/_04_shared_session_state/shared_session_state_with_agent.py">
    See how agents interact with shared session state
  </Card>

<Card title="Team Examples" icon="users" href="https://github.com/agno-agi/agno/tree/main/cookbook/workflows/_06_advanced_concepts/_04_shared_session_state/shared_session_state_with_team.py">
    Learn how teams coordinate using shared state
  </Card>
</CardGroup>

**Examples:**

Example 1 (unknown):
```unknown
### 3. `session_state` as a parameter for custom python functions step in workflow

You can use the `session_state` as a parameter for custom python functions step in workflow to access and modify the session state.

<Note>
  On the function of the custom python function step for a workflow
```

Example 2 (unknown):
```unknown
</Note>

See [examples](/examples/concepts/workflows/06_workflows_advanced_concepts/access_session_state_in_custom_python_function_step) for more details.

The `session_state` is also available as a parameter in the evaluator and selector functions of the `Condition` and `Router` steps:
```

Example 3 (unknown):
```unknown

```

---

## Giphy

**URL:** llms-txt#giphy

**Contents:**
- Prerequisites
- Example
- Toolkit Params
- Toolkit Functions
- Developer Resources

Source: https://docs.agno.com/concepts/tools/toolkits/others/giphy

**GiphyTools** enables an Agent to search for GIFs on GIPHY.

The following agent will search GIPHY for a GIF appropriate for a birthday message.

| Parameter            | Type   | Default | Description                                       |
| -------------------- | ------ | ------- | ------------------------------------------------- |
| `api_key`            | `str`  | `None`  | If you want to manually supply the GIPHY API key. |
| `limit`              | `int`  | `1`     | The number of GIFs to return in a search.         |
| `enable_search_gifs` | `bool` | `True`  | Enable the search\_gifs functionality.            |
| `all`                | `bool` | `False` | Enable all functionality.                         |

| Function      | Description                                         |
| ------------- | --------------------------------------------------- |
| `search_gifs` | Searches GIPHY for a GIF based on the query string. |

## Developer Resources

* View [Tools](https://github.com/agno-agi/agno/blob/main/libs/agno/agno/tools/giphy.py)
* View [Cookbook](https://github.com/agno-agi/agno/tree/main/cookbook/tools/giphy_tools.py)

**Examples:**

Example 1 (unknown):
```unknown
## Example

The following agent will search GIPHY for a GIF appropriate for a birthday message.
```

---

## Alternative example with different topic

**URL:** llms-txt#alternative-example-with-different-topic

**Contents:**
- Usage

alternative_research = ResearchTopic(
    topic="Distributed Systems",
    focus_areas=["Microservices", "Event-Driven Architecture", "Scalability"],
    target_audience="Backend Engineers",
    sources_required=5,
)

team.print_response(input=alternative_research)
bash  theme={null}
    pip install agno pydantic
    bash  theme={null}
    export OPENAI_API_KEY=****
    bash  theme={null}
    python cookbook/examples/teams/structured_input_output/01_pydantic_model_as_input.py
    ```
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Install required libraries">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Set environment variables">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run the agent">
```

---

## Github

**URL:** llms-txt#github

**Contents:**
- Prerequisites
- Example
- Toolkit Params
- Toolkit Functions
- Developer Resources

Source: https://docs.agno.com/concepts/tools/toolkits/others/github

**GithubTools** enables an Agent to access Github repositories and perform tasks such as listing open pull requests, issues and more.

The following examples requires the `PyGithub` library and a Github access token which can be obtained from [here](https://github.com/settings/tokens).

The following agent will search Google for the latest news about "Mistral AI":

| Parameter      | Type            | Default | Description                                                                                                   |
| -------------- | --------------- | ------- | ------------------------------------------------------------------------------------------------------------- |
| `access_token` | `Optional[str]` | `None`  | GitHub access token for authentication. If not provided, will use GITHUB\_ACCESS\_TOKEN environment variable. |
| `base_url`     | `Optional[str]` | `None`  | Optional base URL for GitHub Enterprise installations.                                                        |

| Function                   | Description                                          |
| -------------------------- | ---------------------------------------------------- |
| `search_repositories`      | Searches Github repositories based on a query.       |
| `list_repositories`        | Lists repositories for a given user or organization. |
| `get_repository`           | Gets details about a specific repository.            |
| `list_pull_requests`       | Lists pull requests for a repository.                |
| `get_pull_request`         | Gets details about a specific pull request.          |
| `get_pull_request_changes` | Gets the file changes in a pull request.             |
| `create_issue`             | Creates a new issue in a repository.                 |

You can use `include_tools` or `exclude_tools` to modify the list of tools the agent has access to. Learn more about [selecting tools](/concepts/tools/selecting-tools).

## Developer Resources

* View [Tools](https://github.com/agno-agi/agno/blob/main/libs/agno/agno/tools/github.py)
* View [Cookbook](https://github.com/agno-agi/agno/tree/main/cookbook/tools/github_tools.py)

**Examples:**

Example 1 (unknown):
```unknown

```

Example 2 (unknown):
```unknown
## Example

The following agent will search Google for the latest news about "Mistral AI":
```

---

## print(run_response.metrics)

**URL:** llms-txt#print(run_response.metrics)

**Contents:**
- Usage

bash  theme={null}
    export OPENAI_API_KEY=xxx
    bash  theme={null}
    pip install -U openai agno
    bash Mac theme={null}
        python cookbook/models/openai/chat/basic.py
      bash Windows theme={null}
        python cookbook/models/openai/chat/basic.py
      ```
    </CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Set your API key">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Trafilatura Tools

**URL:** llms-txt#trafilatura-tools

**Contents:**
- Code
- Usage

Source: https://docs.agno.com/examples/concepts/tools/web_scrape/trafilatura

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Set your API key">
    
  </Step>

<Step title="Install libraries">
    
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Set your API key">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## - "Get the top 5 stories (you can try accepting and declining the confirmation)"

**URL:** llms-txt#--"get-the-top-5-stories-(you-can-try-accepting-and-declining-the-confirmation)"

**Contents:**
- Usage

response = agent.run("What are the top 2 hackernews stories?")
if response.is_paused:
    for tool in response.tools:  # type: ignore
        # Ask for confirmation
        console.print(
            f"Tool name [bold blue]{tool.tool_name}({tool.tool_args})[/] requires confirmation."
        )
        message = (
            Prompt.ask("Do you want to continue?", choices=["y", "n"], default="y")
            .strip()
            .lower()
        )

if message == "n":
            break
        else:
            # We update the tools in place
            tool.confirmed = True

run_response = agent.continue_run(run_response=response)
    pprint.pprint_run_response(run_response)
bash  theme={null}
    pip install openai agno
    bash  theme={null}
    python human_in_the_loop.py
    ```
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Install libraries">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Run the agent">
```

---

## File Generation Tools

**URL:** llms-txt#file-generation-tools

**Contents:**
- Code
- Usage

Source: https://docs.agno.com/examples/concepts/tools/file-generation

This cookbook shows how to use the FileGenerationTool to generate various file types (JSON, CSV, PDF, TXT).

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Set your API key">
    
  </Step>

<Step title="Install libraries">
    
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Set your API key">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Router Steps

**URL:** llms-txt#router-steps

Source: https://docs.agno.com/reference/workflows/router-steps

| Parameter     | Type                                                                                                                                                   | Default  | Description                                                                   |
| ------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------ | -------- | ----------------------------------------------------------------------------- |
| `selector`    | `Union[Callable[[StepInput], Union[WorkflowSteps, List[WorkflowSteps]]], Callable[[StepInput], Awaitable[Union[WorkflowSteps, List[WorkflowSteps]]]]]` | Required | Function to select steps dynamically (supports both sync and async functions) |
| `choices`     | `WorkflowSteps`                                                                                                                                        | Required | Available steps for selection                                                 |
| `name`        | `Optional[str]`                                                                                                                                        | `None`   | Name of the router step                                                       |
| `description` | `Optional[str]`                                                                                                                                        | `None`   | Description of the router step                                                |

---

## Every reader implements these core methods

**URL:** llms-txt#every-reader-implements-these-core-methods

**Contents:**
  - The Reading Process
  - Content Types and Specialization
- Reader Configuration
  - Chunking Control
  - Content Processing Options
  - Encoding Control
  - Metadata and Naming
- The Document Output
- Chunking Integration
  - Automatic Chunking

class Reader:
    def read(self, obj, name=None) -> List[Document]:
        """Synchronously read and process content"""
        pass

async def async_read(self, obj, name=None) -> List[Document]:
        """Asynchronously read and process content"""
        pass
python  theme={null}
@classmethod
def get_supported_content_types(cls) -> List[ContentType]:
    """Returns the content types this reader can handle"""
    return [ContentType.PDF]  # Example for PDFReader
python  theme={null}
reader = PDFReader(
    chunk=True,                    # Enable/disable chunking
    chunk_size=1000,              # Size of each chunk
    chunking_strategy=MyStrategy() # Custom chunking logic
)
python  theme={null}
reader = PDFReader(
    split_on_pages=True,          # Create separate documents per page
    password="secret123",         # Handle encrypted PDFs
    read_images=True             # Extract text from images via OCR
)
python  theme={null}
reader = TextReader(
    encoding="utf-8"              # Override default encoding
)

reader = CSVReader(
    encoding="latin-1"            # Handle files with specific encodings
)

reader = MarkdownReader(
    encoding="cp1252"             # Windows-specific encoding
)
python  theme={null}
documents = reader.read(
    file_path,
    name="custom_document_name",  # Override default naming
    password="file_password"      # Runtime password override
)
python  theme={null}
Document(
    content="The extracted text content...",
    id="unique_document_identifier",
    name="document_name",
    meta_data={
        "page": 1,                # Page number for PDFs
        "url": "https://...",     # Source URL for web content
        "author": "...",          # Document metadata
    },
    size=len(content)             # Content size in characters
)
python  theme={null}

**Examples:**

Example 1 (unknown):
```unknown
### The Reading Process

When a reader processes content, it follows these steps:

1. **Content Ingestion**: The reader receives raw content (file, URL, text, etc.)
2. **Parsing**: Extract text and metadata using format-specific logic
3. **Document Creation**: Convert parsed content into `Document` objects
4. **Chunking**: Apply chunking strategies to break content into smaller pieces
5. **Return**: Provide a list of processed documents ready for embedding

### Content Types and Specialization

Each reader specializes in handling specific content types:
```

Example 2 (unknown):
```unknown
This specialization allows each reader to:

* Use format-specific parsing libraries
* Extract relevant metadata
* Handle format-specific challenges (encryption, encoding, etc.)
* Optimize processing for that content type

## Reader Configuration

Readers are highly configurable to meet different processing needs:

### Chunking Control
```

Example 3 (unknown):
```unknown
### Content Processing Options
```

Example 4 (unknown):
```unknown
### Encoding Control

For text-based readers, you can override the file encoding:
```

---

## Create and use workflow

**URL:** llms-txt#create-and-use-workflow

if __name__ == "__main__":
    content_creation_workflow = Workflow(
        name="Content Creation Workflow",
        description="Automated content creation from blog posts to social media",
        db=SqliteDb(
            session_table="workflow_session",
            db_file="tmp/workflow.db",
        ),
        steps=[research_step, content_planning_step],
        input_schema=ResearchTopic,  # <-- Define input schema for validation
    )

print("=== Example 1: Valid Pydantic Model Input ===")
    research_topic = ResearchTopic(
        topic="AI trends in 2024",
        focus_areas=[
            "Machine Learning",
            "Natural Language Processing",
            "Computer Vision",
            "AI Ethics",
        ],
        target_audience="Tech professionals and business leaders",
    )

# ✅ This will work properly - input matches the schema
    content_creation_workflow.print_response(
        input=research_topic,
        markdown=True,
    )

print("\n=== Example 2: Valid Dictionary Input ===")
    # ✅ This will also work - dict matches ResearchTopic structure
    content_creation_workflow.print_response(
        input={
            "topic": "AI trends in 2024",
            "focus_areas": ["Machine Learning", "Computer Vision"],
            "target_audience": "Tech professionals",
            "sources_required": 8
        },
        markdown=True,
    )

print("\n=== Example 3: Missing Required Fields (Commented - Would Fail) ===")
    # ❌ This would fail - missing required 'target_audience' field
    # Uncomment to see validation error:
    # content_creation_workflow.print_response(
    #     input=ResearchTopic(
    #         topic="AI trends in 2024",
    #         focus_areas=[
    #             "Machine Learning",
    #             "Natural Language Processing",
    #             "Computer Vision",
    #             "AI Ethics",
    #         ],
    #         # target_audience missing - will raise ValidationError
    #     ),
    #     markdown=True,
    # )

print("\n=== Example 4: Wrong Model Type (Commented - Would Fail) ===")
    # ❌ This would fail - different Pydantic model provided
    # Uncomment to see validation error:
    # content_creation_workflow.print_response(
    #     input=DifferentModel(name="test"),
    #     markdown=True,
    # )

print("\n=== Example 5: Type Mismatch (Commented - Would Fail) ===")
    # ❌ This would fail - wrong data types
    # Uncomment to see validation error:
    # content_creation_workflow.print_response(
    #     input={
    #         "topic": 123,  # Should be string
    #         "focus_areas": "Machine Learning",  # Should be List[str]
    #         "target_audience": ["audience1", "audience2"],  # Should be string
    #     },
    #     markdown=True,
    # )
```

---

## Llama Essay Writer

**URL:** llms-txt#llama-essay-writer

**Contents:**
- Code
- Usage

Source: https://docs.agno.com/examples/models/huggingface/llama_essay_writer

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Set your API key">
    
  </Step>

<Step title="Install libraries">
    
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Set your API key">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Method 1: Set enable_session_summaries to True

**URL:** llms-txt#method-1:-set-enable_session_summaries-to-true

agent = Agent(
    model=OpenAIChat(id="gpt-5-mini"),
)

team = Team(
    model=OpenAIChat(id="gpt-5-mini"),
    members=[agent],
    db=db,
    enable_session_summaries=True,
)

team.print_response("Hi my name is John and I live in New York")
team.print_response("I like to play basketball and hike in the mountains")

---

## Install dependencies and build

**URL:** llms-txt#install-dependencies-and-build

**Contents:**
  - 2. Install Python Dependencies
  - 3. Set Environment Variables
- Code Example
- Available Tools

npm install
npm run build
bash  theme={null}
pip install agno mcp openai
bash  theme={null}
export BROWSERBASE_API_KEY=your_browserbase_api_key
export BROWSERBASE_PROJECT_ID=your_browserbase_project_id
export OPENAI_API_KEY=your_openai_api_key
python  theme={null}
import asyncio
from os import environ
from textwrap import dedent

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.mcp import MCPTools
from mcp import StdioServerParameters

async def run_agent(message: str) -> None:
    server_params = StdioServerParameters(
        command="node",
        # Update this path to the location where you cloned the repository
        args=["mcp-server-browserbase/stagehand/dist/index.js"],
        env=environ.copy(),
    )

async with MCPTools(server_params=server_params, timeout_seconds=60) as mcp_tools:
        agent = Agent(
            model=OpenAIChat(id="gpt-5-mini"),
            tools=[mcp_tools],
            instructions=dedent("""\
                You are a web scraping assistant that creates concise reader's digests from Hacker News.

CRITICAL INITIALIZATION RULES - FOLLOW EXACTLY:
                1. NEVER use screenshot tool until AFTER successful navigation
                2. ALWAYS start with stagehand_navigate first
                3. Wait for navigation success message before any other actions
                4. If you see initialization errors, restart with navigation only
                5. Use stagehand_observe and stagehand_extract to explore pages safely

Available tools and safe usage order:
                - stagehand_navigate: Use FIRST to initialize browser
                - stagehand_extract: Use to extract structured data from pages
                - stagehand_observe: Use to find elements and understand page structure
                - stagehand_act: Use to click links and navigate to comments
                - screenshot: Use ONLY after navigation succeeds and page loads

Your goal is to create a comprehensive but concise digest that includes:
                - Top headlines with brief summaries
                - Key themes and trends
                - Notable comments and insights
                - Overall tech news landscape overview

Be methodical, extract structured data, and provide valuable insights.
            """),
            markdown=True,
                    )
        await agent.aprint_response(message, stream=True)

if __name__ == "__main__":
    asyncio.run(
        run_agent(
            "Create a comprehensive Hacker News Reader's Digest from https://news.ycombinator.com"
        )
    )
```

The Stagehand MCP server provides several tools for web automation:

| Tool                 | Purpose                                     | Usage Notes                            |
| -------------------- | ------------------------------------------- | -------------------------------------- |
| `stagehand_navigate` | Navigate to web pages                       | **Use first** for initialization       |
| `stagehand_extract`  | Extract structured data                     | Safe for content extraction            |
| `stagehand_observe`  | Find elements and understand page structure | Good for exploration                   |
| `stagehand_act`      | Interact with page elements                 | Click, type, scroll actions            |
| `screenshot`         | Take screenshots                            | **Use only after** navigation succeeds |

**Examples:**

Example 1 (unknown):
```unknown
### 2. Install Python Dependencies
```

Example 2 (unknown):
```unknown
### 3. Set Environment Variables
```

Example 3 (unknown):
```unknown
## Code Example
```

---

## Initialize the X toolkit

**URL:** llms-txt#initialize-the-x-toolkit

x_tools = XTools(
    wait_on_rate_limit=True # Retry when rate limits are reached
)

---

## Apify

**URL:** llms-txt#apify

**Contents:**
- What is Apify?
- Prerequisites
- Basic Usage

Source: https://docs.agno.com/concepts/tools/toolkits/others/apify

This guide demonstrates how to integrate and use [Apify](https://apify.com/actors) Actors within the Agno framework to enhance your AI agents with web scraping, crawling, data extraction, and web automation capabilities.

[Apify](https://apify.com/) is a platform that provides:

* Data collection services for AI Agents, specializing in extracting data from social media, search engines, online maps, e-commerce sites, travel portals, or general websites
* A marketplace of ready-to-use Actors (specialized tools) for various data tasks
* Infrastructure to run and monetize our own AI Agents

1. Sign up for an [Apify account](https://console.apify.com/sign-up)
2. Obtain your Apify API token (can be obtained from [Apify](https://docs.apify.com/platform/integrations/api))
3. Install the required packages:

The Agno framework makes it easy to integrate Apify Actors into your agents. Here's a simple example:

```python  theme={null}
from agno.agent import Agent
from agno.tools.apify import ApifyTools

**Examples:**

Example 1 (unknown):
```unknown
## Basic Usage

The Agno framework makes it easy to integrate Apify Actors into your agents. Here's a simple example:
```

---

## Print the metrics

**URL:** llms-txt#print-the-metrics

**Contents:**
- Usage

print("---" * 5, "Collected Metrics", "---" * 5)
pprint(run_output.metrics)  # type: ignore
bash  theme={null}
    export GROQ_API_KEY=xxx
    bash  theme={null}
    pip install -U groq yfinance agno
    bash Mac theme={null}
      python cookbook/models/groq/metrics.py
      bash Windows theme={null}
      python cookbook/models/groq/metrics.py
      ```
    </CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Set your API key">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## AWS SES Tools

**URL:** llms-txt#aws-ses-tools

**Contents:**
- Code

Source: https://docs.agno.com/examples/concepts/tools/others/aws_ses

```python cookbook/tools/aws_ses_tools.py theme={null}
from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.aws_ses import AWSSESTool
from agno.tools.duckduckgo import DuckDuckGoTools

---

## Fully Python Workflow

**URL:** llms-txt#fully-python-workflow

Source: https://docs.agno.com/concepts/workflows/workflow-patterns/fully-python-workflow

Keep it Simple with Pure Python, in v1 workflows style

**Keep it Simple with Pure Python**: If you prefer the Workflows 1.0 approach or need maximum flexibility, you can still use a single Python function to handle everything.
This approach gives you complete control over the execution flow while still benefiting from workflow features like storage, streaming, and session management.

Replace all the steps in the workflow with a single executable function where you can control everything.

* [Function-Based Workflow](/examples/concepts/workflows/01-basic-workflows/function_instead_of_steps) - Complete function-based workflow

For migration from 1.0 style workflows, refer to the page for [Migrating to Workflows 2.0](/how-to/v2-migration)

---

## 2. Get the token from BotFather.

**URL:** llms-txt#2.-get-the-token-from-botfather.

---

## Importing our GoogleSearchTools ToolKit, containing multiple web search tools

**URL:** llms-txt#importing-our-googlesearchtools-toolkit,-containing-multiple-web-search-tools

**Contents:**
- Tool Built-in Parameters
  - Session State Parameter

from agno.tools.googlesearch import GoogleSearchTools

agent = Agent(
    model=OpenAIChat(id="gpt-5-mini"),
    tools=[
        GoogleSearchTools(),
    ],
)

agent.print_response("What's the latest about OpenAIs GPT-5?", markdown=True)
python  theme={null}
from agno.agent import Agent
from agno.db.sqlite import SqliteDb
from agno.models.openai import OpenAIChat

def add_item(session_state, item: str) -> str:
    """Add an item to the shopping list."""
    session_state["shopping_list"].append(item)  # type: ignore
    return f"The shopping list is now {session_state['shopping_list']}"  # type: ignore

**Examples:**

Example 1 (unknown):
```unknown
In this example, the `GoogleSearchTools` toolkit is added to the agent. This ToolKit comes pre-configured with the `google_search` function.

## Tool Built-in Parameters

Agno automatically provides special parameters to your tools that give access to the agent's state. These parameters are injected automatically - you don't pass them when calling the tool.

### Session State Parameter

The built-in parameter `session_state` allows tools to access and modify persistent data across conversations.

This is useful in cases where a tool result is relevant for the next steps of the conversation.

Add `session_state` as a parameter in your tool function to access the agent's persistent state:
```

---

## When you add content

**URL:** llms-txt#when-you-add-content

knowledge.add_content(
    name="Product Manual",
    path="docs/manual.pdf",
    metadata={"department": "engineering", "version": "2.1"}
)

---

## Execute Workflow

**URL:** llms-txt#execute-workflow

Source: https://docs.agno.com/reference-api/schema/workflows/execute-workflow

post /workflows/{workflow_id}/runs
Execute a workflow with the provided input data. Workflows can run in streaming or batch mode.

**Execution Modes:**
- **Streaming (`stream=true`)**: Real-time step-by-step execution updates via SSE
- **Non-Streaming (`stream=false`)**: Complete workflow execution with final result

**Workflow Execution Process:**
1. Input validation against workflow schema
2. Sequential or parallel step execution based on workflow design
3. Data flow between steps with transformation
4. Error handling and automatic retries where configured
5. Final result compilation and response

**Session Management:**
Workflows support session continuity for stateful execution across multiple runs.

---

## Team

**URL:** llms-txt#team

**Contents:**
- Parameters
- Functions
  - `run`
  - `arun`
  - `print_response`
  - `aprint_response`
  - `cli_app`
  - `acli_app`
  - `get_session_summary`
  - `get_user_memories`

Source: https://docs.agno.com/reference/teams/team

| Parameter                          | Type                                                             | Default    | Description                                                                                                                                                                                                          |
| ---------------------------------- | ---------------------------------------------------------------- | ---------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| `members`                          | `List[Union[Agent, Team]]`                                       | -          | List of agents or teams that make up this team                                                                                                                                                                       |
| `id`                               | `Optional[str]`                                                  | `None`     | Team UUID (autogenerated if not set)                                                                                                                                                                                 |
| `model`                            | `Optional[Model]`                                                | `None`     | Model to use for the team                                                                                                                                                                                            |
| `name`                             | `Optional[str]`                                                  | `None`     | Name of the team                                                                                                                                                                                                     |
| `role`                             | `Optional[str]`                                                  | `None`     | Role of the team within its parent team                                                                                                                                                                              |
| `respond_directly`                 | `bool`                                                           | `False`    | If True, the team leader won't process responses from the members and instead will return them directly                                                                                                              |
| `determine_input_for_members`      | `bool`                                                           | `True`     | Set to False if you want to send the run input directly to the member agents                                                                                                                                         |
| `delegate_task_to_all_members`     | `bool`                                                           | `False`    | If True, the team leader will delegate tasks to all members automatically, without any decision from the team leader                                                                                                 |
| `user_id`                          | `Optional[str]`                                                  | `None`     | Default user ID for this team                                                                                                                                                                                        |
| `session_id`                       | `Optional[str]`                                                  | `None`     | Default session ID for this team (autogenerated if not set)                                                                                                                                                          |
| `session_state`                    | `Optional[Dict[str, Any]]`                                       | `None`     | Session state (stored in the database to persist across runs)                                                                                                                                                        |
| `add_session_state_to_context`     | `bool`                                                           | `False`    | Set to True to add the session\_state to the context                                                                                                                                                                 |
| `enable_agentic_state`             | `bool`                                                           | `False`    | Set to True to give the team tools to update the session\_state dynamically                                                                                                                                          |
| `overwrite_db_session_state`       | `bool`                                                           | `False`    | Set to True to overwrite the session state in the database with the session state provided in the run                                                                                                                |
| `cache_session`                    | `bool`                                                           | `False`    | If True, cache the current Team session in memory for faster access                                                                                                                                                  |
| `resolve_in_context`               | `bool`                                                           | `True`     | If True, resolve the session\_state, dependencies, and metadata in the user and system messages                                                                                                                      |
| `description`                      | `Optional[str]`                                                  | `None`     | A description of the Team that is added to the start of the system message                                                                                                                                           |
| `instructions`                     | `Optional[Union[str, List[str], Callable]]`                      | `None`     | List of instructions for the team                                                                                                                                                                                    |
| `expected_output`                  | `Optional[str]`                                                  | `None`     | Provide the expected output from the Team                                                                                                                                                                            |
| `additional_context`               | `Optional[str]`                                                  | `None`     | Additional context added to the end of the system message                                                                                                                                                            |
| `markdown`                         | `bool`                                                           | `False`    | If markdown=true, add instructions to format the output using markdown                                                                                                                                               |
| `add_datetime_to_context`          | `bool`                                                           | `False`    | If True, add the current datetime to the instructions to give the team a sense of time                                                                                                                               |
| `add_location_to_context`          | `bool`                                                           | `False`    | If True, add the current location to the instructions to give the team a sense of location                                                                                                                           |
| `timezone_identifier`              | `Optional[str]`                                                  | `None`     | Allows for custom timezone for datetime instructions following the TZ Database format                                                                                                                                |
| `add_name_to_context`              | `bool`                                                           | `False`    | If True, add the team name to the instructions                                                                                                                                                                       |
| `add_member_tools_to_context`      | `bool`                                                           | `True`     | If True, add the tools available to team members to the context                                                                                                                                                      |
| `system_message`                   | `Optional[Union[str, Callable, Message]]`                        | `None`     | Provide the system message as a string or function                                                                                                                                                                   |
| `system_message_role`              | `str`                                                            | `"system"` | Role for the system message                                                                                                                                                                                          |
| `additional_input`                 | `Optional[List[Union[str, Dict, BaseModel, Message]]]`           | `None`     | A list of extra messages added after the system message and before the user message                                                                                                                                  |
| `db`                               | `Optional[BaseDb]`                                               | `None`     | Database to use for this team                                                                                                                                                                                        |
| `memory_manager`                   | `Optional[MemoryManager]`                                        | `None`     | Memory manager to use for this team                                                                                                                                                                                  |
| `dependencies`                     | `Optional[Dict[str, Any]]`                                       | `None`     | User provided dependencies                                                                                                                                                                                           |
| `add_dependencies_to_context`      | `bool`                                                           | `False`    | If True, add the dependencies to the user prompt                                                                                                                                                                     |
| `knowledge`                        | `Optional[Knowledge]`                                            | `None`     | Add a knowledge base to the team                                                                                                                                                                                     |
| `knowledge_filters`                | `Optional[Dict[str, Any]]`                                       | `None`     | Filters to apply to knowledge base searches                                                                                                                                                                          |
| `enable_agentic_knowledge_filters` | `Optional[bool]`                                                 | `False`    | Let the team choose the knowledge filters                                                                                                                                                                            |
| `update_knowledge`                 | `bool`                                                           | `False`    | Add a tool that allows the Team to update Knowledge                                                                                                                                                                  |
| `add_knowledge_to_context`         | `bool`                                                           | `False`    | If True, add references to the user prompt                                                                                                                                                                           |
| `knowledge_retriever`              | `Optional[Callable[..., Optional[List[Union[Dict, str]]]]]`      | `None`     | Retrieval function to get references                                                                                                                                                                                 |
| `references_format`                | `Literal["json", "yaml"]`                                        | `"json"`   | Format of the references                                                                                                                                                                                             |
| `share_member_interactions`        | `bool`                                                           | `False`    | If True, send all member interactions (request/response) during the current run to members that have been delegated a task to                                                                                        |
| `get_member_information_tool`      | `bool`                                                           | `False`    | If True, add a tool to get information about the team members                                                                                                                                                        |
| `search_knowledge`                 | `bool`                                                           | `True`     | Add a tool to search the knowledge base (aka Agentic RAG)                                                                                                                                                            |
| `send_media_to_model`              | `bool`                                                           | `True`     | If False, media (images, videos, audio, files) is only available to tools and not sent to the LLM                                                                                                                    |
| `store_media`                      | `bool`                                                           | `True`     | If True, store media in the database                                                                                                                                                                                 |
| `store_tool_messages`              | `bool`                                                           | `True`     | If True, store tool results in the database                                                                                                                                                                          |
| `store_history_messages`           | `bool`                                                           | `True`     | If True, store history messages in the database                                                                                                                                                                      |
| `tools`                            | `Optional[List[Union[Toolkit, Callable, Function, Dict]]]`       | `None`     | A list of tools provided to the Model                                                                                                                                                                                |
| `tool_choice`                      | `Optional[Union[str, Dict[str, Any]]]`                           | `None`     | Controls which (if any) tool is called by the team model                                                                                                                                                             |
| `tool_call_limit`                  | `Optional[int]`                                                  | `None`     | Maximum number of tool calls allowed                                                                                                                                                                                 |
| `max_tool_calls_from_history`      | `Optional[int]`                                                  | `None`     | Maximum number of tool calls from history to keep in context. If None, all tool calls from history are included. If set to N, only the last N tool calls from history are added to the context for memory management |
| `tool_hooks`                       | `Optional[List[Callable]]`                                       | `None`     | A list of hooks to be called before and after the tool call                                                                                                                                                          |
| `pre_hooks`                        | `Optional[Union[List[Callable[..., Any]], List[BaseGuardrail]]]` | `None`     | Functions called right after team session is loaded, before processing starts                                                                                                                                        |
| `post_hooks`                       | `Optional[Union[List[Callable[..., Any]], List[BaseGuardrail]]]` | `None`     | Functions called after output is generated but before the response is returned                                                                                                                                       |
| `input_schema`                     | `Optional[Type[BaseModel]]`                                      | `None`     | Input schema for validating input                                                                                                                                                                                    |
| `output_schema`                    | `Optional[Type[BaseModel]]`                                      | `None`     | Output schema for the team response                                                                                                                                                                                  |
| `parser_model`                     | `Optional[Model]`                                                | `None`     | Provide a secondary model to parse the response from the primary model                                                                                                                                               |
| `parser_model_prompt`              | `Optional[str]`                                                  | `None`     | Provide a prompt for the parser model                                                                                                                                                                                |
| `output_model`                     | `Optional[Model]`                                                | `None`     | Provide an output model to parse the response from the team                                                                                                                                                          |
| `output_model_prompt`              | `Optional[str]`                                                  | `None`     | Provide a prompt for the output model                                                                                                                                                                                |
| `use_json_mode`                    | `bool`                                                           | `False`    | If `output_schema` is set, sets the response mode of the model                                                                                                                                                       |
| `parse_response`                   | `bool`                                                           | `True`     | If True, parse the response                                                                                                                                                                                          |
| `enable_agentic_memory`            | `bool`                                                           | `False`    | Enable the team to manage memories of the user                                                                                                                                                                       |
| `enable_user_memories`             | `bool`                                                           | `False`    | If True, the team creates/updates user memories at the end of runs                                                                                                                                                   |
| `add_memories_to_context`          | `Optional[bool]`                                                 | `None`     | If True, the team adds a reference to the user memories in the response                                                                                                                                              |
| `enable_session_summaries`         | `bool`                                                           | `False`    | If True, the team creates/updates session summaries at the end of runs                                                                                                                                               |
| `session_summary_manager`          | `Optional[SessionSummaryManager]`                                | `None`     | Session summary manager                                                                                                                                                                                              |
| `add_session_summary_to_context`   | `Optional[bool]`                                                 | `None`     | If True, the team adds session summaries to the context                                                                                                                                                              |
| `add_history_to_context`           | `bool`                                                           | `False`    | Add messages from the chat history to the messages list sent to the Model. This only applies to the team leader, not the members.                                                                                    |
| `num_history_runs`                 | `int`                                                            | `3`        | Number of historical runs to include in the messages                                                                                                                                                                 |
| `add_team_history_to_members`      | `bool`                                                           | `False`    | If True, send the team-level history to the members, not the agent-level history                                                                                                                                     |
| `num_team_history_runs`            | `int`                                                            | `3`        | Number of historical runs to include in the messages sent to the members                                                                                                                                             |
| `search_session_history`           | `Optional[bool]`                                                 | `False`    | If True, adds a tool to allow searching through previous sessions                                                                                                                                                    |
| `num_history_sessions`             | `Optional[int]`                                                  | `None`     | Number of past sessions to include in the search                                                                                                                                                                     |
| `read_team_history`                | `bool`                                                           | `False`    | If True, adds a tool to allow the team to read the team history (deprecated and will be removed in a future version)                                                                                                 |
| `read_chat_history`                | `bool`                                                           | `False`    | If True, adds a tool to allow the team to read the chat history                                                                                                                                                      |
| `metadata`                         | `Optional[Dict[str, Any]]`                                       | `None`     | Metadata stored with this team                                                                                                                                                                                       |
| `reasoning`                        | `bool`                                                           | `False`    | Enable reasoning for the team                                                                                                                                                                                        |
| `reasoning_model`                  | `Optional[Model]`                                                | `None`     | Model to use for reasoning                                                                                                                                                                                           |
| `reasoning_agent`                  | `Optional[Agent]`                                                | `None`     | Agent to use for reasoning                                                                                                                                                                                           |
| `reasoning_min_steps`              | `int`                                                            | `1`        | Minimum number of reasoning steps                                                                                                                                                                                    |
| `reasoning_max_steps`              | `int`                                                            | `10`       | Maximum number of reasoning steps                                                                                                                                                                                    |
| `stream`                           | `Optional[bool]`                                                 | `None`     | Stream the response from the Team                                                                                                                                                                                    |
| `stream_events`                    | `bool`                                                           | `False`    | Stream the intermediate steps from the Team                                                                                                                                                                          |
| `stream_member_events`             | `bool`                                                           | `True`     | Stream the member events from the Team members                                                                                                                                                                       |
| `store_events`                     | `bool`                                                           | `False`    | Store the events from the Team                                                                                                                                                                                       |
| `events_to_skip`                   | `Optional[List[Union[RunEvent, TeamRunEvent]]]`                  | `None`     | List of events to skip from the Team                                                                                                                                                                                 |
| `store_member_responses`           | `bool`                                                           | `False`    | Store member agent runs inside the team's RunOutput                                                                                                                                                                  |
| `debug_mode`                       | `bool`                                                           | `False`    | Enable debug logs                                                                                                                                                                                                    |
| `debug_level`                      | `Literal[1, 2]`                                                  | `1`        | Debug level: 1 = basic, 2 = detailed                                                                                                                                                                                 |
| `show_members_responses`           | `bool`                                                           | `False`    | Enable member logs - Sets the debug\_mode for team and members                                                                                                                                                       |
| `retries`                          | `int`                                                            | `0`        | Number of retries to attempt                                                                                                                                                                                         |
| `delay_between_retries`            | `int`                                                            | `1`        | Delay between retries (in seconds)                                                                                                                                                                                   |
| `exponential_backoff`              | `bool`                                                           | `False`    | Exponential backoff: if True, the delay between retries is doubled each time                                                                                                                                         |
| `telemetry`                        | `bool`                                                           | `True`     | Log minimal telemetry for analytics                                                                                                                                                                                  |

* `input` (Union\[str, List, Dict, Message, BaseModel, List\[Message]]): The input to send to the team
* `stream` (Optional\[bool]): Whether to stream the response
* `stream_events` (Optional\[bool]): Whether to stream intermediate steps
* `session_id` (Optional\[str]): Session ID to use
* `session_state` (Optional\[Dict\[str, Any]]): Session state to use. By default, merged with the session state in the db.
* `user_id` (Optional\[str]): User ID to use
* `retries` (Optional\[int]): Number of retries to attempt
* `audio` (Optional\[Sequence\[Audio]]): Audio files to include
* `images` (Optional\[Sequence\[Image]]): Image files to include
* `videos` (Optional\[Sequence\[Video]]): Video files to include
* `files` (Optional\[Sequence\[File]]): Files to include
* `knowledge_filters` (Optional\[Dict\[str, Any]]): Knowledge filters to apply
* `add_history_to_context` (Optional\[bool]): Whether to add history to context
* `add_dependencies_to_context` (Optional\[bool]): Whether to add dependencies to context
* `add_session_state_to_context` (Optional\[bool]): Whether to add session state to context
* `dependencies` (Optional\[Dict\[str, Any]]): Dependencies to use for this run
* `metadata` (Optional\[Dict\[str, Any]]): Metadata to use for this run
* `debug_mode` (Optional\[bool]): Whether to enable debug mode
* `yield_run_response` (bool): Whether to yield the run response (only for streaming)

* `Union[TeamRunOutput, Iterator[Union[RunOutputEvent, TeamRunOutputEvent]]]`: Either a TeamRunOutput or an iterator of events, depending on the `stream` parameter

Run the team asynchronously.

* `input` (Union\[str, List, Dict, Message, BaseModel, List\[Message]]): The input to send to the team
* `stream` (Optional\[bool]): Whether to stream the response
* `stream_events` (Optional\[bool]): Whether to stream intermediate steps
* `session_id` (Optional\[str]): Session ID to use
* `session_state` (Optional\[Dict\[str, Any]]): Session state to use. By default, merged with the session state in the db.
* `user_id` (Optional\[str]): User ID to use
* `retries` (Optional\[int]): Number of retries to attempt
* `audio` (Optional\[Sequence\[Audio]]): Audio files to include
* `images` (Optional\[Sequence\[Image]]): Image files to include
* `videos` (Optional\[Sequence\[Video]]): Video files to include
* `files` (Optional\[Sequence\[File]]): Files to include
* `knowledge_filters` (Optional\[Dict\[str, Any]]): Knowledge filters to apply
* `add_history_to_context` (Optional\[bool]): Whether to add history to context
* `add_dependencies_to_context` (Optional\[bool]): Whether to add dependencies to context
* `add_session_state_to_context` (Optional\[bool]): Whether to add session state to context
* `dependencies` (Optional\[Dict\[str, Any]]): Dependencies to use for this run
* `metadata` (Optional\[Dict\[str, Any]]): Metadata to use for this run
* `debug_mode` (Optional\[bool]): Whether to enable debug mode
* `yield_run_response` (bool): Whether to yield the run response (only for streaming)

* `Union[TeamRunOutput, AsyncIterator[Union[RunOutputEvent, TeamRunOutputEvent]]]`: Either a TeamRunOutput or an async iterator of events, depending on the `stream` parameter

Run the team and print the response.

* `input` (Union\[List, Dict, str, Message, BaseModel, List\[Message]]): The input to send to the team
* `stream` (Optional\[bool]): Whether to stream the response
* `stream_events` (Optional\[bool]): Whether to stream intermediate steps
* `session_id` (Optional\[str]): Session ID to use
* `session_state` (Optional\[Dict\[str, Any]]): Session state to use. By default, merged with the session state in the db.
* `user_id` (Optional\[str]): User ID to use
* `show_message` (bool): Whether to show the message (default: True)
* `show_reasoning` (bool): Whether to show reasoning (default: True)
* `show_full_reasoning` (bool): Whether to show full reasoning (default: False)
* `console` (Optional\[Any]): Console to use for output
* `tags_to_include_in_markdown` (Optional\[Set\[str]]): Tags to include in markdown content
* `audio` (Optional\[Sequence\[Audio]]): Audio files to include
* `images` (Optional\[Sequence\[Image]]): Image files to include
* `videos` (Optional\[Sequence\[Video]]): Video files to include
* `files` (Optional\[Sequence\[File]]): Files to include
* `markdown` (Optional\[bool]): Whether to format output as markdown
* `knowledge_filters` (Optional\[Dict\[str, Any]]): Knowledge filters to apply
* `add_history_to_context` (Optional\[bool]): Whether to add history to context
* `dependencies` (Optional\[Dict\[str, Any]]): Dependencies to use for this run
* `add_dependencies_to_context` (Optional\[bool]): Whether to add dependencies to context
* `add_session_state_to_context` (Optional\[bool]): Whether to add session state to context
* `metadata` (Optional\[Dict\[str, Any]]): Metadata to use for this run
* `debug_mode` (Optional\[bool]): Whether to enable debug mode

### `aprint_response`

Run the team and print the response asynchronously.

* `input` (Union\[List, Dict, str, Message, BaseModel, List\[Message]]): The input to send to the team
* `stream` (Optional\[bool]): Whether to stream the response
* `stream_events` (Optional\[bool]): Whether to stream intermediate steps
* `session_id` (Optional\[str]): Session ID to use
* `session_state` (Optional\[Dict\[str, Any]]): Session state to use. By default, merged with the session state in the db.
* `user_id` (Optional\[str]): User ID to use
* `show_message` (bool): Whether to show the message (default: True)
* `show_reasoning` (bool): Whether to show reasoning (default: True)
* `show_full_reasoning` (bool): Whether to show full reasoning (default: False)
* `console` (Optional\[Any]): Console to use for output
* `tags_to_include_in_markdown` (Optional\[Set\[str]]): Tags to include in markdown content
* `audio` (Optional\[Sequence\[Audio]]): Audio files to include
* `images` (Optional\[Sequence\[Image]]): Image files to include
* `videos` (Optional\[Sequence\[Video]]): Video files to include
* `files` (Optional\[Sequence\[File]]): Files to include
* `markdown` (Optional\[bool]): Whether to format output as markdown
* `knowledge_filters` (Optional\[Dict\[str, Any]]): Knowledge filters to apply
* `add_history_to_context` (Optional\[bool]): Whether to add history to context
* `dependencies` (Optional\[Dict\[str, Any]]): Dependencies to use for this run
* `add_dependencies_to_context` (Optional\[bool]): Whether to add dependencies to context
* `add_session_state_to_context` (Optional\[bool]): Whether to add session state to context
* `metadata` (Optional\[Dict\[str, Any]]): Metadata to use for this run
* `debug_mode` (Optional\[bool]): Whether to enable debug mode

Run an interactive command-line interface to interact with the team.

* `input` (Optional\[str]): The input to send to the team
* `user` (str): Name for the user (default: "User")
* `emoji` (str): Emoji for the user (default: ":sunglasses:")
* `stream` (bool): Whether to stream the response (default: False)
* `markdown` (bool): Whether to format output as markdown (default: False)
* `exit_on` (Optional\[List\[str]]): List of commands to exit the CLI
* `**kwargs`: Additional keyword arguments

Run an interactive command-line interface to interact with the team asynchronously.

* `input` (Optional\[str]): The input to send to the team
* `session_id` (Optional\[str]): Session ID to use
* `user_id` (Optional\[str]): User ID to use
* `user` (str): Name for the user (default: "User")
* `emoji` (str): Emoji for the user (default: ":sunglasses:")
* `stream` (bool): Whether to stream the response (default: False)
* `markdown` (bool): Whether to format output as markdown (default: False)
* `exit_on` (Optional\[List\[str]]): List of commands to exit the CLI
* `**kwargs`: Additional keyword arguments

### `get_session_summary`

Get the session summary for the given session ID.

* `session_id` (Optional\[str]): Session ID to use (if not provided, the current session is used)

* Session summary for the given session

### `get_user_memories`

Get the user memories for the given user ID.

* `user_id` (Optional\[str]): User ID to use (if not provided, the current user is used)

* `Optional[List[UserMemory]]`: The user memories

Add a tool to the team.

* `tool` (Union\[Toolkit, Callable, Function, Dict]): The tool to add

Replace the tools of the team.

* `tools` (List\[Union\[Toolkit, Callable, Function, Dict]]): The tools to set

Cancel a run by run ID.

* `run_id` (str): The run ID to cancel

* `bool`: True if the run was successfully cancelled

Get the run output for the given run ID.

* `run_id` (str): The run ID
* `session_id` (Optional\[str]): Session ID to use

* `Optional[Union[TeamRunOutput, RunOutput]]`: The run output

### `get_last_run_output`

Get the last run output for the session.

* `session_id` (Optional\[str]): Session ID to use (if not provided, the current session is used)

* `Optional[TeamRunOutput]`: The last run output

Get the session for the given session ID.

* `session_id` (Optional\[str]): Session ID to use (if not provided, the current session is used)

* `Optional[TeamSession]`: The team session

Save a session to the database.

* `session` (TeamSession): The session to save

Save a session to the database asynchronously.

* `session` (TeamSession): The session to save

* `session_id` (str): Session ID to delete

### `get_session_name`

Get the session name for the given session ID.

* `session_id` (Optional\[str]): Session ID to use (if not provided, the current session is used)

* `str`: The session name

### `set_session_name`

Set the session name.

* `session_id` (Optional\[str]): Session ID to use (if not provided, the current session is used)
* `autogenerate` (bool): Whether to autogenerate the name
* `session_name` (Optional\[str]): The name to set

* `TeamSession`: The updated session

### `get_session_state`

Get the session state for the given session ID.

* `session_id` (Optional\[str]): Session ID to use (if not provided, the current session is used)

* `Dict[str, Any]`: The session state

### `update_session_state`

Update the session state for the given session ID.

* `session_id` (str): Session ID to use
* `session_state_updates` (Dict\[str, Any]): The session state keys and values to update. Overwrites the existing session state.

* `Dict[str, Any]`: The updated session state

### `get_session_metrics`

Get the session metrics for the given session ID.

* `session_id` (Optional\[str]): Session ID to use (if not provided, the current session is used)

* `Optional[Metrics]`: The session metrics

### `get_chat_history`

Get the chat history for the given session ID.

* `session_id` (Optional\[str]): Session ID to use (if not provided, the current session is used)

* `List[Message]`: The chat history

### `get_messages_for_session`

Get the messages for the given session ID.

* `session_id` (Optional\[str]): Session ID to use (if not provided, the current session is used)

* `List[Message]`: The messages for the session

---

## Baidu Search Tools

**URL:** llms-txt#baidu-search-tools

**Contents:**
- Code
- Usage

Source: https://docs.agno.com/examples/concepts/tools/search/baidusearch

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Set your API key">
    
  </Step>

<Step title="Install libraries">
    
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Set your API key">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## HackerNews Team

**URL:** llms-txt#hackernews-team

**Contents:**
- Code
- Usage

Source: https://docs.agno.com/examples/use-cases/teams/hackernews_team

This example shows how to create a HackerNews team that can aggregate, curate, and discuss trending topics from HackerNews.

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Install required libraries">
    
  </Step>

<Step title="Set environment variables">
    
  </Step>

<Step title="Run the agent">
    
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Install required libraries">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Set environment variables">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run the agent">
```

---

## Pre-hooks and Post-hooks

**URL:** llms-txt#pre-hooks-and-post-hooks

**Contents:**
- Pre-hooks
  - Common Use Cases
  - Basic Example

Source: https://docs.agno.com/concepts/teams/pre-hooks-and-post-hooks

Learn about using pre-hooks and post-hooks with your teams.

Pre-hooks and post-hooks are a simple way to validate or modify the input and output of an Team run.

Pre-hooks execute **before** your Team processes the input, giving you complete control over what reaches the LLM.

They're perfect for implementing validation, security checks, or any data preprocessing task against the input your Team receives.

* Validate format, length, content or any other property of the input.
* Remove or mask sensitive information.
* Normalize input data.

**Data Preprocessing**

* Transform input format or structure.
* Enrich input with additional context.
* Apply any other business logic before sending the input to the LLM.

Let's create a very simple pre-hook that validates the input length and raises an error if it's too long:

````python  theme={null}
from agno.team import Team
from agno.exceptions import CheckTrigger, InputCheckError

---

## What is Reasoning?

**URL:** llms-txt#what-is-reasoning?

**Contents:**
- ReAct: Reason and Act
- Reasoning Models
  - Example

Source: https://docs.agno.com/concepts/reasoning/overview

Reasoning gives Agents the ability to "think" before responding and "analyze" the results of their actions (i.e. tool calls), greatly improving the Agents' ability to solve problems that require sequential tool calls.

Reasoning Agents go through an internal chain of thought before responding, working through different ideas, validating and correcting as needed.

## ReAct: Reason and Act

At the core of effective reasoning lies the **ReAct** (Reason and Act) methodology - a paradigm where agents alternate between reasoning about a problem and taking actions (like calling tools) to gather information or execute tasks. This iterative process allows agents to break down complex problems into manageable steps, validate their assumptions through action, and adjust their approach based on real-world feedback.

In Agno, ReAct principles are embedded throughout our reasoning implementations.
Whether an agent is using reasoning models to think through a problem, or employing reasoning tools to structure their thought process, they follow this fundamental pattern of reasoning → acting → observing → reasoning again until reaching a solution.

Agno supports 3 approaches to reasoning:

1. [Reasoning Models](#reasoning-models)
2. [Reasoning Tools](#reasoning-tools)
3. [Reasoning Agents](#reasoning-agents)

Which approach works best will depend on your use case, we recommend trying them all and immersing yourself in this new era of Reasoning Agents!

Reasoning models are a separate class of large language models pre-trained to think before they answer. They produce an internal chain of thought before responding. Examples of reasoning models include OpenAI o-series, Claude 3.7 sonnet in extended-thinking mode, Gemini 2.0 flash thinking and DeepSeek-R1.

Reasoning at the model layer is all about what the model does **before it starts generating a final response**. Reasoning models excel at single-shot use-cases. They're perfect for solving hard problems (coding, math, physics) that don't require multiple turns, or calling tools sequentially.

You can try any supported Agno model and if that model has reasoning capabilities, it will be used to reason about the problem.

```python o3_mini.py theme={null}
from agno.agent import Agent
from agno.models.openai import OpenAIChat

---

## OpenAI Responses

**URL:** llms-txt#openai-responses

**Contents:**
- Authentication
- Example
- Parameters

Source: https://docs.agno.com/concepts/models/openai-responses

Learn how to use OpenAI Responses with Agno.

`OpenAIResponses` is a class for interacting with OpenAI models using the Responses API. This class provides a streamlined interface for working with OpenAI's newer Responses API, which is distinct from the traditional Chat API. It supports advanced features like tool use, file processing, and knowledge retrieval.

Set your `OPENAI_API_KEY` environment variable. You can get one [from OpenAI here](https://platform.openai.com/account/api-keys).

Use `OpenAIResponses` with your `Agent`:

<CodeGroup>
  
</CodeGroup>

<Note> View more examples [here](/examples/models/openai/responses/basic). </Note>

For more information, please refer to the [OpenAI Responses docs](https://platform.openai.com/docs/api-reference/responses) as well.

| Parameter               | Type                                               | Default             | Description                                                                       |
| ----------------------- | -------------------------------------------------- | ------------------- | --------------------------------------------------------------------------------- |
| `id`                    | `str`                                              | `"gpt-5-mini"`      | The id of the OpenAI model to use with Responses API                              |
| `name`                  | `str`                                              | `"OpenAIResponses"` | The name of the model                                                             |
| `provider`              | `str`                                              | `"OpenAI"`          | The provider of the model                                                         |
| `instructions`          | `Optional[str]`                                    | `None`              | System-level instructions for the assistant                                       |
| `response_format`       | `Optional[Union[str, Dict]]`                       | `None`              | Response format specification for structured outputs                              |
| `temperature`           | `Optional[float]`                                  | `None`              | Controls randomness in the model's output (0.0 to 2.0)                            |
| `top_p`                 | `Optional[float]`                                  | `None`              | Controls diversity via nucleus sampling (0.0 to 1.0)                              |
| `max_completion_tokens` | `Optional[int]`                                    | `None`              | Maximum number of completion tokens to generate                                   |
| `truncation_strategy`   | `Optional[Dict[str, Any]]`                         | `None`              | Strategy for truncating messages when they exceed context limits                  |
| `tool_choice`           | `Optional[Union[str, Dict]]`                       | `None`              | Controls which function is called by the model                                    |
| `parallel_tool_calls`   | `Optional[bool]`                                   | `None`              | Whether to enable parallel function calling                                       |
| `metadata`              | `Optional[Dict[str, str]]`                         | `None`              | Developer-defined metadata to associate with the response                         |
| `api_key`               | `Optional[str]`                                    | `None`              | The API key for authenticating with OpenAI (defaults to OPENAI\_API\_KEY env var) |
| `organization`          | `Optional[str]`                                    | `None`              | The organization ID to use for requests                                           |
| `base_url`              | `Optional[Union[str, httpx.URL]]`                  | `None`              | The base URL for the OpenAI API                                                   |
| `timeout`               | `Optional[float]`                                  | `None`              | Request timeout in seconds                                                        |
| `max_retries`           | `Optional[int]`                                    | `None`              | Maximum number of retries for failed requests                                     |
| `default_headers`       | `Optional[Any]`                                    | `None`              | Default headers to include in all requests                                        |
| `http_client`           | `Optional[Union[httpx.Client, httpx.AsyncClient]]` | `None`              | HTTP client instance for making requests                                          |

`OpenAIResponses` is a subclass of the [Model](/reference/models/model) class and has access to the same params.

**Examples:**

Example 1 (unknown):
```unknown

```

Example 2 (unknown):
```unknown
</CodeGroup>

## Example

Use `OpenAIResponses` with your `Agent`:

<CodeGroup>
```

---

## Now returns Step(s) to execute

**URL:** llms-txt#now-returns-step(s)-to-execute

def research_router(step_input: StepInput) -> List[Step]:
    """
    Decide which research method to use based on the input topic.
    Returns a list containing the step(s) to execute.
    """
    # Use the original workflow message if this is the first step
    topic = step_input.previous_step_content or step_input.input or ""
    topic = topic.lower()

# Check if the topic is tech/startup related - use HackerNews
    tech_keywords = [
        "startup",
        "programming",
        "ai",
        "machine learning",
        "software",
        "developer",
        "coding",
        "tech",
        "silicon valley",
        "venture capital",
        "cryptocurrency",
        "blockchain",
        "open source",
        "github",
    ]

if any(keyword in topic for keyword in tech_keywords):
        print(f"🔍 Tech topic detected: Using HackerNews research for '{topic}'")
        return [research_hackernews]
    else:
        print(f"🌐 General topic detected: Using web research for '{topic}'")
        return [research_web]

workflow = Workflow(
    name="Intelligent Research Workflow",
    description="Automatically selects the best research method based on topic, then publishes content",
    steps=[
        Router(
            name="research_strategy_router",
            selector=research_router,
            choices=[research_hackernews, research_web],
            description="Intelligently selects research method based on topic",
        ),
        publish_content,
    ],
)

if __name__ == "__main__":
    workflow.print_response(
        "Latest developments in artificial intelligence and machine learning"
    )
```

This was a synchronous non-streaming example of this pattern. To checkout async and streaming versions, see the cookbooks-

* [Router Steps Workflow (sync streaming)](https://github.com/agno-agi/agno/tree/main/cookbook/workflows/_05_workflows_conditional_branching/sync/router_steps_workflow_stream.py)
* [Router Steps Workflow (async non-streaming)](https://github.com/agno-agi/agno/tree/main/cookbook/workflows/_05_workflows_conditional_branching/async/router_steps_workflow.py)
* [Router Steps Workflow (async streaming)](https://github.com/agno-agi/agno/tree/main/cookbook/workflows/_05_workflows_conditional_branching/async/router_steps_workflow_stream.py)

---

## Define the custom async retriever

**URL:** llms-txt#define-the-custom-async-retriever

**Contents:**
  - Explanation
- Developer Resources

async def retriever(
    query: str, agent: Optional[Agent] = None, num_documents: int = 5, **kwargs
) -> Optional[list[dict]]:
    """
    Custom async retriever function to search the vector database for relevant documents.
    """
    try:
        qdrant_client = AsyncQdrantClient(path="tmp/qdrant")
        query_embedding = embedder.get_embedding(query)
        results = await qdrant_client.query_points(
            collection_name="thai-recipes",
            query=query_embedding,
            limit=num_documents,
        )
        results_dict = results.model_dump()
        if "points" in results_dict:
            return results_dict["points"]
        else:
            return None
    except Exception as e:
        print(f"Error during vector database search: {str(e)}")
        return None

async def main():
    """Async main function to demonstrate agent usage."""
    agent = Agent(
        knowledge_retriever=retriever,
        search_knowledge=True,
        instructions="Search the knowledge base for information",
    )
    # Example query
    query = "List down the ingredients to make Massaman Gai"
    await agent.aprint_response(query, markdown=True)

if __name__ == "__main__":
    asyncio.run(main())
```

1. **Embedder and Vector Database Setup**: We start by defining an embedder and initializing a connection to a vector database. This setup is crucial for converting queries into embeddings and storing them in the database.

2. **Loading the Knowledge Base**: The knowledge base is loaded with PDF documents. This step involves converting the documents into embeddings and storing them in the vector database.

3. **Custom Retriever Function**: The `retriever` function is defined to handle the retrieval of documents. It takes a query, converts it into an embedding, and searches the vector database for relevant documents.

4. **Agent Initialization**: An agent is initialized with the custom retriever. The agent uses this retriever to search the knowledge base and retrieve information.

5. **Example Query**: The `main` function demonstrates how to use the agent to perform a query and retrieve information from the knowledge base.

## Developer Resources

* View [Sync Retriever](https://github.com/agno-agi/agno/tree/main/cookbook/agent_concepts/knowledge/custom/retriever.py)
* View [Async Retriever](https://github.com/agno-agi/agno/tree/main/cookbook/agent_concepts/knowledge/custom/async_retriever.py)

---

## Batch processing for large datasets

**URL:** llms-txt#batch-processing-for-large-datasets

**Contents:**
  - Issue: Running Out of Memory

def load_content_in_batches(knowledge, content_dir, batch_size=10):
    files = [f for f in os.listdir(content_dir) if f.endswith('.pdf')]
    
    for i in range(0, len(files), batch_size):
        batch_files = files[i:i+batch_size]
        print(f"Processing batch {i//batch_size + 1}")
        
        for file in batch_files:
            knowledge.add_content(
                path=os.path.join(content_dir, file),
                skip_if_exists=True
            )
python  theme={null}

**Examples:**

Example 1 (unknown):
```unknown
### Issue: Running Out of Memory

**What's happening:** Loading too many large files at once, or chunk sizes are too large.

**Quick fixes:**

1. Process content in smaller batches (see code above)
2. Reduce chunk size in your chunking strategy
3. Use `include` and `exclude` patterns to limit what gets processed
4. Clear old/outdated content regularly with `knowledge.remove_content_by_id()`
```

---

## Run workflow with additional_data

**URL:** llms-txt#run-workflow-with-additional_data

**Contents:**
- Developer Resources

workflow.print_response(
    input="AI trends in 2024",
    additional_data={
        "user_email": "kaustubh@agno.com",
        "priority": "high",
        "client_type": "enterprise",
        "budget": "$50000",
        "deadline": "2024-12-15"
    },
    markdown=True,
    stream=True
)
```

## Developer Resources

* [Step with Function and Additional Data](/examples/concepts/workflows/06_workflows_advanced_concepts/step_with_function_additional_data)

---

## Maxim

**URL:** llms-txt#maxim

**Contents:**
- Integrating Agno with Maxim
- Prerequisites
- Sending Traces to Maxim
  - Example: Basic Maxim Integration

Source: https://docs.agno.com/integrations/observability/maxim

Connect Agno with Maxim to monitor, trace, and evaluate your agent's activity and performance.

## Integrating Agno with Maxim

Maxim AI provides comprehensive agent monitoring, evaluation, and observability for your Agno applications. With Maxim's one-line integration, you can easily trace and analyse agent interactions, performance metrics, and more.

1. **Install Dependencies**

Ensure you have the necessary packages installed:

Or install Maxim separately:

2. **Setup Maxim Account**

* Sign up for an account at [Maxim](https://getmaxim.ai/).
   * Generate your API key from the Maxim dashboard.
   * Create a repository to store your traces.

3. **Set Environment Variables**

Configure your environment with the Maxim API key:

## Sending Traces to Maxim

### Example: Basic Maxim Integration

This example demonstrates how to instrument your Agno agent with Maxim and send traces for monitoring and evaluation.

```python  theme={null}
from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.yfinance import YFinanceTools

try:
    from maxim import Maxim
    from maxim.logger.agno import instrument_agno
except ImportError:
    raise ImportError(
        "`maxim` not installed. Please install using `pip install maxim-py`"
    )

**Examples:**

Example 1 (unknown):
```unknown
Or install Maxim separately:
```

Example 2 (unknown):
```unknown
2. **Setup Maxim Account**

   * Sign up for an account at [Maxim](https://getmaxim.ai/).
   * Generate your API key from the Maxim dashboard.
   * Create a repository to store your traces.

3. **Set Environment Variables**

   Configure your environment with the Maxim API key:
```

Example 3 (unknown):
```unknown
## Sending Traces to Maxim

### Example: Basic Maxim Integration

This example demonstrates how to instrument your Agno agent with Maxim and send traces for monitoring and evaluation.
```

---

## Extract and cut video segments

**URL:** llms-txt#extract-and-cut-video-segments

def extract_segments(response_text):
    import re

segments_pattern = r"\|\s*(\d+:\d+)\s*\|\s*(\d+:\d+)\s*\|\s*(.*?)\s*\|\s*(\d+)\s*\|"
    segments: list[dict] = []

for match in re.finditer(segments_pattern, str(response_text)):
        start_time = match.group(1)
        end_time = match.group(2)
        description = match.group(3)
        score = int(match.group(4))

start_seconds = sum(x * int(t) for x, t in zip([60, 1], start_time.split(":")))
        end_seconds = sum(x * int(t) for x, t in zip([60, 1], end_time.split(":")))
        duration = end_seconds - start_seconds

if 15 <= duration <= 60 and score > 7:
            output_path = output_dir / f"short_{len(segments) + 1}.mp4"

command = [
                "ffmpeg",
                "-ss",
                str(start_seconds),
                "-i",
                video_path,
                "-t",
                str(duration),
                "-vf",
                "scale=1080:1920,setsar=1:1",
                "-c:v",
                "libx264",
                "-c:a",
                "aac",
                "-y",
                str(output_path),
            ]

try:
                subprocess.run(command, check=True)
                segments.append(
                    {"path": output_path, "description": description, "score": score}
                )
            except subprocess.CalledProcessError:
                print(f"Failed to process segment: {start_time} - {end_time}")

logger.debug(f"{response.content}")

---

## Create coordinated reasoning RAG team

**URL:** llms-txt#create-coordinated-reasoning-rag-team

**Contents:**
- Usage

coordinated_reasoning_team = Team(
    name="Coordinated Reasoning RAG Team",
    model=Claude(id="claude-sonnet-4-20250514"),
    members=[
        information_gatherer,
        reasoning_analyst,
        evidence_evaluator,
        response_coordinator,
    ],
    instructions=[
        "Work together to provide comprehensive, well-reasoned responses.",
        "Information Gatherer: First search and gather all relevant information.",
        "Reasoning Analyst: Then apply structured reasoning to analyze the information.",
        "Evidence Evaluator: Evaluate the evidence quality and identify any gaps.",
        "Response Coordinator: Finally synthesize everything into a clear, reasoned response.",
        "All agents should use reasoning tools to structure their contributions.",
        "Show your reasoning process transparently in responses.",
    ],
    show_members_responses=True,
    markdown=True,
)

async def async_reasoning_demo():
    """Demonstrate async coordinated reasoning RAG with streaming."""
    print("🧠 Async Coordinated Reasoning RAG Team Demo")
    print("=" * 60)

query = "What are Agents and how do they work with tools? Explain the reasoning behind their design."

# Add documentation content
    await knowledge.add_contents_async(urls=["https://docs.agno.com/introduction/agents.md"])

# Run async with streaming and reasoning
    await coordinated_reasoning_team.aprint_response(
        query, stream=True, stream_events=True, show_full_reasoning=True
    )

def sync_reasoning_demo():
    """Demonstrate sync coordinated reasoning RAG."""
    print("🧠 Coordinated Reasoning RAG Team Demo")
    print("=" * 50)

query = "What are Agents and how do they work with tools? Explain the reasoning behind their design."

# Add documentation content
    knowledge.add_contents(urls=["https://docs.agno.com/introduction/agents.md"])

# Run with detailed reasoning output
    coordinated_reasoning_team.print_response(
        query, stream=True, stream_events=True, show_full_reasoning=True
    )

if __name__ == "__main__":
    # Choose which demo to run
    # asyncio.run(async_reasoning_demo())

sync_reasoning_demo()
bash  theme={null}
    pip install agno cohere lancedb tantivy sqlalchemy
    bash  theme={null}
    export ANTHROPIC_API_KEY=****
    export CO_API_KEY=****
    bash  theme={null}
    python cookbook/examples/teams/search_coordination/02_coordinated_reasoning_rag.py
    ```
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Install required libraries">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Set environment variables">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run the agent">
```

---

## JSON Reader

**URL:** llms-txt#json-reader

Source: https://docs.agno.com/reference/knowledge/reader/json

JSONReader is a reader class that allows you to read data from JSON files.

<Snippet file="json-reader-reference.mdx" />

---

## Use in workflow

**URL:** llms-txt#use-in-workflow

**Contents:**
- Developer Resources

workflow = Workflow(
    name="Enhanced Research Workflow",
    steps=[
        Step(name="research_hackernews", agent=hackernews_agent),
        Step(name="research_web", agent=web_agent),
        Step(name="comprehensive_report", executor=create_comprehensive_report),  # Accesses both previous steps
        Step(name="final_reasoning", agent=reasoning_agent),
    ],
)
python  theme={null}
  parallel_step_output = step_input.get_step_content("parallel_step_name")
  python  theme={null}
  {
      "individual_step_name_1": "output_from_individual_step_1",
      "individual_step_name_2": "output_from_individual_step_2",
  }
  ```
</Note>

## Developer Resources

* [Access Multiple Previous Steps Output](/examples/concepts/workflows/06_workflows_advanced_concepts/access_multiple_previous_steps_output)

**Examples:**

Example 1 (unknown):
```unknown
**Available Methods**

* `step_input.get_step_content("step_name")` - Get content from specific step by name
* `step_input.get_all_previous_content()` - Get all previous step content combined
* `step_input.workflow_message` - Access the original workflow input message
* `step_input.previous_step_content` - Get content from immediate previous step

<Note>
  In case of `Parallel` step, when you do `step_input.get_step_content("parallel_step_name")`, it will return a dict with each key as `individual_step_name` for all the outputs from the steps defined in parallel.
  Example:
```

Example 2 (unknown):
```unknown
`parallel_step_output` will be a dict with each key as `individual_step_name` for all the outputs from the steps defined in parallel.
```

---

## Async Accuracy Evaluation

**URL:** llms-txt#async-accuracy-evaluation

**Contents:**
- Code

Source: https://docs.agno.com/examples/concepts/evals/accuracy/accuracy_async

Learn how to run accuracy evaluations asynchronously for better performance.

This example shows how to run an Accuracy evaluation asynchronously.

```python  theme={null}
"""This example shows how to run an Accuracy evaluation asynchronously."""

import asyncio
from typing import Optional

from agno.agent import Agent
from agno.eval.accuracy import AccuracyEval, AccuracyResult
from agno.models.openai import OpenAIChat
from agno.tools.calculator import CalculatorTools

evaluation = AccuracyEval(
    model=OpenAIChat(id="o4-mini"),
    agent=Agent(
        model=OpenAIChat(id="gpt-5-mini"),
        tools=[CalculatorTools()],
    ),
    input="What is 10*5 then to the power of 2? do it step by step",
    expected_output="2500",
    additional_guidelines="Agent output should include the steps and the final answer.",
    num_iterations=3,
)

---

## First give information to the team

**URL:** llms-txt#first-give-information-to-the-team

**Contents:**
- Ask question in German

## Ask question in German
multi_lingual_q_and_a_team.print_response(
    "Hallo, wie heißt du? Meine Name ist John.", stream=True, session_id=session_id
)

---

## News Agency Team

**URL:** llms-txt#news-agency-team

**Contents:**
- Code
- Usage

Source: https://docs.agno.com/examples/use-cases/teams/news_agency_team

This example shows how to create a news agency team that can search the web, write an article, and edit it.

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Install required libraries">
    
  </Step>

<Step title="Set environment variables">
    
  </Step>

<Step title="Run the agent">
    
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Install required libraries">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Set environment variables">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run the agent">
```

---

## Searxng

**URL:** llms-txt#searxng

**Contents:**
- Example

Source: https://docs.agno.com/concepts/tools/toolkits/search/searxng

**Searxng** enables an Agent to search the web for a query, scrape a website, or crawl a website.

```python cookbook/tools/searxng_tools.py theme={null}
from agno.agent import Agent
from agno.tools.searxng import SearxngTools

---

## Configure Scenario defaults (model for user simulator and judge)

**URL:** llms-txt#configure-scenario-defaults-(model-for-user-simulator-and-judge)

**Contents:**
- Usage

scenario.configure(default_model="openai/gpt-4.1-mini")

@pytest.mark.agent_test
@pytest.mark.asyncio
async def test_vegetarian_recipe_agent() -> None:
    # 1. Define an AgentAdapter to wrap your agent
    class VegetarianRecipeAgentAdapter(scenario.AgentAdapter):
        agent: Agent

def __init__(self) -> None:
            self.agent = Agent(
                model=OpenAIChat(id="gpt-4.1-mini"),
                markdown=True,
                debug_mode=True,
                instructions="You are a vegetarian recipe agent.",
            )

async def call(self, input: scenario.AgentInput) -> scenario.AgentReturnTypes:
            response = self.agent.run(
                message=input.last_new_user_message_str(), # Pass only the last user message
                session_id=input.thread_id, # Pass the thread id, this allows the agent to track history
            )
            return response.content

# 2. Run the scenario simulation
    result = await scenario.run(
        name="dinner recipe request",
        description="User is looking for a vegetarian dinner idea.",
        agents=[
            VegetarianRecipeAgentAdapter(),
            scenario.UserSimulatorAgent(),
            scenario.JudgeAgent(
                criteria=[
                    "Agent should not ask more than two follow-up questions",
                    "Agent should generate a recipe",
                    "Recipe should include a list of ingredients",
                    "Recipe should include step-by-step cooking instructions",
                    "Recipe should be vegetarian and not include any sort of meat",
                ]
            ),
        ],
    )

# 3. Assert and inspect the result
    assert result.success
bash  theme={null}
    export OPENAI_API_KEY=xxx
    export LANGWATCH_API_KEY=xxx # Optional, required for Simulation monitoring
    bash  theme={null}
    pip install -U openai agno langwatch-scenario pytest pytest-asyncio
    # or
    uv add agno langwatch-scenario openai pytest
    bash  theme={null}
    pytest cookbook/agent_concepts/other/scenario_testing.py
    ```
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Set your API key">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
```

---

## Apify Tools

**URL:** llms-txt#apify-tools

**Contents:**
- Code
- Usage

Source: https://docs.agno.com/examples/concepts/tools/others/apify

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Set your API key">
    
  </Step>

<Step title="Install libraries">
    
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Set your API key">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Browserbase

**URL:** llms-txt#browserbase

**Contents:**
- Prerequisites
- Example
- Toolkit Params
- Toolkit Functions
- Developer Resources

Source: https://docs.agno.com/concepts/tools/toolkits/web_scrape/browserbase

**BrowserbaseTools** enable an Agent to automate browser interactions using Browserbase, a headless browser service.

The following example requires Browserbase API credentials after you signup [here](https://www.browserbase.com/), and the Playwright library.

The following agent will use Browserbase to visit `https://quotes.toscrape.com` and extract content. Then navigate to page two of the website and get quotes from there as well.

<Tip>View the [Startup Analyst MCP agent](/examples/concepts/tools/mcp/stagehand)</Tip>

| Parameter                 | Type   | Default | Description                                                                                                                                                                                           |
| ------------------------- | ------ | ------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| `api_key`                 | `str`  | `None`  | Browserbase API key. If not provided, uses BROWSERBASE\_API\_KEY env var.                                                                                                                             |
| `project_id`              | `str`  | `None`  | Browserbase project ID. If not provided, uses BROWSERBASE\_PROJECT\_ID env var.                                                                                                                       |
| `base_url`                | `str`  | `None`  | Custom Browserbase API endpoint URL. Only use this if you're using a self-hosted Browserbase instance or need to connect to a different region. If not provided, uses BROWSERBASE\_BASE\_URL env var. |
| `enable_navigate_to`      | `bool` | `True`  | Enable the navigate\_to functionality.                                                                                                                                                                |
| `enable_screenshot`       | `bool` | `True`  | Enable the screenshot functionality.                                                                                                                                                                  |
| `enable_get_page_content` | `bool` | `True`  | Enable the get\_page\_content functionality.                                                                                                                                                          |
| `enable_close_session`    | `bool` | `True`  | Enable the close\_session functionality.                                                                                                                                                              |
| `all`                     | `bool` | `False` | Enable all functionality.                                                                                                                                                                             |

| Function           | Description                                                                                                                                           |
| ------------------ | ----------------------------------------------------------------------------------------------------------------------------------------------------- |
| `navigate_to`      | Navigates to a URL. Takes a URL and an optional connect\_url parameter.                                                                               |
| `screenshot`       | Takes a screenshot of the current page. Takes a path to save the screenshot, a boolean for full-page capture, and an optional connect\_url parameter. |
| `get_page_content` | Gets the HTML content of the current page. Takes an optional connect\_url parameter.                                                                  |
| `close_session`    | Closes a browser session.                                                                                                                             |

## Developer Resources

* View [Tools](https://github.com/agno-agi/agno/blob/main/libs/agno/agno/tools/browserbase.py)
* View [Cookbook](https://github.com/agno-agi/agno/tree/main/cookbook/tools/browserbase_tools.py)

**Examples:**

Example 1 (unknown):
```unknown
## Example

The following agent will use Browserbase to visit `https://quotes.toscrape.com` and extract content. Then navigate to page two of the website and get quotes from there as well.
```

---

## Create research team with tools and context management

**URL:** llms-txt#create-research-team-with-tools-and-context-management

**Contents:**
- Usage

research_team = Team(
    name="Research Team",
    model=OpenAIChat("gpt-4o"),
    members=[tech_researcher, business_analyst],
    tools=[DuckDuckGoTools()],  # Team uses DuckDuckGo for research
    description="Research team that investigates topics and provides analysis.",
    instructions=dedent("""
        You are a research coordinator that investigates topics comprehensively.
        
        Your Process:
        1. Use DuckDuckGo to search for information on the topic
        2. Delegate detailed analysis to the appropriate specialist
        3. Synthesize research findings with specialist insights
        
        Guidelines:
        - Always start with web research using your DuckDuckGo tools
        - Choose the right specialist based on the topic (tech vs business)
        - Combine your research with specialist analysis
        - Provide comprehensive, well-sourced responses
    """).strip(),
    db=SqliteDb(db_file="tmp/research_team.db"),
    session_id="research_session",
    add_history_to_context=True,
    num_history_runs=6,  # Load last 6 research queries
    max_tool_calls_from_history=3,  # Keep only last 3 research results
    markdown=True,
)

research_team.print_response("What are the latest developments in AI agents?", stream=True)
research_team.print_response("How is the tech market performing this quarter?", stream=True)
research_team.print_response("What are the trends in LLM applications?", stream=True)
research_team.print_response("What companies are leading in AI infrastructure?", stream=True)
bash  theme={null}
    pip install -U agno openai ddgs sqlalchemy
    bash Mac/Linux theme={null}
        export OPENAI_API_KEY="your_openai_api_key_here"
      bash Windows theme={null}
        $Env:OPENAI_API_KEY="your_openai_api_key_here"
      bash  theme={null}
    touch filter_tool_calls_from_history.py
    bash Mac/Linux theme={null}
      python filter_tool_calls_from_history.py
      bash Windows theme={null}
      python filter_tool_calls_from_history.py
      ```
    </CodeGroup>
  </Step>

<Step title="Find All Cookbooks">
    Explore all the available cookbooks in the Agno repository. Click the link below to view the code on GitHub:

<Link href="https://github.com/agno-agi/agno/tree/main/cookbook/teams/context_management" target="_blank">
      Agno Cookbooks on GitHub
    </Link>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Install libraries">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Export your OpenAI API key">
    <CodeGroup>
```

Example 3 (unknown):
```unknown

```

Example 4 (unknown):
```unknown
</CodeGroup>
  </Step>

  <Step title="Create a Python file">
    Create a Python file and add the above code.
```

---

## GCS for Workflows

**URL:** llms-txt#gcs-for-workflows

**Contents:**
- Usage

Source: https://docs.agno.com/examples/concepts/db/gcs/gcs_for_workflow

Agno supports using Google Cloud Storage (GCS) as a storage backend for Workflows using the `GcsJsonDb` class. This storage backend stores session data as JSON blobs in a GCS bucket.

Configure your workflow with GCS storage to enable cloud-based session persistence.

```python gcs_for_workflow.py theme={null}
import uuid
import google.auth
from agno.agent import Agent
from agno.db.gcs_json import GcsJsonDb
from agno.models.openai import OpenAIChat
from agno.team import Team
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.hackernews import HackerNewsTools
from agno.workflow.step import Step
from agno.workflow.workflow import Workflow

---

## Search with metadata filters for more precise results

**URL:** llms-txt#search-with-metadata-filters-for-more-precise-results

results = knowledge.search(
    query="vacation policy",
    max_results=5,
    filters={"department": "hr", "type": "policy"}
)

---

## Create coordinated RAG team

**URL:** llms-txt#create-coordinated-rag-team

**Contents:**
- Usage

coordinated_rag_team = Team(
    name="Coordinated RAG Team",
    model=Claude(id="claude-3-7-sonnet-latest"),
    members=[knowledge_searcher, content_analyzer, response_synthesizer],
    instructions=[
        "Work together to provide comprehensive responses using the knowledge base.",
        "Knowledge Searcher: First search for relevant information thoroughly.",
        "Content Analyzer: Then analyze and organize the retrieved content.",
        "Response Synthesizer: Finally create a well-structured response with sources.",
        "Ensure all responses include proper citations and are factually accurate.",
    ],
    show_members_responses=True,
    markdown=True,
)

def main():
    """Run the coordinated agentic RAG team example."""
    print("🤖 Coordinated Agentic RAG Team Demo")
    print("=" * 50)

# Example query that benefits from coordinated search and analysis
    query = "What are Agents and how do they work with tools and knowledge?"

# Run the coordinated team
    coordinated_rag_team.print_response(
        query, stream=True, stream_events=True
    )

if __name__ == "__main__":
    main()
bash  theme={null}
    pip install agno cohere lancedb tantivy sqlalchemy
    bash  theme={null}
    export ANTHROPIC_API_KEY=****
    export CO_API_KEY=****
    bash  theme={null}
    python cookbook/examples/teams/search_coordination/01_coordinated_agentic_rag.py
    ```
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Install required libraries">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Set environment variables">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run the agent">
```

---

## team.print_response("I like to play basketball and hike in the mountains")

**URL:** llms-txt#team.print_response("i-like-to-play-basketball-and-hike-in-the-mountains")

**Contents:**
- Usage

bash  theme={null}
    pip install agno psycopg2-binary
    bash  theme={null}
    export OPENAI_API_KEY=****
    bash  theme={null}
    cookbook/run_pgvector.sh
    bash  theme={null}
    python 03_session_summary.py
    ```
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Install required libraries">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Set environment variables">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Start PostgreSQL database">
```

Example 4 (unknown):
```unknown
</Step>

  <Step title="Run the agent">
```

---

## Firestore

**URL:** llms-txt#firestore

**Contents:**
- Usage

Source: https://docs.agno.com/concepts/db/firestore

Learn to use Firestore as a database for your Agents

Agno supports using [Firestore](https://cloud.google.com/firestore) as a database with the `FirestoreDb` class.

You can get started with Firestore following their [Get Started guide](https://firebase.google.com/docs/firestore/quickstart).

You need to provide a `project_id` parameter to the `FirestoreDb` class. Firestore will connect automatically using your Google Cloud credentials.

```python firestore_for_agent.py theme={null}
from agno.agent import Agent
from agno.db.firestore import FirestoreDb

PROJECT_ID = "agno-os-test"  # Use your project ID here

---

## SingleStoreDb

**URL:** llms-txt#singlestoredb

Source: https://docs.agno.com/reference/storage/singlestore

`SingleStoreDb` is a class that implements the Db interface using SingleStore  as the backend storage system. It provides high-performance, distributed storage for agent sessions with support for JSON data types and schema versioning.

<Snippet file="db-singlestore-params.mdx" />

<Snippet file="db-new-bulk-methods.mdx" />

---

## SQLite

**URL:** llms-txt#sqlite

**Contents:**
- Usage

Source: https://docs.agno.com/concepts/db/sqlite

Learn to use Sqlite as a database for your Agents

Agno supports using [Sqlite](https://www.sqlite.org) as a database with the `SqliteDb` class.

```python sqlite_for_agent.py theme={null}
from agno.agent import Agent
from agno.db.sqlite import SqliteDb

---

## JWT Middleware

**URL:** llms-txt#jwt-middleware

**Contents:**
- Quick Start
- Configuration Options
- Token Sources
- Parameter Injection
- Security Features
- Excluded Routes
- Developer Resources

Source: https://docs.agno.com/agent-os/customize/middleware/jwt

Add JWT authentication and parameter injection to your AgentOS application

AgentOS provides built-in JWT middleware for authentication, parameter injection, and automatic claims extraction. This middleware can extract JWT tokens from Authorization headers or cookies and automatically inject values into the AgentOS endpoints.

The JWT middleware provides two main features:

1. **Token Validation**: Validates JWT tokens and handles authentication
2. **Parameter Injection**: Automatically injects user\_id, session\_id, and custom claims into endpoint parameters

## Configuration Options

| Parameter              | Description                                                                                  | Default                                                                  |
| ---------------------- | -------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------ |
| `secret_key`           | Secret key for JWT verification                                                              | Optional, will use `JWT_SECRET_KEY` environment variable if not provided |
| `algorithm`            | JWT algorithm (HS256, RS256, etc.)                                                           | "HS256"                                                                  |
| `token_source`         | Where to extract token from. `HEADER`, `COOKIE`, or `BOTH`.                                  | `TokenSource.HEADER`                                                     |
| `token_header_key`     | Key to use for the Authorization header (only used when token\_source is `HEADER` or `BOTH`) | "Authorization"                                                          |
| `cookie_name`          | Cookie name when using cookies (only used when token\_source is `COOKIE` or `BOTH`)          | "access\_token"                                                          |
| `validate`             | Enable token validation                                                                      | `True`                                                                   |
| `excluded_route_paths` | Routes to skip middleware (useful for health checks, etc.)                                   | `[]`                                                                     |
| `scopes_claim`         | JWT claim for scopes                                                                         | `None`                                                                   |
| `user_id_claim`        | JWT claim for user ID                                                                        | "sub"                                                                    |
| `session_id_claim`     | JWT claim for session ID                                                                     | "session\_id"                                                            |
| `dependencies_claims`  | List of additional claims to extract for `dependencies` parameter                            | `[]`                                                                     |
| `session_state_claims` | List of additional claims to extract for `session_state` parameter                           | `[]`                                                                     |

The middleware supports three token sources:

<Tabs>
  <Tab title="Authorization Header">
    Extract JWT from `Authorization: Bearer <token>` header.

<Tab title="HTTP-Only Cookies">
    Extract JWT from HTTP-only cookies for web applications.

<Tab title="Both Sources">
    Try both header and cookie (header takes precedence).

## Parameter Injection

The middleware automatically injects JWT claims into the request object flowing across your FastAPI state. This is a great way to resolve data from your token into parameters received by your endpoints.

These are the parameters automatically injected by our JWT middleware into your endpoints:

* `user_id`
* `session_id`
* `dependencies`
* `session_state`
  For example, in `/agents/{agent_id}/runs`, the `user_id`, `session_id`, `dependencies` and `session_state` are automatically used if they were extracted from the JWT token.

* Automatically using the `user_id` and `session_id` from your JWT token when running an agent
* Automatically filtering sessions retrieved from `/sessions` endpoints by `user_id` (where applicable)
* Automatically injecting `dependencies` from claims in your JWT token into the agent run, which then is available on tools called by your agent

See the [full example](/examples/agent-os/middleware/jwt-middleware) for more details.

<Note>
  Remember to always use strong secret keys, don't hardcode them anywhere in your code and enable validation in production.
</Note>

**Token Validation**: When `validate=True`, the middleware:

* Verifies JWT signature using the secret key
* Checks token expiration (`exp` claim)
* Returns 401 errors for invalid/expired tokens

<Tip>
  **HTTP-Only Cookies**: When using cookies:

* Set `httponly=True` to prevent JavaScript access (XSS protection)
  * Set `secure=True` for HTTPS-only transmission
  * Set `samesite="strict"` for CSRF protection
</Tip>

Skip middleware for specific routes:

## Developer Resources

* [JWT Middleware with Headers Example](/examples/agent-os/middleware/jwt-middleware)
* [JWT Middleware with Cookies Example](/examples/agent-os/middleware/jwt-cookies)
* [Custom FastAPI with JWT Example](/examples/agent-os/middleware/custom-fastapi-jwt)
* [PyJWT Documentation](https://pyjwt.readthedocs.io/)

**Examples:**

Example 1 (unknown):
```unknown
## Configuration Options

| Parameter              | Description                                                                                  | Default                                                                  |
| ---------------------- | -------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------ |
| `secret_key`           | Secret key for JWT verification                                                              | Optional, will use `JWT_SECRET_KEY` environment variable if not provided |
| `algorithm`            | JWT algorithm (HS256, RS256, etc.)                                                           | "HS256"                                                                  |
| `token_source`         | Where to extract token from. `HEADER`, `COOKIE`, or `BOTH`.                                  | `TokenSource.HEADER`                                                     |
| `token_header_key`     | Key to use for the Authorization header (only used when token\_source is `HEADER` or `BOTH`) | "Authorization"                                                          |
| `cookie_name`          | Cookie name when using cookies (only used when token\_source is `COOKIE` or `BOTH`)          | "access\_token"                                                          |
| `validate`             | Enable token validation                                                                      | `True`                                                                   |
| `excluded_route_paths` | Routes to skip middleware (useful for health checks, etc.)                                   | `[]`                                                                     |
| `scopes_claim`         | JWT claim for scopes                                                                         | `None`                                                                   |
| `user_id_claim`        | JWT claim for user ID                                                                        | "sub"                                                                    |
| `session_id_claim`     | JWT claim for session ID                                                                     | "session\_id"                                                            |
| `dependencies_claims`  | List of additional claims to extract for `dependencies` parameter                            | `[]`                                                                     |
| `session_state_claims` | List of additional claims to extract for `session_state` parameter                           | `[]`                                                                     |

## Token Sources

The middleware supports three token sources:

<Tabs>
  <Tab title="Authorization Header">
    Extract JWT from `Authorization: Bearer <token>` header.
```

Example 2 (unknown):
```unknown
</Tab>

  <Tab title="HTTP-Only Cookies">
    Extract JWT from HTTP-only cookies for web applications.
```

Example 3 (unknown):
```unknown
</Tab>

  <Tab title="Both Sources">
    Try both header and cookie (header takes precedence).
```

Example 4 (unknown):
```unknown
</Tab>
</Tabs>

## Parameter Injection

The middleware automatically injects JWT claims into the request object flowing across your FastAPI state. This is a great way to resolve data from your token into parameters received by your endpoints.

These are the parameters automatically injected by our JWT middleware into your endpoints:

* `user_id`
* `session_id`
* `dependencies`
* `session_state`
  For example, in `/agents/{agent_id}/runs`, the `user_id`, `session_id`, `dependencies` and `session_state` are automatically used if they were extracted from the JWT token.

This is useful for:

* Automatically using the `user_id` and `session_id` from your JWT token when running an agent
* Automatically filtering sessions retrieved from `/sessions` endpoints by `user_id` (where applicable)
* Automatically injecting `dependencies` from claims in your JWT token into the agent run, which then is available on tools called by your agent

See the [full example](/examples/agent-os/middleware/jwt-middleware) for more details.

## Security Features

<Note>
  Remember to always use strong secret keys, don't hardcode them anywhere in your code and enable validation in production.
</Note>

**Token Validation**: When `validate=True`, the middleware:

* Verifies JWT signature using the secret key
* Checks token expiration (`exp` claim)
* Returns 401 errors for invalid/expired tokens

<Tip>
  **HTTP-Only Cookies**: When using cookies:

  * Set `httponly=True` to prevent JavaScript access (XSS protection)
  * Set `secure=True` for HTTPS-only transmission
  * Set `samesite="strict"` for CSRF protection
</Tip>

## Excluded Routes

Skip middleware for specific routes:
```

---

## Toolkit Index

**URL:** llms-txt#toolkit-index

**Contents:**
- Search
- Social
- Web Scraping
- Data
- Local
- Native Model Toolkit
- Additional Toolkits

Source: https://docs.agno.com/concepts/tools/toolkits/toolkits

A **Toolkit** is a collection of functions that can be added to an Agent. The functions in a Toolkit are designed to work together, share internal state and provide a better development experience.

The following **Toolkits** are available to use

<CardGroup cols={3}>
  <Card title="Arxiv" icon="book" iconType="duotone" href="/concepts/tools/toolkits/search/arxiv">
    Tools to read arXiv papers.
  </Card>

<Card title="BaiduSearch" icon="magnifying-glass" iconType="duotone" href="/concepts/tools/toolkits/search/baidusearch">
    Tools to search the web using Baidu.
  </Card>

<Card title="DuckDuckGo" icon="duck" iconType="duotone" href="/concepts/tools/toolkits/search/duckduckgo">
    Tools to search the web using DuckDuckGo.
  </Card>

<Card title="Exa" icon="magnifying-glass" iconType="duotone" href="/concepts/tools/toolkits/search/exa">
    Tools to search the web using Exa.
  </Card>

<Card title="Google Search" icon="google" iconType="duotone" href="/concepts/tools/toolkits/search/googlesearch">
    Tools to search Google.
  </Card>

<Card title="HackerNews" icon="newspaper" iconType="duotone" href="/concepts/tools/toolkits/search/hackernews">
    Tools to read Hacker News articles.
  </Card>

<Card title="Pubmed" icon="file-medical" iconType="duotone" href="/concepts/tools/toolkits/search/pubmed">
    Tools to search Pubmed.
  </Card>

<Card title="SearxNG" icon="magnifying-glass" iconType="duotone" href="/concepts/tools/toolkits/search/searxng">
    Tools to search the web using SearxNG.
  </Card>

<Card title="SerperApi" icon="magnifying-glass" iconType="duotone" href="/concepts/tools/toolkits/search/serpapi">
    Tools to search Google using SerperApi.
  </Card>

<Card title="Serpapi" icon="magnifying-glass" iconType="duotone" href="/concepts/tools/toolkits/search/serper">
    Tools to search Google, YouTube, and more using Serpapi.
  </Card>

<Card title="Tavily" icon="magnifying-glass" iconType="duotone" href="/concepts/tools/toolkits/search/tavily">
    Tools to search the web using Tavily.
  </Card>

<Card title="Linkup" icon="magnifying-glass" iconType="duotone" href="/concepts/tools/toolkits/search/linkup">
    Tools to search the web using Linkup.
  </Card>

<Card title="Valyu" icon="magnifying-glass" iconType="duotone" href="/concepts/tools/toolkits/search/valyu">
    Tools to search academic papers and web content using Valyu.
  </Card>

<Card title="Wikipedia" icon="book" iconType="duotone" href="/concepts/tools/toolkits/search/wikipedia">
    Tools to search Wikipedia.
  </Card>
</CardGroup>

<CardGroup cols={3}>
  <Card title="Discord" icon="comment" iconType="duotone" href="/concepts/tools/toolkits/social/discord">
    Tools to interact with Discord.
  </Card>

<Card title="Email" icon="envelope" iconType="duotone" href="/concepts/tools/toolkits/social/email">
    Tools to send emails.
  </Card>

<Card title="Gmail" icon="envelope" iconType="duotone" href="/concepts/tools/toolkits/social/gmail">
    Tools to interact with Gmail.
  </Card>

<Card title="Slack" icon="slack" iconType="duotone" href="/concepts/tools/toolkits/social/slack">
    Tools to interact with Slack.
  </Card>

<Card title="Telegram" icon="telegram" iconType="brands" href="/concepts/tools/toolkits/social/telegram">
    Tools to interact with Telegram.
  </Card>

<Card title="Twilio" icon="mobile-screen-button" iconType="duotone" href="/concepts/tools/toolkits/social/twilio">
    Tools to interact with Twilio services.
  </Card>

<Card title="WhatsApp" icon="whatsapp" iconType="brands" href="/concepts/tools/toolkits/social/whatsapp">
    Tools to interact with WhatsApp.
  </Card>

<Card title="Webex" icon="message" iconType="duotone" href="/concepts/tools/toolkits/social/webex">
    Tools to interact with Cisco Webex.
  </Card>

<Card title="X (Twitter)" icon="x-twitter" iconType="brands" href="/concepts/tools/toolkits/social/x">
    Tools to interact with X.
  </Card>

<Card title="Reddit" icon="reddit" iconType="brands" href="/concepts/tools/toolkits/social/reddit">
    Tools to interact with Reddit.
  </Card>

<Card title="Zoom" icon="video" iconType="duotone" href="/concepts/tools/toolkits/social/zoom">
    Tools to interact with Zoom.
  </Card>
</CardGroup>

<CardGroup cols={3}>
  <Card title="AgentQL" icon="magnifying-glass" iconType="duotone" href="/concepts/tools/toolkits/web_scrape/agentql">
    Browse and scrape websites using AgentQL.
  </Card>

<Card title="BrowserBase" icon="browser" iconType="duotone" href="/concepts/tools/toolkits/web_scrape/browserbase">
    Tools to interact with BrowserBase.
  </Card>

<Card title="Crawl4AI" icon="spider" iconType="duotone" href="/concepts/tools/toolkits/web_scrape/crawl4ai">
    Tools to crawl web data.
  </Card>

<Card title="Jina Reader" icon="robot" iconType="duotone" href="/concepts/tools/toolkits/web_scrape/jina_reader">
    Tools for neural search and AI services using Jina.
  </Card>

<Card title="Newspaper" icon="newspaper" iconType="duotone" href="/concepts/tools/toolkits/web_scrape/newspaper">
    Tools to read news articles.
  </Card>

<Card title="Newspaper4k" icon="newspaper" iconType="duotone" href="/concepts/tools/toolkits/web_scrape/newspaper4k">
    Tools to read articles using Newspaper4k.
  </Card>

<Card title="Website" icon="globe" iconType="duotone" href="/concepts/tools/toolkits/web_scrape/website">
    Tools to scrape websites.
  </Card>

<Card title="Firecrawl" icon="fire" iconType="duotone" href="/concepts/tools/toolkits/web_scrape/firecrawl">
    Tools to crawl the web using Firecrawl.
  </Card>

<Card title="Spider" icon="spider" iconType="duotone" href="/concepts/tools/toolkits/web_scrape/spider">
    Tools to crawl websites.
  </Card>

<Card title="Trafilatura" icon="text" iconType="duotone" href="/concepts/tools/toolkits/web_scrape/trafilatura">
    Tools to extract text content from web pages.
  </Card>

<Card title="BrightData" icon="screen-users" iconType="duotone" href="/concepts/tools/toolkits/web_scrape/brightdata">
    Tools to scrape websites using BrightData.
  </Card>
</CardGroup>

<CardGroup cols={3}>
  <Card title="CSV" icon="file-csv" iconType="duotone" href="/concepts/tools/toolkits/database/csv">
    Tools to work with CSV files.
  </Card>

<Card title="DuckDb" icon="server" iconType="duotone" href="/concepts/tools/toolkits/database/duckdb">
    Tools to run SQL using DuckDb.
  </Card>

<Card title="Pandas" icon="table" iconType="duotone" href="/concepts/tools/toolkits/database/pandas">
    Tools to manipulate data using Pandas.
  </Card>

<Card title="Postgres" icon="database" iconType="duotone" href="/concepts/tools/toolkits/database/postgres">
    Tools to interact with PostgreSQL databases.
  </Card>

<Card title="SQL" icon="database" iconType="duotone" href="/concepts/tools/toolkits/database/sql">
    Tools to run SQL queries.
  </Card>

<Card title="Google BigQuery" icon="database" iconType="duotone" href="/concepts/tools/toolkits/database/google_bigquery">
    Tools to query large datasets using Google BigQuery.
  </Card>

<Card title="Neo4j" icon="project-diagram" iconType="duotone" href="/concepts/tools/toolkits/database/neo4j">
    Tools to interact with Neo4j graph databases.
  </Card>

<Card title="Zep" icon="memory" iconType="duotone" href="/concepts/tools/toolkits/database/zep">
    Tools to interact with Zep.
  </Card>

<Card title="MCP Toolbox" icon="database" iconType="duotone" href="/concepts/tools/mcp/mcp-toolbox">
    Tools to connect to MCP Toolbox for Databases with filtering capabilities.
  </Card>
</CardGroup>

<CardGroup cols={3}>
  <Card title="Calculator" icon="calculator" iconType="duotone" href="/concepts/tools/toolkits/local/calculator">
    Tools to perform calculations.
  </Card>

<Card title="Docker" icon="docker" iconType="duotone" href="/concepts/tools/toolkits/local/docker">
    Tools to interact with Docker.
  </Card>

<Card title="File" icon="file" iconType="duotone" href="/concepts/tools/toolkits/local/file">
    Tools to read and write files.
  </Card>

<Card title="Python" icon="code" iconType="duotone" href="/concepts/tools/toolkits/local/python">
    Tools to write and run Python code.
  </Card>

<Card title="Shell" icon="terminal" iconType="duotone" href="/concepts/tools/toolkits/local/shell">
    Tools to run shell commands.
  </Card>

<Card title="Local File System" icon="file" iconType="duotone" href="/concepts/tools/toolkits/local/local_file_system">
    Tools to write files to the local file system.
  </Card>

<Card title="Sleep" icon="bed" iconType="duotone" href="/concepts/tools/toolkits/local/sleep">
    Tools to pause execution for a given number of seconds.
  </Card>
</CardGroup>

## Native Model Toolkit

<CardGroup cols={3}>
  <Card title="Azure OpenAI" icon="microsoft" iconType="brands" href="/concepts/tools/toolkits/models/azure_openai">
    Tools to generate images using Azure OpenAI DALL-E.
  </Card>

<Card title="Groq" icon="groq" iconType="brands" href="/concepts/tools/toolkits/models/groq">
    Tools to interact with Groq.
  </Card>

<Card title="Morph" icon="code" iconType="duotone" href="/concepts/tools/toolkits/models/morph">
    Tools to modify code using Morph AI.
  </Card>

<Card title="Nebius" icon="image" iconType="duotone" href="/concepts/tools/toolkits/models/nebius">
    Tools to generate images using Nebius AI Studio.
  </Card>
</CardGroup>

## Additional Toolkits

<CardGroup cols={3}>
  <Card title="Airflow" icon="wind" iconType="duotone" href="/concepts/tools/toolkits/others/airflow">
    Tools to manage Airflow DAGs.
  </Card>

<Card title="Apify" icon="gear" iconType="duotone" href="/concepts/tools/toolkits/others/apify">
    Tools to use Apify Actors.
  </Card>

<Card title="AWS Lambda" icon="server" iconType="duotone" href="/concepts/tools/toolkits/others/aws_lambda">
    Tools to run serverless functions using AWS Lambda.
  </Card>

<Card title="AWS SES" icon="envelope" iconType="duotone" href="/concepts/tools/toolkits/others/aws_ses">
    Tools to send emails using AWS SES
  </Card>

<Card title="CalCom" icon="calendar" iconType="duotone" href="/concepts/tools/toolkits/others/calcom">
    Tools to interact with the Cal.com API.
  </Card>

<Card title="Cartesia" icon="waveform" iconType="duotone" href="/concepts/tools/toolkits/others/cartesia">
    Tools for integrating voice AI.
  </Card>

<Card title="Composio" icon="code-branch" iconType="duotone" href="/concepts/tools/toolkits/others/composio">
    Tools to compose complex workflows.
  </Card>

<Card title="Confluence" icon="file" iconType="duotone" href="/concepts/tools/toolkits/others/confluence">
    Tools to manage Confluence pages.
  </Card>

<Card title="Custom API" icon="puzzle-piece" iconType="duotone" href="/concepts/tools/toolkits/others/custom_api">
    Tools to call any custom HTTP API.
  </Card>

<Card title="Dalle" icon="eye" iconType="duotone" href="/concepts/tools/toolkits/others/dalle">
    Tools to interact with Dalle.
  </Card>

<Card title="Eleven Labs" icon="headphones" iconType="duotone" href="/concepts/tools/toolkits/others/eleven_labs">
    Tools to generate audio using Eleven Labs.
  </Card>

<Card title="E2B" icon="server" iconType="duotone" href="/concepts/tools/toolkits/others/e2b">
    Tools to interact with E2B.
  </Card>

<Card title="Fal" icon="video" iconType="duotone" href="/concepts/tools/toolkits/others/fal">
    Tools to generate media using Fal.
  </Card>

<Card title="Financial Datasets" icon="dollar-sign" iconType="duotone" href="/concepts/tools/toolkits/others/financial_datasets">
    Tools to access and analyze financial data.
  </Card>

<Card title="Giphy" icon="image" iconType="duotone" href="/concepts/tools/toolkits/others/giphy">
    Tools to search for GIFs on Giphy.
  </Card>

<Card title="GitHub" icon="github" iconType="brands" href="/concepts/tools/toolkits/others/github">
    Tools to interact with GitHub.
  </Card>

<Card title="Google Maps" icon="map" iconType="duotone" href="/concepts/tools/toolkits/others/google_maps">
    Tools to search for places on Google Maps.
  </Card>

<Card title="Google Calendar" icon="calendar" iconType="duotone" href="/concepts/tools/toolkits/others/googlecalendar">
    Tools to manage Google Calendar events.
  </Card>

<Card title="Google Sheets" icon="google" iconType="duotone" href="/concepts/tools/toolkits/others/google_sheets">
    Tools to work with Google Sheets.
  </Card>

<Card title="Jira" icon="jira" iconType="brands" href="/concepts/tools/toolkits/others/jira">
    Tools to interact with Jira.
  </Card>

<Card title="Linear" icon="list" iconType="duotone" href="/concepts/tools/toolkits/others/linear">
    Tools to interact with Linear.
  </Card>

<Card title="Lumalabs" icon="lightbulb" iconType="duotone" href="/concepts/tools/toolkits/others/lumalabs">
    Tools to interact with Lumalabs.
  </Card>

<Card title="MLX Transcribe" icon="headphones" iconType="duotone" href="/concepts/tools/toolkits/others/mlx_transcribe">
    Tools to transcribe audio using MLX.
  </Card>

<Card title="ModelsLabs" icon="video" iconType="duotone" href="/concepts/tools/toolkits/others/models_labs">
    Tools to generate videos using ModelsLabs.
  </Card>

<Card title="OpenBB" icon="chart-bar" iconType="duotone" href="/concepts/tools/toolkits/others/openbb">
    Tools to search for stock data using OpenBB.
  </Card>

<Card title="Openweather" icon="cloud-sun" iconType="duotone" href="/concepts/tools/toolkits/others/openweather">
    Tools to search for weather data using Openweather.
  </Card>

<Card title="Replicate" icon="robot" iconType="duotone" href="/concepts/tools/toolkits/others/replicate">
    Tools to interact with Replicate.
  </Card>

<Card title="Resend" icon="paper-plane" iconType="duotone" href="/concepts/tools/toolkits/others/resend">
    Tools to send emails using Resend.
  </Card>

<Card title="Todoist" icon="list" iconType="duotone" href="/concepts/tools/toolkits/others/todoist">
    Tools to interact with Todoist.
  </Card>

<Card title="YFinance" icon="dollar-sign" iconType="duotone" href="/concepts/tools/toolkits/others/yfinance">
    Tools to search Yahoo Finance.
  </Card>

<Card title="YouTube" icon="youtube" iconType="brands" href="/concepts/tools/toolkits/others/youtube">
    Tools to search YouTube.
  </Card>

<Card title="Bitbucket" icon="bitbucket" iconType="brands" href="/concepts/tools/toolkits/others/bitbucket">
    Tools to interact with Bitbucket repositories.
  </Card>

<Card title="Brandfetch" icon="trademark" iconType="duotone" href="/concepts/tools/toolkits/others/brandfetch">
    Tools to retrieve brand information and logos.
  </Card>

<Card title="ClickUp" icon="tasks" iconType="duotone" href="/concepts/tools/toolkits/others/clickup">
    Tools to manage ClickUp tasks and projects.
  </Card>

<Card title="Desi Vocal" icon="microphone" iconType="duotone" href="/concepts/tools/toolkits/others/desi_vocal">
    Tools for Indian text-to-speech synthesis.
  </Card>

<Card title="EVM" icon="coins" iconType="duotone" href="/concepts/tools/toolkits/others/evm">
    Tools to interact with Ethereum blockchain.
  </Card>

<Card title="Knowledge" icon="brain" iconType="duotone" href="/concepts/tools/toolkits/others/knowledge">
    Tools to search and analyze knowledge bases.
  </Card>

<Card title="Mem0" icon="memory" iconType="duotone" href="/concepts/tools/toolkits/others/mem0">
    Tools for advanced memory management.
  </Card>

<Card title="Memori" icon="brain" iconType="duotone" href="/concepts/tools/toolkits/others/memori">
    Tools for persistent conversation memory.
  </Card>

<Card title="OpenCV" icon="camera" iconType="duotone" href="/concepts/tools/toolkits/others/opencv">
    Tools for computer vision and camera operations.
  </Card>

<Card title="Reasoning" icon="brain" iconType="duotone" href="/concepts/tools/toolkits/others/reasoning">
    Tools for structured logical analysis.
  </Card>

<Card title="User Control Flow" icon="user-check" iconType="duotone" href="/concepts/tools/toolkits/others/user_control_flow">
    Tools for interactive user input collection.
  </Card>

<Card title="Visualization" icon="chart-bar" iconType="duotone" href="/concepts/tools/toolkits/others/visualization">
    Tools for data visualization and charting.
  </Card>

<Card title="WebTools" icon="globe" iconType="duotone" href="/concepts/tools/toolkits/others/webtools">
    Tools for URL expansion and web utilities.
  </Card>

<Card title="Zendesk" icon="headphones" iconType="duotone" href="/concepts/tools/toolkits/others/zendesk">
    Tools to search Zendesk.
  </Card>
</CardGroup>

---

## OpenAI-compatible models

**URL:** llms-txt#openai-compatible-models

**Contents:**
- Example
- Parameters

Source: https://docs.agno.com/concepts/models/openai-like

Learn how to use any OpenAI-like compatible endpoint with Agno

Many providers support the OpenAI API format. Use the `OpenAILike` model to access them by replacing the `base_url`.

<CodeGroup>
  
</CodeGroup>

| Parameter  | Type            | Default         | Description                                                        |
| ---------- | --------------- | --------------- | ------------------------------------------------------------------ |
| `id`       | `str`           | `"gpt-4o-mini"` | The id of the model to use                                         |
| `name`     | `str`           | `"OpenAILike"`  | The name of the model                                              |
| `provider` | `str`           | `"OpenAILike"`  | The provider of the model                                          |
| `api_key`  | `Optional[str]` | `None`          | The API key for the service (defaults to OPENAI\_API\_KEY env var) |
| `base_url` | `Optional[str]` | `None`          | The base URL for the API service                                   |

`OpenAILike` extends the OpenAI-compatible interface and supports all parameters from [OpenAIChat](/concepts/models/openai). Simply change the `base_url` and `api_key` to point to your preferred OpenAI-compatible service.

---

## WorkflowRunOutput

**URL:** llms-txt#workflowrunoutput

**Contents:**
- WorkflowRunOutput Attributes
- WorkflowRunOutputEvent Types and Attributes
  - BaseWorkflowRunOutputEvent Attributes
  - WorkflowStartedEvent Attributes
  - WorkflowCompletedEvent Attributes
  - WorkflowCancelledEvent Attributes
  - StepStartedEvent Attributes
  - StepCompletedEvent Attributes
  - ConditionExecutionStartedEvent Attributes
  - ConditionExecutionCompletedEvent Attributes

Source: https://docs.agno.com/reference/workflows/workflow_run_output

## WorkflowRunOutput Attributes

| Parameter            | Type                                                              | Default             | Description                                                           |
| -------------------- | ----------------------------------------------------------------- | ------------------- | --------------------------------------------------------------------- |
| `content`            | `Optional[Union[str, Dict[str, Any], List[Any], BaseModel, Any]]` | `None`              | Main content/output from the workflow execution                       |
| `content_type`       | `str`                                                             | `"str"`             | Type of the content (e.g., "str", "json", etc.)                       |
| `workflow_id`        | `Optional[str]`                                                   | `None`              | Unique identifier of the executed workflow                            |
| `workflow_name`      | `Optional[str]`                                                   | `None`              | Name of the executed workflow                                         |
| `run_id`             | `Optional[str]`                                                   | `None`              | Unique identifier for this specific run                               |
| `session_id`         | `Optional[str]`                                                   | `None`              | Session UUID associated with this run                                 |
| `images`             | `Optional[List[Image]]`                                           | `None`              | List of image artifacts generated                                     |
| `videos`             | `Optional[List[Video]]`                                           | `None`              | List of video artifacts generated                                     |
| `audio`              | `Optional[List[Audio]]`                                           | `None`              | List of audio artifacts generated                                     |
| `response_audio`     | `Optional[Audio]`                                                 | `None`              | Audio response from the workflow                                      |
| `step_results`       | `List[Union[StepOutput, List[StepOutput]]]`                       | `[]`                | Actual step execution results as StepOutput objects                   |
| `step_executor_runs` | `Optional[List[Union[RunOutput, TeamRunOutput]]]`                 | `None`              | Store agent/team responses separately with parent\_run\_id references |
| `events`             | `Optional[List[WorkflowRunOutputEvent]]`                          | `None`              | Events captured during workflow execution                             |
| `metrics`            | `Optional[WorkflowMetrics]`                                       | `None`              | Workflow metrics aggregated from all steps                            |
| `metadata`           | `Optional[Dict[str, Any]]`                                        | `None`              | Additional metadata stored with the response                          |
| `created_at`         | `int`                                                             | `int(time())`       | Unix timestamp when the response was created                          |
| `status`             | `RunStatus`                                                       | `RunStatus.pending` | Current status of the workflow run                                    |

## WorkflowRunOutputEvent Types and Attributes

### BaseWorkflowRunOutputEvent Attributes

| Parameter        | Type            | Default       | Description                                              |
| ---------------- | --------------- | ------------- | -------------------------------------------------------- |
| `created_at`     | `int`           | `int(time())` | Unix timestamp when the event was created                |
| `event`          | `str`           | `""`          | Type of the event (e.g., "WorkflowStarted")              |
| `workflow_id`    | `Optional[str]` | `None`        | Unique identifier of the workflow                        |
| `workflow_name`  | `Optional[str]` | `None`        | Name of the workflow                                     |
| `session_id`     | `Optional[str]` | `None`        | Session UUID associated with the workflow                |
| `run_id`         | `Optional[str]` | `None`        | Unique identifier for the workflow run                   |
| `step_id`        | `Optional[str]` | `None`        | Unique identifier for the current step                   |
| `parent_step_id` | `Optional[str]` | `None`        | Unique identifier for the parent step (for nested steps) |

### WorkflowStartedEvent Attributes

| Parameter                                               | Type  | Default                                   | Description           |
| ------------------------------------------------------- | ----- | ----------------------------------------- | --------------------- |
| `event`                                                 | `str` | `WorkflowRunEvent.workflow_started.value` | Event type identifier |
| *Inherits all fields from `BaseWorkflowRunOutputEvent`* |       |                                           |                       |

### WorkflowCompletedEvent Attributes

<Snippet file="workflow-completed-event.mdx" />

### WorkflowCancelledEvent Attributes

| Parameter                                               | Type                       | Default                                     | Description                                 |
| ------------------------------------------------------- | -------------------------- | ------------------------------------------- | ------------------------------------------- |
| `event`                                                 | `str`                      | `WorkflowRunEvent.workflow_completed.value` | Event type identifier                       |
| `content`                                               | `Optional[Any]`            | `None`                                      | Final output content from the workflow      |
| `content_type`                                          | `str`                      | `"str"`                                     | Type of the content                         |
| `step_results`                                          | `List[StepOutput]`         | `[]`                                        | List of all step execution results          |
| `metadata`                                              | `Optional[Dict[str, Any]]` | `None`                                      | Additional metadata from workflow execution |
| *Inherits all fields from `BaseWorkflowRunOutputEvent`* |                            |                                             |                                             |

### StepStartedEvent Attributes

| Parameter                                               | Type                          | Default                               | Description                    |
| ------------------------------------------------------- | ----------------------------- | ------------------------------------- | ------------------------------ |
| `event`                                                 | `str`                         | `WorkflowRunEvent.step_started.value` | Event type identifier          |
| `step_name`                                             | `Optional[str]`               | `None`                                | Name of the step being started |
| `step_index`                                            | `Optional[Union[int, tuple]]` | `None`                                | Index or position of the step  |
| *Inherits all fields from `BaseWorkflowRunOutputEvent`* |                               |                                       |                                |

### StepCompletedEvent Attributes

| Parameter                                               | Type                          | Default                                 | Description                           |
| ------------------------------------------------------- | ----------------------------- | --------------------------------------- | ------------------------------------- |
| `event`                                                 | `str`                         | `WorkflowRunEvent.step_completed.value` | Event type identifier                 |
| `step_name`                                             | `Optional[str]`               | `None`                                  | Name of the step that completed       |
| `step_index`                                            | `Optional[Union[int, tuple]]` | `None`                                  | Index or position of the step         |
| `content`                                               | `Optional[Any]`               | `None`                                  | Content output from the step          |
| `content_type`                                          | `str`                         | `"str"`                                 | Type of the content                   |
| `images`                                                | `Optional[List[Image]]`       | `None`                                  | Image artifacts from the step         |
| `videos`                                                | `Optional[List[Video]]`       | `None`                                  | Video artifacts from the step         |
| `audio`                                                 | `Optional[List[Audio]]`       | `None`                                  | Audio artifacts from the step         |
| `response_audio`                                        | `Optional[Audio]`             | `None`                                  | Audio response from the step          |
| `step_response`                                         | `Optional[StepOutput]`        | `None`                                  | Complete step execution result object |
| *Inherits all fields from `BaseWorkflowRunOutputEvent`* |                               |                                         |                                       |

### ConditionExecutionStartedEvent Attributes

| Parameter                                               | Type                          | Default                                              | Description                        |
| ------------------------------------------------------- | ----------------------------- | ---------------------------------------------------- | ---------------------------------- |
| `event`                                                 | `str`                         | `WorkflowRunEvent.condition_execution_started.value` | Event type identifier              |
| `step_name`                                             | `Optional[str]`               | `None`                                               | Name of the condition step         |
| `step_index`                                            | `Optional[Union[int, tuple]]` | `None`                                               | Index or position of the condition |
| `condition_result`                                      | `Optional[bool]`              | `None`                                               | Result of the condition evaluation |
| *Inherits all fields from `BaseWorkflowRunOutputEvent`* |                               |                                                      |                                    |

### ConditionExecutionCompletedEvent Attributes

| Parameter                                               | Type                          | Default                                                | Description                                 |
| ------------------------------------------------------- | ----------------------------- | ------------------------------------------------------ | ------------------------------------------- |
| `event`                                                 | `str`                         | `WorkflowRunEvent.condition_execution_completed.value` | Event type identifier                       |
| `step_name`                                             | `Optional[str]`               | `None`                                                 | Name of the condition step                  |
| `step_index`                                            | `Optional[Union[int, tuple]]` | `None`                                                 | Index or position of the condition          |
| `condition_result`                                      | `Optional[bool]`              | `None`                                                 | Result of the condition evaluation          |
| `executed_steps`                                        | `Optional[int]`               | `None`                                                 | Number of steps executed based on condition |
| `step_results`                                          | `List[StepOutput]`            | `[]`                                                   | Results from executed steps                 |
| *Inherits all fields from `BaseWorkflowRunOutputEvent`* |                               |                                                        |                                             |

### ParallelExecutionStartedEvent Attributes

| Parameter                                               | Type                          | Default                                             | Description                            |
| ------------------------------------------------------- | ----------------------------- | --------------------------------------------------- | -------------------------------------- |
| `event`                                                 | `str`                         | `WorkflowRunEvent.parallel_execution_started.value` | Event type identifier                  |
| `step_name`                                             | `Optional[str]`               | `None`                                              | Name of the parallel step              |
| `step_index`                                            | `Optional[Union[int, tuple]]` | `None`                                              | Index or position of the parallel step |
| `parallel_step_count`                                   | `Optional[int]`               | `None`                                              | Number of steps to execute in parallel |
| *Inherits all fields from `BaseWorkflowRunOutputEvent`* |                               |                                                     |                                        |

### ParallelExecutionCompletedEvent Attributes

| Parameter                                               | Type                          | Default                                               | Description                            |
| ------------------------------------------------------- | ----------------------------- | ----------------------------------------------------- | -------------------------------------- |
| `event`                                                 | `str`                         | `WorkflowRunEvent.parallel_execution_completed.value` | Event type identifier                  |
| `step_name`                                             | `Optional[str]`               | `None`                                                | Name of the parallel step              |
| `step_index`                                            | `Optional[Union[int, tuple]]` | `None`                                                | Index or position of the parallel step |
| `parallel_step_count`                                   | `Optional[int]`               | `None`                                                | Number of steps executed in parallel   |
| `step_results`                                          | `List[StepOutput]`            | `field(default_factory=list)`                         | Results from all parallel steps        |
| *Inherits all fields from `BaseWorkflowRunOutputEvent`* |                               |                                                       |                                        |

### LoopExecutionStartedEvent Attributes

| Parameter                                               | Type                          | Default                                         | Description                          |
| ------------------------------------------------------- | ----------------------------- | ----------------------------------------------- | ------------------------------------ |
| `event`                                                 | `str`                         | `WorkflowRunEvent.loop_execution_started.value` | Event type identifier                |
| `step_name`                                             | `Optional[str]`               | `None`                                          | Name of the loop step                |
| `step_index`                                            | `Optional[Union[int, tuple]]` | `None`                                          | Index or position of the loop        |
| `max_iterations`                                        | `Optional[int]`               | `None`                                          | Maximum number of iterations allowed |
| *Inherits all fields from `BaseWorkflowRunOutputEvent`* |                               |                                                 |                                      |

### LoopIterationStartedEvent Attributes

| Parameter                                               | Type                          | Default                                         | Description                          |
| ------------------------------------------------------- | ----------------------------- | ----------------------------------------------- | ------------------------------------ |
| `event`                                                 | `str`                         | `WorkflowRunEvent.loop_iteration_started.value` | Event type identifier                |
| `step_name`                                             | `Optional[str]`               | `None`                                          | Name of the loop step                |
| `step_index`                                            | `Optional[Union[int, tuple]]` | `None`                                          | Index or position of the loop        |
| `iteration`                                             | `int`                         | `0`                                             | Current iteration number             |
| `max_iterations`                                        | `Optional[int]`               | `None`                                          | Maximum number of iterations allowed |
| *Inherits all fields from `BaseWorkflowRunOutputEvent`* |                               |                                                 |                                      |

### LoopIterationCompletedEvent Attributes

| Parameter                                               | Type                          | Default                                           | Description                          |
| ------------------------------------------------------- | ----------------------------- | ------------------------------------------------- | ------------------------------------ |
| `event`                                                 | `str`                         | `WorkflowRunEvent.loop_iteration_completed.value` | Event type identifier                |
| `step_name`                                             | `Optional[str]`               | `None`                                            | Name of the loop step                |
| `step_index`                                            | `Optional[Union[int, tuple]]` | `None`                                            | Index or position of the loop        |
| `iteration`                                             | `int`                         | `0`                                               | Current iteration number             |
| `max_iterations`                                        | `Optional[int]`               | `None`                                            | Maximum number of iterations allowed |
| `iteration_results`                                     | `List[StepOutput]`            | `[]`                                              | Results from this iteration          |
| `should_continue`                                       | `bool`                        | `True`                                            | Whether the loop should continue     |
| *Inherits all fields from `BaseWorkflowRunOutputEvent`* |                               |                                                   |                                      |

### LoopExecutionCompletedEvent Attributes

| Parameter                                               | Type                          | Default                                           | Description                          |
| ------------------------------------------------------- | ----------------------------- | ------------------------------------------------- | ------------------------------------ |
| `event`                                                 | `str`                         | `WorkflowRunEvent.loop_execution_completed.value` | Event type identifier                |
| `step_name`                                             | `Optional[str]`               | `None`                                            | Name of the loop step                |
| `step_index`                                            | `Optional[Union[int, tuple]]` | `None`                                            | Index or position of the loop        |
| `total_iterations`                                      | `int`                         | `0`                                               | Total number of iterations completed |
| `max_iterations`                                        | `Optional[int]`               | `None`                                            | Maximum number of iterations allowed |
| `all_results`                                           | `List[List[StepOutput]]`      | `[]`                                              | Results from all iterations          |
| *Inherits all fields from `BaseWorkflowRunOutputEvent`* |                               |                                                   |                                      |

### RouterExecutionStartedEvent Attributes

| Parameter                                               | Type                          | Default                                           | Description                           |
| ------------------------------------------------------- | ----------------------------- | ------------------------------------------------- | ------------------------------------- |
| `event`                                                 | `str`                         | `WorkflowRunEvent.router_execution_started.value` | Event type identifier                 |
| `step_name`                                             | `Optional[str]`               | `None`                                            | Name of the router step               |
| `step_index`                                            | `Optional[Union[int, tuple]]` | `None`                                            | Index or position of the router       |
| `selected_steps`                                        | `List[str]`                   | `field(default_factory=list)`                     | Names of steps selected by the router |
| *Inherits all fields from `BaseWorkflowRunOutputEvent`* |                               |                                                   |                                       |

### RouterExecutionCompletedEvent Attributes

| Parameter                                               | Type                          | Default                                             | Description                       |
| ------------------------------------------------------- | ----------------------------- | --------------------------------------------------- | --------------------------------- |
| `event`                                                 | `str`                         | `WorkflowRunEvent.router_execution_completed.value` | Event type identifier             |
| `step_name`                                             | `Optional[str]`               | `None`                                              | Name of the router step           |
| `step_index`                                            | `Optional[Union[int, tuple]]` | `None`                                              | Index or position of the router   |
| `selected_steps`                                        | `List[str]`                   | `field(default_factory=list)`                       | Names of steps that were selected |
| `executed_steps`                                        | `Optional[int]`               | `None`                                              | Number of steps executed          |
| `step_results`                                          | `List[StepOutput]`            | `field(default_factory=list)`                       | Results from executed steps       |
| *Inherits all fields from `BaseWorkflowRunOutputEvent`* |                               |                                                     |                                   |

### StepsExecutionStartedEvent Attributes

| Parameter                                               | Type                          | Default                                          | Description                          |
| ------------------------------------------------------- | ----------------------------- | ------------------------------------------------ | ------------------------------------ |
| `event`                                                 | `str`                         | `WorkflowRunEvent.steps_execution_started.value` | Event type identifier                |
| `step_name`                                             | `Optional[str]`               | `None`                                           | Name of the steps group              |
| `step_index`                                            | `Optional[Union[int, tuple]]` | `None`                                           | Index or position of the steps group |
| `steps_count`                                           | `Optional[int]`               | `None`                                           | Number of steps in the group         |
| *Inherits all fields from `BaseWorkflowRunOutputEvent`* |                               |                                                  |                                      |

### StepsExecutionCompletedEvent Attributes

| Parameter                                               | Type                          | Default                                            | Description                          |
| ------------------------------------------------------- | ----------------------------- | -------------------------------------------------- | ------------------------------------ |
| `event`                                                 | `str`                         | `WorkflowRunEvent.steps_execution_completed.value` | Event type identifier                |
| `step_name`                                             | `Optional[str]`               | `None`                                             | Name of the steps group              |
| `step_index`                                            | `Optional[Union[int, tuple]]` | `None`                                             | Index or position of the steps group |
| `steps_count`                                           | `Optional[int]`               | `None`                                             | Number of steps in the group         |
| `executed_steps`                                        | `Optional[int]`               | `None`                                             | Number of steps actually executed    |
| `step_results`                                          | `List[StepOutput]`            | `field(default_factory=list)`                      | Results from all executed steps      |
| *Inherits all fields from `BaseWorkflowRunOutputEvent`* |                               |                                                    |                                      |

### StepOutputEvent Attributes

| Parameter                                               | Type                                                              | Default        | Description                                  |
| ------------------------------------------------------- | ----------------------------------------------------------------- | -------------- | -------------------------------------------- |
| `event`                                                 | `str`                                                             | `"StepOutput"` | Event type identifier                        |
| `step_name`                                             | `Optional[str]`                                                   | `None`         | Name of the step that produced output        |
| `step_index`                                            | `Optional[Union[int, tuple]]`                                     | `None`         | Index or position of the step                |
| `step_output`                                           | `Optional[StepOutput]`                                            | `None`         | Complete step execution result               |
| *Inherits all fields from `BaseWorkflowRunOutputEvent`* |                                                                   |                |                                              |
| **Properties (read-only):**                             |                                                                   |                |                                              |
| `content`                                               | `Optional[Union[str, Dict[str, Any], List[Any], BaseModel, Any]]` | -              | Content from the step output                 |
| `images`                                                | `Optional[List[Image]]`                                           | -              | Images from the step output                  |
| `videos`                                                | `Optional[List[Video]]`                                           | -              | Videos from the step output                  |
| `audio`                                                 | `Optional[List[Audio]]`                                           | -              | Audio from the step output                   |
| `success`                                               | `bool`                                                            | -              | Whether the step succeeded                   |
| `error`                                                 | `Optional[str]`                                                   | -              | Error message if step failed                 |
| `stop`                                                  | `bool`                                                            | -              | Whether the step requested early termination |

---

## Db

**URL:** llms-txt#db

**Contents:**
- Code

Source: https://docs.agno.com/examples/models/openai/responses/db

```python cookbook/models/openai/responses/db.py theme={null}
"""Run `pip install ddgs sqlalchemy openai` to install dependencies."""

from agno.agent import Agent
from agno.db.postgres import PostgresDb
from agno.models.openai import OpenAIResponses
from agno.tools.duckduckgo import DuckDuckGoTools

---

## 3. Generate Video Analysis

**URL:** llms-txt#3.-generate-video-analysis

response = agent.run(query, videos=[Video(filepath=video_path)])

---

## "A startup has $500,000 in funding and needs to decide between spending it on marketing or "

**URL:** llms-txt#"a-startup-has-$500,000-in-funding-and-needs-to-decide-between-spending-it-on-marketing-or-"

---

## Create a team for collaborative structured output generation

**URL:** llms-txt#create-a-team-for-collaborative-structured-output-generation

**Contents:**
- Usage

movie_team = Team(
    name="Movie Script Team",
    members=[image_analyst, script_writer],
    model=OpenAIChat(id="gpt-5-mini"),
    instructions=[
        "Create structured movie scripts from visual content.",
        "Image Analyst: First analyze the image for visual elements and context.",
        "Script Writer: Transform analysis into structured movie concepts.",
        "Ensure all output follows the MovieScript schema precisely.",
    ],
    output_schema=MovieScript,
)

response = movie_team.run(
    "Write a movie about this image",
    images=[
        Image(
            url="https://upload.wikimedia.org/wikipedia/commons/0/0c/GoldenGateBridge-001.jpg"
        )
    ],
    stream=True,
)

for event in response:
    pprint(event.content)
bash  theme={null}
    pip install agno pydantic rich
    bash  theme={null}
    export OPENAI_API_KEY=****
    bash  theme={null}
    python cookbook/examples/teams/multimodal/image_to_structured_output.py
    ```
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Install required libraries">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Set environment variables">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run the agent">
```

---

## SearxNG Tools

**URL:** llms-txt#searxng-tools

**Contents:**
- Code
- Usage

Source: https://docs.agno.com/examples/concepts/tools/search/searxng

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Set your API key">
    
  </Step>

<Step title="Install libraries">
    
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Set your API key">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Pipedream Google Calendar

**URL:** llms-txt#pipedream-google-calendar

**Contents:**
- Code

Source: https://docs.agno.com/examples/concepts/tools/mcp/pipedream_google_calendar

This example shows how to use the Google Calendar Pipedream MCP server with Agno Agents.

---

## Define steps using different executor types

**URL:** llms-txt#define-steps-using-different-executor-types

research_step = Step(
    name="Research Step",
    team=research_team,
)

content_planning_step = Step(
    name="Content Planning Step",
    executor=custom_content_planning_function,
)

---

## LanceDb

**URL:** llms-txt#lancedb

Source: https://docs.agno.com/reference/vector_db/lancedb

<Snippet file="vector-db-lancedb-reference.mdx" />

---

## Langtrace

**URL:** llms-txt#langtrace

**Contents:**
- Integrating Agno with Langtrace
- Prerequisites
- Sending Traces to Langtrace

Source: https://docs.agno.com/integrations/observability/langtrace

Integrate Agno with Langtrace to send traces and gain insights into your agent's performance.

## Integrating Agno with Langtrace

Langtrace provides a powerful platform for tracing and monitoring AI model calls. By integrating Agno with Langtrace, you can gain insights into your agent's performance and behavior.

1. **Install Dependencies**

Ensure you have the necessary package installed:

2. **Create a Langtrace Account**

* Sign up for an account at [Langtrace](https://app.langtrace.ai/).
   * Obtain your API key from the Langtrace dashboard.

3. **Set Environment Variables**

Configure your environment with the Langtrace API key:

## Sending Traces to Langtrace

This example demonstrates how to instrument your Agno agent with Langtrace.

```python  theme={null}
from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.yfinance import YFinanceTools
from langtrace_python_sdk import langtrace
from langtrace_python_sdk.utils.with_root_span import with_langtrace_root_span

**Examples:**

Example 1 (unknown):
```unknown
2. **Create a Langtrace Account**

   * Sign up for an account at [Langtrace](https://app.langtrace.ai/).
   * Obtain your API key from the Langtrace dashboard.

3. **Set Environment Variables**

   Configure your environment with the Langtrace API key:
```

Example 2 (unknown):
```unknown
## Sending Traces to Langtrace

This example demonstrates how to instrument your Agno agent with Langtrace.
```

---

## Azure OpenAI Embedder

**URL:** llms-txt#azure-openai-embedder

**Contents:**
- Code

Source: https://docs.agno.com/examples/concepts/knowledge/embedders/azure-embedder

```python  theme={null}
from agno.knowledge.embedder.azure_openai import AzureOpenAIEmbedder
from agno.knowledge.knowledge import Knowledge
from agno.vectordb.pgvector import PgVector

embeddings = AzureOpenAIEmbedder(id="text-embedding-3-small").get_embedding(
    "The quick brown fox jumps over the lazy dog."
)

---

## Save the response audio to a file

**URL:** llms-txt#save-the-response-audio-to-a-file

**Contents:**
- Usage

if run_output.response_audio:
    write_audio_to_file(
        audio=run_output.response_audio.content,
        filename="tmp/scary_story_sequal.wav",
    )
bash  theme={null}
    export OPENAI_API_KEY=xxx
    bash  theme={null}
    pip install -U openai agno
    bash Mac theme={null}
      python cookbook/models/openai/chat/audio_output_agent.py
      bash Windows theme={null}
      python cookbook/models/openai/chat/audio_output_agent.py
      ```
    </CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Set your API key">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## LangDB Embedder

**URL:** llms-txt#langdb-embedder

**Contents:**
- Code

Source: https://docs.agno.com/examples/concepts/knowledge/embedders/langdb-embedder

```python  theme={null}

from agno.knowledge.embedder.langdb import LangDBEmbedder
from agno.knowledge.knowledge import Knowledge
from agno.vectordb.pgvector import PgVector

embeddings = LangDBEmbedder().get_embedding("Embed me")

---

## Google BigQuery Tools

**URL:** llms-txt#google-bigquery-tools

**Contents:**
- Code
- Usage

Source: https://docs.agno.com/examples/concepts/tools/database/google_bigquery

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Set your credentials">
    
  </Step>

<Step title="Install libraries">
    
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Set your credentials">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Run team and return the response as a stream

**URL:** llms-txt#run-team-and-return-the-response-as-a-stream

**Contents:**
  - Streaming all events

stream: Iterator[TeamRunOutputEvent] = team.run("What is the weather in Tokyo?", stream=True)
for chunk in stream:
    if chunk.event == "TeamRunContent":
        print(chunk.content)
python  theme={null}

**Examples:**

Example 1 (unknown):
```unknown
<Tip>
  When your team is running asynchronously (using `arun`), the members will run concurrently if the team leader delegates to multiple members in one request.

  This means you will receive member events concurrently and the order of the events is not guaranteed.
</Tip>

### Streaming all events

By default, when you stream a response, only the `RunContent` events will be streamed.

You can also stream all run events by setting `stream_events=True`.

This will provide real-time updates about the team's internal processes, like tool calling or reasoning:
```

---

## Team with Output Model

**URL:** llms-txt#team-with-output-model

**Contents:**
- Code
- Usage

Source: https://docs.agno.com/examples/concepts/teams/structured_input_output/team_with_output_model

This example shows how to use the output\_model parameter to specify the model that should be used to generate the final response from a team.

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Install required libraries">
    
  </Step>

<Step title="Set environment variables">
    
  </Step>

<Step title="Run the agent">
    
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Install required libraries">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Set environment variables">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run the agent">
```

---

## VoyageAI Embedder

**URL:** llms-txt#voyageai-embedder

**Contents:**
- Code

Source: https://docs.agno.com/examples/concepts/knowledge/embedders/voyageai-embedder

```python  theme={null}
from agno.knowledge.embedder.voyageai import VoyageAIEmbedder
from agno.knowledge.knowledge import Knowledge
from agno.vectordb.pgvector import PgVector

embeddings = VoyageAIEmbedder().get_embedding(
    "The quick brown fox jumps over the lazy dog."
)

---

## Daytona Code Execution

**URL:** llms-txt#daytona-code-execution

**Contents:**
- Code

Source: https://docs.agno.com/examples/concepts/tools/others/daytona

Learn to use Agno's Daytona integration to run your Agent-generated code in a secure sandbox.

```python cookbook/tools/daytona_tools.py theme={null}
from agno.agent import Agent
from agno.models.anthropic import Claude
from agno.tools.daytona import DaytonaTools

daytona_tools = DaytonaTools()

---

## Switch to budget model within same provider (safe)

**URL:** llms-txt#switch-to-budget-model-within-same-provider-(safe)

budget_agent = Agent(
    model=OpenAIChat(id="gpt-4o-mini"),
    instructions="You are a helpful assistant for technical discussions.",
    db=db,
    add_history_to_context=True,
)

---

## DuckDB Tools

**URL:** llms-txt#duckdb-tools

**Contents:**
- Code
- Usage

Source: https://docs.agno.com/examples/concepts/tools/database/duckdb

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Set your API key">
    
  </Step>

<Step title="Install libraries">
    
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Set your API key">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Async Basic.Py

**URL:** llms-txt#async-basic.py

**Contents:**
- Code
- Usage

Source: https://docs.agno.com/examples/models/huggingface/async_basic

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Set your API key">
    
  </Step>

<Step title="Install libraries">
    
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Set your API key">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## AWS Lambda Tools

**URL:** llms-txt#aws-lambda-tools

**Contents:**
- Code
- Usage

Source: https://docs.agno.com/examples/concepts/tools/others/aws_lambda

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Set your AWS credentials">
    
  </Step>

<Step title="Install libraries">
    
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Set your AWS credentials">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## What are Tools?

**URL:** llms-txt#what-are-tools?

Source: https://docs.agno.com/concepts/tools/overview

Tools are functions your Agno Agents can use to get things done.

Tools are what make Agents capable of real-world action. While using LLMs directly you can only generate text, Agents equipped with tools can interact with external systems and perform practical actions.

They are used to enable Agents to interact with external systems, and perform actions like searching the web, running SQL, sending an email or calling APIs.

Agno comes with 120+ pre-built toolkits, which you can use to give your Agents all kind of abilities. You can also write your own tools, to give your Agents even more capabilities. The general syntax is:

```python  theme={null}
import random

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools import tool

---

## Run team and print response to the terminal

**URL:** llms-txt#run-team-and-print-response-to-the-terminal

**Contents:**
- Interactive CLI

team.print_response("What is the weather in Tokyo?")
python  theme={null}
from agno.team import Team
from agno.agent import Agent
from agno.db.sqlite import SqliteDb
from agno.models.openai import OpenAIChat

news_agent = Agent(name="News Agent", role="Get the latest news")
weather_agent = Agent(name="Weather Agent", role="Get the weather for the next 7 days")

team = Team(
    name="News and Weather Team",
    members=[news_agent, weather_agent],
    model=OpenAIChat(id="gpt-4o"),
    db=SqliteDb(db_file="tmp/data.db"),
    add_history_to_context=True,
    num_history_runs=3,
)

**Examples:**

Example 1 (unknown):
```unknown
<Tip>
  You can set `debug_level=2` to get even more detailed logs.
</Tip>

## Interactive CLI

Agno also comes with a pre-built interactive CLI that runs your Team as a command-line application. You can use this to test back-and-forth conversations with your team.
```

---

## Generate unique IDs

**URL:** llms-txt#generate-unique-ids

user_id = str(uuid4())
id = str(uuid4())

---

## Using 'schedule' instead of deprecated 'schedule_interval'

**URL:** llms-txt#using-'schedule'-instead-of-deprecated-'schedule_interval'

**Contents:**
- Usage

with DAG(
    'example_dag',
    default_args=default_args,
    description='A simple example DAG',
    schedule='@daily',  # Changed from schedule_interval
    catchup=False
) as dag:

def print_hello():
        print("Hello from Airflow!")
        return "Hello task completed"

task = PythonOperator(
        task_id='hello_task',
        python_callable=print_hello,
        dag=dag,
    )
"""

agent.run(f"Save this DAG file as 'example_dag.py': {dag_content}")
agent.print_response("Read the contents of 'example_dag.py'")
bash  theme={null}
    pip install -U apache-airflow openai agno
    bash  theme={null}
    export OPENAI_API_KEY=xxx
    bash Mac theme={null}
      python cookbook/tools/airflow_tools.py
      bash Windows theme={null}
      python cookbook/tools/airflow_tools.py
      ```
    </CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Install libraries">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Set your API key">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Slack

**URL:** llms-txt#slack

**Contents:**
- Prerequisites
- Example

Source: https://docs.agno.com/concepts/tools/toolkits/social/slack

The following example requires the `slack-sdk` library.

Get a Slack token from [here](https://api.slack.com/tutorials/tracks/getting-a-token).

The following agent will use Slack to send a message to a channel, list all channels, and get the message history of a specific channel.

```python cookbook/tools/slack_tools.py theme={null}
import os

from agno.agent import Agent
from agno.tools.slack import SlackTools

slack_tools = SlackTools()

agent = Agent(tools=[slack_tools])

**Examples:**

Example 1 (unknown):
```unknown
Get a Slack token from [here](https://api.slack.com/tutorials/tracks/getting-a-token).
```

Example 2 (unknown):
```unknown
## Example

The following agent will use Slack to send a message to a channel, list all channels, and get the message history of a specific channel.
```

---

## Then watch them recall the information (the question below states: "Tell me a 2-sentence story using my name")

**URL:** llms-txt#then-watch-them-recall-the-information-(the-question-below-states:-"tell-me-a-2-sentence-story-using-my-name")

**Contents:**
- Follow up in Spanish
- Usage

## Follow up in Spanish
multi_lingual_q_and_a_team.print_response(
    "Cuéntame una historia de 2 oraciones usando mi nombre real.",
    stream=True,
    session_id=session_id,
)
bash  theme={null}
    pip install agno openai
    `bash  theme={null}
    export OPENAI_API_KEY=****
    </Step>

<Step title="Run the agent">
    `
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Install required libraries">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Set environment variables">
```

Example 3 (bash):
```bash
python team_history.py
```

---

## Storage

**URL:** llms-txt#storage

**Contents:**
- Session table schema
- Developer Resources

Source: https://docs.agno.com/concepts/teams/storage

Use Storage to persist Team sessions and state to a database or file.

**Why do we need Session Storage?**

Teams are ephemeral and stateless. When you run a Team, no state is persisted automatically.

In production environments, we serve (or trigger) Teams via an API and need to continue the same session across multiple requests.

Storage persists the session history and state in a database and allows us to pick up where we left off.

Here is a simple example showing how to configure storage for a Team:

See [Agent Session Storage](/concepts/agents/storage) for more details on how sessions are stored in general.

## Session table schema

If you have a `db` configured for your team, the sessions will be stored in the sessions table in your database.

The schema for the sessions table is as follows:

| Field           | Type   | Description                                      |
| --------------- | ------ | ------------------------------------------------ |
| `session_id`    | `str`  | The unique identifier for the session.           |
| `session_type`  | `str`  | The type of the session.                         |
| `agent_id`      | `str`  | The agent ID of the session.                     |
| `team_id`       | `str`  | The team ID of the session.                      |
| `workflow_id`   | `str`  | The workflow ID of the session.                  |
| `user_id`       | `str`  | The user ID of the session.                      |
| `session_data`  | `dict` | The data of the session.                         |
| `agent_data`    | `dict` | The data of the agent.                           |
| `team_data`     | `dict` | The data of the team.                            |
| `workflow_data` | `dict` | The data of the workflow.                        |
| `metadata`      | `dict` | The metadata of the session.                     |
| `runs`          | `list` | The runs of the session.                         |
| `summary`       | `dict` | The summary of the session.                      |
| `created_at`    | `int`  | The timestamp when the session was created.      |
| `updated_at`    | `int`  | The timestamp when the session was last updated. |

This data is best displayed on the [sessions page of the AgentOS UI](https://os.agno.com/sessions).

## Developer Resources

* View the [Team schema](/reference/teams/team)
* View [Cookbook](https://github.com/agno-agi/agno/tree/main/cookbook/db/)

---

## FireCrawl Reader

**URL:** llms-txt#firecrawl-reader

Source: https://docs.agno.com/reference/knowledge/reader/firecrawl

FireCrawlReader is a reader class that allows you to read data from websites using Firecrawl.

<Snippet file="firecrawl-reader-reference.mdx" />

---

## You can get the cached session:

**URL:** llms-txt#you-can-get-the-cached-session:

**Contents:**
- Usage

session = agent.get_session()
bash  theme={null}
    pip install -U agno openai psycopg
    bash  theme={null}
    # Make sure PostgreSQL is running
    # Update connection string in the code as needed
    bash Mac/Linux theme={null}
        export OPENAI_API_KEY="your_openai_api_key_here"
      bash Windows theme={null}
        $Env:OPENAI_API_KEY="your_openai_api_key_here"
      bash  theme={null}
    touch 08_cache_session.py
    bash Mac theme={null}
      python 08_cache_session.py
      bash Windows theme={null}
      python 08_cache_session.py
      ```
    </CodeGroup>
  </Step>

<Step title="Find All Cookbooks">
    Explore all the available cookbooks in the Agno repository. Click the link below to view the code on GitHub:

<Link href="https://github.com/agno-agi/agno/tree/main/cookbook/agents/session" target="_blank">
      Agno Cookbooks on GitHub
    </Link>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Install libraries">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Setup PostgreSQL">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Export your OpenAI API key">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Will load the session state from the session with the id "user_1_session_1"

**URL:** llms-txt#will-load-the-session-state-from-the-session-with-the-id-"user_1_session_1"

team.print_response("How old am I?", session_id="user_1_session_1", user_id="user_1")

---

## Get Team Details

**URL:** llms-txt#get-team-details

Source: https://docs.agno.com/reference-api/schema/teams/get-team-details

get /teams/{team_id}
Retrieve detailed configuration and member information for a specific team.

---

## ms-marco-MiniLM-L-12-v2

**URL:** llms-txt#ms-marco-minilm-l-12-v2

**Contents:**
- Usage

3. Export API Keys
export ANTHROPIC_API_KEY="your-anthropic-api-key"

4. Run the Example
python cookbook/agent_concepts/agentic_search/agentic_rag_infinity_reranker.py

About Infinity Reranker:
- Provides fast, local reranking without external API calls
- Supports multiple state-of-the-art reranking models
- Can be deployed on GPU for better performance
- Offers both sync and async reranking capabilities
- More deployment options: https://michaelfeil.eu/infinity/0.0.76/deploy/
"""

from agno.agent import Agent
from agno.knowledge.embedder.cohere import CohereEmbedder
from agno.knowledge.knowledge import Knowledge
from agno.knowledge.reranker import InfinityReranker
from agno.models.anthropic import Claude
from agno.vectordb.lancedb import LanceDb, SearchType

knowledge = Knowledge(
    # Use LanceDB as the vector database, store embeddings in the `agno_docs_infinity` table
    vector_db=LanceDb(
        uri="tmp/lancedb",
        table_name="agno_docs_infinity",
        search_type=SearchType.hybrid,
        embedder=CohereEmbedder(id="embed-v4.0"),
        # Use Infinity reranker for local, fast reranking
        reranker=InfinityReranker(
            model="BAAI/bge-reranker-base",  # You can change this to other models
            host="localhost",
            port=7997,
            top_n=5,  # Return top 5 reranked documents
        ),
    ),
)

asyncio.run(
    knowledge.add_contents(
        urls=[
            "https://docs.agno.com/introduction/agents.md",
            "https://docs.agno.com/agents/tools.md",
            "https://docs.agno.com/agents/knowledge.md",
        ]
    )
)

agent = Agent(
    model=Claude(id="claude-3-7-sonnet-latest"),
    # Agentic RAG is enabled by default when `knowledge` is provided to the Agent.
    knowledge=knowledge,
    # search_knowledge=True gives the Agent the ability to search on demand
    # search_knowledge is True by default
    search_knowledge=True,
    instructions=[
        "Include sources in your response.",
        "Always search your knowledge before answering the question.",
        "Provide detailed and accurate information based on the retrieved documents.",
    ],
    markdown=True,
)

def test_infinity_connection():
    """Test if Infinity server is running and accessible"""
    try:
        from infinity_client import Client

_ = Client(base_url="http://localhost:7997")
        print("✅ Successfully connected to Infinity server at localhost:7997")
        return True
    except Exception as e:
        print(f"❌ Failed to connect to Infinity server: {e}")
        print(
            "\nPlease make sure Infinity server is running. See setup instructions above."
        )
        return False

if __name__ == "__main__":
    print("🚀 Agentic RAG with Infinity Reranker Example")
    print("=" * 50)

# Test Infinity connection first
    if not test_infinity_connection():
        exit(1)

print("\n🤖 Starting agent interaction...")
    print("=" * 50)

# Example questions to test the reranking capabilities
    questions = [
        "What are Agents and how do they work?",
        "How do I use tools with agents?",
        "What is the difference between knowledge and tools?",
    ]

for i, question in enumerate(questions, 1):
        print(f"\n🔍 Question {i}: {question}")
        print("-" * 40)
        agent.print_response(question, stream=True)
        print("\n" + "=" * 50)

print("\n🎉 Example completed!")
    print("\nThe Infinity reranker helped improve the relevance of retrieved documents")
    print("by reranking them based on semantic similarity to your queries.")
bash  theme={null}
    pip install -U agno anthropic infinity-client lancedb "infinity-emb[all]"
    bash  theme={null}
    # Run infinity server with reranking model
    infinity_emb v2 --model-id BAAI/bge-reranker-base --port 7997
    bash Mac/Linux theme={null}
        export ANTHROPIC_API_KEY="your_anthropic_api_key_here"
      bash Windows theme={null}
        $Env:ANTHROPIC_API_KEY="your_anthropic_api_key_here"
      bash  theme={null}
    touch agentic_rag_infinity_reranker.py
    bash Mac theme={null}
      python agentic_rag_infinity_reranker.py
      bash Windows theme={null}
      python agentic_rag_infinity_reranker.py
      ```
    </CodeGroup>
  </Step>

<Step title="Find All Cookbooks">
    Explore all the available cookbooks in the Agno repository. Click the link below to view the code on GitHub:

<Link href="https://github.com/agno-agi/agno/tree/main/cookbook/agents/agentic_search" target="_blank">
      Agno Cookbooks on GitHub
    </Link>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Install libraries">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Setup Infinity Server">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Export your ANTHROPIC API key">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Security dependency

**URL:** llms-txt#security-dependency

security = HTTPBearer()

async def verify_token(token: str = Depends(security)):
    if token.credentials != "your-secret-token":
        raise HTTPException(status_code=401, detail="Invalid token")
    return token

---

## MLX Transcribe

**URL:** llms-txt#mlx-transcribe

**Contents:**
- Prerequisites
- Example

Source: https://docs.agno.com/concepts/tools/toolkits/others/mlx_transcribe

**MLX Transcribe** is a tool for transcribing audio files using MLX Whisper.

1. **Install ffmpeg**

* macOS: `brew install ffmpeg`
   * Ubuntu: `sudo apt-get install ffmpeg`
   * Windows: Download from [https://ffmpeg.org/download.html](https://ffmpeg.org/download.html)

2. **Install mlx-whisper library**

3. **Prepare audio files**

* Create a 'storage/audio' directory
   * Place your audio files in this directory
   * Supported formats: mp3, mp4, wav, etc.

4. **Download sample audio** (optional)
   * Visit the [audio-samples](https://audio-samples.github.io/) (as an example) and save the audio file to the `storage/audio` directory.

The following agent will use MLX Transcribe to transcribe audio files.

```python cookbook/tools/mlx_transcribe_tools.py theme={null}

from pathlib import Path
from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.mlx_transcribe import MLXTranscribeTools

**Examples:**

Example 1 (unknown):
```unknown
3. **Prepare audio files**

   * Create a 'storage/audio' directory
   * Place your audio files in this directory
   * Supported formats: mp3, mp4, wav, etc.

4. **Download sample audio** (optional)
   * Visit the [audio-samples](https://audio-samples.github.io/) (as an example) and save the audio file to the `storage/audio` directory.

## Example

The following agent will use MLX Transcribe to transcribe audio files.
```

---

## Router function that selects between simple web research or deep tech research loop

**URL:** llms-txt#router-function-that-selects-between-simple-web-research-or-deep-tech-research-loop

def research_strategy_router(step_input: StepInput) -> List[Step]:
    """
    Decide between simple web research or deep tech research loop based on the input topic.
    Returns either a single web research step or a tech research loop.
    """
    # Use the original workflow message if this is the first step
    topic = step_input.previous_step_content or step_input.input or ""
    topic = topic.lower()

# Check if the topic requires deep tech research
    deep_tech_keywords = [
        "startup trends",
        "ai developments",
        "machine learning research",
        "programming languages",
        "developer tools",
        "silicon valley",
        "venture capital",
        "cryptocurrency analysis",
        "blockchain technology",
        "open source projects",
        "github trends",
        "tech industry",
        "software engineering",
    ]

# Check if it's a complex tech topic that needs deep research
    if any(keyword in topic for keyword in deep_tech_keywords) or (
        "tech" in topic and len(topic.split()) > 3
    ):
        print(
            f"🔬 Deep tech topic detected: Using iterative research loop for '{topic}'"
        )
        return [deep_tech_research_loop]
    else:
        print(f"🌐 Simple topic detected: Using basic web research for '{topic}'")
        return [research_web]

workflow = Workflow(
    name="Adaptive Research Workflow",
    description="Intelligently selects between simple web research or deep iterative tech research based on topic complexity",
    steps=[
        Router(
            name="research_strategy_router",
            selector=research_strategy_router,
            choices=[research_web, deep_tech_research_loop],
            description="Chooses between simple web research or deep tech research loop",
        ),
        publish_content,
    ],
)

if __name__ == "__main__":
    print("=== Testing with deep tech topic ===")
    workflow.print_response(
        "Latest developments in artificial intelligence and machine learning and deep tech research trends"
    )
```

To checkout async version, see the cookbook-

* [Router with Loop Steps Workflow (async)](https://github.com/agno-agi/agno/tree/main/cookbook/workflows/_05_workflows_conditional_branching/async/router_with_loop_steps.py)

---

## team = Team(

**URL:** llms-txt#team-=-team(

---

## Tokens-per-minute rate limiting

**URL:** llms-txt#tokens-per-minute-rate-limiting

Source: https://docs.agno.com/faq/tpm-issues

<img src="https://mintcdn.com/agno-v2/yeT29TzCG5roT0hQ/images/tpm_issues.png?fit=max&auto=format&n=yeT29TzCG5roT0hQ&q=85&s=6c4a68b6662597c61b761f782dbe5f65" alt="Chat with pdf" data-og-width="698" width="698" data-og-height="179" height="179" data-path="images/tpm_issues.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/agno-v2/yeT29TzCG5roT0hQ/images/tpm_issues.png?w=280&fit=max&auto=format&n=yeT29TzCG5roT0hQ&q=85&s=9d57cc77b1620f3ebc6ed5ac2348cd53 280w, https://mintcdn.com/agno-v2/yeT29TzCG5roT0hQ/images/tpm_issues.png?w=560&fit=max&auto=format&n=yeT29TzCG5roT0hQ&q=85&s=62e01716372bec142658c779175b6d1e 560w, https://mintcdn.com/agno-v2/yeT29TzCG5roT0hQ/images/tpm_issues.png?w=840&fit=max&auto=format&n=yeT29TzCG5roT0hQ&q=85&s=0840a597cda52c0c883f722e8f0cf13c 840w, https://mintcdn.com/agno-v2/yeT29TzCG5roT0hQ/images/tpm_issues.png?w=1100&fit=max&auto=format&n=yeT29TzCG5roT0hQ&q=85&s=a3f1e2f454802a9143f82e893eb45af0 1100w, https://mintcdn.com/agno-v2/yeT29TzCG5roT0hQ/images/tpm_issues.png?w=1650&fit=max&auto=format&n=yeT29TzCG5roT0hQ&q=85&s=f5556252c255a94e36218a003e929a40 1650w, https://mintcdn.com/agno-v2/yeT29TzCG5roT0hQ/images/tpm_issues.png?w=2500&fit=max&auto=format&n=yeT29TzCG5roT0hQ&q=85&s=ac5204f428d2ee70c268d6ec9e68b44b 2500w" />

If you face any problems with proprietary models (like OpenAI models) where you are rate limited, we provide the option to set `exponential_backoff=True` and to change `delay_between_retries` to a value in seconds (defaults to 1 second).

See our [models documentation](/concepts/models) for specific information about rate limiting.

In the case of OpenAI, they have tier based rate limits. See the [docs](https://platform.openai.com/docs/guides/rate-limits/usage-tiers) for more information.

---

## Send a direct message

**URL:** llms-txt#send-a-direct-message

agent.print_response(
    "Send direct message to the user @AgnoAgi telling them I want to learn more about them and a link to their community.",
    markdown=True,
)

---

## pprint(event)

**URL:** llms-txt#pprint(event)

**Contents:**
- Usage

bash  theme={null}
    pip install agno pydantic rich
    bash  theme={null}
    export OPENAI_API_KEY=****
    export ANTHROPIC_API_KEY=****
    bash  theme={null}
    python cookbook/examples/teams/structured_input_output/02_team_with_parser_model.py
    ```
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Install required libraries">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Set environment variables">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run the agent">
```

---

## from rich.pretty import pprint

**URL:** llms-txt#from-rich.pretty-import-pprint

---

## Get the path to our configuration file

**URL:** llms-txt#get-the-path-to-our-configuration-file

cwd = Path(__file__).parent
config_file_path = str(cwd.joinpath("configuration.yaml"))

---

## Obtain the default credentials and project id from your gcloud CLI session.

**URL:** llms-txt#obtain-the-default-credentials-and-project-id-from-your-gcloud-cli-session.

credentials, project_id = google.auth.default()

---

## Crawl4ai Tools

**URL:** llms-txt#crawl4ai-tools

**Contents:**
- Code
- Usage

Source: https://docs.agno.com/examples/concepts/tools/search/crawl4ai

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Set your API key">
    
  </Step>

<Step title="Install libraries">
    
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Set your API key">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Newspaper Tools

**URL:** llms-txt#newspaper-tools

**Contents:**
- Code
- Usage

Source: https://docs.agno.com/examples/concepts/tools/web_scrape/newspaper

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Set your API key">
    
  </Step>

<Step title="Install libraries">
    
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Set your API key">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Use the function to log a model call

**URL:** llms-txt#use-the-function-to-log-a-model-call

**Contents:**
- Notes

run("Share a 2 sentence horror story")
```

* **Environment Variables**: Ensure your environment variable is correctly set for the WandB API key.
* **Initialization**: Call `weave.init("project-name")` to initialize Weave with your project name.
* **Decorators**: Use `@weave.op()` to decorate functions you want to log with Weave.

By following these steps, you can effectively integrate Agno with Weave, enabling comprehensive logging and visualization of your AI agents' model calls.

---

## --- Workflow Definition ---

**URL:** llms-txt#----workflow-definition----

blog_generator_workflow = Workflow(
    name="Blog Post Generator",
    description="Advanced blog post generator with research and content creation capabilities",
    db=SqliteDb(
        session_table="workflow_session",
        db_file="tmp/blog_generator.db",
    ),
    steps=blog_generation_execution,
    session_state={},  # Initialize empty session state for caching
)

if __name__ == "__main__":
    import random

async def main():
        # Fun example topics to showcase the generator's versatility
        example_topics = [
            "The Rise of Artificial General Intelligence: Latest Breakthroughs",
            "How Quantum Computing is Revolutionizing Cybersecurity",
            "Sustainable Living in 2024: Practical Tips for Reducing Carbon Footprint",
            "The Future of Work: AI and Human Collaboration",
            "Space Tourism: From Science Fiction to Reality",
            "Mindfulness and Mental Health in the Digital Age",
            "The Evolution of Electric Vehicles: Current State and Future Trends",
            "Why Cats Secretly Run the Internet",
            "The Science Behind Why Pizza Tastes Better at 2 AM",
            "How Rubber Ducks Revolutionized Software Development",
        ]

# Test with a random topic
        topic = random.choice(example_topics)

print("🧪 Testing Blog Post Generator v2.0")
        print("=" * 60)
        print(f"📝 Topic: {topic}")
        print()

# Generate the blog post
        resp = await blog_generator_workflow.arun(
            topic=topic,
            use_search_cache=True,
            use_scrape_cache=True,
            use_blog_cache=True,
        )

pprint_run_response(resp, markdown=True, show_time=True)

asyncio.run(main())
```

For more examples and advanced patterns, see [here](/examples/concepts/workflows/01-basic-workflows). Each file demonstrates a specific pattern with detailed comments and real-world use cases.

---

## Team Session

**URL:** llms-txt#team-session

**Contents:**
- TeamSession Attributes
- TeamSession Methods
  - `upsert_run(run: TeamRunOutput)`
  - `get_run(run_id: str) -> Optional[RunOutput]`
  - `get_messages_from_last_n_runs(...) -> List[Message]`
  - `get_session_summary() -> Optional[SessionSummary]`
  - `get_chat_history() -> List[Message]`

Source: https://docs.agno.com/reference/teams/session

## TeamSession Attributes

| Parameter      | Type                                              | Default  | Description                                             |
| -------------- | ------------------------------------------------- | -------- | ------------------------------------------------------- |
| `session_id`   | `str`                                             | Required | Session UUID                                            |
| `team_id`      | `Optional[str]`                                   | `None`   | ID of the team that this session is associated with     |
| `user_id`      | `Optional[str]`                                   | `None`   | ID of the user interacting with this team               |
| `workflow_id`  | `Optional[str]`                                   | `None`   | ID of the workflow that this session is associated with |
| `team_data`    | `Optional[Dict[str, Any]]`                        | `None`   | Team Data: name, team\_id, model, and mode              |
| `session_data` | `Optional[Dict[str, Any]]`                        | `None`   | Session Data: session\_state, images, videos, audio     |
| `metadata`     | `Optional[Dict[str, Any]]`                        | `None`   | Metadata stored with this team                          |
| `runs`         | `Optional[List[Union[TeamRunOutput, RunOutput]]]` | `None`   | List of all runs in the session                         |
| `summary`      | `Optional[SessionSummary]`                        | `None`   | Summary of the session                                  |
| `created_at`   | `Optional[int]`                                   | `None`   | The unix timestamp when this session was created        |
| `updated_at`   | `Optional[int]`                                   | `None`   | The unix timestamp when this session was last updated   |

## TeamSession Methods

### `upsert_run(run: TeamRunOutput)`

Adds a TeamRunOutput to the runs list. If a run with the same `run_id` already exists, it updates the existing run.

### `get_run(run_id: str) -> Optional[RunOutput]`

Retrieves a specific run by its `run_id`.

### `get_messages_from_last_n_runs(...) -> List[Message]`

Gets messages from the last N runs with various filtering options:

* `agent_id`: Filter by agent ID
* `team_id`: Filter by team ID
* `last_n`: Number of recent runs to include
* `skip_role`: Skip messages with specific role
* `skip_status`: Skip runs with specific statuses
* `skip_history_messages`: Whether to skip history messages

### `get_session_summary() -> Optional[SessionSummary]`

Get the session summary for the session

### `get_chat_history() -> List[Message]`

Get the chat history for the session

---

## Cohere Reranker

**URL:** llms-txt#cohere-reranker

Source: https://docs.agno.com/reference/knowledge/reranker/cohere

<Snippet file="reranker-cohere-params.mdx" />

---

## Note: Gemini may not properly interpret OpenAI's message history format

**URL:** llms-txt#note:-gemini-may-not-properly-interpret-openai's-message-history-format

**Contents:**
- Learn More

* [All supported models](/concepts/models/overview)
* [Environment variables setup](/faq/environment-variables)

---

## Meta

**URL:** llms-txt#meta

**Contents:**
- Parameters

Source: https://docs.agno.com/reference/models/meta

The Meta model provides access to Meta's language models.

| Parameter  | Type            | Default                                     | Description                                               |
| ---------- | --------------- | ------------------------------------------- | --------------------------------------------------------- |
| `id`       | `str`           | `"meta-llama/Meta-Llama-3.1-405B-Instruct"` | The id of the Meta model to use                           |
| `name`     | `str`           | `"MetaLlama"`                               | The name of the model                                     |
| `provider` | `str`           | `"Meta"`                                    | The provider of the model                                 |
| `api_key`  | `Optional[str]` | `None`                                      | The API key for Meta (defaults to META\_API\_KEY env var) |
| `base_url` | `str`           | `"https://api.llama-api.com"`               | The base URL for the Meta API                             |

Meta extends the OpenAI-compatible interface and supports most parameters from OpenAI.

---

## Create Ollama embedder

**URL:** llms-txt#create-ollama-embedder

embedder = OllamaEmbedder(id="nomic-embed-text", dimensions=768)

---

## Code

**URL:** llms-txt#code

```python  theme={null}
from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.team.team import Team
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.yfinance import YFinanceTools

try:
    from maxim import Maxim
    from maxim.logger.agno import instrument_agno
except ImportError:
    raise ImportError(
        "`maxim` not installed. Please install using `pip install maxim-py`"
    )

---

## null

**URL:** llms-txt#null

Source: https://docs.agno.com/reference-api/schema/sessions/update-session

patch /sessions/{session_id}

---

## Example 3: Process multiple responses

**URL:** llms-txt#example-3:-process-multiple-responses

**Contents:**
- Usage

print("\n" + "=" * 50)
print("BATCH PROCESSING")
print("=" * 50)

companies = ["AAPL", "GOOGL", "MSFT"]
responses = []

for company in companies:
    response = team.run(f"Analyze {company} stock")
    responses.append(response)
    print(f"Processed {company}: {type(response.content).__name__}")

print(f"Total responses processed: {len(responses)}")
bash  theme={null}
    pip install agno openai exa
    bash  theme={null}
    export OPENAI_API_KEY=****
    export EXA_API_KEY=****
    bash  theme={null}
    python cookbook/examples/teams/basic/response_as_variable.py
    ```
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Install required libraries">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Set environment variables">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run the agent">
```

---

## Smaller dimensions = faster search, lower cost

**URL:** llms-txt#smaller-dimensions-=-faster-search,-lower-cost

**Contents:**
- Monitoring Performance

embedder = OpenAIEmbedder(
    id="text-embedding-3-large",
    dimensions=1024  # Instead of full 3072
)
python  theme={null}

**Examples:**

Example 1 (unknown):
```unknown
## Monitoring Performance

Keep an eye on these metrics:
```

---

## Additional Data and Metadata

**URL:** llms-txt#additional-data-and-metadata

**Contents:**
- Example

Source: https://docs.agno.com/concepts/workflows/additional-data

How to pass additional data to workflows

**When to Use**
Pass metadata, configuration, or contextual information to specific steps without cluttering the main workflow message flow.

* **Separation of Concerns**: Keep workflow logic separate from metadata
* **Step-Specific Context**: Access additional information in custom functions
* **Clean Message Flow**: Main message stays focused on content
* **Flexible Configuration**: Pass user info, priorities, settings, and more

**Access Pattern**
Use `step_input.additional_data` for dictionary access to all additional data passed to the workflow.

```python  theme={null}
from agno.workflow import Step, Workflow, StepInput, StepOutput

def custom_content_planning_function(step_input: StepInput) -> StepOutput:
    """Custom function that uses additional_data for enhanced context"""
    
    # Access the main workflow message
    message = step_input.input
    previous_content = step_input.previous_step_content
    
    # Access additional_data that was passed with the workflow
    additional_data = step_input.additional_data or {}
    user_email = additional_data.get("user_email", "No email provided")
    priority = additional_data.get("priority", "normal")
    client_type = additional_data.get("client_type", "standard")
    
    # Create enhanced planning prompt with context
    planning_prompt = f"""
        STRATEGIC CONTENT PLANNING REQUEST:
        
        Core Topic: {message}
        Research Results: {previous_content[:500] if previous_content else "No research results"}
        
        Additional Context:
        - Client Type: {client_type}
        - Priority Level: {priority}
        - Contact Email: {user_email}
        
        {"🚨 HIGH PRIORITY - Expedited delivery required" if priority == "high" else "📝 Standard delivery timeline"}
        
        Please create a detailed, actionable content plan.
    """
    
    response = content_planner.run(planning_prompt)
    
    enhanced_content = f"""
        ## Strategic Content Plan
        
        **Planning Topic:** {message}
        **Client Details:** {client_type} | {priority.upper()} priority | {user_email}
        
        **Content Strategy:**
        {response.content}
    """
    
    return StepOutput(content=enhanced_content)

---

## Example 1: Create a task

**URL:** llms-txt#example-1:-create-a-task

print("\n=== Create a task ===")
todoist_agent.print_response("Create a todoist task to buy groceries tomorrow at 10am")

---

## os.environ["OTEL_EXPORTER_OTLP_ENDPOINT"]="http://localhost:3000/api/public/otel" # 🏠 Local deployment (>= v3.22.0)

**URL:** llms-txt#os.environ["otel_exporter_otlp_endpoint"]="http://localhost:3000/api/public/otel"-#-🏠-local-deployment-(>=-v3.22.0)

os.environ["OTEL_EXPORTER_OTLP_HEADERS"] = f"Authorization=Basic {LANGFUSE_AUTH}"

from opentelemetry.exporter.otlp.proto.http.trace_exporter import (  # noqa: E402
    OTLPSpanExporter,
)
from opentelemetry.sdk.trace import TracerProvider  # noqa: E402
from opentelemetry.sdk.trace.export import SimpleSpanProcessor  # noqa: E402

trace_provider = TracerProvider()
trace_provider.add_span_processor(SimpleSpanProcessor(OTLPSpanExporter()))

---

## 4. Get the chat_id by going to the URL:

**URL:** llms-txt#4.-get-the-chat_id-by-going-to-the-url:

---

## It should only call the first tool and fail to call the second tool.

**URL:** llms-txt#it-should-only-call-the-first-tool-and-fail-to-call-the-second-tool.

**Contents:**
- Usage

agent.print_response(
    "Find me the current price of TSLA, then after that find me the latest news about Tesla.",
    stream=True,
)
bash  theme={null}
    pip install -U agno anthropic ddgs
    bash Mac/Linux theme={null}
        export ANTHROPIC_API_KEY="your_anthropic_api_key_here"
      bash Windows theme={null}
        $Env:ANTHROPIC_API_KEY="your_anthropic_api_key_here"
      bash  theme={null}
    touch tool_call_limit.py
    bash Mac theme={null}
      python tool_call_limit.py
      bash Windows   theme={null}
      python tool_call_limit.py
      ```
    </CodeGroup>
  </Step>

<Step title="Find All Cookbooks">
    Explore all the available cookbooks in the Agno repository. Click the link below to view the code on GitHub:

<Link href="https://github.com/agno-agi/agno/tree/main/cookbook/agents/other" target="_blank">
      Agno Cookbooks on GitHub
    </Link>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Install libraries">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Export your ANTHROPIC API key">
    <CodeGroup>
```

Example 3 (unknown):
```unknown

```

Example 4 (unknown):
```unknown
</CodeGroup>
  </Step>

  <Step title="Create a Python file">
    Create a Python file and add the above code.
```

---

## Image Ocr With Structured Output

**URL:** llms-txt#image-ocr-with-structured-output

**Contents:**
- Code
- Usage

Source: https://docs.agno.com/examples/models/mistral/image_ocr_with_structured_output

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Set your API key">
    
  </Step>

<Step title="Install libraries">
    
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Set your API key">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Image Input Bytes Content

**URL:** llms-txt#image-input-bytes-content

**Contents:**
- Code

Source: https://docs.agno.com/examples/models/vertexai/claude/image_input_bytes

```python cookbook/models/vertexai/claude/image_input_bytes.py theme={null}
from pathlib import Path

from agno.agent import Agent
from agno.media import Image
from agno.models.vertexai.claude import Claude
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.utils.media import download_image

agent = Agent(
    model=Claude(id="claude-sonnet-4@20250514"),
    tools=[DuckDuckGoTools()],
    markdown=True,
)

image_path = Path(__file__).parent.joinpath("sample.jpg")

download_image(
    url="https://upload.wikimedia.org/wikipedia/commons/0/0c/GoldenGateBridge-001.jpg",
    output_path=str(image_path),
)

---

## Tool Result Caching

**URL:** llms-txt#tool-result-caching

**Contents:**
- On Toolkit
- On @tool

Source: https://docs.agno.com/concepts/tools/caching

Learn how to cache tool results in Agno.

Tool result caching is designed to avoid unnecessary recomputation by storing the results of function calls on disk.
This is useful during development and testing to speed up the development process, avoid rate limiting, and reduce costs.

<Check>
  This is supported for all Agno Toolkits
</Check>

Pass `cache_results=True` to the Toolkit constructor to enable caching for that Toolkit.

Pass `cache_results=True` to the `@tool` decorator to enable caching for that tool.

**Examples:**

Example 1 (unknown):
```unknown
## On @tool

Pass `cache_results=True` to the `@tool` decorator to enable caching for that tool.
```

---

## DuckDuckGo Search

**URL:** llms-txt#duckduckgo-search

**Contents:**
- Code

Source: https://docs.agno.com/examples/concepts/tools/search/duckduckgo

```python cookbook/tools/duckduckgo_tools.py theme={null}
from agno.agent import Agent
from agno.tools.duckduckgo import DuckDuckGoTools

agent = Agent(tools=[DuckDuckGoTools()])
agent.print_response("Whats happening in France?", markdown=True)

---

## 5. Extract and cut video segments - Optional

**URL:** llms-txt#5.-extract-and-cut-video-segments---optional

def extract_segments(response_text):
    import re

segments_pattern = r"\|\s*(\d+:\d+)\s*\|\s*(\d+:\d+)\s*\|\s*(.*?)\s*\|\s*(\d+)\s*\|"
    segments: list[dict] = []

for match in re.finditer(segments_pattern, str(response_text)):
        start_time = match.group(1)
        end_time = match.group(2)
        description = match.group(3)
        score = int(match.group(4))

# Convert timestamps to seconds
        start_seconds = sum(x * int(t) for x, t in zip([60, 1], start_time.split(":")))
        end_seconds = sum(x * int(t) for x, t in zip([60, 1], end_time.split(":")))
        duration = end_seconds - start_seconds

# Only process high-scoring segments
        if 15 <= duration <= 60 and score > 7:
            output_path = output_dir / f"short_{len(segments) + 1}.mp4"

# FFmpeg command to cut video
            command = [
                "ffmpeg",
                "-ss",
                str(start_seconds),
                "-i",
                video_path,
                "-t",
                str(duration),
                "-vf",
                "scale=1080:1920,setsar=1:1",
                "-c:v",
                "libx264",
                "-c:a",
                "aac",
                "-y",
                str(output_path),
            ]

try:
                subprocess.run(command, check=True)
                segments.append(
                    {"path": output_path, "description": description, "score": score}
                )
            except subprocess.CalledProcessError:
                print(f"Failed to process segment: {start_time} - {end_time}")

logger.debug(f"{response.content}")

---

## Simple Accuracy

**URL:** llms-txt#simple-accuracy

**Contents:**
- Code

Source: https://docs.agno.com/examples/concepts/evals/accuracy/basic

Learn to check how complete, correct and accurate an Agno Agent's response is.

This example shows a more complex evaluation that compares the full output of the agent for correctness.

---

## Example 3: Adding more ingredients

**URL:** llms-txt#example-3:-adding-more-ingredients

print("Example 3: Adding Fresh Ingredients")
print("-" * 50)
shopping_team.print_response(
    "I need apples and oranges for my fruit salad", stream=True
)
print(f"Session state: {shopping_team.get_session_state()}")
print()

---

## Generate Video using Replicate

**URL:** llms-txt#generate-video-using-replicate

**Contents:**
- Code
- Usage

Source: https://docs.agno.com/examples/concepts/multimodal/generate-video-replicate

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Set your API key">
    
  </Step>

<Step title="Install libraries">
    
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Set your API key">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## By default, it stores data in /tmp/lancedb

**URL:** llms-txt#by-default,-it-stores-data-in-/tmp/lancedb

vector_db = LanceDb(
    table_name="recipes",
    uri="tmp/lancedb",  # You can change this path to store data elsewhere
)

---

## Example 1: Send a message to a Slack channel

**URL:** llms-txt#example-1:-send-a-message-to-a-slack-channel

agent.print_response("Send a message 'Hello from Agno!' to the channel #general", markdown=True)

---

## Run team and return the response as a variable

**URL:** llms-txt#run-team-and-return-the-response-as-a-variable

response = team.run(input="What is the weather in Tokyo?")

---

## Custom Chunking

**URL:** llms-txt#custom-chunking

**Contents:**
- Usage
- Custom Chunking Params

Source: https://docs.agno.com/concepts/knowledge/chunking/custom-chunking

Custom chunking allows you to implement your own chunking strategy by creating a class that inherits from `ChunkingStrategy`. This is useful when you need to split documents based on specific separators, apply custom logic, or handle domain-specific content formats.

## Custom Chunking Params

<Snippet file="chunking-custom.mdx" />

**Examples:**

Example 1 (unknown):
```unknown
## Usage
```

---

## Pdf Input Url

**URL:** llms-txt#pdf-input-url

**Contents:**
- Code

Source: https://docs.agno.com/examples/models/openai/responses/pdf_input_url

```python cookbook/models/openai/responses/pdf_input_url.py theme={null}
from agno.agent import Agent
from agno.db.postgres import PostgresDb
from agno.media import File
from agno.models.openai.responses import OpenAIResponses

---

## MongoDB

**URL:** llms-txt#mongodb

Source: https://docs.agno.com/reference/storage/mongodb

`MongoDb` is a class that implements the Db interface using MongoDB as the backend storage system. It provides scalable, document-based storage for agent sessions with support for indexing and efficient querying.

<Snippet file="db-mongodb-params.mdx" />

<Snippet file="db-new-bulk-methods.mdx" />

---

## Wait for the vector index to sync with KV

**URL:** llms-txt#wait-for-the-vector-index-to-sync-with-kv

---

## StepInput

**URL:** llms-txt#stepinput

**Contents:**
- Helper Functions

Source: https://docs.agno.com/reference/workflows/step_input

| Parameter               | Type                                                         | Description                                                                |
| ----------------------- | ------------------------------------------------------------ | -------------------------------------------------------------------------- |
| `input`                 | `Optional[Union[str, Dict[str, Any], List[Any], BaseModel]]` | Primary input message (can be any format)                                  |
| `previous_step_content` | `Optional[Any]`                                              | Content from the last step                                                 |
| `previous_step_outputs` | `Optional[Dict[str, StepOutput]]`                            | All previous step outputs by name                                          |
| `additional_data`       | `Optional[Dict[str, Any]]`                                   | Additional context data                                                    |
| `images`                | `Optional[List[Image]]`                                      | Media inputs - images (accumulated from workflow input and previous steps) |
| `videos`                | `Optional[List[Video]]`                                      | Media inputs - videos (accumulated from workflow input and previous steps) |
| `audio`                 | `Optional[List[Audio]]`                                      | Media inputs - audio (accumulated from workflow input and previous steps)  |
| `files`                 | `Optional[List[File]]`                                       | File inputs (accumulated from workflow input and previous steps)           |

| Method                                        | Return Type                            | Description                                                     |
| --------------------------------------------- | -------------------------------------- | --------------------------------------------------------------- |
| `get_step_output(step_name: str)`             | `Optional[StepOutput]`                 | Get the complete StepOutput object from a specific step by name |
| `get_step_content(step_name: str)`            | `Optional[Union[str, Dict[str, str]]]` | Get content from a specific step by name                        |
| `get_all_previous_content()`                  | `str`                                  | Get all previous step content combined                          |
| `get_last_step_content()`                     | `Optional[str]`                        | Get content from the immediate previous step                    |
| `get_workflow_history(num_runs: int)`         | `List[Tuple[str, str]]`                | Get the workflow history as a list of tuples                    |
| `get_workflow_history_context(num_runs: int)` | `str`                                  | Get the workflow history as a formatted context string          |

---

## First run - this will create the cache

**URL:** llms-txt#first-run---this-will-create-the-cache

response = agent.run(
    "Explain the difference between REST and GraphQL APIs with examples"
)
if response and response.metrics:
    print(f"First run cache write tokens = {response.metrics.cache_write_tokens}")

---

## PDF Reader

**URL:** llms-txt#pdf-reader

Source: https://docs.agno.com/reference/knowledge/reader/pdf

PDFReader is a reader class that allows you to read data from PDF files.

<Snippet file="pdf-reader-reference.mdx" />

---

## Firecrawl

**URL:** llms-txt#firecrawl

**Contents:**
- Prerequisites
- Example
- Toolkit Params
- Toolkit Functions
- Developer Resources

Source: https://docs.agno.com/concepts/tools/toolkits/web_scrape/firecrawl

Use Firecrawl with Agno to scrape and crawl the web.

**FirecrawlTools** enable an Agent to perform web crawling and scraping tasks.

The following example requires the `firecrawl-py` library and an API key which can be obtained from [Firecrawl](https://firecrawl.dev).

The following agent will scrape the content from [https://finance.yahoo.com/](https://finance.yahoo.com/) and return a summary of the content:

| Parameter        | Type             | Default                     | Description                                                                  |
| ---------------- | ---------------- | --------------------------- | ---------------------------------------------------------------------------- |
| `api_key`        | `str`            | `None`                      | API key for authentication. Uses FIRECRAWL\_API\_KEY env var if not provided |
| `enable_scrape`  | `bool`           | `True`                      | Enables website scraping functionality                                       |
| `enable_crawl`   | `bool`           | `False`                     | Enables website crawling functionality                                       |
| `enable_mapping` | `bool`           | `False`                     | Enables website mapping functionality                                        |
| `enable_search`  | `bool`           | `False`                     | Enables web search functionality                                             |
| `all`            | `bool`           | `False`                     | Enables all functionality when set to True                                   |
| `formats`        | `List[str]`      | `None`                      | List of formats to be used for operations                                    |
| `limit`          | `int`            | `10`                        | Maximum number of items to retrieve                                          |
| `poll_interval`  | `int`            | `30`                        | Interval in seconds between polling for results                              |
| `search_params`  | `Dict[str, Any]` | `None`                      | Parameters for search operations                                             |
| `api_url`        | `str`            | `https://api.firecrawl.dev` | API URL to use for the Firecrawl app                                         |

| Function         | Description                                                                                                                                                                                                                                             |
| ---------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| `scrape_website` | Scrapes a website using Firecrawl. Parameters include `url` to specify the URL to scrape. The function supports optional formats if specified. Returns the results of the scraping in JSON format.                                                      |
| `crawl_website`  | Crawls a website using Firecrawl. Parameters include `url` to specify the URL to crawl, and an optional `limit` to define the maximum number of pages to crawl. The function supports optional formats and returns the crawling results in JSON format. |
| `map_website`    | Maps a website structure using Firecrawl. Parameters include `url` to specify the URL to map. Returns the mapping results in JSON format.                                                                                                               |
| `search`         | Performs a web search using Firecrawl. Parameters include `query` for the search term and optional `limit` for maximum results. Returns search results in JSON format.                                                                                  |

## Developer Resources

* View [Tools](https://github.com/agno-agi/agno/blob/main/libs/agno/agno/tools/firecrawl.py)
* View [Cookbook](https://github.com/agno-agi/agno/tree/main/cookbook/tools/firecrawl_tools.py)

**Examples:**

Example 1 (unknown):
```unknown

```

Example 2 (unknown):
```unknown
## Example

The following agent will scrape the content from [https://finance.yahoo.com/](https://finance.yahoo.com/) and return a summary of the content:
```

---

## Initialize Pinecone

**URL:** llms-txt#initialize-pinecone

api_key = getenv("PINECONE_API_KEY")
index_name = "filtering-index"

vector_db = PineconeDb(
    name=index_name,
    dimension=1536,
    metric="cosine",
    spec={"serverless": {"cloud": "aws", "region": "us-east-1"}},
    api_key=api_key,
)

---

## session_id=session_id,

**URL:** llms-txt#session_id=session_id,

---

## Custom function step that has access to ALL previous step outputs

**URL:** llms-txt#custom-function-step-that-has-access-to-all-previous-step-outputs

def create_comprehensive_report(step_input: StepInput) -> StepOutput:
    """
    Custom function that creates a report using data from multiple previous steps.
    This function has access to ALL previous step outputs and the original workflow message.
    """

# Access original workflow input
    original_topic = step_input.input or ""

# Access specific step outputs by name
    hackernews_data = step_input.get_step_content("research_hackernews") or ""
    web_data = step_input.get_step_content("research_web") or ""

# Or access ALL previous content
    _ = step_input.get_all_previous_content()

# Create a comprehensive report combining all sources
    report = f"""
        # Comprehensive Research Report: {original_topic}

## Executive Summary
        Based on research from HackerNews and web sources, here's a comprehensive analysis of {original_topic}.

## HackerNews Insights
        {hackernews_data[:500]}...

## Web Research Findings
        {web_data[:500]}...
    """

return StepOutput(
        step_name="comprehensive_report", content=report.strip(), success=True
    )

comprehensive_report_step = Step(
    name="comprehensive_report",
    executor=create_comprehensive_report,
    description="Create comprehensive report from all research sources",
)

---

## X (Twitter)

**URL:** llms-txt#x-(twitter)

**Contents:**
- Prerequisites
- Setup
- Example

Source: https://docs.agno.com/concepts/tools/toolkits/social/x

**XTools** allows an Agent to interact with X, providing functionality for posting, messaging, and searching tweets.

Install the required library:

<Info>Tweepy is a Python library for interacting with the X API.</Info>

1. **Create X Developer Account**

* Visit [developer.x.com](https://developer.x.com) and apply for developer access
   * Create a new project and app in your developer portal

2. **Generate API Credentials**

* Navigate to your app's "Keys and tokens" section
   * Generate and copy these credentials:
     * API Key & Secret
     * Bearer Token
     * Access Token & Secret

3. **Configure Environment**

```python cookbook/tools/x_tools.py theme={null}
from agno.agent import Agent
from agno.tools.x import XTools

**Examples:**

Example 1 (unknown):
```unknown
<Info>Tweepy is a Python library for interacting with the X API.</Info>

## Setup

1. **Create X Developer Account**

   * Visit [developer.x.com](https://developer.x.com) and apply for developer access
   * Create a new project and app in your developer portal

2. **Generate API Credentials**

   * Navigate to your app's "Keys and tokens" section
   * Generate and copy these credentials:
     * API Key & Secret
     * Bearer Token
     * Access Token & Secret

3. **Configure Environment**
```

Example 2 (unknown):
```unknown
## Example
```

---

## Example usage with academic research request

**URL:** llms-txt#example-usage-with-academic-research-request

if __name__ == "__main__":
    research_scholar.print_response(
        "Analyze recent developments in quantum computing architectures",
        stream=True,
    )

---

## Initialize Searxng with your Searxng instance URL

**URL:** llms-txt#initialize-searxng-with-your-searxng-instance-url

searxng = SearxngTools(
    host="http://localhost:53153",
    engines=[],
    fixed_max_results=5,
    news=True,
    science=True
)

---

## from agno.models.mistral.mistral import MistralChat

**URL:** llms-txt#from-agno.models.mistral.mistral-import-mistralchat

**Contents:**
- Usage

from agno.models.openai import OpenAIChat
from agno.team import Team, TeamRunEvent
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.hackernews import HackerNewsTools

wikipedia_agent = Agent(
    id="hacker-news-agent",
    name="Hacker News Agent",
    role="Search Hacker News for information",
    # model=MistralChat(id="mistral-large-latest"),
    tools=[HackerNewsTools()],
    instructions=[
        "Find articles about the company in the Hacker News",
    ],
)

website_agent = Agent(
    id="website-agent",
    name="Website Agent",
    role="Search the website for information",
    model=OpenAIChat(id="gpt-5-mini"),
    tools=[DuckDuckGoTools()],
    instructions=[
        "Search the website for information",
    ],
)

user_id = str(uuid4())
id = str(uuid4())

company_info_team = Team(
    name="Company Info Team",
    id=id,
    user_id=user_id,
    model=Claude(id="claude-3-7-sonnet-latest"),
    members=[
        wikipedia_agent,
        website_agent,
    ],
    markdown=True,
    instructions=[
        "You are a team that finds information about a company.",
        "First search the web and wikipedia for information about the company.",
        "If you can find the company's website URL, then scrape the homepage and the about page.",
    ],
    show_members_responses=True,
)

async def run_team_with_events(prompt: str):
    content_started = False
    async for run_output_event in company_info_team.arun(
        prompt,
        stream=True,
        stream_events=True,
    ):
        if run_output_event.event in [
            TeamRunEvent.run_started,
            TeamRunEvent.run_completed,
        ]:
            print(f"\nTEAM EVENT: {run_output_event.event}")

if run_output_event.event in [TeamRunEvent.tool_call_started]:
            print(f"\nTEAM EVENT: {run_output_event.event}")
            print(f"TOOL CALL: {run_output_event.tool.tool_name}")
            print(f"TOOL CALL ARGS: {run_output_event.tool.tool_args}")

if run_output_event.event in [TeamRunEvent.tool_call_completed]:
            print(f"\nTEAM EVENT: {run_output_event.event}")
            print(f"TOOL CALL: {run_output_event.tool.tool_name}")
            print(f"TOOL CALL RESULT: {run_output_event.tool.result}")

# Member events
        if run_output_event.event in [RunEvent.tool_call_started]:
            print(f"\nMEMBER EVENT: {run_output_event.event}")
            print(f"AGENT ID: {run_output_event.agent_id}")
            print(f"TOOL CALL: {run_output_event.tool.tool_name}")
            print(f"TOOL CALL ARGS: {run_output_event.tool.tool_args}")

if run_output_event.event in [RunEvent.tool_call_completed]:
            print(f"\nMEMBER EVENT: {run_output_event.event}")
            print(f"AGENT ID: {run_output_event.agent_id}")
            print(f"TOOL CALL: {run_output_event.tool.tool_name}")
            print(f"TOOL CALL RESULT: {run_output_event.tool.result}")

if run_output_event.event in [TeamRunEvent.run_content]:
            if not content_started:
                print("CONTENT")
                content_started = True
            else:
                print(run_output_event.content, end="")

if __name__ == "__main__":
    asyncio.run(
        run_team_with_events(
            "Write me a full report on everything you can find about Agno, the company building AI agent infrastructure.",
        )
    )
bash  theme={null}
    pip install agno ddgs
    bash  theme={null}
    export ANTHROPIC_API_KEY=****
    export OPENAI_API_KEY=****
    bash  theme={null}
    python cookbook/examples/teams/streaming/02_events.py
    ```
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Install required libraries">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Set environment variables">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run the agent">
```

---

## Workflow-level: limit history for all steps

**URL:** llms-txt#workflow-level:-limit-history-for-all-steps

workflow = Workflow(
    add_workflow_history_to_steps=True,
    num_history_runs=5  # Only last 5 runs
)

---

## Secure deployment pipeline

**URL:** llms-txt#secure-deployment-pipeline

workflow = Workflow(
    name="Secure Deployment Pipeline",
    steps=[
        Step(name="Security Scan", agent=security_scanner),
        Step(name="Security Gate", executor=security_gate),  # May stop here
        Step(name="Deploy Code", agent=code_deployer),       # Only if secure
        Step(name="Setup Monitoring", agent=monitoring_agent), # Only if deployed
    ]
)

---

## Multi Language Team

**URL:** llms-txt#multi-language-team

**Contents:**
- Code
- Usage

Source: https://docs.agno.com/examples/use-cases/teams/multi_language_team

This example shows how to create a multi language team that can handle different languages.

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Install required libraries">
    
  </Step>

<Step title="Set environment variables">
    
  </Step>

<Step title="Run the agent">
    
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Install required libraries">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Set environment variables">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run the agent">
```

---

## Create distributed search team

**URL:** llms-txt#create-distributed-search-team

**Contents:**
- Usage

distributed_search_team = Team(
    name="Distributed Search Team with Infinity Reranker",
    model=Claude(id="claude-3-7-sonnet-latest"),
    members=[
        primary_searcher,
        secondary_searcher,
        cross_reference_validator,
        result_synthesizer,
    ],
    instructions=[
        "Work together to provide comprehensive search results using distributed processing.",
        "Primary Searcher: Conduct broad comprehensive search first.",
        "Secondary Searcher: Perform targeted specialized search.",
        "Cross-Reference Validator: Validate consistency across all results.",
        "Result Synthesizer: Combine everything into a ranked, comprehensive response.",
        "Leverage the infinity reranker for high-performance result ranking.",
        "Ensure all results are properly attributed and ranked by relevance.",
    ],
    show_members_responses=True,
    markdown=True,
)

async def async_distributed_search():
    """Demonstrate async distributed search with infinity reranking."""
    print("⚡ Async Distributed Search with Infinity Reranker Demo")
    print("=" * 65)

query = "How do Agents work with tools and what are the performance considerations?"

# Add content to both knowledge bases
    await knowledge_primary.add_contents_async(
        urls=["https://docs.agno.com/introduction/agents.md"]
    )
    await knowledge_secondary.add_contents_async(
        urls=["https://docs.agno.com/introduction/agents.md"]
    )

# Run async distributed search
    await distributed_search_team.aprint_response(
        query, stream=True, stream_events=True
    )

def sync_distributed_search():
    """Demonstrate sync distributed search with infinity reranking."""
    print("⚡ Distributed Search with Infinity Reranker Demo")
    print("=" * 55)

query = "How do Agents work with tools and what are the performance considerations?"

# Add content to both knowledge bases
    knowledge_primary.add_contents(
        urls=["https://docs.agno.com/introduction/agents.md"]
    )
    knowledge_secondary.add_contents(
        urls=["https://docs.agno.com/introduction/agents.md"]
    )

# Run distributed search
    distributed_search_team.print_response(
        query, stream=True, stream_events=True
    )

if __name__ == "__main__":
    # Choose which demo to run

try:
        # asyncio.run(async_distributed_search())

sync_distributed_search()
    except Exception as e:
        print(f"❌ Error: {e}")
        print("\n💡 Make sure Infinity server is running:")
        print("   pip install 'infinity-emb[all]'")
        print("   infinity_emb v2 --model-id BAAI/bge-reranker-base --port 7997")
bash  theme={null}
    pip install agno cohere lancedb "infinity-emb[all]"
    bash  theme={null}
    infinity_emb v2 --model-id BAAI/bge-reranker-base --port 7997
    bash  theme={null}
    export ANTHROPIC_API_KEY=****
    export CO_API_KEY=****
    bash  theme={null}
    python cookbook/examples/teams/search_coordination/03_distributed_infinity_search.py
    ```
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Install required libraries">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Set up Infinity server">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Set environment variables">
```

Example 4 (unknown):
```unknown
</Step>

  <Step title="Run the agent">
```

---

## Understands intent and context

**URL:** llms-txt#understands-intent-and-context

user: "Can I send back this item I bought last month?"

---

## Morph

**URL:** llms-txt#morph

**Contents:**
- Example
- Toolkit Params
- Toolkit Functions
- Developer Resources

Source: https://docs.agno.com/concepts/tools/toolkits/models/morph

MorphTools provides advanced code editing capabilities using Morph's Fast Apply API for intelligent code modifications.

The following agent can perform intelligent code editing using Morph:

| Parameter          | Type            | Default                         | Description                                     |
| ------------------ | --------------- | ------------------------------- | ----------------------------------------------- |
| `api_key`          | `Optional[str]` | `None`                          | Morph API key. Uses MORPH\_API\_KEY if not set. |
| `base_url`         | `str`           | `"https://api.morphllm.com/v1"` | Morph API base URL.                             |
| `model`            | `str`           | `"morph-v3-large"`              | Morph model to use for code editing.            |
| `instructions`     | `Optional[str]` | `None`                          | Custom instructions for code editing behavior.  |
| `add_instructions` | `bool`          | `True`                          | Whether to add instructions to the agent.       |

| Function    | Description                                                        |
| ----------- | ------------------------------------------------------------------ |
| `edit_file` | Apply intelligent code modifications using Morph's Fast Apply API. |

## Developer Resources

* View [Tools Source](https://github.com/agno-agi/agno/blob/main/libs/agno/agno/tools/models/morph.py)
* [Morph API Documentation](https://docs.morphllm.com/)
* [Fast Apply API Reference](https://api.morphllm.com/docs)

---

## Reddit

**URL:** llms-txt#reddit

**Contents:**
- Example
- Toolkit Params
- Toolkit Functions
- Developer Resources

Source: https://docs.agno.com/concepts/tools/toolkits/social/reddit

RedditTools enables agents to interact with Reddit for browsing posts, comments, and subreddit information.

The following agent can browse and analyze Reddit content:

| Parameter         | Type                    | Default              | Description                                                   |
| ----------------- | ----------------------- | -------------------- | ------------------------------------------------------------- |
| `reddit_instance` | `Optional[praw.Reddit]` | `None`               | Existing Reddit instance to use.                              |
| `client_id`       | `Optional[str]`         | `None`               | Reddit client ID. Uses REDDIT\_CLIENT\_ID if not set.         |
| `client_secret`   | `Optional[str]`         | `None`               | Reddit client secret. Uses REDDIT\_CLIENT\_SECRET if not set. |
| `user_agent`      | `Optional[str]`         | `"RedditTools v1.0"` | User agent string for API requests.                           |
| `username`        | `Optional[str]`         | `None`               | Reddit username for authenticated access.                     |
| `password`        | `Optional[str]`         | `None`               | Reddit password for authenticated access.                     |

| Function              | Description                                              |
| --------------------- | -------------------------------------------------------- |
| `get_subreddit_info`  | Get information about a specific subreddit.              |
| `get_subreddit_posts` | Get posts from a subreddit with various sorting options. |
| `search_subreddits`   | Search for subreddits by name or topic.                  |
| `get_post_details`    | Get detailed information about a specific post.          |
| `get_post_comments`   | Get comments from a specific post.                       |
| `search_posts`        | Search for posts across Reddit or within subreddits.     |
| `get_user_info`       | Get information about a Reddit user.                     |
| `get_user_posts`      | Get posts submitted by a specific user.                  |
| `get_user_comments`   | Get comments made by a specific user.                    |
| `create_post`         | Create a new post (requires authentication).             |
| `create_comment`      | Create a comment on a post (requires authentication).    |
| `vote_on_post`        | Vote on a post (requires authentication).                |
| `vote_on_comment`     | Vote on a comment (requires authentication).             |

You can use `include_tools` or `exclude_tools` to modify the list of tools the agent has access to. Learn more about [selecting tools](/concepts/tools/selecting-tools).

## Developer Resources

* View [Tools Source](https://github.com/agno-agi/agno/blob/main/libs/agno/agno/tools/reddit.py)
* [Reddit API Documentation](https://www.reddit.com/dev/api/)
* [PRAW Documentation](https://praw.readthedocs.io/)

---

## vLLM

**URL:** llms-txt#vllm

**Contents:**
- Prerequisites
- Example
- Advanced Usage
  - With Tools
- Params

Source: https://docs.agno.com/concepts/models/vllm

[vLLM](https://docs.vllm.ai/en/latest/) is a fast and easy-to-use library for LLM inference and serving, designed for high-throughput and memory-efficient LLM serving.

Install vLLM and start serving a model:

This spins up the vLLM server with an OpenAI-compatible API.

<Note>The default vLLM server URL is `http://localhost:8000/`</Note>

<CodeGroup>
  
</CodeGroup>

vLLM models work seamlessly with Agno tools:

<Note> View more examples [here](/examples/models/vllm/basic). </Note>

For the full list of supported models, see the [vLLM documentation](https://docs.vllm.ai/en/latest/models/supported_models.html).

| Parameter  | Type            | Default                       | Description                                     |
| ---------- | --------------- | ----------------------------- | ----------------------------------------------- |
| `id`       | `str`           | `"microsoft/DialoGPT-medium"` | The id of the model to use with vLLM            |
| `name`     | `str`           | `"vLLM"`                      | The name of the model                           |
| `provider` | `str`           | `"vLLM"`                      | The provider of the model                       |
| `api_key`  | `Optional[str]` | `None`                        | The API key (usually not needed for local vLLM) |
| `base_url` | `str`           | `"http://localhost:8000/v1"`  | The base URL for the vLLM server                |

`VLLM` is a subclass of the [Model](/reference/models/model) class and has access to the same params.

**Examples:**

Example 1 (unknown):
```unknown

```

Example 2 (unknown):
```unknown
This spins up the vLLM server with an OpenAI-compatible API.

<Note>The default vLLM server URL is `http://localhost:8000/`</Note>

## Example

Basic Agent

<CodeGroup>
```

Example 3 (unknown):
```unknown
</CodeGroup>

## Advanced Usage

### With Tools

vLLM models work seamlessly with Agno tools:
```

---

## Grouped Steps Workflow

**URL:** llms-txt#grouped-steps-workflow

**Contents:**
- Basic Example

Source: https://docs.agno.com/concepts/workflows/workflow-patterns/grouped-steps-workflow

Organize multiple steps into reusable, logical sequences for complex workflows with clean separation of concerns

**Key Benefits**: Reusable sequences, cleaner branching logic, modular workflow design

Grouped steps enable modular workflow architecture with reusable components and clear logical boundaries.

```python grouped_steps_workflow.py theme={null}
from agno.workflow import Steps, Step, Workflow

---

## Qdrant FastEmbed Embedder

**URL:** llms-txt#qdrant-fastembed-embedder

**Contents:**
- Code

Source: https://docs.agno.com/examples/concepts/knowledge/embedders/qdrant-fastembed

```python  theme={null}
from agno.knowledge.embedder.fastembed import FastEmbedEmbedder
from agno.knowledge.knowledge import Knowledge
from agno.vectordb.pgvector import PgVector

embeddings = FastEmbedEmbedder().get_embedding(
    "The quick brown fox jumps over the lazy dog."
)

---

## pprint(response)

**URL:** llms-txt#pprint(response)

**Contents:**
- Usage

bash  theme={null}
    pip install -U agno openai ddgs rich
    bash Mac/Linux theme={null}
        export OPENAI_API_KEY="your_openai_api_key_here"
      bash Windows theme={null}
        $Env:OPENAI_API_KEY="your_openai_api_key_here"
      bash  theme={null}
    touch response_as_variable.py
    bash Mac theme={null}
      python response_as_variable.py
      bash Windows theme={null}
      python response_as_variable.py
      ```
    </CodeGroup>
  </Step>

<Step title="Find All Cookbooks">
    Explore all the available cookbooks in the Agno repository. Click the link below to view the code on GitHub:

<Link href="https://github.com/agno-agi/agno/tree/main/cookbook/agents/input_and_output" target="_blank">
      Agno Cookbooks on GitHub
    </Link>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Install libraries">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Export your OpenAI API key">
    <CodeGroup>
```

Example 3 (unknown):
```unknown

```

Example 4 (unknown):
```unknown
</CodeGroup>
  </Step>

  <Step title="Create a Python file">
    Create a Python file and add the above code.
```

---

## IBM WatsonX

**URL:** llms-txt#ibm-watsonx

**Contents:**
- Parameters

Source: https://docs.agno.com/reference/models/ibm-watsonx

The IBM WatsonX model provides access to IBM's language models.

| Parameter    | Type            | Default                                               | Description                                                               |
| ------------ | --------------- | ----------------------------------------------------- | ------------------------------------------------------------------------- |
| `id`         | `str`           | `"meta-llama/llama-3-1-70b-instruct"`                 | The id of the IBM WatsonX model to use                                    |
| `name`       | `str`           | `"IBMWatsonx"`                                        | The name of the model                                                     |
| `provider`   | `str`           | `"IBM"`                                               | The provider of the model                                                 |
| `api_key`    | `Optional[str]` | `None`                                                | The API key for IBM WatsonX (defaults to WATSONX\_API\_KEY env var)       |
| `base_url`   | `str`           | `"https://us-south.ml.cloud.ibm.com/ml/v1/text/chat"` | The base URL for the IBM WatsonX API                                      |
| `project_id` | `Optional[str]` | `None`                                                | The project ID for IBM WatsonX (defaults to WATSONX\_PROJECT\_ID env var) |

IBM WatsonX extends the OpenAI-compatible interface and supports most parameters from OpenAI.

---

## Async Reliability Evaluation

**URL:** llms-txt#async-reliability-evaluation

**Contents:**
- Code

Source: https://docs.agno.com/examples/concepts/evals/reliability/reliability_async

Learn how to run reliability evaluations asynchronously.

This example shows how to run a Reliability evaluation asynchronously.

---

## images=[Image(url="YOUR_IMAGE_URL")],

**URL:** llms-txt#images=[image(url="your_image_url")],

---

## Run As Cli

**URL:** llms-txt#run-as-cli

**Contents:**
- Code
- Usage

Source: https://docs.agno.com/examples/use-cases/agents/run_as_cli

This example shows how to create an interactive CLI app with an agent.

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Set your API key">
    
  </Step>

<Step title="Install libraries">
    
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Set your API key">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Audio Input Output

**URL:** llms-txt#audio-input-output

**Contents:**
- Code

Source: https://docs.agno.com/examples/concepts/multimodal/audio-input-output

```python  theme={null}
import requests
from agno.agent import Agent
from agno.media import Audio
from agno.models.openai import OpenAIChat
from agno.utils.audio import write_audio_to_file

---

## or

**URL:** llms-txt#or

**Contents:**
- Usage

vector_db.delete_by_metadata({"doc_type": "recipe_book"})
bash  theme={null}
    pip install -U weaviate-client pypdf openai agno
    bash Weaviate Cloud theme={null}
      # 1. Create account at https://console.weaviate.cloud/
      # 2. Create a cluster and copy the "REST endpoint" and "Admin" API Key
      # 3. Set environment variables:
      export WCD_URL="your-cluster-url" 
      export WCD_API_KEY="your-api-key"
      # 4. Set local=False in the code
      bash Local Development theme={null}
      # 1. Install Docker from https://docs.docker.com/get-docker/
      # 2. Run Weaviate locally:
      docker run -d \
          -p 8080:8080 \
          -p 50051:50051 \
          --name weaviate \
          cr.weaviate.io/semitechnologies/weaviate:1.28.4
      # 3. Set local=True in the code
      bash  theme={null}
    export OPENAI_API_KEY=xxx
    bash Mac theme={null}
      python cookbook/knowledge/vector_db/weaviate_db/weaviate_db.py
      bash Windows theme={null}
      python cookbook/knowledge/vector_db/weaviate_db/weaviate_db.py
      ```
    </CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Install libraries">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Setup Weaviate">
    <CodeGroup>
```

Example 3 (unknown):
```unknown

```

Example 4 (unknown):
```unknown
</CodeGroup>
  </Step>

  <Step title="Set environment variables">
```

---

## We have to create a tool with the correct name, arguments and docstring

**URL:** llms-txt#we-have-to-create-a-tool-with-the-correct-name,-arguments-and-docstring

---

## Pass a dictionary - it will be automatically validated against ResearchProject schema

**URL:** llms-txt#pass-a-dictionary---it-will-be-automatically-validated-against-researchproject-schema

research_team.print_response(
    input={
        "project_name": "AI Framework Comparison 2024",
        "research_topics": ["LangChain", "CrewAI", "AutoGen", "Agno"],
        "target_audience": "AI Engineers and Developers",
        "depth_level": "intermediate",
        "max_sources": 15,
        "include_recent_only": True,
    }
)

print("\n=== Example 2: Pydantic Model Input (direct pass-through) ===")

---

## X (Twitter) Tools

**URL:** llms-txt#x-(twitter)-tools

**Contents:**
- Code
- Usage

Source: https://docs.agno.com/examples/concepts/tools/social/x

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Set your API key">
    
  </Step>

<Step title="Set your X credentials">
    
  </Step>

<Step title="Install libraries">
    
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Set your API key">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Set your X credentials">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 4 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

---

## Compatibility Overview

**URL:** llms-txt#compatibility-overview

**Contents:**
  - Multimodal Support

Source: https://docs.agno.com/concepts/models/compatibility

All models on Agno supports:

* Streaming responses
* Tool calling
* Structured outputs
* Async execution

<Note>
  HuggingFace supports tool calling through the Agno framework, but not for streaming
  responses.
</Note>

<Note>
  Perplexity supports tool calling through the Agno framework, but their models don't
  natively support tool calls in a straightforward way. This means tool usage may
  be less reliable compared to other providers.
</Note>

<Note>
  Vercel V0 doesn't support native structured output, but does support `use_json_mode=True`.
</Note>

### Multimodal Support

| Agno Supported Models | Image Input | Audio Input | Audio Responses | Video Input | File Upload |
| --------------------- | :---------: | :---------: | :-------------: | :---------: | :---------: |
| AIMLAPI               |      ✅      |             |                 |             |             |
| Anthropic Claude      |      ✅      |             |                 |             |      ✅      |
| AWS Bedrock           |      ✅      |             |                 |             |      ✅      |
| AWS Bedrock Claude    |      ✅      |             |                 |             |      ✅      |
| Azure AI Foundry      |      ✅      |             |                 |             |             |
| Azure OpenAI          |      ✅      |             |                 |             |             |
| Cerebras              |             |             |                 |             |             |
| Cerebras OpenAI       |             |             |                 |             |             |
| Cohere                |      ✅      |             |                 |             |             |
| CometAPI              |      ✅      |             |                 |             |             |
| DashScope             |      ✅      |             |                 |             |             |
| DeepInfra             |             |             |                 |             |             |
| DeepSeek              |             |             |                 |             |             |
| Fireworks             |             |             |                 |             |             |
| Gemini                |      ✅      |      ✅      |                 |      ✅      |      ✅      |
| Groq                  |      ✅      |             |                 |             |             |
| HuggingFace           |      ✅      |             |                 |             |             |
| IBM WatsonX           |      ✅      |             |                 |             |             |
| InternLM              |             |             |                 |             |             |
| LangDB                |      ✅      |      ✅      |                 |             |             |
| LiteLLM               |      ✅      |      ✅      |                 |             |             |
| LiteLLMOpenAI         |             |      ✅      |                 |             |             |
| LlamaCpp              |             |             |                 |             |             |
| LM Studio             |      ✅      |             |                 |             |             |
| Llama                 |      ✅      |             |                 |             |             |
| LlamaOpenAI           |      ✅      |             |                 |             |             |
| Mistral               |      ✅      |             |                 |             |             |
| Nebius                |             |             |                 |             |             |
| Nexus                 |             |             |                 |             |             |
| Nvidia                |             |             |                 |             |             |
| Ollama                |      ✅      |             |                 |             |             |
| OpenAIChat            |      ✅      |      ✅      |        ✅        |             |             |
| OpenAIResponses       |      ✅      |      ✅      |        ✅        |             |      ✅      |
| OpenRouter            |             |             |                 |             |             |
| Perplexity            |             |             |                 |             |             |
| Portkey               |             |             |                 |             |             |
| Requesty              |             |             |                 |             |             |
| Sambanova             |             |             |                 |             |             |
| Siliconflow           |             |             |                 |             |             |
| Together              |      ✅      |             |                 |             |             |
| Vercel V0             |             |             |                 |             |             |
| VLLM                  |             |             |                 |             |             |
| Vertex AI Claude      |      ✅      |             |                 |             |             |
| XAI                   |      ✅      |             |                 |             |             |

---

## Financial Datasets API

**URL:** llms-txt#financial-datasets-api

**Contents:**
- Prerequisites
- Example

Source: https://docs.agno.com/concepts/tools/toolkits/others/financial_datasets

**FinancialDatasetsTools** provide a comprehensive API for retrieving and analyzing diverse financial datasets, including stock prices, financial statements, company information, SEC filings, and cryptocurrency data from multiple providers.

The toolkit requires a Financial Datasets API key that can be obtained by creating an account at [financialdatasets.ai](https://financialdatasets.ai).

Set your API key as an environment variable:

Basic usage of the Financial Datasets toolkit:

```python  theme={null}
from agno.agent import Agent
from agno.tools.financial_datasets import FinancialDatasetsTools

agent = Agent(
    name="Financial Data Agent",
    tools=[FinancialDatasetsTools()],
    description="You are a financial data specialist that helps analyze financial information for stocks and cryptocurrencies.",
    instructions=[
        "When given a financial query:",
        "1. Use appropriate Financial Datasets methods based on the query type",
        "2. Format financial data clearly and highlight key metrics",
        "3. For financial statements, compare important metrics with previous periods when relevant",
        "4. Calculate growth rates and trends when appropriate",
        "5. Handle errors gracefully and provide meaningful feedback",
    ],
    markdown=True,
    )

**Examples:**

Example 1 (unknown):
```unknown
Set your API key as an environment variable:
```

Example 2 (unknown):
```unknown
## Example

Basic usage of the Financial Datasets toolkit:
```

---

## BAAI/bge-reranker-v2-m3

**URL:** llms-txt#baai/bge-reranker-v2-m3

---

## Step

**URL:** llms-txt#step

Source: https://docs.agno.com/reference/workflows/step

| Parameter              | Type                     | Default | Description                                                                                       |
| ---------------------- | ------------------------ | ------- | ------------------------------------------------------------------------------------------------- |
| `name`                 | `Optional[str]`          | `None`  | Name of the step for identification                                                               |
| `agent`                | `Optional[Agent]`        | `None`  | Agent to execute for this step                                                                    |
| `team`                 | `Optional[Team]`         | `None`  | Team to execute for this step                                                                     |
| `executor`             | `Optional[StepExecutor]` | `None`  | Custom function to execute for this step                                                          |
| `step_id`              | `Optional[str]`          | `None`  | Unique identifier for the step (auto-generated if not provided)                                   |
| `description`          | `Optional[str]`          | `None`  | Description of the step's purpose                                                                 |
| `max_retries`          | `int`                    | `3`     | Maximum number of retry attempts on failure                                                       |
| `timeout_seconds`      | `Optional[int]`          | `None`  | Timeout for step execution in seconds                                                             |
| `skip_on_failure`      | `bool`                   | `False` | Whether to skip this step if it fails after all retries                                           |
| `add_workflow_history` | `bool`                   | `False` | If True, add the workflow history to the step                                                     |
| `num_history_runs`     | `int`                    | `None`  | Number of runs to include in the workflow history, if not provided, all history runs are included |

---

## Create a team for collaborative image generation

**URL:** llms-txt#create-a-team-for-collaborative-image-generation

**Contents:**
- Usage

image_team = Team(
    name="Image Generation Team",
    model=OpenAIChat(id="gpt-5-mini"),
    members=[prompt_engineer, image_generator],
    instructions=[
        "Generate high-quality images from user prompts.",
        "Prompt Engineer: First enhance and optimize the user's prompt.",
        "Image Creator: Generate images using the enhanced prompt with DALL-E.",
    ],
    markdown=True,
)

run_stream: Iterator[RunOutputEvent] = image_team.run(
    "Create an image of a yellow siamese cat",
    stream=True,
    stream_events=True,
)
for chunk in run_stream:
    pprint(dataclass_to_dict(chunk, exclude={"messages"}))
    print("---" * 20)
bash  theme={null}
    pip install agno rich
    bash  theme={null}
    export OPENAI_API_KEY=****
    bash  theme={null}
    python cookbook/examples/teams/multimodal/generate_image_with_team.py
    ```
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Install required libraries">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Set environment variables">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run the agent">
```

---

## Recursive Chunking

**URL:** llms-txt#recursive-chunking

Source: https://docs.agno.com/reference/knowledge/chunking/recursive

Recursive chunking is a method of splitting documents into smaller chunks by recursively applying a chunking strategy.
This is useful when you want to process large documents in smaller, manageable pieces.

<Snippet file="chunking-recursive.mdx" />

---

## Supabase

**URL:** llms-txt#supabase

**Contents:**
- Usage

Source: https://docs.agno.com/concepts/db/supabase

Learn to use Supabase as a database provider for your Agents

Agno supports using [Supabase](https://supabase.com/) with the `PostgresDb` class.

You can get started with Supabase following their [Get Started guide](https://supabase.com/docs/guides/getting-started).

You can read more about the [`PostgresDb` class](/concepts/db/postgres) in its section.

```python supabase_for_agent.py theme={null}
from agno.agent import Agent
from agno.db.postgres import PostgresDb
from os import getenv

---

## Teams with Langfuse Via Openinference

**URL:** llms-txt#teams-with-langfuse-via-openinference

**Contents:**
- Code

Source: https://docs.agno.com/examples/concepts/integrations/observability/langfuse_via_openinference_team

```python cookbook/integrations/observability/langfuse_via_openinference_team.py theme={null}
import base64
import os
from uuid import uuid4

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.team import Team
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.newspaper4k import Newspaper4kTools
from openinference.instrumentation.agno import AgnoInstrumentor
from opentelemetry import trace as trace_api
from opentelemetry.exporter.otlp.proto.http.trace_exporter import OTLPSpanExporter
from opentelemetry.sdk.trace import TracerProvider
from opentelemetry.sdk.trace.export import SimpleSpanProcessor

LANGFUSE_AUTH = base64.b64encode(
    f"{os.getenv('LANGFUSE_PUBLIC_KEY')}:{os.getenv('LANGFUSE_SECRET_KEY')}".encode()
).decode()
os.environ["OTEL_EXPORTER_OTLP_ENDPOINT"] = (
    "https://us.cloud.langfuse.com/api/public/otel"  # 🇺🇸 US data region
)

---

## === CONDITION EVALUATOR ===

**URL:** llms-txt#===-condition-evaluator-===

def needs_fact_checking(step_input: StepInput) -> bool:
    """Determine if the research contains claims that need fact-checking"""
    summary = step_input.previous_step_content or ""

# Look for keywords that suggest factual claims
    fact_indicators = [
        "study shows",
        "research indicates",
        "according to",
        "statistics",
        "data shows",
        "survey",
        "report",
        "million",
        "billion",
        "percent",
        "%",
        "increase",
        "decrease",
    ]

return any(indicator in summary.lower() for indicator in fact_indicators)

---

## Using Docker Compose

**URL:** llms-txt#using-docker-compose

docker-compose exec db psql -U toolbox_user -d toolbox_db -c "SELECT COUNT(*) FROM hotels;"

---

## Reply to a post

**URL:** llms-txt#reply-to-a-post

agent.print_response(
    "Can you reply to this [post ID] post as a general message as to how great this project is: https://x.com/AgnoAgi",
    markdown=True,
)

---

## Show team capabilities

**URL:** llms-txt#show-team-capabilities

**Contents:**
- Usage

print("\n🔧 Team Tools Available:")
for t in team.tools:
    print(f"   - {t.name}: {t.description}")

print("\n👥 Team Members:")
for member in team.members:
    print(f"   - {member.name}: {member.role}")
bash  theme={null}
    pip install agno ddgs
    bash  theme={null}
    export OPENAI_API_KEY=****
    bash  theme={null}
    python cookbook/examples/teams/tools/01_team_with_custom_tools.py
    ```
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Install required libraries">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Set environment variables">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run the agent">
```

---

## DynamoDB Workflow Storage

**URL:** llms-txt#dynamodb-workflow-storage

**Contents:**
- Usage

Source: https://docs.agno.com/examples/concepts/db/dynamodb/dynamodb_for_workflow

Agno supports using DynamoDB as a storage backend for Workflows using the `DynamoDb` class.

You need to provide `aws_access_key_id` and `aws_secret_access_key` parameters to the `DynamoDb` class.

```python dynamo_for_workflow.py theme={null}
from agno.agent import Agent
from agno.db.dynamodb import DynamoDb
from agno.models.openai import OpenAIChat
from agno.team import Team
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.hackernews import HackerNewsTools
from agno.workflow.step import Step
from agno.workflow.workflow import Workflow

---

## Verify db contents

**URL:** llms-txt#verify-db-contents

**Contents:**
- Params
- Developer Resources

print("\nVerifying db contents...")
all_sessions = db.get_sessions(session_type=SessionType.AGENT)
print(f"Total sessions in Redis: {len(all_sessions)}")

if all_sessions:
    print("\nSession details:")
    session = all_sessions[0]
    print(f"The stored session: {session}")

<Snippet file="db-redis-params.mdx" />

## Developer Resources

* View [Cookbook](https://github.com/agno-agi/agno/tree/main/cookbook/db/redis/redis_for_agent.py)

---

## Jina Reader

**URL:** llms-txt#jina-reader

**Contents:**
- Prerequisites
- Example
- Toolkit Params
- Toolkit Functions
- Developer Resources

Source: https://docs.agno.com/concepts/tools/toolkits/web_scrape/jina_reader

**JinaReaderTools** enable an Agent to perform web search tasks using Jina.

The following example requires the `jina` library.

The following agent will use Jina API to summarize the content of [https://github.com/AgnoAgi](https://github.com/AgnoAgi)

| Parameter              | Type            | Default                | Description                                                                                         |
| ---------------------- | --------------- | ---------------------- | --------------------------------------------------------------------------------------------------- |
| `api_key`              | `Optional[str]` | `None`                 | The API key for authentication purposes. If not provided, uses JINA\_API\_KEY environment variable. |
| `base_url`             | `str`           | `"https://r.jina.ai/"` | The base URL of the API.                                                                            |
| `search_url`           | `str`           | `"https://s.jina.ai/"` | The URL used for search queries.                                                                    |
| `max_content_length`   | `int`           | `10000`                | The maximum length of content allowed.                                                              |
| `timeout`              | `Optional[int]` | `None`                 | Timeout in seconds for API requests.                                                                |
| `search_query_content` | `bool`          | `True`                 | Include content in search query results.                                                            |
| `enable_read_url`      | `bool`          | `True`                 | Enable the read\_url functionality.                                                                 |
| `enable_search_query`  | `bool`          | `False`                | Enable the search\_query functionality.                                                             |
| `all`                  | `bool`          | `False`                | Enable all functionality.                                                                           |

| Function       | Description                                                                                                                                                                                            |
| -------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |
| `read_url`     | Reads the content of a specified URL using Jina Reader API. Parameters include `url` for the URL to read. Returns the truncated content or an error message if the request fails.                      |
| `search_query` | Performs a web search using Jina Reader API based on a specified query. Parameters include `query` for the search term. Returns the truncated search results or an error message if the request fails. |

## Developer Resources

* View [Tools](https://github.com/agno-agi/agno/blob/main/libs/agno/agno/tools/jina_reader.py)
* View [Cookbook](https://github.com/agno-agi/agno/tree/main/cookbook/tools/jina_reader_tools.py)

**Examples:**

Example 1 (unknown):
```unknown
## Example

The following agent will use Jina API to summarize the content of [https://github.com/AgnoAgi](https://github.com/AgnoAgi)
```

---

## Technology trends

**URL:** llms-txt#technology-trends

tech_prompt = """\
Analyze media trends for:
Keywords: artificial intelligence, machine learning, automation
Sources: techcrunch.com, arstechnica.com, wired.com
"""

---

## Setup the Team with our custom tool.

**URL:** llms-txt#setup-the-team-with-our-custom-tool.

team = Team(
    members=[agent],
    tools=[get_customer_profile],
    model=OpenAIChat(id="gpt-4o"),
    instructions="You are a team that handles customer requests.",
)

async def run_team():
    # Running the Team: The team should call our custom tool and yield the custom event
    async for event in team.arun(
        "Hello, can you get me the customer profile for customer with ID 123?",
        stream=True,
    ):
        if isinstance(event, CustomEvent):
            print(f"✅ Custom event emitted: {event}")

asyncio.run(run_team())
```

---

## This will raise an InputCheckError

**URL:** llms-txt#this-will-raise-an-inputcheckerror

**Contents:**
- Developer Resources

team.run("Can you check what's in https://fake.com?")
```

## Developer Resources

* View [Examples](/examples/concepts/teams/guardrails)
* View [Reference](/reference/hooks/base-guardrail)
* View [Cookbook](https://github.com/agno-agi/agno/tree/main/cookbook/teams/guardrails)

---

## Configuration for the Chat page

**URL:** llms-txt#configuration-for-the-chat-page

chat:
  quick_prompts:
    <AGENT_ID>:
      - <PROMPT_1>
      - <PROMPT_2>
      - <PROMPT_3>
      ...
    ...

---

## Print the run metrics

**URL:** llms-txt#print-the-run-metrics

print("---" * 5, "Run Metrics", "---" * 5)
if run_response and run_response.metrics:
    pprint(run_response.metrics)
else:
    print("No run metrics available")

---

## Send lots of messages...

**URL:** llms-txt#send-lots-of-messages...

**Contents:**
- Search the session history
- Developer Resources

team.print_response("What was my first message?", stream=True)
python session_history_search.py theme={null}
import os
from agno.agent import Agent
from agno.team import Team
from agno.models.openai import OpenAIChat
from agno.db.sqlite import SqliteDb

if os.path.exists("tmp/data.db"):
    os.remove("tmp/data.db")

db = SqliteDb(db_file="tmp/data.db")

team = Team(
    members=[],
    model=OpenAIChat(id="gpt-5-mini"),
    user_id="user_1",
    db=db,
    search_session_history=True,  # allow searching previous sessions
    num_history_sessions=2,  # only include the last 2 sessions in the search to avoid context length issues
)

session_1_id = "session_1_id"
session_2_id = "session_2_id"
session_3_id = "session_3_id"
session_4_id = "session_4_id"
session_5_id = "session_5_id"

team.print_response("What is the capital of South Africa?", session_id=session_1_id)
team.print_response("What is the capital of China?", session_id=session_2_id)
team.print_response("What is the capital of France?", session_id=session_3_id)
team.print_response("What is the capital of Japan?", session_id=session_4_id)
team.print_response(
    "What did I discuss in my previous conversations?", session_id=session_5_id
)  # It should only include information from the last 2 sessions
```

## Developer Resources

* View the [Team schema](/reference/teams/team)
* View the [Session schema](/reference/teams/session)
* View [Cookbook](https://github.com/agno-agi/agno/tree/main/cookbook/teams/basic_flows/)

**Examples:**

Example 1 (unknown):
```unknown
## Search the session history

In some scenarios, you might want to fetch messages from across multiple sessions to provide context or continuity in conversations.

To enable fetching messages from the last N sessions, you need to use the following flags:

* `search_session_history`: Set this to `True` to allow searching through previous sessions.
* `num_history_sessions`: Specify the number of past sessions to include in the search. In the example below, it is set to `2` to include only the last 2 sessions.

It's advisable to keep this number low (2 or 3), as a larger number might fill up the context length of the model, potentially leading to performance issues.

Here's an example of searching through the last 2 sessions:
```

---

## Print the reasoning_content

**URL:** llms-txt#print-the-reasoning_content

print("\n--- reasoning_content from response ---")
if hasattr(response, "reasoning_content") and response.reasoning_content:
    print("✅ reasoning_content FOUND in non-streaming response")
    print(f"   Length: {len(response.reasoning_content)} characters")
    print("\n=== reasoning_content preview (non-streaming) ===")
    preview = response.reasoning_content[:1000]
    if len(response.reasoning_content) > 1000:
        preview += "..."
    print(preview)
else:
    print("❌ reasoning_content NOT FOUND in non-streaming response")

print("\n\n=== Example 3: Processing stream with reasoning=True ===\n")

---

## Example 2: Get company news analysis as a variable

**URL:** llms-txt#example-2:-get-company-news-analysis-as-a-variable

print("\n" + "=" * 50)
print("COMPANY NEWS ANALYSIS")
print("=" * 50)

news_response = team.run("What is in the news about NVDA?")
assert isinstance(news_response.content, CompanyAnalysis)
print(f"Response type: {type(news_response.content)}")
print(f"Company: {news_response.content.company_name}")
print(f"Analysis: {news_response.content.analysis}")
pprint_run_response(news_response)

---

## Redis for Team

**URL:** llms-txt#redis-for-team

**Contents:**
- Usage
  - Run Redis
- Params
- Developer Resources

Source: https://docs.agno.com/examples/concepts/db/redis/redis_for_team

Agno supports using Redis as a storage backend for Teams using the `RedisDb` class.

Install [docker desktop](https://docs.docker.com/desktop/install/mac-install/) and run **Redis** on port **6379** using:

<Snippet file="db-redis-params.mdx" />

## Developer Resources

* View [Cookbook](https://github.com/agno-agi/agno/tree/main/cookbook/db/redis/redis_for_team.py)

**Examples:**

Example 1 (unknown):
```unknown

```

---

## Neo4j

**URL:** llms-txt#neo4j

**Contents:**
- Prerequisites
- Example
- Toolkit Params
- Toolkit Functions
- Developer Resources

Source: https://docs.agno.com/concepts/tools/toolkits/database/neo4j

**Neo4jTools** enables agents to interact with Neo4j graph databases for querying and managing graph data.

The following example requires the `neo4j` library.

You will also need a Neo4j database. The following example uses a Neo4j database running in a Docker container.

Make sure to set the `NEO4J_URI` environment variable to the URI of the Neo4j database.

The following agent can interact with Neo4j graph databases:

| Parameter                   | Type            | Default | Description                                       |
| --------------------------- | --------------- | ------- | ------------------------------------------------- |
| `uri`                       | `Optional[str]` | `None`  | Neo4j connection URI. Uses NEO4J\_URI if not set. |
| `user`                      | `Optional[str]` | `None`  | Neo4j username. Uses NEO4J\_USERNAME if not set.  |
| `password`                  | `Optional[str]` | `None`  | Neo4j password. Uses NEO4J\_PASSWORD if not set.  |
| `database`                  | `Optional[str]` | `None`  | Specific database name to connect to.             |
| `enable_list_labels`        | `bool`          | `True`  | Enable listing node labels.                       |
| `enable_list_relationships` | `bool`          | `True`  | Enable listing relationship types.                |
| `enable_get_schema`         | `bool`          | `True`  | Enable schema information retrieval.              |
| `enable_run_cypher`         | `bool`          | `True`  | Enable Cypher query execution.                    |

| Function             | Description                                           |
| -------------------- | ----------------------------------------------------- |
| `list_labels`        | List all node labels in the graph database.           |
| `list_relationships` | List all relationship types in the graph database.    |
| `get_schema`         | Get comprehensive schema information about the graph. |
| `run_cypher`         | Execute Cypher queries on the graph database.         |

## Developer Resources

* View [Tools Source](https://github.com/agno-agi/agno/blob/main/libs/agno/agno/tools/neo4j.py)
* [Neo4j Documentation](https://neo4j.com/docs/)
* [Cypher Query Language](https://neo4j.com/docs/cypher-manual/current/)

**Examples:**

Example 1 (unknown):
```unknown
You will also need a Neo4j database. The following example uses a Neo4j database running in a Docker container.
```

Example 2 (unknown):
```unknown
Make sure to set the `NEO4J_URI` environment variable to the URI of the Neo4j database.
```

Example 3 (unknown):
```unknown
Install libraries
```

Example 4 (unknown):
```unknown
Run the agent
```

---

## --- PDF utility ---

**URL:** llms-txt#----pdf-utility----

def extract_text_from_pdf(url: str) -> str:
    try:
        resp = requests.get(url)
        resp.raise_for_status()
        reader = PdfReader(io.BytesIO(resp.content))
        return "\n".join(page.extract_text() or "" for page in reader.pages)
    except Exception as e:
        print(f"Error extracting PDF from {url}: {e}")
        return ""

---

## Field Labeled CSV Reader

**URL:** llms-txt#field-labeled-csv-reader

Source: https://docs.agno.com/reference/knowledge/reader/field-labeled-csv

FieldLabeledCSVReader is a reader class that converts CSV rows into field-labeled text documents.

<Snippet file="field-labeled-csv-reader-reference.mdx" />

---

## "current_context": get_current_context,

**URL:** llms-txt#"current_context":-get_current_context,

---

## Add CORS middleware

**URL:** llms-txt#add-cors-middleware

app.add_middleware(
    CORSMiddleware,
    allow_origins=["https://yourdomain.com"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

---

## "Analyze this landmark's architecture and recent news.",

**URL:** llms-txt#"analyze-this-landmark's-architecture-and-recent-news.",

---

## DuckDb

**URL:** llms-txt#duckdb

**Contents:**
- Prerequisites
- Example
- Toolkit Params
- Toolkit Functions
- Developer Resources

Source: https://docs.agno.com/concepts/tools/toolkits/database/duckdb

**DuckDbTools** enable an Agent to run SQL and analyze data using DuckDb.

The following example requires DuckDB library. To install DuckDB, run the following command:

For more installation options, please refer to [DuckDB documentation](https://duckdb.org/docs/installation).

The following agent will analyze the movies file using SQL and return the result.

| Parameter       | Type                 | Default | Description                                                   |
| --------------- | -------------------- | ------- | ------------------------------------------------------------- |
| `db_path`       | `str`                | `None`  | Specifies the path to the database file.                      |
| `connection`    | `DuckDBPyConnection` | `None`  | Provides an existing DuckDB connection object.                |
| `init_commands` | `List`               | `None`  | A list of initial SQL commands to run on database connection. |
| `read_only`     | `bool`               | `False` | Configures the database connection to be read-only.           |
| `config`        | `dict`               | `None`  | Configuration options for the database connection.            |

| Function                   | Description                                                                                                                                                                                                                                    |
| -------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| `show_tables`              | Function to show tables in the database                                                                                                                                                                                                        |
| `describe_table`           | Function to describe a table                                                                                                                                                                                                                   |
| `inspect_query`            | Function to inspect a query and return the query plan. Always inspect your query before running them.                                                                                                                                          |
| `run_query`                | Function that runs a query and returns the result.                                                                                                                                                                                             |
| `summarize_table`          | Function to compute a number of aggregates over a table. The function launches a query that computes a number of aggregates over all columns, including min, max, avg, std and approx\_unique.                                                 |
| `get_table_name_from_path` | Get the table name from a path                                                                                                                                                                                                                 |
| `create_table_from_path`   | Creates a table from a path                                                                                                                                                                                                                    |
| `export_table_to_path`     | Save a table in a desired format (default: parquet). If the path is provided, the table will be saved under that path. Eg: If path is /tmp, the table will be saved as /tmp/table.parquet. Otherwise it will be saved in the current directory |
| `load_local_path_to_table` | Load a local file into duckdb                                                                                                                                                                                                                  |
| `load_local_csv_to_table`  | Load a local CSV file into duckdb                                                                                                                                                                                                              |
| `load_s3_path_to_table`    | Load a file from S3 into duckdb                                                                                                                                                                                                                |
| `load_s3_csv_to_table`     | Load a CSV file from S3 into duckdb                                                                                                                                                                                                            |
| `create_fts_index`         | Create a full text search index on a table                                                                                                                                                                                                     |
| `full_text_search`         | Full text Search in a table column for a specific text/keyword                                                                                                                                                                                 |

You can use `include_tools` or `exclude_tools` to modify the list of tools the agent has access to. Learn more about [selecting tools](/concepts/tools/selecting-tools).

## Developer Resources

* View [Tools](https://github.com/agno-agi/agno/blob/main/libs/agno/agno/tools/duckdb.py)
* View [Cookbook](https://github.com/agno-agi/agno/tree/main/cookbook/tools/duckdb_tools.py)

**Examples:**

Example 1 (unknown):
```unknown
For more installation options, please refer to [DuckDB documentation](https://duckdb.org/docs/installation).

## Example

The following agent will analyze the movies file using SQL and return the result.
```

---

## Define a TypedDict schema

**URL:** llms-txt#define-a-typeddict-schema

class ResearchTopicDict(TypedDict):
    topic: str
    focus_areas: List[str]
    target_audience: str
    sources_required: int

---

## Daytona

**URL:** llms-txt#daytona

**Contents:**
- Prerequisites
- Example

Source: https://docs.agno.com/concepts/tools/toolkits/others/daytona

Enable your Agents to run code in a remote, secure sandbox.

**Daytona** offers secure and elastic infrastructure for runnning your AI-generated code. At Agno, we integrate with it to enable your Agents and Teams to run code in your Daytona sandboxes.

The Daytona tools require the `daytona_sdk` Python package:

You will also need a Daytona API key. You can get it from your [Daytona account](https://app.daytona.io/account):

The following example demonstrates how to create an agent that can run Python code in a Daytona sandbox:

```python cookbook/tools/daytona_tools.py theme={null}
from agno.agent import Agent
from agno.models.anthropic import Claude
from agno.tools.daytona import DaytonaTools

daytona_tools = DaytonaTools()

**Examples:**

Example 1 (unknown):
```unknown
You will also need a Daytona API key. You can get it from your [Daytona account](https://app.daytona.io/account):
```

Example 2 (unknown):
```unknown
## Example

The following example demonstrates how to create an agent that can run Python code in a Daytona sandbox:
```

---

## Example: Sandbox management

**URL:** llms-txt#example:-sandbox-management

agent.print_response("What's the current status of our sandbox and how much time is left before timeout?")

---

## Create steps

**URL:** llms-txt#create-steps

manage_items_step = Step(
    name="manage_items",
    description="Help manage shopping list items (add/remove)",
    agent=shopping_assistant,
)

view_list_step = Step(
    name="view_list",
    description="View and manage the complete shopping list",
    agent=list_manager,
)

---

## Steps

**URL:** llms-txt#steps

Source: https://docs.agno.com/reference/workflows/steps-step

| Parameter     | Type                  | Default | Description                                                        |
| ------------- | --------------------- | ------- | ------------------------------------------------------------------ |
| `name`        | `Optional[str]`       | `None`  | Name of the steps group for identification                         |
| `description` | `Optional[str]`       | `None`  | Description of the steps group's purpose                           |
| `steps`       | `Optional[List[Any]]` | `[]`    | List of steps to execute sequentially (empty list if not provided) |

---

## ag infra down

**URL:** llms-txt#ag-infra-down

**Contents:**
- Params

Source: https://docs.agno.com/reference/agno-infra/cli/ws/down

Delete resources for active infra

<ResponseField name="resources_filter" type="str">
  Resource filter. Format - ENV:INFRA:GROUP:NAME:TYPE
</ResponseField>

<ResponseField name="env_filter" type="str">
  Filter the environment to deploy `--env` `-e`
</ResponseField>

<ResponseField name="infra_filter" type="str">
  Filter the infra to deploy. `--infra` `-i`
</ResponseField>

<ResponseField name="group_filter" type="str">
  Filter resources using group name. `--group` `-g`
</ResponseField>

<ResponseField name="name_filter" type="str">
  Filter resource using name. `--name` `-n`
</ResponseField>

<ResponseField name="type_filter" type="str">
  Filter resource using type `--type` `-t`
</ResponseField>

<ResponseField name="dry_run" type="bool">
  Print resources and exit. `--dry-run` `-dr`
</ResponseField>

<ResponseField name="auto_confirm" type="bool">
  Skip the confirmation before deploying resources. `--yes` `-y`
</ResponseField>

<ResponseField name="print_debug_log" type="bool">
  Print debug logs. `--debug` `-d`
</ResponseField>

<ResponseField name="force" type="bool">
  Force `--force` `-f`
</ResponseField>

---

## Updating Tools

**URL:** llms-txt#updating-tools

**Contents:**
- Agent Example

Source: https://docs.agno.com/concepts/tools/attaching-tools

Learn how to add/update tools on Agents and Teams after they have been created.

Tools can be added to Agents and Teams post-creation. This gives you the flexibility to add tools to an existing Agent or Team instance after initialization, which is useful for dynamic tool management or when you need to conditionally add tools based on runtime requirements.
The whole collection of tools available to an Agent or Team can also be updated by using the `set_tools` call. Note that this will remove any other tools already assigned to your Agent or Team and override it with the list of tools provided to `set_tools`.

Create your own tool, for example `get_weather`. Then call `add_tool` to attach it to your Agent.

---

## Add YouTube video content synchronously

**URL:** llms-txt#add-youtube-video-content-synchronously

knowledge.add_content(
    metadata={"source": "youtube", "type": "educational"},
    urls=[
        "https://www.youtube.com/watch?v=dQw4w9WgXcQ",  # Replace with actual educational video
        "https://www.youtube.com/watch?v=example123"   # Replace with actual video URL
    ],
    reader=YouTubeReader(),
)

---

## Dependencies

**URL:** llms-txt#dependencies

**Contents:**
- Adding the entire context to the user message
- Developer Resources

Source: https://docs.agno.com/concepts/teams/dependencies

Learn how to use dependencies in your teams.

**Dependencies** is a way to inject variables into your Team Context. `dependencies` is a dictionary that contains a set of functions (or static variables) that are resolved before the team runs.

<Note>
  You can use dependencies to inject memories, dynamic few-shot examples, "retrieved" documents, etc.
</Note>

<Check>
  Dependencies are automatically resolved when the team is run.
</Check>

## Adding the entire context to the user message

Set `add_dependencies_to_context=True` to add the entire list of dependencies to the user message. This way you don't have to manually add the dependencies to the instructions.

<Tip>
  You can pass `dependencies` and `add_dependencies_to_context` to the `run`, `arun`, `print_response` and `aprint_response` methods.
</Tip>

## Developer Resources

* View the [Team schema](/reference/teams/team)
* View [Cookbook](https://github.com/agno-agi/agno/tree/main/cookbook/teams/dependencies)

**Examples:**

Example 1 (unknown):
```unknown
<Check>
  Dependencies are automatically resolved when the team is run.
</Check>

## Adding the entire context to the user message

Set `add_dependencies_to_context=True` to add the entire list of dependencies to the user message. This way you don't have to manually add the dependencies to the instructions.
```

---

## Test the team

**URL:** llms-txt#test-the-team

team.print_response("What is the capital of France?", stream=True)

---

## Vercel v0

**URL:** llms-txt#vercel-v0

**Contents:**
- Parameters

Source: https://docs.agno.com/reference/models/vercel

The Vercel v0 model provides access to Vercel's language models.

| Parameter  | Type            | Default                       | Description                                                   |
| ---------- | --------------- | ----------------------------- | ------------------------------------------------------------- |
| `id`       | `str`           | `"v0"`                        | The id of the Vercel model to use                             |
| `name`     | `str`           | `"VercelV0"`                  | The name of the model                                         |
| `provider` | `str`           | `"Vercel"`                    | The provider of the model                                     |
| `api_key`  | `Optional[str]` | `None`                        | The API key for Vercel (defaults to VERCEL\_API\_KEY env var) |
| `base_url` | `str`           | `"https://api.vercel.com/v1"` | The base URL for the Vercel API                               |

Vercel extends the OpenAI-compatible interface and supports most parameters from OpenAI.

---

## Persistent Session with History Context

**URL:** llms-txt#persistent-session-with-history-context

**Contents:**
- Code
- Usage

Source: https://docs.agno.com/examples/concepts/teams/session/persistent_session_history

This example shows how to use the session history to store conversation history and add it to the context with configurable history limits.

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Install required libraries">
    
  </Step>

<Step title="Set environment variables">
    
  </Step>

<Step title="Start PostgreSQL database">
    
  </Step>

<Step title="Run the agent">
    
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Install required libraries">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Set environment variables">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Start PostgreSQL database">
```

Example 4 (unknown):
```unknown
</Step>

  <Step title="Run the agent">
```

---

## Replicate

**URL:** llms-txt#replicate

**Contents:**
- Prerequisites
- Example
- Toolkit Params
- Toolkit Functions
- Developer Resources

Source: https://docs.agno.com/concepts/tools/toolkits/others/replicate

**ReplicateTools** enables an Agent to generate media using the [Replicate platform](https://replicate.com/).

The following example requires the `replicate` library. To install the Replicate client, run the following command:

The following agent will use Replicate to generate images or videos requested by the user.

| Parameter               | Type   | Default            | Description                                                          |
| ----------------------- | ------ | ------------------ | -------------------------------------------------------------------- |
| `api_key`               | `str`  | `None`             | If you want to manually supply the Replicate API key.                |
| `model`                 | `str`  | `minimax/video-01` | The replicate model to use. Find out more on the Replicate platform. |
| `enable_generate_media` | `bool` | `True`             | Enable the generate\_media functionality.                            |
| `all`                   | `bool` | `False`            | Enable all functionality.                                            |

| Function         | Description                                                                         |
| ---------------- | ----------------------------------------------------------------------------------- |
| `generate_media` | Generate either an image or a video from a prompt. The output depends on the model. |

## Developer Resources

* View [Tools](https://github.com/agno-agi/agno/blob/main/libs/agno/agno/tools/replicate.py)
* View [Cookbook](https://github.com/agno-agi/agno/tree/main/cookbook/tools/replicate_tools.py)

**Examples:**

Example 1 (unknown):
```unknown
The following example requires the `replicate` library. To install the Replicate client, run the following command:
```

Example 2 (unknown):
```unknown
## Example

The following agent will use Replicate to generate images or videos requested by the user.
```

---

## Get Workflow Details

**URL:** llms-txt#get-workflow-details

Source: https://docs.agno.com/reference-api/schema/workflows/get-workflow-details

get /workflows/{workflow_id}
Retrieve detailed configuration and step information for a specific workflow.

---

## Define workflow steps

**URL:** llms-txt#define-workflow-steps

suggestion_step = Step(
    name="Meal Suggestion",
    agent=meal_suggester,
)

preference_analysis_step = Step(
    name="Preference Analysis",
    executor=analyze_food_preferences,
)

recipe_step = Step(
    name="Recipe Recommendations",
    agent=recipe_specialist,
)

---

## Example 2: Generate an image with custom settings

**URL:** llms-txt#example-2:-generate-an-image-with-custom-settings

**Contents:**
- Toolkit Params
- Toolkit Functions
- Developer Resources

custom_dalle = Dalle(model="dall-e-3", size="1792x1024", quality="hd", style="natural")

agent_custom = Agent(
    tools=[custom_dalle],
    name="Custom DALL-E Generator",
    )

agent_custom.print_response("Create a panoramic nature scene showing a peaceful mountain lake at sunset", markdown=True)
```

| Parameter             | Type   | Default       | Description                                                       |
| --------------------- | ------ | ------------- | ----------------------------------------------------------------- |
| `model`               | `str`  | `"dall-e-3"`  | The DALL-E model to use                                           |
| `enable_create_image` | `bool` | `True`        | Enable the create image functionality                             |
| `n`                   | `int`  | `1`           | Number of images to generate                                      |
| `size`                | `str`  | `"1024x1024"` | Image size (256x256, 512x512, 1024x1024, 1792x1024, or 1024x1792) |
| `quality`             | `str`  | `"standard"`  | Image quality (standard or hd)                                    |
| `style`               | `str`  | `"vivid"`     | Image style (vivid or natural)                                    |
| `api_key`             | `str`  | `None`        | The OpenAI API key for authentication                             |
| `all`                 | `bool` | `False`       | Enable all functionality when set to True                         |

| Function         | Description                               |
| ---------------- | ----------------------------------------- |
| `generate_image` | Generates an image based on a text prompt |

## Developer Resources

* View [Tools](https://github.com/agno-agi/agno/blob/main/libs/agno/agno/tools/dalle.py)
* View [Cookbook](https://github.com/agno-agi/agno/tree/main/cookbook/tools/dalle_tools.py)

---

## Docx Reader

**URL:** llms-txt#docx-reader

Source: https://docs.agno.com/reference/knowledge/reader/docx

DocxReader is a reader class that allows you to read data from Docx files.

<Snippet file="docx-reader-reference.mdx" />

---

## Website Reader

**URL:** llms-txt#website-reader

Source: https://docs.agno.com/reference/knowledge/reader/website

WebsiteReader is a reader class that allows you to read data from websites.

<Snippet file="website-reader-reference.mdx" />

---

## Agno

**URL:** llms-txt#agno

python cookbook/evals/performance/instantiate_agent_with_tool.py

---

## Workflow Tools

**URL:** llms-txt#workflow-tools

**Contents:**
- Example

Source: https://docs.agno.com/concepts/workflows/workflow-tools

How to execute a workflow inside an Agent or Team

You can give a workflow to an Agent or Team to execute using `WorkflowTools`.

```python  theme={null}
from agno.agent import Agent    
from agno.models.openai import OpenAIChat
from agno.tools.workflow import WorkflowTools

---

## Conversation History

**URL:** llms-txt#conversation-history

**Contents:**
- History Reference
- Add history to the team context
- Send team history to members

Source: https://docs.agno.com/concepts/teams/chat_history

Learn about Team session history and managing conversation history.

Teams with storage enabled automatically have access to the run history of the session (also called the "conversation history" or "chat history").

<Note>
  For all forms of session history, you need to have a database assigned to the team. See [Storage](/concepts/teams/storage) for more details.
</Note>

We can give the Team access to the chat history in the following ways:

* **Team-Level History**:
  * You can set `add_history_to_context=True` and `num_history_runs=5` to add the inputs and responses from the last 5 runs automatically to every request sent to the team leader.
  * You can set `read_chat_history=True` to provide a `get_chat_history()` tool to your team allowing it to read any message in the entire chat history.
  * We can also set `read_tool_call_history=True` to provide a `get_tool_call_history()` tool to your team allowing it to read tool calls in reverse chronological order.
  * You can enable `search_session_history` to allow searching through previous sessions.
  * You can set `add_team_history_to_members=True` and `num_team_history_runs=5` to add the inputs and responses from the last 5 runs (that is the team-level inputs and responses) automatically to every message sent to the team members.
* **Member-Level History**:
  * You can also enable `add_history_to_context` for individual team members. This will only add the inputs and outputs for that member to all requests sent to that member, giving it access to its own history.

<Tip>
  Working with team history can be tricky. Experiment with the above settings to find the best fit for your use case.
  See the [History Reference](#history-reference) for help on how to use the different history features.
</Tip>

<Tabs>
  <Tab title="Simple History">
    Start with **Team History in Context** for basic conversation continuity:

<Tab title="Member Coordination">
    Use **Team History to Members** for shared context:

<Tab title="Share Interaction Information">
    Share **Member Interactions** during a run:

<Tab title="Long Conversations">
    Add **Chat History Tool** when agents need to search history:

<Tab title="Multi-Session Memory">
    Enable **Multi-Session Search** for cross-session continuity:

<Note>
  **Database Requirement**: All history features require a database configured on the team. See [Storage](/concepts/teams/storage) for setup.
</Note>

<Tip>
  **Performance Tip**: More history = larger context = slower and costlier requests. Start with `num_history_runs=3` and increase only if needed.
</Tip>

## Add history to the team context

To add the history of the conversation to the context, you can set `add_history_to_context=True`.
This will add the inputs and responses from the last 3 runs (that is the default) to the context of the team leader.
You can change the number of runs by setting `num_history_runs=n` where `n` is the number of runs to include.

Take a look at this example:

See the full example in the [Add history to the team context](/examples/concepts/teams/basic/respond_directly_with_history) documentation.

## Send team history to members

To send the team history to the members, you can set `add_team_history_to_members=True`.
This will send the inputs and responses from the last 3 team-level runs (that is the default) to the members when tasks are delegated to them.
You can change the number of runs by setting `num_team_history_runs=n` where `n` is the number of runs to include.

Take a look at this example:

```python  theme={null}
from agno.agent import Agent
from agno.db.sqlite import SqliteDb
from agno.models.openai import OpenAIChat
from agno.team.team import Team

german_agent = Agent(
    name="German Agent",
    role="You answer German questions.",
    model=OpenAIChat(id="o3-mini"),
)

spanish_agent = Agent(
    name="Spanish Agent",
    role="You answer Spanish questions.",
    model=OpenAIChat(id="o3-mini"),
)

multi_lingual_q_and_a_team = Team(
    name="Multi Lingual Q and A Team",
    model=OpenAIChat("o3-mini"),
    members=[german_agent, spanish_agent],
    instructions=[
        "You are a multi lingual Q and A team that can answer questions in English and Spanish. You MUST delegate the task to the appropriate member based on the language of the question.",
        "If the question is in German, delegate to the German agent. If the question is in Spanish, delegate to the Spanish agent.",
        "Always translate the response from the appropriate language to English and show both the original and translated responses.",
    ],
    db=SqliteDb(
        db_file="tmp/multi_lingual_q_and_a_team.db"
    ),  # Add a database to store the conversation history. This is a requirement for history to work correctly.
    determine_input_for_members=False,  # Send the input directly to the member agents without the team leader synthesizing its own input.
    respond_directly=True,  # The team leader will not process responses from the members and instead will return them directly.
    add_team_history_to_members=True,  # Send all interactions between the user and the team to the member agents.
)

**Examples:**

Example 1 (unknown):
```unknown
</Tab>

  <Tab title="Member Coordination">
    Use **Team History to Members** for shared context:
```

Example 2 (unknown):
```unknown
</Tab>

  <Tab title="Share Interaction Information">
    Share **Member Interactions** during a run:
```

Example 3 (unknown):
```unknown
</Tab>

  <Tab title="Long Conversations">
    Add **Chat History Tool** when agents need to search history:
```

Example 4 (unknown):
```unknown
</Tab>

  <Tab title="Multi-Session Memory">
    Enable **Multi-Session Search** for cross-session continuity:
```

---

## Check if team has session state and display information

**URL:** llms-txt#check-if-team-has-session-state-and-display-information

print("\n📊 Team Session Info:")
session = team.get_session()
print(f"   Session ID: {session.session_id}")
print(f"   Session State: {session.session_data['session_state']}")

---

## Basic Stream

**URL:** llms-txt#basic-stream

**Contents:**
- Code

Source: https://docs.agno.com/examples/models/xai/basic_stream

```python cookbook/models/xai/basic_stream.py theme={null}
from typing import Iterator  # noqa
from agno.agent import Agent, RunOutputEvent  # noqa
from agno.models.xai import xAI

agent = Agent(model=xAI(id="grok-2"), markdown=True)

---

## Multimodal Query for Video Analysis

**URL:** llms-txt#multimodal-query-for-video-analysis

query = """
You are an expert in video content creation, specializing in crafting engaging short-form content for platforms like YouTube Shorts and Instagram Reels. Your task is to analyze the provided video and identify segments that maximize viewer engagement.

For each video, you'll:

1. Identify key moments that will capture viewers' attention, focusing on:
   - High-energy sequences
   - Emotional peaks
   - Surprising or unexpected moments
   - Strong visual and audio elements
   - Clear narrative segments with compelling storytelling

2. Extract segments that work best for short-form content, considering:
   - Optimal length (strictly 15–60 seconds)
   - Natural start and end points that ensure smooth transitions
   - Engaging pacing that maintains viewer attention
   - Audio-visual harmony for an immersive experience
   - Vertical format compatibility and adjustments if necessary

3. Provide a detailed analysis of each segment, including:
   - Precise timestamps (Start Time | End Time in MM:SS format)
   - A clear description of why the segment would be engaging
   - Suggestions on how to enhance the segment for short-form content
   - An importance score (1-10) based on engagement potential

Your goal is to identify moments that are visually compelling, emotionally engaging, and perfectly optimized for short-form platforms.
"""

---

## Including and excluding tools

**URL:** llms-txt#including-and-excluding-tools

**Contents:**
- Example

Source: https://docs.agno.com/concepts/tools/selecting-tools

Learn how to include and exclude tools from a Toolkit.

You can specify which tools to include or exclude from a `Toolkit` by using the `include_tools` and `exclude_tools` parameters. This can be very useful to limit the number of tools that are available to an Agent.

For example, here's how to include only the `get_latest_emails` tool in the `GmailTools` toolkit:

Similarly, here's how to exclude the `create_draft_email` tool from the `GmailTools` toolkit:

Here's an example of how to use the `include_tools` and `exclude_tools` parameters to limit the number of tools that are available to an Agent:

**Examples:**

Example 1 (unknown):
```unknown
Similarly, here's how to exclude the `create_draft_email` tool from the `GmailTools` toolkit:
```

Example 2 (unknown):
```unknown
## Example

Here's an example of how to use the `include_tools` and `exclude_tools` parameters to limit the number of tools that are available to an Agent:
```

---

## Example 4: Distance Matrix

**URL:** llms-txt#example-4:-distance-matrix

print("\n=== Distance Matrix Example ===")
agent.print_response(
    """Calculate the travel time and distance between these locations in Phoenix:
    Origins: ['Phoenix Sky Harbor Airport', 'Downtown Phoenix']
    Destinations: ['Desert Botanical Garden', 'Phoenix Zoo']""",
    markdown=True,
    stream=True,
)

---

## Airflow

**URL:** llms-txt#airflow

**Contents:**
- Example

Source: https://docs.agno.com/concepts/tools/toolkits/others/airflow

The following agent will use Airflow to save and read a DAG file.

```python cookbook/tools/airflow_tools.py theme={null}
from agno.agent import Agent
from agno.tools.airflow import AirflowTools

agent = Agent(
    tools=[AirflowTools(dags_dir="dags", save_dag=True, read_dag=True)], markdown=True
)

dag_content = """
from airflow import DAG
from airflow.operators.python import PythonOperator
from datetime import datetime, timedelta
default_args = {
    'owner': 'airflow',
    'depends_on_past': False,
    'start_date': datetime(2024, 1, 1),
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 1,
    'retry_delay': timedelta(minutes=5),
}

---

## pprint(run.content)

**URL:** llms-txt#pprint(run.content)

**Contents:**
- Usage

agent.print_response("New York")
bash  theme={null}
    export AZURE_API_KEY=xxx
    export AZURE_ENDPOINT=xxx
    bash  theme={null}
    pip install -U azure-ai-inference agno
    bash Mac theme={null}
      python cookbook/models/azure/ai_foundry/structured_output.py
      bash Windows theme={null}
      python cookbook/models/azure/ai_foundry/structured_output.py
      ```
    </CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Set your API key">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Zoom

**URL:** llms-txt#zoom

**Contents:**
- Prerequisites
- Example Usage

Source: https://docs.agno.com/concepts/tools/toolkits/social/zoom

**Zoom** enables an Agent to interact with Zoom, allowing it to schedule meetings, manage recordings, and handle various meeting-related operations through the Zoom API. The toolkit uses Zoom's Server-to-Server OAuth authentication for secure API access.

The Zoom toolkit requires the following setup:

1. Install required dependencies:

2. Set up Server-to-Server OAuth app in Zoom Marketplace:

* Go to [Zoom Marketplace](https://marketplace.zoom.us/)
   * Click "Develop" → "Build App"
   * Choose "Server-to-Server OAuth" app type
   * Configure the app with required scopes:
     * `/meeting:write:admin`
     * `/meeting:read:admin`
     * `/recording:read:admin`
   * Note your Account ID, Client ID, and Client Secret

3. Set up environment variables:

```python  theme={null}
from agno.agent import Agent
from agno.tools.zoom import ZoomTools

**Examples:**

Example 1 (unknown):
```unknown
2. Set up Server-to-Server OAuth app in Zoom Marketplace:

   * Go to [Zoom Marketplace](https://marketplace.zoom.us/)
   * Click "Develop" → "Build App"
   * Choose "Server-to-Server OAuth" app type
   * Configure the app with required scopes:
     * `/meeting:write:admin`
     * `/meeting:read:admin`
     * `/recording:read:admin`
   * Note your Account ID, Client ID, and Client Secret

3. Set up environment variables:
```

Example 2 (unknown):
```unknown
## Example Usage
```

---

## Reddit Post Generator

**URL:** llms-txt#reddit-post-generator

**Contents:**
- Usage

Source: https://docs.agno.com/examples/use-cases/agents/reddit-post-generator

**Reddit Post Generator** is a team of agents that can research topics on the web and make posts for a subreddit on Reddit.

Create a file `reddit_post_generator_team.py` with the following code:

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Install libraries">
    
  </Step>

<Step title="Set environment variables">
    
  </Step>

<Step title="Run the agent">
    
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Install libraries">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Set environment variables">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run the agent">
```

---

## and +91 1234567890 with the recipient's WhatsApp ID

**URL:** llms-txt#and-+91-1234567890-with-the-recipient's-whatsapp-id

**Contents:**
- Toolkit Params
- Toolkit Functions
- Developer Resources

agent.print_response(
    "Send a template message using the '''hello_world''' template in English to +91 1234567890"
)
```

| Parameter         | Type            | Default   | Description                                                                                                               |
| ----------------- | --------------- | --------- | ------------------------------------------------------------------------------------------------------------------------- |
| `access_token`    | `Optional[str]` | `None`    | WhatsApp Business API access token. If not provided, uses `WHATSAPP_ACCESS_TOKEN` environment variable.                   |
| `phone_number_id` | `Optional[str]` | `None`    | WhatsApp Business Account phone number ID. If not provided, uses `WHATSAPP_PHONE_NUMBER_ID` environment variable.         |
| `version`         | `str`           | `"v22.0"` | API version to use. If not provided, uses `WHATSAPP_VERSION` environment variable or defaults to "v22.0".                 |
| `recipient_waid`  | `Optional[str]` | `None`    | Default recipient WhatsApp ID (e.g., "1234567890"). If not provided, uses `WHATSAPP_RECIPIENT_WAID` environment variable. |
| `async_mode`      | `bool`          | `False`   | Enable asynchronous methods for sending messages.                                                                         |

| Function                      | Description                                                                                                                                                                                           |
| ----------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| `send_text_message_sync`      | Sends a text message to a WhatsApp user (synchronous). Parameters: `text` (str), `recipient` (Optional\[str]), `preview_url` (bool), `recipient_type` (str).                                          |
| `send_template_message_sync`  | Sends a template message to a WhatsApp user (synchronous). Parameters: `recipient` (Optional\[str]), `template_name` (str), `language_code` (str), `components` (Optional\[List\[Dict\[str, Any]]]).  |
| `send_text_message_async`     | Sends a text message to a WhatsApp user (asynchronous). Parameters: `text` (str), `recipient` (Optional\[str]), `preview_url` (bool), `recipient_type` (str).                                         |
| `send_template_message_async` | Sends a template message to a WhatsApp user (asynchronous). Parameters: `recipient` (Optional\[str]), `template_name` (str), `language_code` (str), `components` (Optional\[List\[Dict\[str, Any]]]). |

## Developer Resources

* View [Tools](https://github.com/agno-agi/agno/blob/main/libs/agno/agno/tools/whatsapp.py)
* View [Cookbook](https://github.com/agno-agi/agno/tree/main/cookbook/tools/whatsapp_tools.py)

---

## Azure AI Foundry

**URL:** llms-txt#azure-ai-foundry

**Contents:**
- Parameters

Source: https://docs.agno.com/reference/models/azure

The Azure AI Foundry model provides access to Azure-hosted AI Foundry models.

| Parameter           | Type                              | Default            | Description                                                                        |
| ------------------- | --------------------------------- | ------------------ | ---------------------------------------------------------------------------------- |
| `id`                | `str`                             | `"gpt-4o"`         | The id of the model to use                                                         |
| `name`              | `str`                             | `"AzureAIFoundry"` | The name of the model                                                              |
| `provider`          | `str`                             | `"Azure"`          | The provider of the model                                                          |
| `temperature`       | `Optional[float]`                 | `None`             | Controls randomness in the model's output (0.0 to 2.0)                             |
| `max_tokens`        | `Optional[int]`                   | `None`             | Maximum number of tokens to generate in the response                               |
| `frequency_penalty` | `Optional[float]`                 | `None`             | Penalizes new tokens based on their frequency in the text so far (-2.0 to 2.0)     |
| `presence_penalty`  | `Optional[float]`                 | `None`             | Penalizes new tokens based on whether they appear in the text so far (-2.0 to 2.0) |
| `top_p`             | `Optional[float]`                 | `None`             | Controls diversity via nucleus sampling (0.0 to 1.0)                               |
| `stop`              | `Optional[Union[str, List[str]]]` | `None`             | Up to 4 sequences where the API will stop generating further tokens                |
| `seed`              | `Optional[int]`                   | `None`             | Random seed for deterministic sampling                                             |
| `model_extras`      | `Optional[Dict[str, Any]]`        | `None`             | Additional model-specific parameters                                               |
| `request_params`    | `Optional[Dict[str, Any]]`        | `None`             | Additional parameters to include in the request                                    |
| `api_key`           | `Optional[str]`                   | `None`             | The API key for Azure AI Foundry (defaults to AZURE\_API\_KEY env var)             |
| `api_version`       | `Optional[str]`                   | `None`             | The API version to use (defaults to AZURE\_API\_VERSION env var)                   |
| `azure_endpoint`    | `Optional[str]`                   | `None`             | The Azure endpoint URL (defaults to AZURE\_ENDPOINT env var)                       |
| `timeout`           | `Optional[float]`                 | `None`             | Request timeout in seconds                                                         |
| `max_retries`       | `Optional[int]`                   | `None`             | Maximum number of retries for failed requests                                      |
| `http_client`       | `Optional[httpx.Client]`          | `None`             | HTTP client instance for making requests                                           |
| `client_params`     | `Optional[Dict[str, Any]]`        | `None`             | Additional parameters for client configuration                                     |

---

## Morph Tools

**URL:** llms-txt#morph-tools

**Contents:**
- Code
- Usage

Source: https://docs.agno.com/examples/concepts/tools/models/morph

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Set your API keys">
    
  </Step>

<Step title="Install libraries">
    
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Set your API keys">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## print(response.content)

**URL:** llms-txt#print(response.content)

**Contents:**
- Usage

bash  theme={null}
    pip install agno openai
    bash  theme={null}
    export OPENAI_API_KEY=****
    bash  theme={null}
    python cookbook/examples/teams/dependencies/add_dependencies_on_run.py
    ```
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Install required libraries">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Set environment variables">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run the agent">
```

---

## Langfuse

**URL:** llms-txt#langfuse

**Contents:**
- Integrating Agno with Langfuse
- Prerequisites
- Sending Traces to Langfuse

Source: https://docs.agno.com/integrations/observability/langfuse

Integrate Agno with Langfuse to send traces and gain insights into your agent's performance.

## Integrating Agno with Langfuse

Langfuse provides a robust platform for tracing and monitoring AI model calls. By integrating Agno with Langfuse, you can utilize OpenInference and OpenLIT to send traces and gain insights into your agent's performance.

1. **Install Dependencies**

Ensure you have the necessary packages installed:

2. **Setup Langfuse Account**

* Either self-host or sign up for an account at [Langfuse](https://us.cloud.langfuse.com).
   * Obtain your public and secret API keys from the Langfuse dashboard.

3. **Set Environment Variables**

Configure your environment with the Langfuse API keys:

## Sending Traces to Langfuse

* ### Example: Using Langfuse with OpenInference

This example demonstrates how to instrument your Agno agent with OpenInference and send traces to Langfuse.

```python  theme={null}
import base64
import os

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.yfinance import YFinanceTools
from openinference.instrumentation.agno import AgnoInstrumentor
from opentelemetry import trace as trace_api
from opentelemetry.exporter.otlp.proto.http.trace_exporter import OTLPSpanExporter
from opentelemetry.sdk.trace import TracerProvider
from opentelemetry.sdk.trace.export import SimpleSpanProcessor

**Examples:**

Example 1 (unknown):
```unknown
2. **Setup Langfuse Account**

   * Either self-host or sign up for an account at [Langfuse](https://us.cloud.langfuse.com).
   * Obtain your public and secret API keys from the Langfuse dashboard.

3. **Set Environment Variables**

   Configure your environment with the Langfuse API keys:
```

Example 2 (unknown):
```unknown
## Sending Traces to Langfuse

* ### Example: Using Langfuse with OpenInference

This example demonstrates how to instrument your Agno agent with OpenInference and send traces to Langfuse.
```

---

## CSV Tools

**URL:** llms-txt#csv-tools

**Contents:**
- Code
- Usage

Source: https://docs.agno.com/examples/concepts/tools/database/csv

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Set your API key">
    
  </Step>

<Step title="Install libraries">
    
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Set your API key">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Production Application

**URL:** llms-txt#production-application

**Contents:**
- Workspace Settings
- Build your production image
  - Create an ECR Repository
  - Update the `InfraSettings`
  - Build a new image
- ECS Task Definition
- ECS Service

Source: https://docs.agno.com/templates/infra-management/production-app

Your production application runs on AWS and its resources are defined in the `infra/prd_resources.py` file. This guide shows how to:

1. [Build a production image](#build-your-production-image)
2. [Update ECS Task Definitions](#ecs-task-definition)
3. [Update ECS Services](#ecs-service)

## Workspace Settings

The `InfraSettings` object in the `infra/settings.py` file defines common settings used by your workspace apps and resources.

## Build your production image

Your application uses the `agno` images by default. To use your own image:

* Create a Repository in `ECR` and authenticate or use `Dockerhub`.
* Open `infra/settings.py` file
* Update the `image_repo` to your image repository
* Set `build_images=True` and `push_images=True`
* Optional - Set `build_images=False` and `push_images=False` to use an existing image in the repository

### Create an ECR Repository

To use ECR, **create the image repo and authenticate with ECR** before pushing images.

**1. Create the image repository in ECR**

The repo name should match the `infra_name`. Meaning if you're using the default infra name, the repo name would be `ai`.

<img src="https://mintcdn.com/agno-v2/Y7twezR0wF2re1xh/images/create-ecr-image.png?fit=max&auto=format&n=Y7twezR0wF2re1xh&q=85&s=c68ceb3a9b6784fd519cc04b0e38caf1" alt="create-ecr-image" data-og-width="1389" width="1389" data-og-height="408" height="408" data-path="images/create-ecr-image.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/agno-v2/Y7twezR0wF2re1xh/images/create-ecr-image.png?w=280&fit=max&auto=format&n=Y7twezR0wF2re1xh&q=85&s=2ce02d48da7e53a6c335736a17ebec6e 280w, https://mintcdn.com/agno-v2/Y7twezR0wF2re1xh/images/create-ecr-image.png?w=560&fit=max&auto=format&n=Y7twezR0wF2re1xh&q=85&s=f0b4d1687849a637c0a595c4a8d0690a 560w, https://mintcdn.com/agno-v2/Y7twezR0wF2re1xh/images/create-ecr-image.png?w=840&fit=max&auto=format&n=Y7twezR0wF2re1xh&q=85&s=26b1f13eb8b6f9b09a06b9e6bb1eeb27 840w, https://mintcdn.com/agno-v2/Y7twezR0wF2re1xh/images/create-ecr-image.png?w=1100&fit=max&auto=format&n=Y7twezR0wF2re1xh&q=85&s=e53f084201341a7c92738fa62efdb64c 1100w, https://mintcdn.com/agno-v2/Y7twezR0wF2re1xh/images/create-ecr-image.png?w=1650&fit=max&auto=format&n=Y7twezR0wF2re1xh&q=85&s=c9e2477e1befaf12f81d4d345dac5a26 1650w, https://mintcdn.com/agno-v2/Y7twezR0wF2re1xh/images/create-ecr-image.png?w=2500&fit=max&auto=format&n=Y7twezR0wF2re1xh&q=85&s=a22af7830053ba9139cfb6d0d4017d4a 2500w" />

**2. Authenticate with ECR**

You can also use a helper script to avoid running the full command

<Note>
  Update the script with your ECR repo before running.
</Note>

<CodeGroup>
  
</CodeGroup>

### Update the `InfraSettings`

<Note>
  The `image_repo` defines the repo for your image.

* If using dockerhub it would be something like `agno`.
  * If using ECR it would be something like `[ACCOUNT_ID].dkr.ecr.us-east-1.amazonaws.com`
</Note>

### Build a new image

Build the production image using:

To `force` rebuild images, use the `--force` or `-f` flag

Because the only docker resources in the production env are docker images, you can also use:

## ECS Task Definition

If you updated the Image, CPU, Memory or Environment Variables, update the Task Definition using:

To redeploy the production application, update the ECS Service using:

<Note>
  If you **ONLY** rebuilt the image, you do not need to update the task definition and can just patch the service to pickup the new image.
</Note>

**Examples:**

Example 1 (unknown):
```unknown
You can also use a helper script to avoid running the full command

<Note>
  Update the script with your ECR repo before running.
</Note>

<CodeGroup>
```

Example 2 (unknown):
```unknown
</CodeGroup>

### Update the `InfraSettings`
```

Example 3 (unknown):
```unknown
<Note>
  The `image_repo` defines the repo for your image.

  * If using dockerhub it would be something like `agno`.
  * If using ECR it would be something like `[ACCOUNT_ID].dkr.ecr.us-east-1.amazonaws.com`
</Note>

### Build a new image

Build the production image using:

<CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Setup

**URL:** llms-txt#setup

**Contents:**
- Set your AWS credentials
- Run PgVector

## Set your AWS credentials

<Note>
  By default, this embedder uses the `cohere.embed-multilingual-v3` model. You must enable access to this model from the AWS Bedrock model catalog before using this embedder.
</Note>

**Examples:**

Example 1 (unknown):
```unknown
<Note>
  By default, this embedder uses the `cohere.embed-multilingual-v3` model. You must enable access to this model from the AWS Bedrock model catalog before using this embedder.
</Note>

## Run PgVector
```

---

## Webhook

**URL:** llms-txt#webhook

Source: https://docs.agno.com/reference-api/schema/whatsapp/webhook

post /whatsapp/webhook
Handle incoming WhatsApp messages

---

## SSH Access

**URL:** llms-txt#ssh-access

**Contents:**
- Dev SSH Access
- Production SSH Access

Source: https://docs.agno.com/templates/infra-management/ssh-access

SSH Access is an important part of the developer workflow.

SSH into the dev containers using the `docker exec` command

## Production SSH Access

Your ECS tasks are already enabled with SSH access. SSH into the production containers using:

**Examples:**

Example 1 (unknown):
```unknown
## Production SSH Access

Your ECS tasks are already enabled with SSH access. SSH into the production containers using:
```

---

## Spider Tools

**URL:** llms-txt#spider-tools

**Contents:**
- Code
- Usage

Source: https://docs.agno.com/examples/concepts/tools/web_scrape/spider

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Set your API key">
    
  </Step>

<Step title="Install libraries">
    
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Set your API key">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## OpenRouter

**URL:** llms-txt#openrouter

**Contents:**
- Parameters

Source: https://docs.agno.com/reference/models/openrouter

The OpenRouter model provides unified access to various language models through OpenRouter.

| Parameter  | Type            | Default                          | Description                                                           |
| ---------- | --------------- | -------------------------------- | --------------------------------------------------------------------- |
| `id`       | `str`           | `"openai/gpt-4o-mini"`           | The id of the OpenRouter model to use                                 |
| `name`     | `str`           | `"OpenRouter"`                   | The name of the model                                                 |
| `provider` | `str`           | `"OpenRouter"`                   | The provider of the model                                             |
| `api_key`  | `Optional[str]` | `None`                           | The API key for OpenRouter (defaults to OPENROUTER\_API\_KEY env var) |
| `base_url` | `str`           | `"https://openrouter.ai/api/v1"` | The base URL for the OpenRouter API                                   |
| `app_name` | `Optional[str]` | `"agno"`                         | Application name for OpenRouter request headers                       |

OpenRouter extends the OpenAI-compatible interface and supports most parameters from OpenAI.

---

## Setup paths

**URL:** llms-txt#setup-paths

cwd = Path(__file__).parent
tmp_dir = cwd.joinpath("tmp")
tmp_dir.mkdir(parents=True, exist_ok=True)

---

## Run the evaluation

**URL:** llms-txt#run-the-evaluation

**Contents:**
- Best Practices
- Next Steps

result: Optional[AccuracyResult] = evaluation.run(print_results=True)
assert result is not None and result.avg_score >= 8
```

* **Start Simple:** Begin with basic accuracy tests before moving to complex performance and reliability evaluations
* **Use Multiple Test Cases:** Don't rely on a single test case - create comprehensive test suites
* **Track Over Time:** Monitor your eval results as you make changes to your agents
* **Combine Dimensions:** Use all three evaluation dimensions for a complete picture of agent quality

Dive deeper into each evaluation dimension:

1. **[Accuracy Evals](/concepts/evals/accuracy)** - Learn LLM-as-a-judge techniques and multiple test case strategies
2. **[Performance Evals](/concepts/evals/performance)** - Measure latency, memory usage, and compare different configurations
3. **[Reliability Evals](/concepts/evals/reliability)** - Test tool calls, error handling, and rate limiting behavior

---

## Create a team for collaborative audio-to-text processing

**URL:** llms-txt#create-a-team-for-collaborative-audio-to-text-processing

**Contents:**
- Usage

audio_team = Team(
    name="Audio Analysis Team",
    model=Gemini(id="gemini-2.0-flash-exp"),
    members=[transcription_specialist, content_analyzer],
    instructions=[
        "Work together to transcribe and analyze audio content.",
        "Transcription Specialist: First convert audio to accurate text with speaker identification.",
        "Content Analyzer: Analyze transcription for insights and key themes.",
    ],
    markdown=True,
)

url = "https://agno-public.s3.us-east-1.amazonaws.com/demo_data/QA-01.mp3"

response = requests.get(url)
audio_content = response.content

audio_team.print_response(
    "Give a transcript of this audio conversation. Use speaker A, speaker B to identify speakers.",
    audio=[Audio(content=audio_content)],
    stream=True,
)
bash  theme={null}
    pip install agno requests google-generativeai
    bash  theme={null}
    export GOOGLE_API_KEY=****
    bash  theme={null}
    python cookbook/examples/teams/multimodal/audio_to_text.py
    ```
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Install required libraries">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Set environment variables">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run the agent">
```

---

## --- File management ---

**URL:** llms-txt#----file-management----

reports_dir = Path(__file__).parent.joinpath("reports", "investment")
if reports_dir.is_dir():
    rmtree(path=reports_dir, ignore_errors=True)
reports_dir.mkdir(parents=True, exist_ok=True)

stock_analyst_report = str(reports_dir.joinpath("stock_analyst_report.md"))
research_analyst_report = str(reports_dir.joinpath("research_analyst_report.md"))
investment_report = str(reports_dir.joinpath("investment_report.md"))

---

## Team Example

**URL:** llms-txt#team-example

**Contents:**
- Related Documentation

Create a list of tools, and assign them to your Team with `set_tools`

<Tip>
  The `add_tool` method allows you to dynamically extend an Agent's or a Team's capabilities. This is particularly useful when you want to add tools based on user input or other runtime conditions.
  The `set_tool` method allows you to override an Agent's or a Team's capabilities. Note that this will remove any existing tools previously assigned to your Agent or Team.
</Tip>

## Related Documentation

* [Tool Decorator](/concepts/tools/custom-tools) - Learn how to create custom tools
* [Available Toolkits](/concepts/tools/toolkits) - Explore pre-built toolkits
* [Selecting Tools](/concepts/tools/selecting-tools) - Learn how to filter tools in toolkits

---

## "Consider both financial and lifestyle factors in your analysis.",

**URL:** llms-txt#"consider-both-financial-and-lifestyle-factors-in-your-analysis.",

---

## For async workflow execution

**URL:** llms-txt#for-async-workflow-execution

workflow_tools = WorkflowTools(
    workflow=my_async_workflow,
    async_mode=True,  # This will use async versions of the tools
    enable_run_workflow=True,
)

agent = Agent(
    model=OpenAIChat(id="gpt-5-mini"),
    tools=[workflow_tools],
)

await agent.arun(...)
```

---

## Create collaborative research team

**URL:** llms-txt#create-collaborative-research-team

team = Team(
    name="Hackernews Research Team",
    model=OpenAIChat(id="gpt-5-mini"),
    members=[hackernews_agent],
    determine_input_for_members=False,
    instructions=[
        "Conduct thorough research based on the structured input",
        "Address all focus areas mentioned in the research topic",
        "Tailor the research to the specified target audience",
        "Provide the requested number of sources",
    ],
    show_members_responses=True,
)

---

## Define the embedder

**URL:** llms-txt#define-the-embedder

embedder = OpenAIEmbedder(id="text-embedding-3-small")

---

## PubMed Tools

**URL:** llms-txt#pubmed-tools

**Contents:**
- Code
- Usage

Source: https://docs.agno.com/examples/concepts/tools/search/pubmed

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Set your API key">
    
  </Step>

<Step title="Install libraries">
    
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Set your API key">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## MongoDB Cosmos vCore

**URL:** llms-txt#mongodb-cosmos-vcore

**Contents:**
- Code
- Usage

Source: https://docs.agno.com/examples/concepts/vectordb/mongo-db/cosmos-mongodb-vcore

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Install libraries">
    
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Install libraries">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

Example 3 (unknown):
```unknown

```

---

## Add a directory of PDFs using the PDF reader

**URL:** llms-txt#add-a-directory-of-pdfs-using-the-pdf-reader

**Contents:**
- Best Practices for Search & Retrieval
  - Content Strategy
  - Technical Optimization
  - Monitoring and Improvement
- Next Steps

knowledge.add_content(path="presentations/", reader=PdfReader())
```

## Best Practices for Search & Retrieval

* Organize logically; group related content
* Use consistent terminology
* Include context and cross-references
* Keep content current; retire outdated docs

### Technical Optimization

* Choose appropriate chunk sizes and strategies
* Select a quality embedder for your domain
* Configure VectorDB search type (vector/keyword/hybrid)
* Add a reranker for better ordering

### Monitoring and Improvement

* Test with real user queries regularly
* Observe what agents retrieve
* Gather feedback when answers lack context
* Iterate on chunking, metadata, and search configuration

<CardGroup cols={2}>
  <Card title="Content Database" icon="database" href="/concepts/knowledge/content_db">
    Learn how content metadata is tracked and managed
  </Card>

<Card title="Vector Databases" icon="vector-square" href="/concepts/vectordb/overview">
    Explore storage options for your knowledge base
  </Card>

<Card title="Hybrid Search" icon="magnifying-glass" href="/concepts/knowledge/advanced/hybrid-search">
    Deep dive into advanced search strategies
  </Card>

<Card title="Performance Tips" icon="gauge" href="/concepts/knowledge/advanced/performance-tips">
    Optimize your knowledge base for speed and accuracy
  </Card>
</CardGroup>

---

## -*- Print memories and session summary

**URL:** llms-txt#-*--print-memories-and-session-summary

if agent.db:
    pprint(agent.db.get_user_memories(user_id="test_user"))
    pprint(
        agent.db.get_session(
            session_id="test_session", session_type=SessionType.AGENT
        ).summary  # type: ignore
    )

---

## Gmail

**URL:** llms-txt#gmail

**Contents:**
- Prerequisites
- Example
- Toolkit Params
- Toolkit Functions
- Developer Resources

Source: https://docs.agno.com/concepts/tools/toolkits/social/gmail

**Gmail** enables an Agent to interact with Gmail, allowing it to read, search, send, and manage emails.

The Gmail toolkit requires Google API client libraries and proper authentication setup. Install the required dependencies:

You'll also need to set up Google Cloud credentials:

1. Go to [Google Cloud Console](https://console.cloud.google.com)
2. Create a project or select an existing one
3. Enable the Gmail API
4. Create OAuth 2.0 credentials
5. Set up environment variables:

| Parameter          | Type          | Default | Description                          |
| ------------------ | ------------- | ------- | ------------------------------------ |
| `creds`            | `Credentials` | `None`  | Pre-fetched OAuth credentials        |
| `credentials_path` | `str`         | `None`  | Path to credentials file             |
| `token_path`       | `str`         | `None`  | Path to token file                   |
| `scopes`           | `List[str]`   | `None`  | Custom OAuth scopes                  |
| `port`             | `int`         | `None`  | Port to use for OAuth authentication |

| Function                | Description                                        |
| ----------------------- | -------------------------------------------------- |
| `get_latest_emails`     | Get the latest X emails from the user's inbox      |
| `get_emails_from_user`  | Get X number of emails from a specific sender      |
| `get_unread_emails`     | Get the latest X unread emails                     |
| `get_starred_emails`    | Get X number of starred emails                     |
| `get_emails_by_context` | Get X number of emails matching a specific context |
| `get_emails_by_date`    | Get emails within a specific date range            |
| `create_draft_email`    | Create and save an email draft                     |
| `send_email`            | Send an email immediately                          |
| `search_emails`         | Search emails using natural language queries       |

You can use `include_tools` or `exclude_tools` to modify the list of tools the agent has access to. Learn more about [selecting tools](/concepts/tools/selecting-tools).

## Developer Resources

* View [Tools](https://github.com/agno-agi/agno/blob/main/libs/agno/agno/tools/gmail.py)
* View [Cookbook](https://github.com/agno-agi/agno/tree/main/cookbook/tools/gmail_tools.py)

**Examples:**

Example 1 (unknown):
```unknown
You'll also need to set up Google Cloud credentials:

1. Go to [Google Cloud Console](https://console.cloud.google.com)
2. Create a project or select an existing one
3. Enable the Gmail API
4. Create OAuth 2.0 credentials
5. Set up environment variables:
```

Example 2 (unknown):
```unknown
## Example
```

---

## AWS Bedrock Embedder

**URL:** llms-txt#aws-bedrock-embedder

**Contents:**
- Code

Source: https://docs.agno.com/examples/concepts/knowledge/embedders/aws-bedrock-embedder

```python cookbook/knowledge/embedders/aws_bedrock_embedder.py theme={null}
import asyncio
from agno.knowledge.embedder.aws_bedrock import AwsBedrockEmbedder
from agno.knowledge.knowledge import Knowledge
from agno.knowledge.reader.pdf_reader import PDFReader
from agno.vectordb.pgvector import PgVector

embeddings = AwsBedrockEmbedder().get_embedding(
    "The quick brown fox jumps over the lazy dog."
)

---

## Milvus Async Hybrid Search

**URL:** llms-txt#milvus-async-hybrid-search

**Contents:**
- Code
- Usage

Source: https://docs.agno.com/examples/concepts/vectordb/milvus-db/async-milvus-db-hybrid-search

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Install libraries">
    
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Install libraries">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

Example 3 (unknown):
```unknown

```

---

## Example usage with a complex reasoning problem

**URL:** llms-txt#example-usage-with-a-complex-reasoning-problem

reasoning_agent.print_response(
    "Solve this logic puzzle: A man has to take a fox, a chicken, and a sack of grain across a river. "
    "The boat is only big enough for the man and one item. If left unattended together, the fox will "
    "eat the chicken, and the chicken will eat the grain. How can the man get everything across safely?",
    stream=True,
)

---

## Trafilatura

**URL:** llms-txt#trafilatura

**Contents:**
- Example
- Toolkit Params
- Toolkit Functions
- Developer Resources

Source: https://docs.agno.com/concepts/tools/toolkits/web_scrape/trafilatura

TrafilaturaTools provides advanced web scraping and text extraction capabilities with support for crawling and content analysis.

The following agent can extract and analyze web content:

| Parameter                 | Type            | Default | Description                                                  |
| ------------------------- | --------------- | ------- | ------------------------------------------------------------ |
| `output_format`           | `str`           | `"txt"` | Default output format (txt, json, xml, markdown, csv, html). |
| `include_comments`        | `bool`          | `False` | Whether to extract comments along with main text.            |
| `include_tables`          | `bool`          | `False` | Whether to include table content.                            |
| `include_images`          | `bool`          | `False` | Whether to include image information (experimental).         |
| `include_formatting`      | `bool`          | `False` | Whether to preserve text formatting.                         |
| `include_links`           | `bool`          | `False` | Whether to preserve links (experimental).                    |
| `with_metadata`           | `bool`          | `False` | Whether to include metadata in extractions.                  |
| `favor_precision`         | `bool`          | `False` | Whether to prefer precision over recall.                     |
| `favor_recall`            | `bool`          | `False` | Whether to prefer recall over precision.                     |
| `target_language`         | `Optional[str]` | `None`  | Target language filter (ISO 639-1 format).                   |
| `deduplicate`             | `bool`          | `True`  | Whether to remove duplicate segments.                        |
| `max_crawl_urls`          | `int`           | `100`   | Maximum number of URLs to crawl per website.                 |
| `max_known_urls`          | `int`           | `1000`  | Maximum number of known URLs during crawling.                |
| `enable_extract_text`     | `bool`          | `True`  | Whether to extract text content.                             |
| `enable_extract_metadata` | `bool`          | `True`  | Whether to extract metadata information.                     |
| `enable_html_to_text`     | `bool`          | `True`  | Whether to convert HTML content to clean text.               |
| `enable_batch_extract`    | `bool`          | `True`  | Whether to extract content from multiple URLs in batch.      |

| Function           | Description                                              |
| ------------------ | -------------------------------------------------------- |
| `extract_text`     | Extract clean text content from a URL or HTML.           |
| `extract_metadata` | Extract metadata information from web pages.             |
| `html_to_text`     | Convert HTML content to clean text.                      |
| `crawl_website`    | Crawl a website and extract content from multiple pages. |
| `batch_extract`    | Extract content from multiple URLs in batch.             |
| `get_page_info`    | Get comprehensive page information including metadata.   |

## Developer Resources

* View [Tools Source](https://github.com/agno-agi/agno/blob/main/libs/agno/agno/tools/trafilatura.py)
* [Trafilatura Documentation](https://trafilatura.readthedocs.io/)
* [Web Scraping Best Practices](https://trafilatura.readthedocs.io/en/latest/corefunctions.html)

---

## Initialize Redis db (use the right db_url for your setup)

**URL:** llms-txt#initialize-redis-db-(use-the-right-db_url-for-your-setup)

db = RedisDb(db_url="redis://localhost:6379")

---

## Video Input (Local File Upload)

**URL:** llms-txt#video-input-(local-file-upload)

**Contents:**
- Code

Source: https://docs.agno.com/examples/models/gemini/video_input_local_file_upload

```python cookbook/models/google/gemini/video_input_local_file_upload.py theme={null}
from pathlib import Path

from agno.agent import Agent
from agno.media import Video
from agno.models.google import Gemini

agent = Agent(
    model=Gemini(id="gemini-2.0-flash-exp"),
    markdown=True,
)

---

## Google Maps

**URL:** llms-txt#google-maps

**Contents:**
- Prerequisites
- Example
- Toolkit Params
- Toolkit Functions
- Rate Limits
- Developer Resources

Source: https://docs.agno.com/concepts/tools/toolkits/others/google_maps

Tools for interacting with Google Maps services including place search, directions, geocoding, and more

**GoogleMapTools** enable an Agent to interact with various Google Maps services for location-based operations including place search, directions, geocoding, and more.

The following example requires the `googlemaps` library and an API key which can be obtained from the [Google Cloud Console](https://console.cloud.google.com/projectselector2/google/maps-apis/credentials).

You'll need to enable the following APIs in your Google Cloud Console:

* Places API
* Directions API
* Geocoding API
* Address Validation API
* Distance Matrix API
* Elevation API
* Time Zone API

Basic usage of the Google Maps toolkit:

For more examples, see the [Google Maps Tools Examples](/examples/concepts/tools/others/google_maps).

| Parameter | Type            | Default | Description                                                                         |
| --------- | --------------- | ------- | ----------------------------------------------------------------------------------- |
| `key`     | `Optional[str]` | `None`  | Optional API key. If not provided, uses GOOGLE\_MAPS\_API\_KEY environment variable |

| Function              | Description                                                                                                                                                                                               |
| --------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| `search_places`       | Search for places using Google Maps Places API. Parameters: `query` (str) for the search query. Returns stringified JSON with place details including name, address, phone, website, rating, and hours.   |
| `get_directions`      | Get directions between locations. Parameters: `origin` (str), `destination` (str), optional `mode` (str) for travel mode, optional `avoid` (List\[str]) for features to avoid. Returns route information. |
| `validate_address`    | Validate an address. Parameters: `address` (str), optional `region_code` (str), optional `locality` (str). Returns address validation results.                                                            |
| `geocode_address`     | Convert address to coordinates. Parameters: `address` (str), optional `region` (str). Returns location information with coordinates.                                                                      |
| `reverse_geocode`     | Convert coordinates to address. Parameters: `lat` (float), `lng` (float), optional `result_type` and `location_type` (List\[str]). Returns address information.                                           |
| `get_distance_matrix` | Calculate distances between locations. Parameters: `origins` (List\[str]), `destinations` (List\[str]), optional `mode` (str) and `avoid` (List\[str]). Returns distance and duration matrix.             |
| `get_elevation`       | Get elevation for a location. Parameters: `lat` (float), `lng` (float). Returns elevation data.                                                                                                           |
| `get_timezone`        | Get timezone for a location. Parameters: `lat` (float), `lng` (float), optional `timestamp` (datetime). Returns timezone information.                                                                     |

You can use `include_tools` or `exclude_tools` to modify the list of tools the agent has access to. Learn more about [selecting tools](/concepts/tools/selecting-tools).

Google Maps APIs have usage limits and quotas that vary by service and billing plan. Please refer to the [Google Maps Platform pricing](https://cloud.google.com/maps-platform/pricing) for details.

## Developer Resources

* View [Tools](https://github.com/agno-agi/agno/blob/main/libs/agno/agno/tools/google_maps.py)
* View [Cookbook](https://github.com/agno-agi/agno/tree/main/cookbook/tools/google_maps_tools.py)

**Examples:**

Example 1 (unknown):
```unknown

```

Example 2 (unknown):
```unknown
You'll need to enable the following APIs in your Google Cloud Console:

* Places API
* Directions API
* Geocoding API
* Address Validation API
* Distance Matrix API
* Elevation API
* Time Zone API

## Example

Basic usage of the Google Maps toolkit:
```

---

## === Request/Response Logging Middleware ===

**URL:** llms-txt#===-request/response-logging-middleware-===

class RequestLoggingMiddleware(BaseHTTPMiddleware):
    """
    Request/response logging middleware with timing and basic info.
    """

def __init__(self, app, log_body: bool = False, log_headers: bool = False):
        super().__init__(app)
        self.log_body = log_body
        self.log_headers = log_headers
        self.request_count = 0

async def dispatch(self, request: Request, call_next) -> Response:
        self.request_count += 1
        start_time = time.time()

# Basic request info
        client_ip = request.client.host if request.client else "unknown"
        print(
            f"🔍 Request #{self.request_count}: {request.method} {request.url.path} from {client_ip}"
        )

# Optional: Log headers
        if self.log_headers:
            print(f"📋 Headers: {dict(request.headers)}")

# Optional: Log request body
        if self.log_body and request.method in ["POST", "PUT", "PATCH"]:
            body = await request.body()
            if body:
                print(f"📝 Body: {body.decode()}")

# Process request
        response = await call_next(request)

# Log response info
        duration = time.time() - start_time
        status_emoji = "✅" if response.status_code < 400 else "❌"
        print(
            f"{status_emoji} Response: {response.status_code} in {duration * 1000:.1f}ms"
        )

# Add request count to response header
        response.headers["X-Request-Count"] = str(self.request_count)

---

## User Control Flow

**URL:** llms-txt#user-control-flow

**Contents:**
- Example
- Toolkit Params
- Toolkit Functions
- Developer Resources

Source: https://docs.agno.com/concepts/tools/toolkits/others/user_control_flow

UserControlFlowTools enable agents to pause execution and request input from users during conversations.

The following agent can request user input during conversations:

| Parameter               | Type            | Default | Description                                        |
| ----------------------- | --------------- | ------- | -------------------------------------------------- |
| `instructions`          | `Optional[str]` | `None`  | Custom instructions for user interaction behavior. |
| `add_instructions`      | `bool`          | `True`  | Whether to add instructions to the agent.          |
| `enable_get_user_input` | `bool`          | `True`  | Enable user input request functionality.           |

| Function         | Description                                            |
| ---------------- | ------------------------------------------------------ |
| `get_user_input` | Pause agent execution and request input from the user. |

## Developer Resources

* View [Tools Source](https://github.com/agno-agi/agno/blob/main/libs/agno/agno/tools/user_control_flow.py)
* [Agno Interactive Agents](https://docs.agno.com/interactive-agents)
* [User Input Patterns](https://docs.agno.com/user-input)

---

## Example questions to try:

**URL:** llms-txt#example-questions-to-try:

---

## Create a team with the analysis tool

**URL:** llms-txt#create-a-team-with-the-analysis-tool

**Contents:**
- Usage

performance_team = Team(
    model=OpenAIChat(id="gpt-4o"),
    members=[data_analyst, team_lead],
    tools=[analyze_team_performance],
    name="Team Performance Analysis Team",
    description="A team specialized in analyzing team performance using integrated data sources.",
    instructions=[
        "You are a team performance analysis unit with access to team metrics and analysis tools.",
        "When asked to analyze any team, use the analyze_team_performance tool first.",
        "This tool has access to team metrics and current context through integrated data sources.",
        "Data Analyst: Focus on the quantitative metrics and trends.",
        "Team Lead: Provide strategic insights and management recommendations.",
        "Work together to provide comprehensive team performance insights.",
    ],
)

print("=== Team Tool Dependencies Access Example ===\n")

response = performance_team.run(
    input="Please analyze the 'engineering_team' performance and provide comprehensive insights about their productivity and recommendations for improvement.",
    dependencies={
        "team_metrics": {
            "team_name": "Engineering Team Alpha",
            "team_size": 8,
            "productivity_score": 7.5,
            "sprint_velocity": 85,
            "bug_resolution_rate": 92,
            "code_review_turnaround": "2.3 days",
            "areas": ["Backend Development", "Frontend Development", "DevOps"],
        },
        "current_context": get_current_context,
    },
    session_id="test_team_tool_dependencies",
)

print(f"\nTeam Response: {response.content}")
bash  theme={null}
    pip install agno openai
    bash  theme={null}
    export OPENAI_API_KEY=****
    bash  theme={null}
    python cookbook/examples/teams/dependencies/access_dependencies_in_tool.py
    ```
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Install required libraries">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Set environment variables">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run the team">
```

---

## Example 1: Sleep for 2 seconds

**URL:** llms-txt#example-1:-sleep-for-2-seconds

agent.print_response("Sleep for 2 seconds")

---

## Better quality for complex content (but slower)

**URL:** llms-txt#better-quality-for-complex-content-(but-slower)

**Contents:**
  - 5. Use Async for Batch Operations
- Common Performance Pitfalls
  - Issue: Search Returns Irrelevant Results

quality_chunking = SemanticChunking(
    chunk_size=1200,
    similarity_threshold=0.5
)
python  theme={null}
import asyncio

async def load_knowledge_efficiently():
    # Load multiple content sources in parallel
    tasks = [
        knowledge.add_content_async(path="docs/hr/"),
        knowledge.add_content_async(path="docs/engineering/"),
        knowledge.add_content_async(url="https://company.com/api-docs"),
    ]
    await asyncio.gather(*tasks)

asyncio.run(load_knowledge_efficiently())
python  theme={null}

**Examples:**

Example 1 (unknown):
```unknown
Learn more about [choosing chunking strategies](/concepts/knowledge/chunking/overview).

### 5. Use Async for Batch Operations

Process multiple items concurrently:
```

Example 2 (unknown):
```unknown
## Common Performance Pitfalls

### Issue: Search Returns Irrelevant Results

**What's happening:** Chunks are too large, too small, or chunking strategy doesn't match your content.

**Quick fixes:**

1. Check your chunking strategy - try semantic chunking for better context
2. Verify content actually loaded: `knowledge.get_content_status(content_id)`
3. Increase `max_results` to see if relevant results are just ranked lower
4. Add metadata filters to narrow the search scope
```

---

## Initialize OpenLIT instrumentation. The disable_batch flag is set to true to process traces immediately.

**URL:** llms-txt#initialize-openlit-instrumentation.-the-disable_batch-flag-is-set-to-true-to-process-traces-immediately.

**Contents:**
- Usage

openlit.init(tracer=tracer, disable_batch=True)

agent = Agent(
    model=OpenAIChat(id="gpt-5-mini"),
    tools=[DuckDuckGoTools()],
    markdown=True,
    debug_mode=True,
)

agent.print_response("What is currently trending on Twitter?")

bash  theme={null}
    export LANGFUSE_PUBLIC_KEY=<your-key>
    export LANGFUSE_SECRET_KEY=<your-key>
    bash  theme={null}
    pip install -U agno openai langfuse openlit opentelemetry-sdk opentelemetry-exporter-otlp ddgs
    bash Mac theme={null}
      python cookbook/integrations/observability/langfuse_via_openlit.py
      bash Windows theme={null}
      python cookbook/integrations/observability/langfuse_via_openlit.py
      ```
    </CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Set your API key">
    Set your Langfuse API key as an environment variables:
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## MongoDB for Team

**URL:** llms-txt#mongodb-for-team

**Contents:**
- Usage
  - Run MongoDB

Source: https://docs.agno.com/examples/concepts/db/mongodb/mongodb_for_team

Agno supports using MongoDB as a storage backend for Teams using the `MongoDb` class.

You need to provide either `db_url` or `client`. The following example uses `db_url`.

Install [docker desktop](https://docs.docker.com/desktop/install/mac-install/) and run **MongoDB** on port **27017** using:

```python mongodb_for_team.py theme={null}
"""
Run: `pip install openai ddgs newspaper4k lxml_html_clean agno` to install the dependencies
"""
from typing import List

from agno.agent import Agent
from agno.db.mongo import MongoDb
from agno.models.openai import OpenAIChat
from agno.team import Team
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.hackernews import HackerNewsTools
from pydantic import BaseModel

**Examples:**

Example 1 (unknown):
```unknown

```

---

## Add your own routes

**URL:** llms-txt#add-your-own-routes

@app.post("/customers")
async def get_customers():
    return [
        {
            "id": 1,
            "name": "John Doe",
            "email": "john.doe@example.com",
        },
        {
            "id": 2,
            "name": "Jane Doe",
            "email": "jane.doe@example.com",
        },
    ]

---

## Structured outputs

**URL:** llms-txt#structured-outputs

**Contents:**
- Structured Outputs vs. JSON Mode
  - Structured Outputs (Default if supported)
- Example
  - JSON Mode
- Example
  - When to use

Source: https://docs.agno.com/faq/structured-outputs

## Structured Outputs vs. JSON Mode

When working with language models, generating responses that match a specific structure is crucial for building reliable applications. Agno Agents support two methods to achieve this: **Structured Outputs** and **JSON mode**.

### Structured Outputs (Default if supported)

"Structured Outputs" is the **preferred** and most **reliable** way to extract well-formed, schema-compliant responses from a Model. If a model class supports it, Agno Agents use Structured Outputs by default.

With structured outputs, we provide a schema to the model (using Pydantic or JSON Schema), and the model’s response is guaranteed to **strictly follow** that schema. This eliminates many common issues like missing fields, invalid enum values, or inconsistent formatting. Structured Outputs are ideal when you need high-confidence, well-structured responses—like entity extraction, content generation for UI rendering, and more.

In this case, the response model is passed as a keyword argument to the model.

In the example above, the model will generate a response that matches the `User` schema using structured outputs via OpenAI's `gpt-5-mini` model. The agent will then return the `User` object as-is.

Some model classes **do not support Structured Outputs**, or you may want to fall back to JSON mode even when the model supports both options. In such cases, you can enable **JSON mode** by setting `use_json_mode=True`.

JSON mode works by injecting a detailed description of the expected JSON structure into the system prompt. The model is then instructed to return a valid JSON object that follows this structure. Unlike Structured Outputs, the response is **not automatically validated** against the schema at the API level.

Use **Structured Outputs** if the model supports it — it’s reliable, clean, and validated automatically.

* When the model doesn't support structured outputs. Agno agents do this by default on your behalf.
* When you need broader compatibility, but are okay validating manually.
* When the model does not support tools with structured outputs.

**Examples:**

Example 1 (unknown):
```unknown
In the example above, the model will generate a response that matches the `User` schema using structured outputs via OpenAI's `gpt-5-mini` model. The agent will then return the `User` object as-is.

***

### JSON Mode

Some model classes **do not support Structured Outputs**, or you may want to fall back to JSON mode even when the model supports both options. In such cases, you can enable **JSON mode** by setting `use_json_mode=True`.

JSON mode works by injecting a detailed description of the expected JSON structure into the system prompt. The model is then instructed to return a valid JSON object that follows this structure. Unlike Structured Outputs, the response is **not automatically validated** against the schema at the API level.

## Example
```

---

## enable_session_summaries=True,

**URL:** llms-txt#enable_session_summaries=true,

---

## print(chunk.content)

**URL:** llms-txt#print(chunk.content)

---

## Add custom middleware

**URL:** llms-txt#add-custom-middleware

**Contents:**
- Usage
- Middleware Features
- Developer Resources

app.add_middleware(
    RateLimitMiddleware,
    requests_per_minute=10,
    window_size=60,
)

app.add_middleware(
    RequestLoggingMiddleware,
    log_body=False,
    log_headers=False,
)

if __name__ == "__main__":
    """
    Run the essential middleware demo using AgentOS serve method.
    
    Features:
    1. Rate Limiting (10 requests/minute)
    2. Request/Response Logging
    """

agent_os.serve(
        app="custom_middleware:app",
        reload=True,
    )
bash  theme={null}
    export OPENAI_API_KEY=your_openai_api_key
    bash  theme={null}
    pip install -U agno openai ddgs fastapi["standard"] uvicorn sqlalchemy pgvector psycopg
    bash  theme={null}
    # Using Docker
    docker run -d \
      --name agno-postgres \
      -e POSTGRES_DB=ai \
      -e POSTGRES_USER=ai \
      -e POSTGRES_PASSWORD=ai \
      -p 5532:5432 \
      pgvector/pgvector:pg17
    bash  theme={null}
    python custom_middleware.py
    bash  theme={null}
    curl http://localhost:7777/config
    bash  theme={null}
    for i in {1..15}; do curl http://localhost:7777/config; done
    bash  theme={null}
    curl -v http://localhost:7777/config
    python  theme={null}
    app.add_middleware(
        RateLimitMiddleware,
        requests_per_minute=100,  # Allow 100 requests per minute
        window_size=60,           # 60-second sliding window
    )
    
    🔍 Request #1: GET /config from 127.0.0.1
    ✅ Response: 200 in 45.2ms
    🔍 Request #2: POST /agents/demo-agent/runs from 127.0.0.1  
    ✅ Response: 200 in 1240.8ms
    python  theme={null}
    app.add_middleware(
        RequestLoggingMiddleware,
        log_body=True,     # Log request bodies
        log_headers=True,  # Log request headers
    )
    ```
  </Tab>
</Tabs>

## Developer Resources

* [Custom Middleware Documentation](/agent-os/customize/middleware/custom)
* [FastAPI Middleware Documentation](https://fastapi.tiangolo.com/tutorial/middleware/)

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Set Environment Variables">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Setup PostgreSQL Database">
```

Example 4 (unknown):
```unknown
</Step>

  <Step title="Run Example">
```

---

## Execute Evaluation

**URL:** llms-txt#execute-evaluation

Source: https://docs.agno.com/reference-api/schema/evals/execute-evaluation

post /eval-runs
Run evaluation tests on agents or teams. Supports accuracy, performance, and reliability evaluations. Requires either agent_id or team_id, but not both.

---

## Environment variables

**URL:** llms-txt#environment-variables

Source: https://docs.agno.com/templates/infra-management/env-vars

Environment variables can be added to resources using the `env_vars` parameter or the `env_file` parameter pointing to a `yaml` file. Examples

The apps in your templates are already configured to read environment variables.

**Examples:**

Example 1 (unknown):
```unknown

```

---

## pprint(structured_output_response.content)

**URL:** llms-txt#pprint(structured_output_response.content)

**Contents:**
- Usage

json_mode_agent.print_response("New York")
bash  theme={null}
    export TOGETHER_API_KEY=xxx
    bash  theme={null}
    pip install -U openai agno
    bash Mac theme={null}
      python cookbook/models/together/structured_output.py
      bash Windows theme={null}
      python cookbook/models/together/structured_output.py
      ```
    </CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Set your API key">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Accuracy Evals

**URL:** llms-txt#accuracy-evals

**Contents:**
- Basic Example
  - Evaluator Agent

Source: https://docs.agno.com/concepts/evals/accuracy

Learn how to evaluate your Agno Agents and Teams for accuracy using LLM-as-a-judge methodology with input/output pairs.

Accuracy evals aim at measuring how well your Agents and Teams perform against a gold-standard answer.

You will provide an input and the ideal, expected output. Then the Agent's real answer will be compared against the given ideal output.

In this example, the `AccuracyEval` will run the Agent with the input, then use a different model (`o4-mini`) to score the Agent's response according to the guidelines provided.

To evaluate the accuracy of the Agent's response, we use another Agent. This strategy is usually referred to as "LLM-as-a-judge".

You can adjust the evaluator Agent to make it fit the criteria you want to evaluate:

```python  theme={null}
from typing import Optional

from agno.agent import Agent
from agno.eval.accuracy import AccuracyAgentResponse, AccuracyEval, AccuracyResult
from agno.models.openai import OpenAIChat
from agno.tools.calculator import CalculatorTools

**Examples:**

Example 1 (unknown):
```unknown
### Evaluator Agent

To evaluate the accuracy of the Agent's response, we use another Agent. This strategy is usually referred to as "LLM-as-a-judge".

You can adjust the evaluator Agent to make it fit the criteria you want to evaluate:
```

---

## OpenAI gpt-5-mini with reasoning effort

**URL:** llms-txt#openai-gpt-5-mini-with-reasoning-effort

**Contents:**
- Code
- Usage

Source: https://docs.agno.com/examples/concepts/reasoning/models/openai/reasoning-effort

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Set your API key">
    
  </Step>

<Step title="Install libraries">
    
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Set your API key">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Sleep

**URL:** llms-txt#sleep

**Contents:**
- Example

Source: https://docs.agno.com/concepts/tools/toolkits/local/sleep

The following agent will use the `sleep` tool to pause execution for a given number of seconds.

```python cookbook/tools/sleep_tools.py theme={null}
from agno.agent import Agent
from agno.tools.sleep import SleepTools

---

## Set Temperature

**URL:** llms-txt#set-temperature

**Contents:**
- Code

Source: https://docs.agno.com/examples/models/ollama/set_temperature

```python cookbook/models/ollama/set_temperature.py theme={null}
from agno.agent import Agent, RunOutput  # noqa
from agno.models.ollama import Ollama

agent = Agent(model=Ollama(id="llama3.2", options={"temperature": 0.5}), markdown=True)

---

## Cancel Team Run

**URL:** llms-txt#cancel-team-run

Source: https://docs.agno.com/reference-api/schema/teams/cancel-team-run

post /teams/{team_id}/runs/{run_id}/cancel
Cancel a currently executing team run. This will attempt to stop the team's execution gracefully.

**Note:** Cancellation may not be immediate for all operations.

---

## Arxiv

**URL:** llms-txt#arxiv

**Contents:**
- Prerequisites
- Example
- Toolkit Params
- Toolkit Functions
- Developer Resources

Source: https://docs.agno.com/concepts/tools/toolkits/search/arxiv

**ArxivTools** enable an Agent to search for publications on Arxiv.

The following example requires the `arxiv` and `pypdf` libraries.

The following agent will run seach arXiv for "language models" and print the response.

| Parameter                  | Type   | Default | Description                                                        |
| -------------------------- | ------ | ------- | ------------------------------------------------------------------ |
| `enable_search_arxiv`      | `bool` | `True`  | Enables the functionality to search the arXiv database.            |
| `enable_read_arxiv_papers` | `bool` | `True`  | Allows reading of arXiv papers directly.                           |
| `download_dir`             | `Path` | -       | Specifies the directory path where downloaded files will be saved. |

| Function                                 | Description                                                                                        |
| ---------------------------------------- | -------------------------------------------------------------------------------------------------- |
| `search_arxiv_and_update_knowledge_base` | This function searches arXiv for a topic, adds the results to the knowledge base and returns them. |
| `search_arxiv`                           | Searches arXiv for a query.                                                                        |

## Developer Resources

* View [Tools](https://github.com/agno-agi/agno/blob/main/libs/agno/agno/tools/arxiv.py)
* View [Cookbook](https://github.com/agno-agi/agno/tree/main/cookbook/tools/arxiv_tools.py)

**Examples:**

Example 1 (unknown):
```unknown
## Example

The following agent will run seach arXiv for "language models" and print the response.
```

---

## Delete Multiple Sessions

**URL:** llms-txt#delete-multiple-sessions

Source: https://docs.agno.com/reference-api/schema/sessions/delete-multiple-sessions

delete /sessions
Delete multiple sessions by their IDs in a single operation. This action cannot be undone and will permanently remove all specified sessions and their runs.

---

## team.print_response(

**URL:** llms-txt#team.print_response(

---

## 2. Continue with approval (use the run_id and tool_call_id from response)

**URL:** llms-txt#2.-continue-with-approval-(use-the-run_id-and-tool_call_id-from-response)

**Contents:**
- Usage
- Learn More

curl -X POST http://localhost:7777/agents/data_manager/runs/{run_id}/continue \
  -F "tools=[{\"tool_call_id\": \"{tool_call_id}\", \"confirmed\": true}]" \
  -F "session_id=test_session" \
  -F "stream=false"
bash  theme={null}
    export OPENAI_API_KEY=your_openai_api_key
    bash  theme={null}
    pip install -U agno fastapi uvicorn sqlalchemy pgvector psycopg openai
    bash  theme={null}
    docker run -d \
      --name agno-postgres \
      -e POSTGRES_DB=ai \
      -e POSTGRES_USER=ai \
      -e POSTGRES_PASSWORD=ai \
      -p 5532:5432 \
      pgvector/pgvector:pg17
    bash  theme={null}
    python hitl_confirmation.py
    ```
  </Step>
</Steps>

<CardGroup cols={2}>
  <Card title="HITL Getting Started" icon="hand" href="/examples/getting-started/12-human-in-the-loop">
    Learn HITL basics with a simple example
  </Card>

<Card title="HITL Concepts" icon="book" href="/concepts/hitl/overview">
    Understand HITL patterns and best practices
  </Card>
</CardGroup>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Set Environment Variables">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Setup PostgreSQL Database">
```

Example 4 (unknown):
```unknown
</Step>

  <Step title="Run Example">
```

---

## With Docker and Docker Compose

**URL:** llms-txt#with-docker-and-docker-compose

---

## OpenAI gpt-5-mini with Tools

**URL:** llms-txt#openai-gpt-5-mini-with-tools

**Contents:**
- Code
- Usage

Source: https://docs.agno.com/examples/concepts/reasoning/models/openai/o3-mini-tools

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Set your API key">
    
  </Step>

<Step title="Install libraries">
    
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Set your API key">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Create distributed reranking RAG team

**URL:** llms-txt#create-distributed-reranking-rag-team

**Contents:**
- Usage

distributed_reranking_team = Team(
    name="Distributed Reranking RAG Team",
    model=OpenAIChat(id="gpt-5-mini"),
    members=[
        initial_retriever,
        reranking_specialist,
        context_analyzer,
        final_synthesizer,
    ],
    instructions=[
        "Work together to provide optimal RAG responses using advanced reranking.",
        "Initial Retriever: First perform broad comprehensive retrieval.",
        "Reranking Specialist: Apply advanced reranking for result optimization.",
        "Context Analyzer: Analyze and validate the reranked results.",
        "Final Synthesizer: Create optimal responses from reranked information.",
        "Leverage advanced reranking for superior result quality.",
        "Demonstrate the benefits of specialized reranking in team coordination.",
    ],
    show_members_responses=True,
    markdown=True,
)

async def async_reranking_rag_demo():
    """Demonstrate async distributed reranking RAG processing."""
    print("🎯 Async Distributed Reranking RAG Demo")
    print("=" * 45)

query = "What's the best way to prepare authentic Tom Kha Gai? I want traditional methods and modern variations."

# Add content to knowledge bases
    await reranked_knowledge.add_contents_async(
        url="https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf"
    )
    await validation_knowledge.add_contents_async(
        url="https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf"
    )

# Run async distributed reranking RAG
    await aprint_response(input=query, team=distributed_reranking_team)

def sync_reranking_rag_demo():
    """Demonstrate sync distributed reranking RAG processing."""
    print("🎯 Distributed Reranking RAG Demo")
    print("=" * 35)

query = "What's the best way to prepare authentic Tom Kha Gai? I want traditional methods and modern variations."

# Add content to knowledge bases
    reranked_knowledge.add_contents(
        url="https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf"
    )
    validation_knowledge.add_contents(
        url="https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf"
    )

# Run distributed reranking RAG
    print_response(distributed_reranking_team, query)

def advanced_culinary_demo():
    """Demonstrate advanced reranking for complex culinary queries."""
    print("👨‍🍳 Advanced Culinary Analysis with Reranking RAG")
    print("=" * 55)

query = """I want to understand the science behind Thai curry pastes. Can you explain:
    - Traditional preparation methods vs modern techniques
    - How different ingredients affect flavor profiles
    - Regional variations and their historical origins
    - Best practices for storage and usage
    - How to adapt recipes for different dietary needs"""

# Add content to knowledge bases
    reranked_knowledge.add_contents(
        url="https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf"
    )
    validation_knowledge.add_contents(
        url="https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf"
    )

print_response(distributed_reranking_team, query)

if __name__ == "__main__":
    # Choose which demo to run
    asyncio.run(async_reranking_rag_demo())

# advanced_culinary_demo()

# sync_reranking_rag_demo()
bash  theme={null}
    pip install agno openai lancedb tantivy pypdf sqlalchemy cohere
    bash  theme={null}
    export OPENAI_API_KEY=****
    export COHERE_API_KEY=****
    bash  theme={null}
    python cookbook/examples/teams/distributed_rag/03_distributed_rag_with_reranking.py
    ```
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Install required libraries">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Set environment variables">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run the agent">
```

---

## PRINT RESPONSE

**URL:** llms-txt#print-response

---

## Print the embeddings and their dimensions

**URL:** llms-txt#print-the-embeddings-and-their-dimensions

print(f"Embeddings: {embeddings[:5]}")
print(f"Dimensions: {len(embeddings)}")

---

## Example usage with a famous landmark

**URL:** llms-txt#example-usage-with-a-famous-landmark

agent.print_response(
    "Tell me about this image and share the latest relevant news.",
    images=[
        Image(
            url="https://upload.wikimedia.org/wikipedia/commons/0/0c/GoldenGateBridge-001.jpg"
        )
    ],
    stream=True,
)

---

## topic="AI",

**URL:** llms-txt#topic="ai",

---

## -*- Print memories

**URL:** llms-txt#-*--print-memories

if agent.db:
    pprint(agent.db.get_user_memories(user_id="test_user"))
    pprint(
        agent.db.get_session(
            session_id="test_session", session_type=SessionType.AGENT
        ).summary  # type: ignore
    )

---

## Common events you might want to skip

**URL:** llms-txt#common-events-you-might-want-to-skip

events_to_skip = [
    WorkflowRunEvent.workflow_started,
    WorkflowRunEvent.workflow_completed,
    WorkflowRunEvent.workflow_cancelled,
    WorkflowRunEvent.step_started,
    WorkflowRunEvent.step_completed,
    WorkflowRunEvent.parallel_execution_started,
    WorkflowRunEvent.parallel_execution_completed,
    WorkflowRunEvent.condition_execution_started,
    WorkflowRunEvent.condition_execution_completed,
    WorkflowRunEvent.loop_execution_started,
    WorkflowRunEvent.loop_execution_completed,
    WorkflowRunEvent.router_execution_started,
    WorkflowRunEvent.router_execution_completed,
]
python  theme={null}

**Examples:**

Example 1 (unknown):
```unknown
**Use Cases**

* **Debugging**: Store all events to analyze workflow execution flow
* **Audit Trails**: Keep records of all workflow activities for compliance
* **Performance Analysis**: Analyze timing and execution patterns
* **Error Investigation**: Review event sequences leading to failures
* **Noise Reduction**: Skip verbose events like `step_started` to focus on results

**Configuration Examples**
```

---

## - Fetch the URL and token from Upstash Console

**URL:** llms-txt#--fetch-the-url-and-token-from-upstash-console

---

## ==============================================================================

**URL:** llms-txt#==============================================================================

def demo_customer_support_cli():
    """Demo customer support workflow with CLI"""
    support_workflow = create_customer_support_workflow()

print("🎧 Customer Support Demo - Type 'exit' to quit")
    print("Try: 'My account is locked and I can't access my billing information'")
    print("-" * 60)

support_workflow.cli_app(
        session_id="support_demo",
        user="Customer",
        emoji="🆘",
        stream=True,
        stream_events=True,
    )

def demo_medical_consultation_cli():
    """Demo medical consultation workflow with CLI"""
    medical_workflow = create_medical_consultation_workflow()

print("🏥 Medical Consultation Demo - Type 'exit' to quit")
    print("Try: 'I've been having chest pain and shortness of breath for 2 days'")
    print("-" * 60)

medical_workflow.cli_app(
        session_id="medical_demo",
        user="Patient",
        emoji="🩺",
        stream=True,
        stream_events=True,
    )

def demo_tutoring_cli():
    """Demo tutoring workflow with CLI"""
    tutoring_workflow = create_tutoring_workflow()

print("📚 Tutoring Session Demo - Type 'exit' to quit")
    print("Try: 'I'm struggling with calculus derivatives and have a test next week'")
    print("-" * 60)

tutoring_workflow.cli_app(
        session_id="tutoring_demo",
        user="Student",
        emoji="🎓",
        stream=True,
        stream_events=True,
    )

if __name__ == "__main__":
    import sys

demos = {
        "support": demo_customer_support_cli,
        "medical": demo_medical_consultation_cli,
        "tutoring": demo_tutoring_cli,
    }

if len(sys.argv) > 1 and sys.argv[1] in demos:
        demos[sys.argv[1]]()
    else:
        print("🚀 Conversational Workflow Demos")
        print("Choose a demo to run:")
        print("")
        for key, func in demos.items():
            print(f"{key:<10} - {func.__doc__}")
        print("")
        print("Or run all demos interactively:")
        choice = input("Enter demo name (or 'all'): ").strip().lower()

if choice == "all":
            for demo_func in demos.values():
                demo_func()
        elif choice in demos:
            demos[choice]()
        else:
            print("Invalid choice!")
```

---

## MongoDB Hybrid Search

**URL:** llms-txt#mongodb-hybrid-search

**Contents:**
- Code
- Usage

Source: https://docs.agno.com/examples/concepts/vectordb/mongo-db/mongo-db-hybrid-search

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Install libraries">
    
  </Step>

<Step title="Run MongoDB">
    
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Install libraries">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Run MongoDB">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## LiteLLM OpenAI

**URL:** llms-txt#litellm-openai

**Contents:**
- Proxy Server Integration
  - Starting the Proxy Server
  - Using the Proxy
  - Configuration Options
- Examples
  - Proxy Examples

Source: https://docs.agno.com/concepts/models/litellm_openai

Use LiteLLM with Agno with an openai-compatible proxy server.

## Proxy Server Integration

LiteLLM can also be used as an OpenAI-compatible proxy server, allowing you to route requests to different models through a unified API.

### Starting the Proxy Server

First, install LiteLLM with proxy support:

Start the proxy server:

The `LiteLLMOpenAI` class connects to the LiteLLM proxy using an OpenAI-compatible interface:

### Configuration Options

The `LiteLLMOpenAI` class accepts the following parameters:

| Parameter  | Type | Description                                                    | Default                                      |
| ---------- | ---- | -------------------------------------------------------------- | -------------------------------------------- |
| `id`       | str  | Model identifier                                               | "gpt-5-mini"                                 |
| `name`     | str  | Display name for the model                                     | "LiteLLM"                                    |
| `provider` | str  | Provider name                                                  | "LiteLLM"                                    |
| `api_key`  | str  | API key (falls back to LITELLM\_API\_KEY environment variable) | None                                         |
| `base_url` | str  | URL of the LiteLLM proxy server                                | "[http://0.0.0.0:4000](http://0.0.0.0:4000)" |

`LiteLLMOpenAI` is a subclass of the [OpenAILike](/concepts/models/openai-like) class and has access to the same params.

Check out these examples in the cookbook:

<Note> View more examples [here](/examples/models/litellm_openai/basic). </Note>

**Examples:**

Example 1 (unknown):
```unknown
Start the proxy server:
```

Example 2 (unknown):
```unknown
### Using the Proxy

The `LiteLLMOpenAI` class connects to the LiteLLM proxy using an OpenAI-compatible interface:
```

---

## Singlestore for Team

**URL:** llms-txt#singlestore-for-team

**Contents:**
- Usage

Source: https://docs.agno.com/examples/concepts/db/singlestore/singlestore_for_team

Agno supports using Singlestore as a storage backend for Teams using the `SingleStoreDb` class.

Obtain the credentials for Singlestore from [here](https://portal.singlestore.com/)

```python singlestore_for_team.py theme={null}
"""
Run: `pip install openai ddgs newspaper4k lxml_html_clean agno` to install the dependencies
"""
from os import getenv
from typing import List

from agno.agent import Agent
from agno.db.singlestore.singlestore import SingleStoreDb
from agno.models.openai import OpenAIChat
from agno.team import Team
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.hackernews import HackerNewsTools
from pydantic import BaseModel

---

## Create team with tool hooks

**URL:** llms-txt#create-team-with-tool-hooks

**Contents:**
- Usage

company_info_team = Team(
    name="Company Info Team",
    model=Claude(id="claude-3-7-sonnet-latest"),
    members=[
        reddit_agent,
        website_agent,
    ],
    markdown=True,
    instructions=[
        "You are a team that finds information about a company.",
        "First search the web and wikipedia for information about the company.",
        "If you can find the company's website URL, then scrape the homepage and the about page.",
    ],
    show_members_responses=True,
    tool_hooks=[logger_hook],
)

if __name__ == "__main__":
    company_info_team.print_response(
        "Write me a full report on everything you can find about Agno, the company building AI agent infrastructure.",
        stream=True,
    )
bash  theme={null}
    pip install agno ddgs
    bash  theme={null}
    export OPENAI_API_KEY=****
    export ANTHROPIC_API_KEY=****
    export REDDIT_CLIENT_ID=****
    export REDDIT_CLIENT_SECRET=****
    export REDDIT_USER_AGENT=****
    bash  theme={null}
    python cookbook/examples/teams/tools/02_team_with_tool_hooks.py
    ```
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Install required libraries">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Set environment variables">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run the agent">
```

---

## Run a hybrid search query

**URL:** llms-txt#run-a-hybrid-search-query

**Contents:**
- Usage

results = hybrid_db.search("chicken coconut soup", limit=5)
print("Hybrid Search Results:", results)
bash  theme={null}
    pip install -U agno sqlalchemy psycopg pgvector
    bash Mac theme={null}
      python cookbook/knowledge/search_type/hybrid_search.py
      bash Windows theme={null}
      python cookbook/knowledge/search_type/hybrid_search.py
      ```
    </CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Step title="Install libraries">
```

Example 2 (unknown):
```unknown
</Step>

  <Snippet file="run-pgvector-step.mdx" />

  <Step title="Run the example">
    <CodeGroup>
```

Example 3 (unknown):
```unknown

```

---

## Mem0 Tools

**URL:** llms-txt#mem0-tools

**Contents:**
- Code
- Usage

Source: https://docs.agno.com/examples/concepts/tools/others/mem0

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Set your API keys">
    
  </Step>

<Step title="Install libraries">
    
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Set your API keys">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Example 1: Business Search

**URL:** llms-txt#example-1:-business-search

print("\n=== Business Search Example ===")
agent.print_response(
    "Find me highly rated Indian restaurants in Phoenix, AZ with their contact details",
    markdown=True,
    stream=True,
)

---

## Entertainment trends

**URL:** llms-txt#entertainment-trends

entertainment_prompt = """\
Analyze media trends for:
Keywords: streaming, gaming, social media
Sources: variety.com, polygon.com, theverge.com
"""
```

---

## Run team as an interactive CLI app

**URL:** llms-txt#run-team-as-an-interactive-cli-app

**Contents:**
- Developer Resources

team.cli_app(stream=True)
```

## Developer Resources

* View the [Team reference](/reference/teams/team)
* View [Team Cookbook](https://github.com/agno-agi/agno/tree/main/cookbook/teams/README.md)

---

## -*- Build container environment

**URL:** llms-txt#-*--build-container-environment

**Contents:**
  - Update the ECS Task Definition
  - Update the ECS Service
- Manually migrate prodution database
- How the migrations directory was created

container_env = {
    ...
    # Migrate database on startup using alembic
    "MIGRATE_DB": ws_settings.prd_db_enabled,
}
...
bash terminal theme={null}
  ag infra patch --env prd --infra aws --name td
  bash shorthand theme={null}
  ag infra patch -e prd -i aws -n td
  bash terminal theme={null}
  ag infra patch --env prd --infra aws --name service
  bash shorthand theme={null}
  ag infra patch -e prd -i aws -n service
  bash  theme={null}
ECS_CLUSTER=ai-app-prd-cluster
TASK_ARN=$(aws ecs list-tasks --cluster ai-app-prd-cluster --query "taskArns[0]" --output text)
CONTAINER_NAME=ai-api-prd

aws ecs execute-command --cluster $ECS_CLUSTER \
    --task $TASK_ARN \
    --container $CONTAINER_NAME \
    --interactive \
    --command "alembic -c db/alembic.ini upgrade head"
bash  theme={null}
docker exec -it ai-api cd db && alembic init migrations
python db/migrations/env.py theme={null}

**Examples:**

Example 1 (unknown):
```unknown
### Update the ECS Task Definition

Because we updated the Environment Variables, we need to update the Task Definition:

<CodeGroup>
```

Example 2 (unknown):
```unknown

```

Example 3 (unknown):
```unknown
</CodeGroup>

### Update the ECS Service

After updating the task definition, redeploy the production application:

<CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Update Content

**URL:** llms-txt#update-content

Source: https://docs.agno.com/reference-api/schema/knowledge/update-content

patch /knowledge/content/{content_id}
Update content properties such as name, description, metadata, or processing configuration. Allows modification of existing content without re-uploading.

---

## Docker

**URL:** llms-txt#docker

**Contents:**
- Prerequisites
- Example
- Toolkit Params
- Toolkit Functions
  - Container Management
  - Image Management
  - Volume Management
  - Network Management
- Developer Resources

Source: https://docs.agno.com/concepts/tools/toolkits/local/docker

**DockerTools** enable an Agent to interact with Docker containers, images, volumes, and networks.

The Docker tools require the `docker` Python package. You'll also need Docker installed and running on your system.

The following example creates an agent that can manage Docker resources:

| Parameter                     | Type   | Default | Description                                                      |
| ----------------------------- | ------ | ------- | ---------------------------------------------------------------- |
| `enable_container_management` | `bool` | `True`  | Enables container management functions (list, start, stop, etc.) |
| `enable_image_management`     | `bool` | `True`  | Enables image management functions (pull, build, etc.)           |
| `enable_volume_management`    | `bool` | `False` | Enables volume management functions                              |
| `enable_network_management`   | `bool` | `False` | Enables network management functions                             |

### Container Management

| Function             | Description                                     |
| -------------------- | ----------------------------------------------- |
| `list_containers`    | Lists all containers or only running containers |
| `start_container`    | Starts a stopped container                      |
| `stop_container`     | Stops a running container                       |
| `remove_container`   | Removes a container                             |
| `get_container_logs` | Retrieves logs from a container                 |
| `inspect_container`  | Gets detailed information about a container     |
| `run_container`      | Creates and starts a new container              |
| `exec_in_container`  | Executes a command inside a running container   |

| Function        | Description                              |
| --------------- | ---------------------------------------- |
| `list_images`   | Lists all images on the system           |
| `pull_image`    | Pulls an image from a registry           |
| `remove_image`  | Removes an image                         |
| `build_image`   | Builds an image from a Dockerfile        |
| `tag_image`     | Tags an image                            |
| `inspect_image` | Gets detailed information about an image |

### Volume Management

| Function         | Description                              |
| ---------------- | ---------------------------------------- |
| `list_volumes`   | Lists all volumes                        |
| `create_volume`  | Creates a new volume                     |
| `remove_volume`  | Removes a volume                         |
| `inspect_volume` | Gets detailed information about a volume |

### Network Management

| Function                            | Description                               |
| ----------------------------------- | ----------------------------------------- |
| `list_networks`                     | Lists all networks                        |
| `create_network`                    | Creates a new network                     |
| `remove_network`                    | Removes a network                         |
| `inspect_network`                   | Gets detailed information about a network |
| `connect_container_to_network`      | Connects a container to a network         |
| `disconnect_container_from_network` | Disconnects a container from a network    |

## Developer Resources

* View [Tools](https://github.com/agno-agi/agno/blob/main/libs/agno/agno/tools/docker.py)
* View [Cookbook](https://github.com/agno-agi/agno/tree/main/cookbook/tools/docker_tools.py)

**Examples:**

Example 1 (unknown):
```unknown
## Example

The following example creates an agent that can manage Docker resources:
```

---

## With Podman

**URL:** llms-txt#with-podman

---

## Running Workflows

**URL:** llms-txt#running-workflows

**Contents:**
- Running your Workflow

Source: https://docs.agno.com/concepts/workflows/running-workflow

Learn how to run a workflow and get the response.

The `Workflow.run()` function runs the agent and generates a response, either as a `WorkflowRunOutput` object or a stream of `WorkflowRunOutput` objects.

Many of our examples use `workflow.print_response()` which is a helper utility to print the response in the terminal. This uses `workflow.run()` under the hood.

## Running your Workflow

Here's how to run your workflow. The response is captured in the `response`.

```python  theme={null}
from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.db.sqlite import SqliteDb
from agno.team import Team
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.hackernews import HackerNewsTools
from agno.workflow import Step, Workflow
from agno.run.workflow import WorkflowRunOutput
from agno.utils.pprint import pprint_run_response

---

## Run the team and capture metrics

**URL:** llms-txt#run-the-team-and-capture-metrics

run_output = team.run("What is the stock price of NVDA")
pprint_run_response(run_output, markdown=True)

---

## Using Podman

**URL:** llms-txt#using-podman

**Contents:**
- Basic Example

podman exec db psql -U toolbox_user -d toolbox_db -c "SELECT COUNT(*) FROM hotels;"
python  theme={null}
import asyncio
from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.mcp_toolbox import MCPToolbox

async def main():
    # Connect to the running MCP Toolbox server and filter to hotel tools only
    async with MCPToolbox(
        url="http://127.0.0.1:5001",
        toolsets=["hotel-management"]  # Only load hotel search tools
    ) as toolbox:
        agent = Agent(
            model=OpenAIChat(),
            tools=[toolbox],
            instructions="You help users find hotels. Always mention hotel ID, name, location, and price tier."
        )
        
        # Ask the agent to find hotels
        await agent.aprint_response("Find luxury hotels in Zurich")

**Examples:**

Example 1 (unknown):
```unknown
## Basic Example

Here's the simplest way to use MCPToolbox (after running the Quick Start setup):
```

---

## Add middleware

**URL:** llms-txt#add-middleware

**Contents:**
- Common Use Cases
- Middleware Execution Order

app.add_middleware(
    JWTMiddleware,
    secret_key="your-secret-key",
    validate=True
)

if __name__ == "__main__":
    agent_os.serve(app="agent_os_with_jwt_middleware:app", reload=True)
python  theme={null}
    class RateLimitMiddleware(BaseHTTPMiddleware):
        def __init__(self, app, requests_per_minute: int = 60):
            super().__init__(app)
            self.requests_per_minute = requests_per_minute
            # ... implementation

app.add_middleware(RateLimitMiddleware, requests_per_minute=100)
    python  theme={null}
    class LoggingMiddleware(BaseHTTPMiddleware):
        async def dispatch(self, request: Request, call_next):
            start_time = time.time()
            response = await call_next(request)
            process_time = time.time() - start_time
            # Log request details...
            return response
    python  theme={null}
app.add_middleware(MiddlewareA)  # Runs third (closest to route)
app.add_middleware(MiddlewareB)  # Runs second
app.add_middleware(MiddlewareC)  # Runs first (outermost)

**Examples:**

Example 1 (unknown):
```unknown
<Note>
  Always test middleware thoroughly in staging environments before production deployment.

  A reminder that middleware adds latency to every request.
</Note>

## Common Use Cases

<Tabs>
  <Tab title="Authentication">
    **Secure your AgentOS with JWT authentication:**

    * Extract tokens from headers or cookies
    * Automatic parameter injection (user\_id, session\_id)
    * Custom claims extraction for `dependencies` and `session_state`
    * Route exclusion for public endpoints

    [Learn more about JWT Middleware](/agent-os/customize/middleware/jwt)
  </Tab>

  <Tab title="Rate Limiting">
    **Prevent API abuse with rate limiting:**
```

Example 2 (unknown):
```unknown
See the [full example](/examples/agent-os/middleware/custom-middleware) for more details.
  </Tab>

  <Tab title="Logging">
    **Monitor requests and responses:**
```

Example 3 (unknown):
```unknown
See the [full example](/examples/agent-os/middleware/custom-middleware) for more details.
  </Tab>
</Tabs>

## Middleware Execution Order

<Warning>
  Middleware is executed in reverse order of addition. The last middleware added runs first.
</Warning>
```

---

## Example 2: List all channels in the Slack workspace

**URL:** llms-txt#example-2:-list-all-channels-in-the-slack-workspace

agent.print_response("List all channels in our Slack workspace", markdown=True)

---

## -----------------------------------------------------------------------------------

**URL:** llms-txt#-----------------------------------------------------------------------------------

---

## Building Workflows

**URL:** llms-txt#building-workflows

**Contents:**
- Building Blocks
- How to make your first workflow?

Source: https://docs.agno.com/concepts/workflows/building-workflow

Learn how to build your workflows.

Workflows are a powerful way to orchestrate your agents and teams. They are a series of steps that are executed in a flow that you control.

1. The **`Workflow`** class is the top-level orchestrator that manages the entire execution process.
2. **`Step`** is the fundamental unit of work in the workflow system. Each step encapsulates exactly one `executor` - either an `Agent`, a `Team`, or a custom Python function. This design ensures clarity and maintainability while preserving the individual characteristics of each executor.
3. **`Loop`** is a construct that allows you to execute one or more steps multiple times. This is useful when you need to repeat a set of steps until a certain condition is met.
4. **`Parallel`** is a construct that allows you to execute one or more steps in parallel. This is useful when you need to execute a set of steps concurrently with the outputs joined together.
5. **`Condition`** makes a step conditional based on criteria you specify.
6. **`Router`** allows you to specify which step(s) to execute next, effectively creating branching logic in your workflow.

<Note>
  When using a custom Python function as an executor for a step, `StepInput` and
  `StepOutput` provides standardized interfaces for data flow between steps:
</Note>

<img className="block dark:hidden" src="https://mintcdn.com/agno-v2/ZJv0T4EM1rVInAsr/images/workflows-step-io-flow-light.png?fit=max&auto=format&n=ZJv0T4EM1rVInAsr&q=85&s=25129f55e1ead21d513c25b51dca412d" alt="Workflows step IO flow diagram" data-og-width="2001" width="2001" data-og-height="756" height="756" data-path="images/workflows-step-io-flow-light.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/agno-v2/ZJv0T4EM1rVInAsr/images/workflows-step-io-flow-light.png?w=280&fit=max&auto=format&n=ZJv0T4EM1rVInAsr&q=85&s=67a933194ccf6f39606314d48784927f 280w, https://mintcdn.com/agno-v2/ZJv0T4EM1rVInAsr/images/workflows-step-io-flow-light.png?w=560&fit=max&auto=format&n=ZJv0T4EM1rVInAsr&q=85&s=3edc1003ffbee7b7767b1a84720bc616 560w, https://mintcdn.com/agno-v2/ZJv0T4EM1rVInAsr/images/workflows-step-io-flow-light.png?w=840&fit=max&auto=format&n=ZJv0T4EM1rVInAsr&q=85&s=cd83f97d789ba6e0a1980e8d25759281 840w, https://mintcdn.com/agno-v2/ZJv0T4EM1rVInAsr/images/workflows-step-io-flow-light.png?w=1100&fit=max&auto=format&n=ZJv0T4EM1rVInAsr&q=85&s=f018b152972888a037a239342bde7602 1100w, https://mintcdn.com/agno-v2/ZJv0T4EM1rVInAsr/images/workflows-step-io-flow-light.png?w=1650&fit=max&auto=format&n=ZJv0T4EM1rVInAsr&q=85&s=bd9ba2232ddd6801f4ece40477975608 1650w, https://mintcdn.com/agno-v2/ZJv0T4EM1rVInAsr/images/workflows-step-io-flow-light.png?w=2500&fit=max&auto=format&n=ZJv0T4EM1rVInAsr&q=85&s=383ce2bcd2751e0b47a1304f797cf2d0 2500w" />

<img className="hidden dark:block" src="https://mintcdn.com/agno-v2/ZJv0T4EM1rVInAsr/images/workflows-step-io-flow.png?fit=max&auto=format&n=ZJv0T4EM1rVInAsr&q=85&s=faa43206bfa64265fa4d32721a12216d" alt="Workflows step IO flow diagram" data-og-width="2001" width="2001" data-og-height="756" height="756" data-path="images/workflows-step-io-flow.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/agno-v2/ZJv0T4EM1rVInAsr/images/workflows-step-io-flow.png?w=280&fit=max&auto=format&n=ZJv0T4EM1rVInAsr&q=85&s=50c47b8c564021b246ba034bc331c982 280w, https://mintcdn.com/agno-v2/ZJv0T4EM1rVInAsr/images/workflows-step-io-flow.png?w=560&fit=max&auto=format&n=ZJv0T4EM1rVInAsr&q=85&s=430d80604ed6927b914c7e9c1fcf8aa6 560w, https://mintcdn.com/agno-v2/ZJv0T4EM1rVInAsr/images/workflows-step-io-flow.png?w=840&fit=max&auto=format&n=ZJv0T4EM1rVInAsr&q=85&s=60dfe1d25956d8895f87b215607466cf 840w, https://mintcdn.com/agno-v2/ZJv0T4EM1rVInAsr/images/workflows-step-io-flow.png?w=1100&fit=max&auto=format&n=ZJv0T4EM1rVInAsr&q=85&s=8ac4c55ff3bb0c2f17baf76f0bae48a8 1100w, https://mintcdn.com/agno-v2/ZJv0T4EM1rVInAsr/images/workflows-step-io-flow.png?w=1650&fit=max&auto=format&n=ZJv0T4EM1rVInAsr&q=85&s=1c4237aa3bf8581c52b97f51f0b524e5 1650w, https://mintcdn.com/agno-v2/ZJv0T4EM1rVInAsr/images/workflows-step-io-flow.png?w=2500&fit=max&auto=format&n=ZJv0T4EM1rVInAsr&q=85&s=9ae7ba15375a9bb435b1765646047770 2500w" />

## How to make your first workflow?

There are different types of patterns you can use to build your workflows.
For example you can combine agents, teams, and functions to build a workflow.

---

## Readme Generator

**URL:** llms-txt#readme-generator

**Contents:**
- Code
- Usage

Source: https://docs.agno.com/examples/use-cases/agents/readme_generator

The README Generator Agent is an intelligent automation tool that creates comprehensive, professional README files for open source projects. This agent leverages the power of AI to analyze GitHub repositories and generate well-structured documentation automatically.

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Set your API key">
    
  </Step>

<Step title="Install libraries">
    
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Set your API key">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Delete Multiple Memories

**URL:** llms-txt#delete-multiple-memories

Source: https://docs.agno.com/reference-api/schema/memory/delete-multiple-memories

delete /memories
Delete multiple user memories by their IDs in a single operation. This action cannot be undone and all specified memories will be permanently removed.

---

## Give a sentiment analysis of this audio conversation. Use speaker A, speaker B to identify speakers.

**URL:** llms-txt#give-a-sentiment-analysis-of-this-audio-conversation.-use-speaker-a,-speaker-b-to-identify-speakers.

**Contents:**
- Key Features
- Use Cases

agent.print_response(
    "Give a sentiment analysis of this audio conversation. Use speaker A, speaker B to identify speakers.",
    audio=[Audio(content=audio_content)],
    stream=True,
)

agent.print_response(
    "What else can you tell me about this audio conversation?",
    stream=True,
)
```

* **Audio Processing**: Downloads and processes audio files from remote URLs
* **Sentiment Analysis**: Analyzes emotional tone and sentiment in conversations
* **Speaker Identification**: Distinguishes between different speakers in the conversation
* **Persistent Sessions**: Maintains conversation history using SQLite database
* **Streaming Response**: Real-time response generation for better user experience

* Customer service call analysis
* Meeting sentiment tracking
* Interview evaluation
* Call center quality monitoring

---

## Custom reader configuration

**URL:** llms-txt#custom-reader-configuration

reader = PDFReader(
    chunk_size=1000,
    chunking_strategy=SemanticChunking(),
)

knowledge_base = Knowledge(
    vector_db=vector_db,
)

---

## TeamRunOutput

**URL:** llms-txt#teamrunoutput

**Contents:**
- TeamRunOutput Attributes
- TeamRunOutputEvent Types
  - Core Events
  - Pre-Hook Events
  - Post-Hook Events
  - Tool Events
  - Reasoning Events
  - Memory Events
  - Session Summary Events
- Event Attributes

Source: https://docs.agno.com/reference/teams/team-response

The `TeamRunOutput` class represents the response from a team run, containing both the team's overall response and individual member responses. It supports streaming and provides real-time events throughout the execution of a team.

## TeamRunOutput Attributes

| Attribute             | Type                                              | Default             | Description                                 |
| --------------------- | ------------------------------------------------- | ------------------- | ------------------------------------------- |
| `content`             | `Any`                                             | `None`              | Content of the response                     |
| `content_type`        | `str`                                             | `"str"`             | Specifies the data type of the content      |
| `messages`            | `List[Message]`                                   | `None`              | A list of messages included in the response |
| `metrics`             | `Metrics`                                         | `None`              | Usage metrics of the run                    |
| `model`               | `str`                                             | `None`              | The model used in the run                   |
| `model_provider`      | `str`                                             | `None`              | The model provider used in the run          |
| `member_responses`    | `List[Union[TeamRunOutput, RunOutput]]`           | `[]`                | Responses from individual team members      |
| `run_id`              | `str`                                             | `None`              | Run Id                                      |
| `team_id`             | `str`                                             | `None`              | Team Id for the run                         |
| `team_name`           | `str`                                             | `None`              | Name of the team                            |
| `session_id`          | `str`                                             | `None`              | Session Id for the run                      |
| `parent_run_id`       | `str`                                             | `None`              | Parent run ID if this is a nested run       |
| `tools`               | `List[ToolExecution]`                             | `None`              | List of tools provided to the model         |
| `images`              | `List[Image]`                                     | `None`              | List of images from member runs             |
| `videos`              | `List[Video]`                                     | `None`              | List of videos from member runs             |
| `audio`               | `List[Audio]`                                     | `None`              | List of audio snippets from member runs     |
| `files`               | `List[File]`                                      | `None`              | List of files from member runs              |
| `response_audio`      | `Audio`                                           | `None`              | The model's raw response in audio           |
| `input`               | `TeamRunInput`                                    | `None`              | Input media and messages from user          |
| `reasoning_content`   | `str`                                             | `None`              | Any reasoning content the model produced    |
| `citations`           | `Citations`                                       | `None`              | Any citations used in the response          |
| `model_provider_data` | `Any`                                             | `None`              | Model provider specific metadata            |
| `metadata`            | `Dict[str, Any]`                                  | `None`              | Additional metadata for the run             |
| `references`          | `List[MessageReferences]`                         | `None`              | Message references                          |
| `additional_input`    | `List[Message]`                                   | `None`              | Additional input messages                   |
| `reasoning_steps`     | `List[ReasoningStep]`                             | `None`              | Reasoning steps taken during execution      |
| `reasoning_messages`  | `List[Message]`                                   | `None`              | Messages related to reasoning               |
| `created_at`          | `int`                                             | Current timestamp   | Unix timestamp of the response creation     |
| `events`              | `List[Union[RunOutputEvent, TeamRunOutputEvent]]` | `None`              | List of events that occurred during the run |
| `status`              | `RunStatus`                                       | `RunStatus.running` | Current status of the run                   |
| `workflow_step_id`    | `str`                                             | `None`              | FK: Points to StepOutput.step\_id           |

## TeamRunOutputEvent Types

The following events are sent by the `Team.run()` function depending on the team's configuration:

| Event Type                   | Description                                             |
| ---------------------------- | ------------------------------------------------------- |
| `TeamRunStarted`             | Indicates the start of a team run                       |
| `TeamRunContent`             | Contains the model's response text as individual chunks |
| `TeamRunContentCompleted`    | Signals completion of content streaming                 |
| `TeamRunIntermediateContent` | Contains intermediate content during the run            |
| `TeamRunCompleted`           | Signals successful completion of the team run           |
| `TeamRunError`               | Indicates an error occurred during the team run         |
| `TeamRunCancelled`           | Signals that the team run was cancelled                 |

| Event Type             | Description                                    |
| ---------------------- | ---------------------------------------------- |
| `TeamPreHookStarted`   | Indicates the start of a pre-run hook          |
| `TeamPreHookCompleted` | Signals completion of a pre-run hook execution |

| Event Type              | Description                                     |
| ----------------------- | ----------------------------------------------- |
| `TeamPostHookStarted`   | Indicates the start of a post-run hook          |
| `TeamPostHookCompleted` | Signals completion of a post-run hook execution |

| Event Type              | Description                                                    |
| ----------------------- | -------------------------------------------------------------- |
| `TeamToolCallStarted`   | Indicates the start of a tool call                             |
| `TeamToolCallCompleted` | Signals completion of a tool call, including tool call results |

| Event Type               | Description                                         |
| ------------------------ | --------------------------------------------------- |
| `TeamReasoningStarted`   | Indicates the start of the team's reasoning process |
| `TeamReasoningStep`      | Contains a single step in the reasoning process     |
| `TeamReasoningCompleted` | Signals completion of the reasoning process         |

| Event Type                  | Description                                    |
| --------------------------- | ---------------------------------------------- |
| `TeamMemoryUpdateStarted`   | Indicates that the team is updating its memory |
| `TeamMemoryUpdateCompleted` | Signals completion of a memory update          |

### Session Summary Events

| Event Type                    | Description                                       |
| ----------------------------- | ------------------------------------------------- |
| `TeamSessionSummaryStarted`   | Indicates the start of session summary generation |
| `TeamSessionSummaryCompleted` | Signals completion of session summary generation  |

### Base TeamRunOutputEvent

All events inherit from `BaseTeamRunEvent` which provides these common attributes:

| Attribute         | Type            | Default           | Description                           |
| ----------------- | --------------- | ----------------- | ------------------------------------- |
| `created_at`      | `int`           | Current timestamp | Unix timestamp of the event creation  |
| `event`           | `str`           | `""`              | The type of event                     |
| `team_id`         | `str`           | `""`              | ID of the team generating the event   |
| `team_name`       | `str`           | `""`              | Name of the team generating the event |
| `run_id`          | `Optional[str]` | `None`            | ID of the current run                 |
| `session_id`      | `Optional[str]` | `None`            | ID of the current session             |
| `workflow_id`     | `Optional[str]` | `None`            | ID of the workflow                    |
| `workflow_run_id` | `Optional[str]` | `None`            | ID of the workflow's run              |
| `step_id`         | `Optional[str]` | `None`            | ID of the workflow step               |
| `step_name`       | `Optional[str]` | `None`            | Name of the workflow step             |
| `step_index`      | `Optional[int]` | `None`            | Index of the workflow step            |
| `content`         | `Optional[Any]` | `None`            | For backwards compatibility           |

| Attribute        | Type  | Default            | Description               |
| ---------------- | ----- | ------------------ | ------------------------- |
| `event`          | `str` | `"TeamRunStarted"` | Event type                |
| `model`          | `str` | `""`               | The model being used      |
| `model_provider` | `str` | `""`               | The provider of the model |

### IntermediateRunContentEvent

| Attribute      | Type            | Default                        | Description                          |
| -------------- | --------------- | ------------------------------ | ------------------------------------ |
| `event`        | `str`           | `"TeamRunIntermediateContent"` | Event type                           |
| `content`      | `Optional[Any]` | `None`                         | Intermediate content of the response |
| `content_type` | `str`           | `"str"`                        | Type of the content                  |

### RunContentCompletedEvent

| Attribute | Type  | Default                     | Description |
| --------- | ----- | --------------------------- | ----------- |
| `event`   | `str` | `"TeamRunContentCompleted"` | Event type  |

| Attribute             | Type                                | Default            | Description                      |
| --------------------- | ----------------------------------- | ------------------ | -------------------------------- |
| `event`               | `str`                               | `"TeamRunContent"` | Event type                       |
| `content`             | `Optional[Any]`                     | `None`             | The content of the response      |
| `content_type`        | `str`                               | `"str"`            | Type of the content              |
| `reasoning_content`   | `Optional[str]`                     | `None`             | Reasoning content produced       |
| `citations`           | `Optional[Citations]`               | `None`             | Citations used in the response   |
| `model_provider_data` | `Optional[Any]`                     | `None`             | Model provider specific metadata |
| `response_audio`      | `Optional[Audio]`                   | `None`             | Model's audio response           |
| `image`               | `Optional[Image]`                   | `None`             | Image attached to the response   |
| `references`          | `Optional[List[MessageReferences]]` | `None`             | Message references               |
| `additional_input`    | `Optional[List[Message]]`           | `None`             | Additional input messages        |
| `reasoning_steps`     | `Optional[List[ReasoningStep]]`     | `None`             | Reasoning steps                  |
| `reasoning_messages`  | `Optional[List[Message]]`           | `None`             | Reasoning messages               |

### RunCompletedEvent

| Attribute             | Type                                    | Default              | Description                             |
| --------------------- | --------------------------------------- | -------------------- | --------------------------------------- |
| `event`               | `str`                                   | `"TeamRunCompleted"` | Event type                              |
| `content`             | `Optional[Any]`                         | `None`               | Final content of the response           |
| `content_type`        | `str`                                   | `"str"`              | Type of the content                     |
| `reasoning_content`   | `Optional[str]`                         | `None`               | Reasoning content produced              |
| `citations`           | `Optional[Citations]`                   | `None`               | Citations used in the response          |
| `model_provider_data` | `Optional[Any]`                         | `None`               | Model provider specific metadata        |
| `images`              | `Optional[List[Image]]`                 | `None`               | Images attached to the response         |
| `videos`              | `Optional[List[Video]]`                 | `None`               | Videos attached to the response         |
| `audio`               | `Optional[List[Audio]]`                 | `None`               | Audio snippets attached to the response |
| `response_audio`      | `Optional[Audio]`                       | `None`               | Model's audio response                  |
| `references`          | `Optional[List[MessageReferences]]`     | `None`               | Message references                      |
| `additional_input`    | `Optional[List[Message]]`               | `None`               | Additional input messages               |
| `reasoning_steps`     | `Optional[List[ReasoningStep]]`         | `None`               | Reasoning steps                         |
| `reasoning_messages`  | `Optional[List[Message]]`               | `None`               | Reasoning messages                      |
| `member_responses`    | `List[Union[TeamRunOutput, RunOutput]]` | `[]`                 | Responses from individual team members  |
| `metadata`            | `Optional[Dict[str, Any]]`              | `None`               | Additional metadata                     |
| `metrics`             | `Optional[Metrics]`                     | `None`               | Usage metrics                           |

| Attribute | Type            | Default          | Description   |
| --------- | --------------- | ---------------- | ------------- |
| `event`   | `str`           | `"TeamRunError"` | Event type    |
| `content` | `Optional[str]` | `None`           | Error message |

### RunCancelledEvent

| Attribute | Type            | Default              | Description             |
| --------- | --------------- | -------------------- | ----------------------- |
| `event`   | `str`           | `"TeamRunCancelled"` | Event type              |
| `reason`  | `Optional[str]` | `None`               | Reason for cancellation |

### PreHookStartedEvent

| Attribute       | Type                     | Default                | Description                         |
| --------------- | ------------------------ | ---------------------- | ----------------------------------- |
| `event`         | `str`                    | `"TeamPreHookStarted"` | Event type                          |
| `pre_hook_name` | `Optional[str]`          | `None`                 | Name of the pre-hook being executed |
| `run_input`     | `Optional[TeamRunInput]` | `None`                 | The run input passed to the hook    |

### PreHookCompletedEvent

| Attribute       | Type                     | Default                  | Description                         |
| --------------- | ------------------------ | ------------------------ | ----------------------------------- |
| `event`         | `str`                    | `"TeamPreHookCompleted"` | Event type                          |
| `pre_hook_name` | `Optional[str]`          | `None`                   | Name of the pre-hook that completed |
| `run_input`     | `Optional[TeamRunInput]` | `None`                   | The run input passed to the hook    |

### PostHookStartedEvent

| Attribute        | Type            | Default                 | Description                          |
| ---------------- | --------------- | ----------------------- | ------------------------------------ |
| `event`          | `str`           | `"TeamPostHookStarted"` | Event type                           |
| `post_hook_name` | `Optional[str]` | `None`                  | Name of the post-hook being executed |

### PostHookCompletedEvent

| Attribute        | Type            | Default                   | Description                          |
| ---------------- | --------------- | ------------------------- | ------------------------------------ |
| `event`          | `str`           | `"TeamPostHookCompleted"` | Event type                           |
| `post_hook_name` | `Optional[str]` | `None`                    | Name of the post-hook that completed |

### ToolCallStartedEvent

| Attribute | Type                      | Default                 | Description           |
| --------- | ------------------------- | ----------------------- | --------------------- |
| `event`   | `str`                     | `"TeamToolCallStarted"` | Event type            |
| `tool`    | `Optional[ToolExecution]` | `None`                  | The tool being called |

### ToolCallCompletedEvent

| Attribute | Type                      | Default                   | Description                 |
| --------- | ------------------------- | ------------------------- | --------------------------- |
| `event`   | `str`                     | `"TeamToolCallCompleted"` | Event type                  |
| `tool`    | `Optional[ToolExecution]` | `None`                    | The tool that was called    |
| `content` | `Optional[Any]`           | `None`                    | Result of the tool call     |
| `images`  | `Optional[List[Image]]`   | `None`                    | Images produced by the tool |
| `videos`  | `Optional[List[Video]]`   | `None`                    | Videos produced by the tool |
| `audio`   | `Optional[List[Audio]]`   | `None`                    | Audio produced by the tool  |

### ReasoningStartedEvent

| Attribute | Type  | Default                  | Description |
| --------- | ----- | ------------------------ | ----------- |
| `event`   | `str` | `"TeamReasoningStarted"` | Event type  |

### ReasoningStepEvent

| Attribute           | Type            | Default               | Description                   |
| ------------------- | --------------- | --------------------- | ----------------------------- |
| `event`             | `str`           | `"TeamReasoningStep"` | Event type                    |
| `content`           | `Optional[Any]` | `None`                | Content of the reasoning step |
| `content_type`      | `str`           | `"str"`               | Type of the content           |
| `reasoning_content` | `str`           | `""`                  | Detailed reasoning content    |

### ReasoningCompletedEvent

| Attribute      | Type            | Default                    | Description                   |
| -------------- | --------------- | -------------------------- | ----------------------------- |
| `event`        | `str`           | `"TeamReasoningCompleted"` | Event type                    |
| `content`      | `Optional[Any]` | `None`                     | Content of the reasoning step |
| `content_type` | `str`           | `"str"`                    | Type of the content           |

### MemoryUpdateStartedEvent

| Attribute | Type  | Default                     | Description |
| --------- | ----- | --------------------------- | ----------- |
| `event`   | `str` | `"TeamMemoryUpdateStarted"` | Event type  |

### MemoryUpdateCompletedEvent

| Attribute | Type  | Default                       | Description |
| --------- | ----- | ----------------------------- | ----------- |
| `event`   | `str` | `"TeamMemoryUpdateCompleted"` | Event type  |

### SessionSummaryStartedEvent

| Attribute | Type  | Default                       | Description |
| --------- | ----- | ----------------------------- | ----------- |
| `event`   | `str` | `"TeamSessionSummaryStarted"` | Event type  |

### SessionSummaryCompletedEvent

| Attribute         | Type            | Default                         | Description                   |
| ----------------- | --------------- | ------------------------------- | ----------------------------- |
| `event`           | `str`           | `"TeamSessionSummaryCompleted"` | Event type                    |
| `session_summary` | `Optional[Any]` | `None`                          | The generated session summary |

### ParserModelResponseStartedEvent

| Attribute | Type  | Default                            | Description |
| --------- | ----- | ---------------------------------- | ----------- |
| `event`   | `str` | `"TeamParserModelResponseStarted"` | Event type  |

### ParserModelResponseCompletedEvent

| Attribute | Type  | Default                              | Description |
| --------- | ----- | ------------------------------------ | ----------- |
| `event`   | `str` | `"TeamParserModelResponseCompleted"` | Event type  |

### OutputModelResponseStartedEvent

| Attribute | Type  | Default                            | Description |
| --------- | ----- | ---------------------------------- | ----------- |
| `event`   | `str` | `"TeamOutputModelResponseStarted"` | Event type  |

### OutputModelResponseCompletedEvent

| Attribute | Type  | Default                              | Description |
| --------- | ----- | ------------------------------------ | ----------- |
| `event`   | `str` | `"TeamOutputModelResponseCompleted"` | Event type  |

| Attribute | Type  | Default         | Description |
| --------- | ----- | --------------- | ----------- |
| `event`   | `str` | `"CustomEvent"` | Event type  |

---

## Debugging Teams

**URL:** llms-txt#debugging-teams

**Contents:**
- Debug Mode

Source: https://docs.agno.com/concepts/teams/debugging-teams

Learn how to debug Agno Teams.

Agno comes with an exceptionally well-built debug mode that takes your team development experience to the next level. It helps you understand the flow of execution and the intermediate steps. For example:

1. Inspect the messages sent to the model and the response it generates.
2. Trace intermediate steps and monitor metrics like token usage, execution time, etc.
3. Inspect tool calls, errors, and their results.
4. Monitor team member interactions and delegation patterns.

To enable debug mode:

1. Set the `debug_mode` parameter on your team, to enable it for all runs, as well as for member runs.
2. Set the `debug_mode` parameter on the `run` method, to enable it for the current run.
3. Set the `AGNO_DEBUG` environment variable to `True`, to enable debug mode for all teams.

```python  theme={null}
from agno.team import Team
from agno.agent import Agent
from agno.models.openai import OpenAIChat

news_agent = Agent(name="News Agent", role="Get the latest news")
weather_agent = Agent(name="Weather Agent", role="Get the weather for the next 7 days")

team = Team(
    name="News and Weather Team",
    members=[news_agent, weather_agent],
    model=OpenAIChat(id="gpt-4o"),
    debug_mode=True,
    # debug_level=2, # Uncomment to get more detailed logs
)

---

## Process streaming responses to find the final one

**URL:** llms-txt#process-streaming-responses-to-find-the-final-one

print("\n\n=== Test 2: Processing stream to find final response ===\n")

---

## Get a response

**URL:** llms-txt#get-a-response

**Contents:**
  - Using Hugging Face Models
  - Configuration Options
  - Examples

agent.print_response("Share a 2 sentence horror story")
python  theme={null}
from agno.agent import Agent
from agno.models.litellm import LiteLLM

agent = Agent(
    model=LiteLLM(
        id="huggingface/mistralai/Mistral-7B-Instruct-v0.2",
        top_p=0.95,
    ),
    markdown=True,
)

agent.print_response("What's happening in France?")
```

### Configuration Options

The `LiteLLM` class accepts the following parameters:

| Parameter        | Type                       | Description                                                                               | Default      |
| ---------------- | -------------------------- | ----------------------------------------------------------------------------------------- | ------------ |
| `id`             | str                        | Model identifier (e.g., "gpt-5-mini" or "huggingface/mistralai/Mistral-7B-Instruct-v0.2") | "gpt-5-mini" |
| `name`           | str                        | Display name for the model                                                                | "LiteLLM"    |
| `provider`       | str                        | Provider name                                                                             | "LiteLLM"    |
| `api_key`        | Optional\[str]             | API key (falls back to LITELLM\_API\_KEY environment variable)                            | None         |
| `api_base`       | Optional\[str]             | Base URL for API requests                                                                 | None         |
| `max_tokens`     | Optional\[int]             | Maximum tokens in the response                                                            | None         |
| `temperature`    | float                      | Sampling temperature                                                                      | 0.7          |
| `top_p`          | float                      | Top-p sampling value                                                                      | 1.0          |
| `request_params` | Optional\[Dict\[str, Any]] | Additional request parameters                                                             | None         |

<Note> View more examples [here](/examples/models/litellm/basic). </Note>

**Examples:**

Example 1 (unknown):
```unknown
### Using Hugging Face Models

LiteLLM can also work with Hugging Face models:
```

---

## Accuracy with Teams

**URL:** llms-txt#accuracy-with-teams

**Contents:**
- Code

Source: https://docs.agno.com/examples/concepts/evals/accuracy/accuracy_with_teams

Learn how to evaluate the accuracy of an Agno Team.

```python  theme={null}
from typing import Optional

from agno.agent import Agent
from agno.eval.accuracy import AccuracyEval, AccuracyResult
from agno.models.openai import OpenAIChat
from agno.team import Team

---

## Continue the session with user 1 (This means "tell me a joke" in French)

**URL:** llms-txt#continue-the-session-with-user-1-(this-means-"tell-me-a-joke"-in-french)

team.print_response("Raconte-moi une blague.", user_id=user_1_id, session_id=user_1_session_id)

---

## Shell

**URL:** llms-txt#shell

**Contents:**
- Example
- Toolkit Params
- Toolkit Functions
- Developer Resources

Source: https://docs.agno.com/concepts/tools/toolkits/local/shell

**ShellTools** enable an Agent to interact with the shell to run commands.

The following agent will run a shell command and show contents of the current directory.

<Note>
  Mention your OS to the agent to make sure it runs the correct command.
</Note>

| Parameter                  | Type   | Default | Description                                 |                                            |
| -------------------------- | ------ | ------- | ------------------------------------------- | ------------------------------------------ |
| `base_dir`                 | \`Path | str\`   | `None`                                      | Base directory for shell command execution |
| `enable_run_shell_command` | `bool` | `True`  | Enables functionality to run shell commands |                                            |
| `all`                      | `bool` | `False` | Enables all functionality when set to True  |                                            |

| Function            | Description                                           |
| ------------------- | ----------------------------------------------------- |
| `run_shell_command` | Runs a shell command and returns the output or error. |

## Developer Resources

* View [Tools](https://github.com/agno-agi/agno/blob/main/libs/agno/agno/tools/shell.py)
* View [Cookbook](https://github.com/agno-agi/agno/tree/main/cookbook/tools/shell_tools.py)

---

## Memories are automatically created from this conversation

**URL:** llms-txt#memories-are-automatically-created-from-this-conversation

agent.print_response("My name is Sarah and I prefer email over phone calls.")

---

## DynamoDB

**URL:** llms-txt#dynamodb

Source: https://docs.agno.com/reference/storage/dynamodb

DynamoDB is a class that implements the Db interface using Amazon DynamoDB as the backend storage system. It provides scalable, managed storage for agent sessions with support for indexing and efficient querying.

<Snippet file="db-dynamodb-params.mdx" />

<Snippet file="db-new-bulk-methods.mdx" />

---

## Check what's been processed and its status

**URL:** llms-txt#check-what's-been-processed-and-its-status

content_list, total_count = knowledge.get_content()
for content in content_list:
    status, message = knowledge.get_content_status(content.id)
    print(f"{content.name}: {status}")

---

## Remove content and vectors by id

**URL:** llms-txt#remove-content-and-vectors-by-id

contents, _ = knowledge.get_content()
for content in contents:
    print(content.id)
    print(" ")
    knowledge.remove_content_by_id(content.id)

---

## Example 1: Financial Statements

**URL:** llms-txt#example-1:-financial-statements

print("\n=== Income Statement Example ===")
agent.print_response(
    "Get the most recent income statement for AAPL and highlight key metrics",
    stream=True,
)

---

## PostgreSQL

**URL:** llms-txt#postgresql

**Contents:**
- Usage

Source: https://docs.agno.com/concepts/db/postgres

Learn to use PostgreSQL as a database for your Agents

Agno supports using [PostgreSQL](https://www.postgresql.org/) as a database with the `PostgresDb` class.

```python postgres_for_agent.py theme={null}
from agno.agent import Agent
from agno.db.postgres import PostgresDb

db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai" # Replace with your own connection string

---

## Create a team

**URL:** llms-txt#create-a-team

research_team = Team(
    name="Research Team",
    description="A team of agents that research the web",
    members=[research_agent, simple_agent],
    model=OpenAIChat(id="gpt-5-mini"),
    id="research_team",
    instructions=[
        "You are the lead researcher of a research team! 🔍",
    ],
    db=db,
    enable_user_memories=True,
    add_datetime_to_context=True,
    markdown=True,
)

---

## Development Application

**URL:** llms-txt#development-application

**Contents:**
- Infra Settings
- Build your development image
  - Build a new image
- Restart all containers
- Recreate development resources

Source: https://docs.agno.com/templates/infra-management/development-app

Your development application runs locally on docker and its resources are defined in the `infra/dev_resources.py` file. This guide shows how to:

1. [Build a development image](#build-your-development-image)
2. [Restart all docker containers](#restart-all-containers)
3. [Recreate development resources](#recreate-development-resources)

The `InfraSettings` object in the `infra/settings.py` file defines common settings used by your Agno Infra apps and resources.

## Build your development image

Your application uses the `agno` docker images by default. To use your own image:

* Open `infra/settings.py` file
* Update the `image_repo` to your image repository
* Set `build_images=True`

### Build a new image

Build the development image using:

To `force` rebuild images, use the `--force` or `-f` flag

## Restart all containers

Restart all docker containers using:

## Recreate development resources

To recreate all dev resources, use the `--force` flag:

**Examples:**

Example 1 (unknown):
```unknown
### Build a new image

Build the development image using:

<CodeGroup>
```

Example 2 (unknown):
```unknown

```

Example 3 (unknown):
```unknown
</CodeGroup>

To `force` rebuild images, use the `--force` or `-f` flag

<CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Metrics

**URL:** llms-txt#metrics

**Contents:**
- Example Usage

Source: https://docs.agno.com/concepts/teams/metrics

Understanding team run and session metrics in Agno

When you run a team in Agno, the response you get (**TeamRunOutput**) includes detailed metrics about the run. These metrics help you understand resource usage (like **token usage** and **time**), performance, and other aspects of the model and tool calls across both the team leader and team members.

Metrics are available at multiple levels:

* **Per-message**: Each message (assistant, tool, etc.) has its own metrics.
* **Per-member run**: Each team member run has its own metrics. You can make member runs available on the `TeamRunOutput` by setting `store_member_responses=True`,
* **Team-level**: The `TeamRunOutput` aggregates metrics across all team leader and team member messages.
* **Session-level**: Aggregated metrics across all runs in the session, for both the team leader and all team members.

Suppose you have a team that performs some tasks and you want to analyze the metrics after running it. Here's how you can access and print the metrics:

```python  theme={null}
from typing import Iterator

from agno.agent import Agent, RunOutput
from agno.models.openai import OpenAIChat
from agno.team.team import Team
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.utils.pprint import pprint_run_response
from rich.pretty import pprint

---

## Developer Resources

**URL:** llms-txt#developer-resources

* View [Cookbook](https://github.com/agno-agi/agno/tree/main/cookbook/knowledge/embedders/aws_bedrock_embedder.py)

---

## VoyageAI

**URL:** llms-txt#voyageai

Source: https://docs.agno.com/reference/knowledge/embedder/voyageai

VoyageAI Embedder is a class that allows you to embed documents using VoyageAI's embedding models, which are specifically designed for high-performance text embeddings.

<Snippet file="embedder-voyageai-reference.mdx" />

---

## Run the workflow

**URL:** llms-txt#run-the-workflow

**Contents:**
- Expected Output

company_description_workflow.print_response(
    input=supplier_request,
)
bash  theme={null}
python run_workflow.py
```

* [Company Analysis](https://github.com/agno-agi/agno/tree/main/cookbook/examples/workflows/company_analysis)
* [Customer Support](https://github.com/agno-agi/agno/tree/main/cookbook/examples/workflows/customer_support)
* [Investment Analyst](https://github.com/agno-agi/agno/tree/main/cookbook/examples/workflows/investment_analyst)

**Examples:**

Example 1 (unknown):
```unknown
## Expected Output

The workflow will:

1. **Gather Information**: Simultaneously crawl the website, search the web, check Wikipedia, and find competitors
2. **Generate Profile**: Create a comprehensive supplier profile with detailed sections
3. **Send Email**: Deliver the formatted HTML report to the specified email address

The generated supplier profile includes:

* Company overview and basic information
* Detailed analysis from multiple data sources
* Structured markdown formatting for readability
* Professional email delivery with HTML formatting

Run the workflow with:
```

---

## Create a team with both shared and private state

**URL:** llms-txt#create-a-team-with-both-shared-and-private-state

shopping_team = Team(
    name="Shopping Team",
    model=OpenAIChat(id="gpt-5-mini"),
    members=[shopping_agent],
    session_state={"shopping_list": [], "chores": []},
    tools=[list_items, add_chore],
    instructions=[
        "You manage a shopping list.",
        "Forward add/remove requests to the Shopping List Agent.",
        "Use list_items to show the current list.",
        "Log completed tasks using add_chore.",
    ],
)

---

## Set the local collector endpoint

**URL:** llms-txt#set-the-local-collector-endpoint

os.environ["PHOENIX_COLLECTOR_ENDPOINT"] = "http://localhost:6006"

---

## ------------------------------------------------------------------------------------------------

**URL:** llms-txt#------------------------------------------------------------------------------------------------

---

## Newspaper

**URL:** llms-txt#newspaper

**Contents:**
- Prerequisites
- Example
- Toolkit Params
- Toolkit Functions
- Developer Resources

Source: https://docs.agno.com/concepts/tools/toolkits/web_scrape/newspaper

**NewspaperTools** enable an Agent to read news articles using the Newspaper4k library.

The following example requires the `newspaper3k` library.

The following agent will summarize the wikipedia article on language models.

| Parameter                 | Type   | Default | Description                                                   |
| ------------------------- | ------ | ------- | ------------------------------------------------------------- |
| `enable_get_article_text` | `bool` | `True`  | Enables the functionality to retrieve the text of an article. |

| Function           | Description                                                                                                                                                                             |
| ------------------ | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| `get_article_text` | Retrieves the text of an article from a specified URL. Parameters include `url` for the URL of the article. Returns the text of the article or an error message if the retrieval fails. |

## Developer Resources

* View [Tools](https://github.com/agno-agi/agno/blob/main/libs/agno/agno/tools/newspaper.py)
* View [Cookbook](https://github.com/agno-agi/agno/tree/main/cookbook/tools/newspaper_tools.py)

**Examples:**

Example 1 (unknown):
```unknown
## Example

The following agent will summarize the wikipedia article on language models.
```

---

## Retry Function Call

**URL:** llms-txt#retry-function-call

**Contents:**
- Code
- Usage

Source: https://docs.agno.com/examples/getting-started/11-retry-function-call

This example shows how to retry a function call if it fails or you do not like the output. This is useful for:

* Handling temporary failures
* Improving output quality through retries
* Implementing human-in-the-loop validation

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Install libraries">
    
  </Step>

<Step title="Run the agent">
    
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Install libraries">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Run the agent">
```

---

## STREAM AND PRETTY PRINT

**URL:** llms-txt#stream-and-pretty-print

**Contents:**
  - Modify what is show on the terminal
- Next Steps
- Developer Resources

stream: Iterator[TeamRunOutputEvent] = team.run("What is the weather in Tokyo?", stream=True)
pprint_run_response(stream, markdown=True)
python  theme={null}
team.print_response("What is the weather in Tokyo?", show_members_responses=True)
```

Next, continue building your team by adding functionality as needed. Common questions:

* **How do I run my team?** -> See the [running teams](/concepts/teams/running-teams) documentation.
* **How do I manage sessions?** -> See the [team sessions](/concepts/teams/sessions) documentation.
* **How do I manage input and capture output?** -> See the [input and output](/concepts/teams/input-output) documentation.
* **How do I give the team context?** -> See the [context engineering](/concepts/teams/context) documentation.
* **How do I add knowledge?** -> See the [knowledge](/concepts/teams/knowledge) documentation.
* **How do I add guardrails?** -> See the [guardrails](/concepts/teams/guardrails) documentation.

## Developer Resources

* View the [Team reference](/reference/teams/team)
* View [Team Cookbook](https://github.com/agno-agi/agno/tree/main/cookbook/teams/README.md)

**Examples:**

Example 1 (unknown):
```unknown
### Modify what is show on the terminal

When using `print_response`, only the team tool calls (typically all of the delegation to members) are printed. If you want to print the responses from the members, you can use the `show_members_responses` parameter.
```

---

## session_id="session_summary",

**URL:** llms-txt#session_id="session_summary",

---

## Define research team for complex analysis

**URL:** llms-txt#define-research-team-for-complex-analysis

research_team = Team(
    name="Research Team",
    members=[hackernews_agent, web_agent],
    instructions="Research tech topics from Hackernews and the web",
)

content_planner = Agent(
    name="Content Planner",
    model=OpenAIChat(id="gpt-5-mini"),
    instructions=[
        "Plan a content schedule over 4 weeks for the provided topic and research content",
        "Ensure that I have posts for 3 posts per week",
    ],
)

---

## Setup Redis

**URL:** llms-txt#setup-redis

---

## SerperApi

**URL:** llms-txt#serperapi

**Contents:**
- Prerequisites
- Example
- Toolkit Params
- Toolkit Functions
- Developer Resources

Source: https://docs.agno.com/concepts/tools/toolkits/search/serper

**SerperApiTools** enable an Agent to search Google for a query.

The following example requires an API key from [SerperApi](https://serper.dev/).

The following agent will search Google for the query: "Whats happening in the USA" and share results.

| Parameter               | Type   | Default | Description                               |
| ----------------------- | ------ | ------- | ----------------------------------------- |
| `api_key`               | `str`  | -       | API key for authentication purposes.      |
| `location`              | `str`  | `"us"`  | Location to search from.                  |
| `enable_search`         | `bool` | `True`  | Enable the search functionality.          |
| `enable_search_news`    | `bool` | `True`  | Enable the search\_news functionality.    |
| `enable_search_scholar` | `bool` | `True`  | Enable the search\_scholar functionality. |
| `enable_scrape_webpage` | `bool` | `True`  | Enable the scrape\_webpage functionality. |
| `all`                   | `bool` | `False` | Enable all functionality.                 |

| Function         | Description                                        |
| ---------------- | -------------------------------------------------- |
| `search_google`  | This function searches Google for a query.         |
| `search_news`    | This function searches Google News for a query.    |
| `search_scholar` | This function searches Google Scholar for a query. |
| `scrape_webpage` | This function scrapes a webpage for a query.       |

## Developer Resources

* View [Tools](https://github.com/agno-agi/agno/blob/main/libs/agno/agno/tools/serper.py)
* View [Cookbook](https://github.com/agno-agi/agno/tree/main/cookbook/tools/serper_tools.py)

**Examples:**

Example 1 (unknown):
```unknown
## Example

The following agent will search Google for the query: "Whats happening in the USA" and share results.
```

---

## Business trends

**URL:** llms-txt#business-trends

business_prompt = """\
Analyze media trends for:
Keywords: remote work, digital transformation, sustainability
Sources: forbes.com, bloomberg.com, hbr.org
"""

---

## Basic debug information

**URL:** llms-txt#basic-debug-information

agent = Agent(
    model=Claude(id="claude-3-5-sonnet-20240620"),
    tools=[DuckDuckGoTools()],
    debug_mode=True,
    debug_level=1,
)

agent.print_response("What is the current price of Tesla?")

---

## Example usage with different types of book queries

**URL:** llms-txt#example-usage-with-different-types-of-book-queries

book_recommendation_agent.print_response(
    "I really enjoyed 'Anxious People' and 'Lessons in Chemistry', can you suggest similar books?",
    stream=True,
)

---

## PgVector Hybrid Search

**URL:** llms-txt#pgvector-hybrid-search

**Contents:**
- Code
- Usage

Source: https://docs.agno.com/examples/concepts/vectordb/pgvector/pgvector-hybrid-search

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Install libraries">
    
  </Step>

<Snippet file="run-pgvector.mdx" />

<Step title="Set environment variables">
    
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Install libraries">
```

Example 2 (unknown):
```unknown
</Step>

  <Snippet file="run-pgvector.mdx" />

  <Step title="Set environment variables">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Linkup

**URL:** llms-txt#linkup

**Contents:**
- Example
- Toolkit Params
- Toolkit Functions
- Developer Resources

Source: https://docs.agno.com/concepts/tools/toolkits/search/linkup

LinkupTools provides advanced web search capabilities with deep search options and structured results.

The following agent can perform advanced web searches using Linkup:

| Parameter                       | Type            | Default           | Description                                        |
| ------------------------------- | --------------- | ----------------- | -------------------------------------------------- |
| `api_key`                       | `Optional[str]` | `None`            | Linkup API key. Uses LINKUP\_API\_KEY if not set.  |
| `depth`                         | `Literal`       | `"standard"`      | Search depth: "standard" or "deep".                |
| `output_type`                   | `Literal`       | `"searchResults"` | Output format: "searchResults" or "sourcedAnswer". |
| `enable_web_search_with_linkup` | `bool`          | `True`            | Enable web search functionality.                   |

| Function                 | Description                                                       |
| ------------------------ | ----------------------------------------------------------------- |
| `web_search_with_linkup` | Perform advanced web searches with configurable depth and format. |

## Developer Resources

* View [Tools Source](https://github.com/agno-agi/agno/blob/main/libs/agno/agno/tools/linkup.py)
* [Linkup SDK Documentation](https://docs.linkup.com/)
* [Linkup API Reference](https://api.linkup.com/docs)

---

## Initialize PostgresTools with connection details

**URL:** llms-txt#initialize-postgrestools-with-connection-details

postgres_tools = PostgresTools(
    host="localhost",
    port=5532,
    db_name="ai",
    user="ai",
    password="ai"
)

---

## Download the .cursorrules file

**URL:** llms-txt#download-the-.cursorrules-file

curl -o .cursorrules https://raw.githubusercontent.com/agno-agi/agno/main/.cursorrules

---

## Async PostgreSQL

**URL:** llms-txt#async-postgresql

**Contents:**
- Usage

Source: https://docs.agno.com/concepts/db/async_postgres

Learn to use Async Postgre as a database for your Agents

Agno supports using [PostgreSQL](https://www.postgresql.org/) asynchronously, with the `AsyncPostgresDb` class.

```python postgres_for_agent.py theme={null}
from agno.agent import Agent
from agno.db.postgres import AsyncPostgresDb

---

## Confluence

**URL:** llms-txt#confluence

**Contents:**
- Prerequisites

Source: https://docs.agno.com/concepts/tools/toolkits/others/confluence

**ConfluenceTools** enable an Agent to retrieve, create, and update pages in Confluence. They also allow you to explore spaces and page details.

The following example requires the `atlassian-python-api` library and Confluence credentials. You can obtain an API token by going [here](https://id.atlassian.com/manage-profile/security).

```shell  theme={null}
export CONFLUENCE_URL="https://your-confluence-instance"
export CONFLUENCE_USERNAME="your-username"
export CONFLUENCE_PASSWORD="your-password"

**Examples:**

Example 1 (unknown):
```unknown

```

---

## Newspaper4k Tools

**URL:** llms-txt#newspaper4k-tools

**Contents:**
- Code
- Usage

Source: https://docs.agno.com/examples/concepts/tools/web_scrape/newspaper4k

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Set your API key">
    
  </Step>

<Step title="Install libraries">
    
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Set your API key">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Fast processing for simple content

**URL:** llms-txt#fast-processing-for-simple-content

fast_chunking = FixedSizeChunking(
    chunk_size=800,
    overlap=80
)

---

## Reasoning Team

**URL:** llms-txt#reasoning-team

**Contents:**
- Code
- Usage

Source: https://docs.agno.com/examples/use-cases/teams/reasoning_team

This example shows how to create a reasoning team that can handle complex queries involving web search and financial data using the `coordinate` mode with reasoning capabilities.

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Install required libraries">
    
  </Step>

<Step title="Set environment variables">
    
  </Step>

<Step title="Run the agent">
    
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Install required libraries">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Set environment variables">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run the agent">
```

---

## Searches for: return policies, time limits, return procedures

**URL:** llms-txt#searches-for:-return-policies,-time-limits,-return-procedures

---

## End condition function for the loop

**URL:** llms-txt#end-condition-function-for-the-loop

def research_quality_check(outputs: List[StepOutput]) -> bool:
    """
    Evaluate if research results are sufficient
    Returns True to break the loop, False to continue
    """
    if not outputs:
        return False

# Check if any output contains substantial content
    for output in outputs:
        if output.content and len(output.content) > 300:
            print(
                f"✅ Research quality check passed - found substantial content ({len(output.content)} chars)"
            )
            return True

print("❌ Research quality check failed - need more substantial research")
    return False

---

## Setup a team with two members

**URL:** llms-txt#setup-a-team-with-two-members

english_agent = Agent(
    name="English Agent",
    role="You only answer in English",
    model=OpenAIChat(id="gpt-5-mini"),
)
spanish_agent = Agent(
    name="Spanish Agent",
    role="You can only answer in Spanish",
    model=OpenAIChat(id="gpt-5-mini"),
)

multi_language_team = Team(
    name="Multi Language Team",
    model=OpenAIChat(id="gpt-5-mini"),
    members=[english_agent, spanish_agent],
    respond_directly=True,
    markdown=True,
    instructions=[
        "You are a language router that directs questions to the appropriate language agent.",
        "If the user asks in a language whose agent is not a team member, respond in English with:",
        "'I can only answer in the following languages: English and Spanish.",
        "Always check the language of the user's input before routing to an agent.",
    ],
)

---

## Example 2: Directions

**URL:** llms-txt#example-2:-directions

print("\n=== Directions Example ===")
agent.print_response(
    """Get driving directions from 'Phoenix Sky Harbor Airport' to 'Desert Botanical Garden', 
    avoiding highways if possible""",
    markdown=True,
    stream=True,
)

---

## OpenAIModerationGuardrail

**URL:** llms-txt#openaimoderationguardrail

**Contents:**
- Parameters
- Moderation categories

Source: https://docs.agno.com/reference/hooks/openai-moderation-guardrail

| Parameter              | Type        | Default                    | Description                                                                               |
| ---------------------- | ----------- | -------------------------- | ----------------------------------------------------------------------------------------- |
| `moderation_model`     | `str`       | `"omni-moderation-latest"` | The model to use for moderation.                                                          |
| `raise_for_categories` | `List[str]` | `None`                     | The categories to raise for.                                                              |
| `api_key`              | `str`       | `None`                     | The API key to use for moderation. Defaults to the OPENAI\_API\_KEY environment variable. |

## Moderation categories

You can check the current list of moderation categories in [OpenAI's docs](https://platform.openai.com/docs/guides/moderation#content-classifications).

---

## Create conversational meal planning workflow

**URL:** llms-txt#create-conversational-meal-planning-workflow

meal_workflow = Workflow(
    name="Conversational Meal Planner",
    description="Smart meal planning with conversation awareness and preference learning",
    db=SqliteDb(
        session_table="workflow_session",
        db_file="tmp/meal_workflow.db",
    ),
    steps=[suggestion_step, preference_analysis_step, recipe_step],
    add_workflow_history_to_steps=True,
)

def demonstrate_conversational_meal_planning():
    """Demonstrate natural conversational meal planning"""
    session_id = "meal_planning_demo"

print("🍽️  Conversational Meal Planning Demo")
    print("=" * 60)

# First interaction
    print("\n👤 User: What should I cook for dinner tonight?")
    meal_workflow.print_response(
        input="What should I cook for dinner tonight?",
        session_id=session_id,
        markdown=True,
    )

# Second interaction - user provides preferences
    print(
        "\n👤 User: I had Italian yesterday, and I'm trying to eat healthier these days"
    )
    meal_workflow.print_response(
        input="I had Italian yesterday, and I'm trying to eat healthier these days",
        session_id=session_id,
        markdown=True,
    )

# Third interaction - more specific request
    print(
        "\n👤 User: Actually, do you have something with fish? I love Asian flavors too"
    )
    meal_workflow.print_response(
        input="Actually, do you have something with fish? I love Asian flavors too",
        session_id=session_id,
        markdown=True,
    )

if __name__ == "__main__":
    demonstrate_conversational_meal_planning()
```

---

## LangSmith

**URL:** llms-txt#langsmith

**Contents:**
- Integrating Agno with LangSmith
- Prerequisites
- Sending Traces to LangSmith

Source: https://docs.agno.com/integrations/observability/langsmith

Integrate Agno with LangSmith to send traces and gain insights into your agent's performance.

## Integrating Agno with LangSmith

LangSmith offers a comprehensive platform for tracing and monitoring AI model calls. By integrating Agno with LangSmith, you can utilize OpenInference to send traces and gain insights into your agent's performance.

1. **Create a LangSmith Account**

* Sign up for an account at [LangSmith](https://smith.langchain.com).
   * Obtain your API key from the LangSmith dashboard.

2. **Set Environment Variables**

Configure your environment with the LangSmith API key and other necessary settings:

3. **Install Dependencies**

Ensure you have the necessary packages installed:

## Sending Traces to LangSmith

This example demonstrates how to instrument your Agno agent with OpenInference and send traces to LangSmith.

```python  theme={null}
import os

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.duckduckgo import DuckDuckGoTools
from openinference.instrumentation.agno import AgnoInstrumentor
from opentelemetry import trace as trace_api
from opentelemetry.exporter.otlp.proto.http.trace_exporter import OTLPSpanExporter
from opentelemetry.sdk.trace import TracerProvider
from opentelemetry.sdk.trace.export import SimpleSpanProcessor

**Examples:**

Example 1 (unknown):
```unknown
3. **Install Dependencies**

   Ensure you have the necessary packages installed:
```

Example 2 (unknown):
```unknown
## Sending Traces to LangSmith

This example demonstrates how to instrument your Agno agent with OpenInference and send traces to LangSmith.
```

---

## Yfinance

**URL:** llms-txt#yfinance

**Contents:**
- Prerequisites
- Example
- Toolkit Params
- Toolkit Functions
- Developer Resources

Source: https://docs.agno.com/concepts/tools/toolkits/others/yfinance

**YFinanceTools** enable an Agent to access stock data, financial information and more from Yahoo Finance.

The following example requires the `yfinance` library.

The following agent will provide information about the stock price and analyst recommendations for NVDA (Nvidia Corporation).

The YFinanceTools toolkit does not require any configuration parameters. All functions are enabled by default and do not have individual enable/disable flags. Simply instantiate the toolkit without any parameters.

| Function                      | Description                                                      |
| ----------------------------- | ---------------------------------------------------------------- |
| `get_current_stock_price`     | This function retrieves the current stock price of a company.    |
| `get_company_info`            | This function retrieves detailed information about a company.    |
| `get_historical_stock_prices` | This function retrieves historical stock prices for a company.   |
| `get_stock_fundamentals`      | This function retrieves fundamental data about a stock.          |
| `get_income_statements`       | This function retrieves income statements of a company.          |
| `get_key_financial_ratios`    | This function retrieves key financial ratios for a company.      |
| `get_analyst_recommendations` | This function retrieves analyst recommendations for a stock.     |
| `get_company_news`            | This function retrieves the latest news related to a company.    |
| `get_technical_indicators`    | This function retrieves technical indicators for stock analysis. |

You can use `include_tools` or `exclude_tools` to modify the list of tools the agent has access to. Learn more about [selecting tools](/concepts/tools/selecting-tools).

## Developer Resources

* View [Tools](https://github.com/agno-agi/agno/blob/main/libs/agno/agno/tools/yfinance.py)
* View [Cookbook](https://github.com/agno-agi/agno/tree/main/cookbook/tools/yfinance_tools.py)

**Examples:**

Example 1 (unknown):
```unknown
## Example

The following agent will provide information about the stock price and analyst recommendations for NVDA (Nvidia Corporation).
```

---

## Upload the file to Anthropic

**URL:** llms-txt#upload-the-file-to-anthropic

uploaded_file = client.beta.files.upload(
    file=Path(pdf_path),
)

if uploaded_file is not None:
    agent = Agent(
        model=Claude(
            id="claude-opus-4-20250514",
            default_headers={"anthropic-beta": "files-api-2025-04-14"},
        ),
        markdown=True,
    )

agent.print_response(
        "Summarize the contents of the attached file.",
        files=[File(external=uploaded_file)],
    )
```

---

## focus_areas=["AI", "Machine Learning"],

**URL:** llms-txt#focus_areas=["ai",-"machine-learning"],

---

## Create team with input_schema for automatic validation

**URL:** llms-txt#create-team-with-input_schema-for-automatic-validation

research_team = Team(
    name="Research Team with Input Validation",
    model=OpenAIChat(id="gpt-5-mini"),
    members=[hackernews_agent, web_researcher],
    input_schema=ResearchProject,
    instructions=[
        "Conduct thorough research based on the validated input",
        "Coordinate between team members to avoid duplicate work",
        "Ensure research depth matches the specified level",
        "Respect the maximum sources limit",
        "Focus on recent sources if requested",
    ],
)

print("=== Example 1: Valid Dictionary Input (will be auto-validated) ===")

---

## import asyncio

**URL:** llms-txt#import-asyncio

---

## Create a financial advisor team with comprehensive hooks

**URL:** llms-txt#create-a-financial-advisor-team-with-comprehensive-hooks

**Contents:**
- Usage

team = Team(
    name="Financial Advisor",
    model=OpenAIChat(id="gpt-5-mini"),
    pre_hooks=[transform_input],
    description="A professional financial advisor providing investment guidance and financial planning advice.",
    instructions=[
        "You are a knowledgeable financial advisor with expertise in:",
        "• Investment strategies and portfolio management",
        "• Retirement planning and savings strategies",
        "• Risk assessment and diversification",
        "• Tax-efficient investing",
        "",
        "Provide clear, actionable advice while being mindful of individual circumstances.",
        "Always remind users to consult with a licensed financial advisor for personalized advice.",
    ],
    debug_mode=True,
)

team.print_response(
    input="I'm 35 years old and want to start investing for retirement. moderate risk tolerance. retirement savings in IRAs/401(k)s= $100,000. total savings is $200,000. my net worth is $300,000",
    session_id="test_session",
    user_id="test_user",
    stream=True,
)
bash  theme={null}
    pip install -U agno openai
    bash Mac theme={null}
      python cookbook/teams/hooks/input_transformation_pre_hook.py
      bash Windows theme={null}
      python cookbook/teams/hooks/input_transformation_pre_hook.py
      ```
    </CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Install libraries">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Run example">
    <CodeGroup>
```

Example 3 (unknown):
```unknown

```

---

## Langwatch

**URL:** llms-txt#langwatch

**Contents:**
- Code

Source: https://docs.agno.com/examples/concepts/integrations/observability/langwatch_op

This example shows how to instrument your agno agent and send traces to LangWatch.

```python cookbook/integrations/observability/langwatch_op.py theme={null}
import langwatch
from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.duckduckgo import DuckDuckGoTools
from openinference.instrumentation.agno import AgnoInstrumentor

---

## Alternative prompt example

**URL:** llms-txt#alternative-prompt-example

crypto_prompt = """\
Analyze media trends for:
Keywords: cryptocurrency, bitcoin, ethereum
Sources: coindesk.com, cointelegraph.com
"""

---

## Get the final app

**URL:** llms-txt#get-the-final-app

app = agent_os.get_app()

---

## Create a client

**URL:** llms-txt#create-a-client

**Contents:**
- Usage

client = Surreal(url=SURREALDB_URL)
client.signin({"username": SURREALDB_USER, "password": SURREALDB_PASSWORD})
client.use(namespace=SURREALDB_NAMESPACE, database=SURREALDB_DATABASE)

surrealdb = SurrealDb(
    client=client,
    collection="recipes",  # Collection name for storing documents
    efc=150,  # HNSW construction time/accuracy trade-off
    m=12,  # HNSW max number of connections per element
    search_ef=40,  # HNSW search time/accuracy trade-off
    embedder=OpenAIEmbedder(),
)

def sync_demo():
    """Demonstrate synchronous usage of SurrealDb"""
    knowledge = Knowledge(
        vector_db=surrealdb,
    )

# Load data synchronously
    knowledge.add_content(
        url="https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf",
    )

agent = Agent(knowledge=knowledge)
    agent.print_response(
        "What are the 3 categories of Thai SELECT is given to restaurants overseas?",
        markdown=True,
    )

if __name__ == "__main__":
    print("Running synchronous demo...")
    sync_demo()
bash  theme={null}
    pip install -U surrealdb pypdf openai agno
    bash  theme={null}
    docker run --rm --pull always -p 8000:8000 surrealdb/surrealdb:latest start --user root --pass root     
    bash Mac theme={null}
      python cookbook/knowledge/vector_db/surrealdb/surreal_db.py
      bash Windows theme={null}
      python cookbook/knowledge/vector_db/surrealdb/surreal_db.py
      ```
    </CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Install libraries">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Run SurrealDB">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Ollama Embedder

**URL:** llms-txt#ollama-embedder

**Contents:**
- Code

Source: https://docs.agno.com/examples/concepts/knowledge/embedders/ollama-embedder

```python  theme={null}
from agno.knowledge.embedder.ollama import OllamaEmbedder
from agno.knowledge.knowledge import Knowledge
from agno.vectordb.pgvector import PgVector

embeddings = OllamaEmbedder().get_embedding(
    "The quick brown fox jumps over the lazy dog."
)

---

## team.print_response("Hi my name is John and I live in New York")

**URL:** llms-txt#team.print_response("hi-my-name-is-john-and-i-live-in-new-york")

---

## Hacker News

**URL:** llms-txt#hacker-news

**Contents:**
- Example
- Toolkit Params
- Toolkit Functions
- Developer Resources

Source: https://docs.agno.com/concepts/tools/toolkits/search/hackernews

**HackerNews** enables an Agent to search Hacker News website.

The following agent will write an engaging summary of the users with the top 2 stories on hackernews along with the stories.

| Parameter                 | Type   | Default | Description                    |
| ------------------------- | ------ | ------- | ------------------------------ |
| `enable_get_top_stories`  | `bool` | `True`  | Enables fetching top stories.  |
| `enable_get_user_details` | `bool` | `True`  | Enables fetching user details. |
| `all`                     | `bool` | `False` | Enables all functionality.     |

| Function                     | Description                                                                                                                                                                      |
| ---------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| `get_top_hackernews_stories` | Retrieves the top stories from Hacker News. Parameters include `num_stories` to specify the number of stories to return (default is 10). Returns the top stories in JSON format. |
| `get_user_details`           | Retrieves the details of a Hacker News user by their username. Parameters include `username` to specify the user. Returns the user details in JSON format.                       |

## Developer Resources

* View [Tools](https://github.com/agno-agi/agno/blob/main/libs/agno/agno/tools/hackernews.py)
* View [Cookbook](https://github.com/agno-agi/agno/tree/main/cookbook/tools/hackernews_tools.py)

---

## 4. Create output directory

**URL:** llms-txt#4.-create-output-directory

output_dir = Path(output_dir)
output_dir.mkdir(parents=True, exist_ok=True)

---

## "Hi, i want to make a 3 course meal. Can you recommend some recipes. "

**URL:** llms-txt#"hi,-i-want-to-make-a-3-course-meal.-can-you-recommend-some-recipes.-"

---

## Cal.com Tools

**URL:** llms-txt#cal.com-tools

**Contents:**
- Code
- Usage

Source: https://docs.agno.com/examples/concepts/tools/others/calcom

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Set your API keys">
    
  </Step>

<Step title="Install libraries">
    
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Set your API keys">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## user_id=john_doe_id,

**URL:** llms-txt#user_id=john_doe_id,

---

## SQLite for development

**URL:** llms-txt#sqlite-for-development

from agno.db.sqlite import SqliteDb
contents_db = SqliteDb(db_file="my_knowledge.db")

---

## Define the vector search index

**URL:** llms-txt#define-the-vector-search-index

search_index = SearchIndex(
    name="vector_search",
    source_type="gocbcore",
    idx_type="fulltext-index",
    source_name="recipe_bucket",
    plan_params={"index_partitions": 1, "num_replicas": 0},
    params={
        "doc_config": {
            "docid_prefix_delim": "",
            "docid_regexp": "",
            "mode": "scope.collection.type_field",
            "type_field": "type",
        },
        "mapping": {
            "default_analyzer": "standard",
            "default_datetime_parser": "dateTimeOptional",
            "index_dynamic": True,
            "store_dynamic": True,
            "default_mapping": {"dynamic": True, "enabled": False},
            "types": {
                "recipe_scope.recipes": {
                    "dynamic": False,
                    "enabled": True,
                    "properties": {
                        "content": {
                            "enabled": True,
                            "fields": [
                                {
                                    "docvalues": True,
                                    "include_in_all": False,
                                    "include_term_vectors": False,
                                    "index": True,
                                    "name": "content",
                                    "store": True,
                                    "type": "text",
                                }
                            ],
                        },
                        "embedding": {
                            "enabled": True,
                            "dynamic": False,
                            "fields": [
                                {
                                    "vector_index_optimized_for": "recall",
                                    "docvalues": True,
                                    "dims": 1536,
                                    "include_in_all": False,
                                    "include_term_vectors": False,
                                    "index": True,
                                    "name": "embedding",
                                    "similarity": "dot_product",
                                    "store": True,
                                    "type": "vector",
                                }
                            ],
                        },
                        "meta": {
                            "dynamic": True,
                            "enabled": True,
                            "properties": {
                                "name": {
                                    "enabled": True,
                                    "fields": [
                                        {
                                            "docvalues": True,
                                            "include_in_all": False,
                                            "include_term_vectors": False,
                                            "index": True,
                                            "name": "name",
                                            "store": True,
                                            "analyzer": "keyword",
                                            "type": "text",
                                        }
                                    ],
                                }
                            },
                        },
                    },
                }
            },
        },
    },
)
vector_db = CouchbaseSearch(
    bucket_name="recipe_bucket",
    scope_name="recipe_scope",
    collection_name="recipes",
    couchbase_connection_string=connection_string,
    cluster_options=cluster_options,
    search_index=search_index,
    embedder=OpenAIEmbedder(
        dimensions=1536,
    ),
    wait_until_index_ready=60,
    overwrite=True,
)

knowledge = Knowledge(
    name="Couchbase Knowledge Base",
    description="This is a knowledge base that uses a Couchbase DB",
    vector_db=vector_db,
)

knowledge.add_content(
    name="Recipes",
    url="https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf",
    metadata={"doc_type": "recipe_book"},
)
agent = Agent(
    knowledge=knowledge,
    search_knowledge=True,
    read_chat_history=True,
)

agent.print_response("List down the ingredients to make Massaman Gai", markdown=True)

vector_db.delete_by_name("Recipes")

---

## Tool Use Gpt 5

**URL:** llms-txt#tool-use-gpt-5

**Contents:**
- Code
- Usage

Source: https://docs.agno.com/examples/models/openai/responses/tool_use_gpt_5

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Set your API key">
    
  </Step>

<Step title="Install libraries">
    
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Set your API key">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Instrument Agno with Maxim for automatic tracing and logging

**URL:** llms-txt#instrument-agno-with-maxim-for-automatic-tracing-and-logging

instrument_agno(Maxim().logger())

---

## Set environment variables for Arize Phoenix

**URL:** llms-txt#set-environment-variables-for-arize-phoenix

os.environ["PHOENIX_CLIENT_HEADERS"] = f"api_key={os.getenv('ARIZE_PHOENIX_API_KEY')}"
os.environ["PHOENIX_COLLECTOR_ENDPOINT"] = "https://app.phoenix.arize.com"

---

## Pass a Pydantic model directly - no additional validation needed

**URL:** llms-txt#pass-a-pydantic-model-directly---no-additional-validation-needed

**Contents:**
- Usage

research_request = ResearchProject(
    project_name="Blockchain Development Tools",
    research_topics=["Ethereum", "Solana", "Web3 Libraries"],
    target_audience="Blockchain Developers",
    depth_level="advanced",
    max_sources=12,
    include_recent_only=False,
)

research_team.print_response(input=research_request)
bash  theme={null}
    pip install agno pydantic ddgs
    bash  theme={null}
    export OPENAI_API_KEY=****
    bash  theme={null}
    python cookbook/examples/teams/structured_input_output/06_input_schema_on_team.py
    ```
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Install required libraries">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Set environment variables">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run the agent">
```

---

## Configuration for the Evals page

**URL:** llms-txt#configuration-for-the-evals-page

evals:
  available_models:
    - <MODEL_STRING>
    ...
  display_name: <DISPLAY_NAME>
  dbs:
    - <DB_ID>
      domain_config:
        available_models:
          - <MODEL_STRING>
          ...
        display_name: <DISPLAY_NAME>
    ...

---

## Langfuse Via Openinference

**URL:** llms-txt#langfuse-via-openinference

**Contents:**
- Code

Source: https://docs.agno.com/examples/concepts/integrations/observability/langfuse_via_openinference

```python cookbook/integrations/observability/langfuse_via_openinference.py theme={null}
import base64
import os

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.duckduckgo import DuckDuckGoTools
from openinference.instrumentation.agno import AgnoInstrumentor
from opentelemetry import trace as trace_api
from opentelemetry.exporter.otlp.proto.http.trace_exporter import OTLPSpanExporter
from opentelemetry.sdk.trace import TracerProvider
from opentelemetry.sdk.trace.export import SimpleSpanProcessor

LANGFUSE_AUTH = base64.b64encode(
    f"{os.getenv('LANGFUSE_PUBLIC_KEY')}:{os.getenv('LANGFUSE_SECRET_KEY')}".encode()
).decode()
os.environ["OTEL_EXPORTER_OTLP_ENDPOINT"] = (
    "https://us.cloud.langfuse.com/api/public/otel"  # 🇺🇸 US data region
)

---

## Confluence Tools

**URL:** llms-txt#confluence-tools

**Contents:**
- Code
- Usage

Source: https://docs.agno.com/examples/concepts/tools/others/confluence

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Set your API credentials">
    
  </Step>

<Step title="Install libraries">
    
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Set your API credentials">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## "What factors should they consider and how should they analyze this decision?",

**URL:** llms-txt#"what-factors-should-they-consider-and-how-should-they-analyze-this-decision?",

---

## Add content with chunking applied

**URL:** llms-txt#add-content-with-chunking-applied

**Contents:**
- Choosing a Strategy

knowledge.add_content(
    path="documents/cookbook.pdf",
    reader=reader,
)
```

## Choosing a Strategy

The choice of chunking strategy depends on your content type and use case:

* **Text documents**: Semantic chunking maintains context and meaning
* **Structured documents**: Document or Markdown chunking preserves hierarchy
* **Tabular data**: CSV Row chunking treats each row as a separate entity
* **Mixed content**: Recursive chunking provides flexibility with multiple separators
* **Uniform processing**: Fixed Size chunking ensures consistent chunk dimensions

Each reader has a default chunking strategy that works well for its content type, but you can override it by specifying a `chunking_strategy` parameter when configuring the reader.

<Note>
  Consider your specific use case and performance requirements when choosing a chunking strategy, since different strategies vary in processing time and memory usage.
</Note>

---

## --- Workflow definition ---

**URL:** llms-txt#----workflow-definition----

**Contents:**
- Usage

startup_validation_workflow = Workflow(
    name="Startup Idea Validator",
    description="Comprehensive startup idea validation with market research and competitive analysis",
    db=SqliteDb(
        session_table="workflow_session",
        db_file="tmp/workflows.db",
    ),
    steps=startup_validation_execution,
    session_state={},  # Initialize empty workflow session state
)

if __name__ == "__main__":

async def main():
        from rich.prompt import Prompt

# Get idea from user
        idea = Prompt.ask(
            "[bold]What is your startup idea?[/bold]\n✨",
            default="A marketplace for Christmas Ornaments made from leather",
        )

print("🧪 Testing Startup Idea Validator with New Workflow Structure")
        print("=" * 70)

result = await startup_validation_workflow.arun(
            input="Please validate this startup idea with comprehensive market research and competitive analysis",
            startup_idea=idea,
        )

pprint_run_response(result, markdown=True)

asyncio.run(main())
bash  theme={null}
    openai agno googlesearch-python
    bash  theme={null}
    python startup_idea_validator.py
    ```
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Install libraries">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Run the workflow">
```

---

## Ollama with Reasoning Tools

**URL:** llms-txt#ollama-with-reasoning-tools

**Contents:**
- Code
- Usage

Source: https://docs.agno.com/examples/concepts/reasoning/tools/ollama-reasoning-tools

This example shows how to use `ReasoningTools` with an Ollama model.

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Install libraries">
    
  </Step>

<Step title="Run Example">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Install libraries">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Run Example">
    <CodeGroup>
```

Example 3 (unknown):
```unknown

```

---

## EVM Tools

**URL:** llms-txt#evm-tools

**Contents:**
- Code
- Usage

Source: https://docs.agno.com/examples/concepts/tools/others/evm

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Set your credentials">
    
  </Step>

<Step title="Install libraries">
    
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Set your credentials">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Infra Name: social-intel

**URL:** llms-txt#infra-name:-social-intel

env  theme={null}
export OPENAI_API_KEY=sk-your-openai-api-key-here
export X_API_KEY=your-x-api-key
export X_API_SECRET=your-x-api-secret
export X_ACCESS_TOKEN=your-access-token
export X_ACCESS_TOKEN_SECRET=your-access-token-secret
export X_BEARER_TOKEN=your-bearer-token
export EXA_API_KEY=your-exa-api-key
```

Our environment is now ready. Let's start building!

**Examples:**

Example 1 (unknown):
```unknown
5. Set your environment variables:
```

---

## Hacker News Tools

**URL:** llms-txt#hacker-news-tools

**Contents:**
- Code
- Usage

Source: https://docs.agno.com/examples/concepts/tools/search/hackernews

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Set your API key">
    
  </Step>

<Step title="Install libraries">
    
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Set your API key">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Example toolkit for handling emails

**URL:** llms-txt#example-toolkit-for-handling-emails

class EmailTools(Toolkit):
    def __init__(self, *args, **kwargs):
        super().__init__(
            name="EmailTools", tools=[self.send_email, self.get_emails], *args, **kwargs
        )

def send_email(self, subject: str, body: str, to_address: str) -> str:
        """Send an email to the given address with the given subject and body.

Args:
            subject (str): The subject of the email.
            body (str): The body of the email.
            to_address (str): The address to send the email to.
        """
        return f"Sent email to {to_address} with subject {subject} and body {body}"

def get_emails(self, date_from: str, date_to: str) -> str:
        """Get all emails between the given dates.

Args:
            date_from (str): The start date.
            date_to (str): The end date.
        """
        return [
            {
                "subject": "Hello",
                "body": "Hello, world!",
                "to_address": "test@test.com",
                "date": date_from,
            },
            {
                "subject": "Random other email",
                "body": "This is a random other email",
                "to_address": "john@doe.com",
                "date": date_to,
            },
        ]

agent = Agent(
    model=OpenAIChat(id="gpt-5-mini"),
    tools=[EmailTools(), UserControlFlowTools()],
    markdown=True,
)

run_response = agent.run("Send an email with the body 'How is it going in Tokyo?'")

---

## Define the quality standards for analysis

**URL:** llms-txt#define-the-quality-standards-for-analysis

**Contents:**
  - 3f. Combine Into Complete Instructions

analysis_principles = dedent("""
    ANALYSIS PRINCIPLES:
    - Evidence-based conclusions with supporting metrics
    - Actionable insights that drive business decisions
    - Cross-platform correlation analysis
    - Influence-weighted sentiment scoring
    - Proactive risk and opportunity identification
""")

print("Analysis principles defined")
python  theme={null}

**Examples:**

Example 1 (unknown):
```unknown
### 3f. Combine Into Complete Instructions
```

---

## Create Git Repo

**URL:** llms-txt#create-git-repo

Source: https://docs.agno.com/templates/infra-management/git-repo

Create a git repository to share your application with your team.

<Steps>
  <Step title="Create a git repository">
    Create a new [git repository](https://github.com/new).
  </Step>

<Step title="Push your code">
    Push your code to the git repository.

<Step title="Ask your team to join">
    Ask your team to follow the [setup steps for new users](/templates/infra-management/new-users) to use this workspace.
  </Step>
</Steps>

---

## Tavily

**URL:** llms-txt#tavily

**Contents:**
- Prerequisites
- Example
- Toolkit Params
- Toolkit Functions
- Developer Resources

Source: https://docs.agno.com/concepts/tools/toolkits/search/tavily

**TavilyTools** enable an Agent to search the web using the Tavily API.

The following examples requires the `tavily-python` library and an API key from [Tavily](https://tavily.com/).

The following agent will run a search on Tavily for "language models" and print the response.

| Parameter               | Type                           | Default      | Description                                                                               |
| ----------------------- | ------------------------------ | ------------ | ----------------------------------------------------------------------------------------- |
| `api_key`               | `Optional[str]`                | `None`       | Tavily API key. If not provided, will use TAVILY\_API\_KEY environment variable.          |
| `enable_search`         | `bool`                         | `True`       | Enable search functionality.                                                              |
| `enable_search_context` | `bool`                         | `False`      | Enable search context functionality using Tavily's context API.                           |
| `all`                   | `bool`                         | `False`      | Enable all available functions in the toolkit.                                            |
| `max_tokens`            | `int`                          | `6000`       | Maximum number of tokens to use in search results.                                        |
| `include_answer`        | `bool`                         | `True`       | Whether to include an AI-generated answer summary in the response.                        |
| `search_depth`          | `Literal['basic', 'advanced']` | `'advanced'` | Depth of search - 'basic' for faster results or 'advanced' for more comprehensive search. |
| `format`                | `Literal['json', 'markdown']`  | `'markdown'` | Output format - 'json' for raw data or 'markdown' for formatted text.                     |

| Function                  | Description                                                                                                                                                                                                                                                                    |
| ------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |
| `web_search_using_tavily` | Search the web for a given query using Tavily API. Parameters include `query` (str) for the search query and `max_results` (int, default=5) for maximum number of results. Returns JSON string of results with titles, URLs, content and relevance scores in specified format. |
| `web_search_with_tavily`  | Alternative search function that uses Tavily's search context API. Parameters include `query` (str) for the search query. Returns contextualized search results. Only available when `enable_search_context` is True.                                                          |

## Developer Resources

* View [Tools](https://github.com/agno-agi/agno/blob/main/libs/agno/agno/tools/tavily.py)
* View [Cookbook](https://github.com/agno-agi/agno/tree/main/cookbook/tools/tavily_tools.py)

**Examples:**

Example 1 (unknown):
```unknown

```

Example 2 (unknown):
```unknown
## Example

The following agent will run a search on Tavily for "language models" and print the response.
```

---

## Crawl4AI

**URL:** llms-txt#crawl4ai

**Contents:**
- Prerequisites
- Example
- Toolkit Params
- Toolkit Functions
- Developer Resources

Source: https://docs.agno.com/concepts/tools/toolkits/web_scrape/crawl4ai

**Crawl4aiTools** enable an Agent to perform web crawling and scraping tasks using the Crawl4ai library.

The following example requires the `crawl4ai` library.

The following agent will scrape the content from the [https://github.com/agno-agi/agno](https://github.com/agno-agi/agno) webpage:

| Parameter           | Type    | Default              | Description                                                                               |
| ------------------- | ------- | -------------------- | ----------------------------------------------------------------------------------------- |
| `max_length`        | `int`   | `1000`               | Specifies the maximum length of the text from the webpage to be returned.                 |
| `timeout`           | `int`   | `60`                 | Timeout in seconds for web crawling operations.                                           |
| `use_pruning`       | `bool`  | `False`              | Enable content pruning to remove less relevant content.                                   |
| `pruning_threshold` | `float` | `0.48`               | Threshold for content pruning relevance scoring.                                          |
| `bm25_threshold`    | `float` | `1.0`                | BM25 scoring threshold for content relevance.                                             |
| `headless`          | `bool`  | `True`               | Run browser in headless mode.                                                             |
| `wait_until`        | `str`   | `"domcontentloaded"` | Browser wait condition before crawling (e.g., "domcontentloaded", "load", "networkidle"). |
| `enable_crawl`      | `bool`  | `True`               | Enable the web crawling functionality.                                                    |
| `all`               | `bool`  | `False`              | Enable all available functions. When True, all enable flags are ignored.                  |

| Function      | Description                                                                                                                                                                                                      |
| ------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| `web_crawler` | Crawls a website using crawl4ai's WebCrawler. Parameters include 'url' for the URL to crawl and an optional 'max\_length' to limit the length of extracted content. The default value for 'max\_length' is 1000. |

## Developer Resources

* View [Tools](https://github.com/agno-agi/agno/blob/main/libs/agno/agno/tools/crawl4ai.py)
* View [Cookbook](https://github.com/agno-agi/agno/tree/main/cookbook/tools/crawl4ai_tools.py)

**Examples:**

Example 1 (unknown):
```unknown
## Example

The following agent will scrape the content from the [https://github.com/agno-agi/agno](https://github.com/agno-agi/agno) webpage:
```

---

## Exceptions & Retries

**URL:** llms-txt#exceptions-&-retries

Source: https://docs.agno.com/concepts/tools/exceptions

If after a tool call we need to "retry" the model with a different set of instructions or stop the agent, we can raise one of the following exceptions:

* `RetryAgentRun`: Use this exception when you want to retry the agent run with a different set of instructions.
* `StopAgentRun`: Use this exception when you want to stop the agent run.
* `AgentRunException`: A generic exception that can be used to retry the tool call.

This example shows how to use the `RetryAgentRun` exception to retry the agent with additional instructions.

<Tip>
  Make sure to set `AGNO_DEBUG=True` to see the debug logs.
</Tip>

---

## ClickUp

**URL:** llms-txt#clickup

**Contents:**
- Example
- Toolkit Params
- Toolkit Functions
- Developer Resources

Source: https://docs.agno.com/concepts/tools/toolkits/others/clickup

ClickUpTools enables agents to interact with ClickUp workspaces for project management and task organization.

The following agent can manage ClickUp tasks and projects:

| Parameter         | Type            | Default | Description                                                 |
| ----------------- | --------------- | ------- | ----------------------------------------------------------- |
| `api_key`         | `Optional[str]` | `None`  | ClickUp API key. Uses CLICKUP\_API\_KEY if not set.         |
| `master_space_id` | `Optional[str]` | `None`  | Default space ID to use. Uses MASTER\_SPACE\_ID if not set. |

| Function      | Description                                                  |
| ------------- | ------------------------------------------------------------ |
| `list_tasks`  | List tasks with optional filtering by status, assignee, etc. |
| `create_task` | Create a new task in a specified list.                       |
| `get_task`    | Get detailed information about a specific task.              |
| `update_task` | Update an existing task's properties.                        |
| `delete_task` | Delete a task from ClickUp.                                  |
| `list_spaces` | List all spaces accessible to the user.                      |
| `list_lists`  | List all lists within a space or folder.                     |

You can use `include_tools` or `exclude_tools` to modify the list of tools the agent has access to. Learn more about [selecting tools](/concepts/tools/selecting-tools).

## Developer Resources

* View [Tools Source](https://github.com/agno-agi/agno/blob/main/libs/agno/agno/tools/clickup.py)
* [ClickUp API Documentation](https://clickup.com/api/)

---

## Generate Images with Intermediate Steps

**URL:** llms-txt#generate-images-with-intermediate-steps

**Contents:**
- Code
- Usage

Source: https://docs.agno.com/examples/concepts/multimodal/generate-image

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Set your API key">
    
  </Step>

<Step title="Install libraries">
    
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Set your API key">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Oxylabs Tools

**URL:** llms-txt#oxylabs-tools

**Contents:**
- Code
- Usage

Source: https://docs.agno.com/examples/concepts/tools/web_scrape/oxylabs

Use Oxylabs with Agno to scrape and crawl the web.

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Set your API keys">
    
  </Step>

<Step title="Install libraries">
    
  </Step>

<Step title="Run the example">
    
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Set your API keys">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run the example">
```

---

## Audio Configuration

**URL:** llms-txt#audio-configuration

SAMPLE_RATE = 24000  # Hz (24kHz)
CHANNELS = 1  # Mono (Change to 2 if Stereo)
SAMPLE_WIDTH = 2  # Bytes (16 bits)

---

## Groq Claude + DeepSeek R1

**URL:** llms-txt#groq-claude-+-deepseek-r1

**Contents:**
- Code
- Usage

Source: https://docs.agno.com/examples/concepts/reasoning/models/groq/groq-plus-claude

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Set your API key">
    
  </Step>

<Step title="Install libraries">
    
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Set your API key">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Vector Search

**URL:** llms-txt#vector-search

**Contents:**
- Code

Source: https://docs.agno.com/examples/concepts/knowledge/search_type/vector-search

```python vector_search.py theme={null}
from agno.knowledge.knowledge import Knowledge
from agno.vectordb.pgvector import PgVector, SearchType

db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"

---

## Brandfetch

**URL:** llms-txt#brandfetch

**Contents:**
- Example
- Toolkit Params
- Toolkit Functions
- Developer Resources

Source: https://docs.agno.com/concepts/tools/toolkits/others/brandfetch

BrandfetchTools provides access to brand data and logo information through the Brandfetch API.

The following agent can search for brand information and retrieve brand data:

| Parameter                     | Type              | Default                          | Description                                                   |
| ----------------------------- | ----------------- | -------------------------------- | ------------------------------------------------------------- |
| `api_key`                     | `Optional[str]`   | `None`                           | Brandfetch API key. Uses BRANDFETCH\_API\_KEY if not set.     |
| `client_id`                   | `Optional[str]`   | `None`                           | Brandfetch Client ID for search. Uses BRANDFETCH\_CLIENT\_ID. |
| `base_url`                    | `str`             | `"https://api.brandfetch.io/v2"` | Brandfetch API base URL.                                      |
| `timeout`                     | `Optional[float]` | `20.0`                           | Request timeout in seconds.                                   |
| `enable_search_by_identifier` | `bool`            | `True`                           | Enable searching brands by domain/identifier.                 |
| `enable_search_by_brand`      | `bool`            | `False`                          | Enable searching brands by name.                              |
| `async_tools`                 | `bool`            | `False`                          | Enable async versions of tools.                               |

| Function                | Description                                               |
| ----------------------- | --------------------------------------------------------- |
| `search_by_identifier`  | Search for brand data using domain or company identifier. |
| `search_by_brand`       | Search for brands by name (requires client\_id).          |
| `asearch_by_identifier` | Async version of search by identifier.                    |
| `asearch_by_brand`      | Async version of search by brand name.                    |

## Developer Resources

* View [Tools Source](https://github.com/agno-agi/agno/blob/main/libs/agno/agno/tools/brandfetch.py)
* [Brandfetch API Documentation](https://docs.brandfetch.com/)

---

## Add JWT middleware configured for cookie-based authentication

**URL:** llms-txt#add-jwt-middleware-configured-for-cookie-based-authentication

app.add_middleware(
    JWTMiddleware,
    secret_key=JWT_SECRET, # or use JWT_SECRET_KEY environment variable
    algorithm="HS256",
    excluded_route_paths=[
        "/set-auth-cookie",
        "/clear-auth-cookie",
    ],
    token_source=TokenSource.COOKIE,  # Extract JWT from cookies
    cookie_name="auth_token",  # Name of the cookie containing the JWT
    user_id_claim="sub",  # Extract user_id from 'sub' claim
    session_id_claim="session_id",  # Extract session_id from 'session_id' claim
    dependencies_claims=[
        "name",
        "email",
        "roles",
        "org",
    ],  # Additional claims to extract
    validate=True,  # We want to ensure the token is valid
)

agent_os = AgentOS(
    description="JWT Cookie-Based AgentOS",
    agents=[profile_agent],
    base_app=app,
)

---

## Google BigQuery

**URL:** llms-txt#google-bigquery

**Contents:**
- Example
- Toolkit Params
- Toolkit Functions
- Developer Resources

Source: https://docs.agno.com/concepts/tools/toolkits/database/google_bigquery

GoogleBigQueryTools enables agents to interact with Google BigQuery for large-scale data analysis and SQL queries.

The following agent can query and analyze BigQuery datasets:

| Parameter               | Type            | Default | Description                                           |
| ----------------------- | --------------- | ------- | ----------------------------------------------------- |
| `dataset`               | `str`           | `None`  | BigQuery dataset name (required).                     |
| `project`               | `Optional[str]` | `None`  | Google Cloud project ID. Uses GOOGLE\_CLOUD\_PROJECT. |
| `location`              | `Optional[str]` | `None`  | BigQuery location. Uses GOOGLE\_CLOUD\_LOCATION.      |
| `credentials`           | `Optional[Any]` | `None`  | Google Cloud credentials object.                      |
| `enable_list_tables`    | `bool`          | `True`  | Enable table listing functionality.                   |
| `enable_describe_table` | `bool`          | `True`  | Enable table description functionality.               |
| `enable_run_sql_query`  | `bool`          | `True`  | Enable SQL query execution functionality.             |

| Function         | Description                                             |
| ---------------- | ------------------------------------------------------- |
| `list_tables`    | List all tables in the specified BigQuery dataset.      |
| `describe_table` | Get detailed schema information about a specific table. |
| `run_sql_query`  | Execute SQL queries on BigQuery datasets.               |

## Developer Resources

* View [Tools Source](https://github.com/agno-agi/agno/blob/main/libs/agno/agno/tools/google_bigquery.py)
* [BigQuery Documentation](https://cloud.google.com/bigquery/docs)
* [BigQuery SQL Reference](https://cloud.google.com/bigquery/docs/reference/standard-sql/)

---

## Add topics from Wikipedia synchronously

**URL:** llms-txt#add-topics-from-wikipedia-synchronously

knowledge.add_content(
    metadata={"source": "wikipedia", "type": "encyclopedia"},
    topics=["Manchester United", "Artificial Intelligence"],
    reader=WikipediaReader(),
)

---

## Image Generation

**URL:** llms-txt#image-generation

**Contents:**
- Code

Source: https://docs.agno.com/examples/getting-started/14-image-generation

This example shows how to create an AI agent that generates images using DALL-E.
You can use this agent to create various types of images, from realistic photos to artistic
illustrations and creative concepts.

Example prompts to try:

* "Create a surreal painting of a floating city in the clouds at sunset"
* "Generate a photorealistic image of a cozy coffee shop interior"
* "Design a cute cartoon mascot for a tech startup"
* "Create an artistic portrait of a cyberpunk samurai"

```python image_generation.py theme={null}
from textwrap import dedent

from agno.agent import Agent
from agno.db.sqlite import SqliteDb
from agno.models.openai import OpenAIChat
from agno.run.agent import RunOutput
from agno.tools.dalle import DalleTools

---

## Economic analysis example

**URL:** llms-txt#economic-analysis-example

---

## MySQLDb

**URL:** llms-txt#mysqldb

Source: https://docs.agno.com/reference/storage/mysql

`MySQLDb` is a class that implements the Db interface using MySQL as the backend storage system. It provides robust, relational storage for agent sessions with support for JSONB data types, schema versioning, and efficient querying.

<Snippet file="db-mysql-params.mdx" />

<Snippet file="db-new-bulk-methods.mdx" />

---

## ag infra config

**URL:** llms-txt#ag-infra-config

**Contents:**
- Params

Source: https://docs.agno.com/reference/agno-infra/cli/ws/config

Prints active infra config

<ResponseField name="print_debug_log" type="bool">
  Print debug logs. `--debug` `-d`
</ResponseField>

---

## Get user profile

**URL:** llms-txt#get-user-profile

**Contents:**
- Toolkit Params
- Toolkit Functions
- Developer Resources

agent.print_response("Get my X profile", markdown=True)
```

Check out the [Tweet Analysis Agent](/examples/use-cases/agents/tweet-analysis-agent)
  for a more advanced example.{" "}
</Note>

| Parameter              | Type   | Default | Description                                                    |
| ---------------------- | ------ | ------- | -------------------------------------------------------------- |
| `bearer_token`         | `str`  | `None`  | Bearer token for authentication                                |
| `consumer_key`         | `str`  | `None`  | Consumer key for authentication                                |
| `consumer_secret`      | `str`  | `None`  | Consumer secret for authentication                             |
| `access_token`         | `str`  | `None`  | Access token for authentication                                |
| `access_token_secret`  | `str`  | `None`  | Access token secret for authentication                         |
| `include_post_metrics` | `bool` | `False` | Include post metrics (likes, retweets, etc.) in search results |
| `wait_on_rate_limit`   | `bool` | `False` | Retry when rate limits are reached                             |

| Function            | Description                                 |
| ------------------- | ------------------------------------------- |
| `create_post`       | Creates and posts a new post                |
| `reply_to_post`     | Replies to an existing post                 |
| `send_dm`           | Sends a direct message to a X user          |
| `get_user_info`     | Retrieves information about a X user        |
| `get_home_timeline` | Gets the authenticated user's home timeline |
| `search_posts`      | Searches for tweets                         |

You can use `include_tools` or `exclude_tools` to modify the list of tools the agent has access to. Learn more about [selecting tools](/concepts/tools/selecting-tools).

## Developer Resources

* View [Tools](https://github.com/agno-agi/agno/blob/main/libs/agno/agno/tools/x.py)
* View [Cookbook](https://github.com/agno-agi/agno/tree/main/cookbook/tools/x_tools.py)
* View [Tweet Analysis Agent Cookbook](https://github.com/agno-agi/agno/tree/main/cookbook/examples/agents/social_media_agent.py)

---

## AWS Bedrock

**URL:** llms-txt#aws-bedrock

**Contents:**
- Parameters

Source: https://docs.agno.com/reference/models/bedrock

Learn how to use AWS Bedrock models in Agno.

The AWS Bedrock model provides access to models hosted on AWS Bedrock.

| Parameter               | Type                       | Default        | Description                                                          |
| ----------------------- | -------------------------- | -------------- | -------------------------------------------------------------------- |
| `id`                    | `str`                      | Required       | The id of the AWS Bedrock model to use                               |
| `name`                  | `str`                      | `"AwsBedrock"` | The name of the model                                                |
| `provider`              | `str`                      | `"AWS"`        | The provider of the model                                            |
| `temperature`           | `Optional[float]`          | `None`         | Controls randomness in the model's output                            |
| `max_tokens`            | `Optional[int]`            | `None`         | Maximum number of tokens to generate                                 |
| `top_p`                 | `Optional[float]`          | `None`         | Controls diversity via nucleus sampling                              |
| `top_k`                 | `Optional[int]`            | `None`         | Controls diversity via top-k sampling                                |
| `stop_sequences`        | `Optional[List[str]]`      | `None`         | A list of strings that the model should stop generating text at      |
| `response_format`       | `Optional[str]`            | `None`         | The format of the response                                           |
| `request_params`        | `Optional[Dict[str, Any]]` | `None`         | Additional parameters to include in the request                      |
| `aws_region`            | `Optional[str]`            | `None`         | The AWS region to use (defaults to AWS\_REGION env var)              |
| `aws_access_key_id`     | `Optional[str]`            | `None`         | AWS access key ID (defaults to AWS\_ACCESS\_KEY\_ID env var)         |
| `aws_secret_access_key` | `Optional[str]`            | `None`         | AWS secret access key (defaults to AWS\_SECRET\_ACCESS\_KEY env var) |
| `aws_session_token`     | `Optional[str]`            | `None`         | AWS session token (defaults to AWS\_SESSION\_TOKEN env var)          |
| `aws_profile`           | `Optional[str]`            | `None`         | AWS profile to use (defaults to AWS\_PROFILE env var)                |
| `client_params`         | `Optional[Dict[str, Any]]` | `None`         | Additional parameters for client configuration                       |

---

## Remove all content

**URL:** llms-txt#remove-all-content

**Contents:**
- Usage

knowledge.remove_all_content()
bash  theme={null}
    pip install -U agno sqlalchemy psycopg pgvector
    bash Mac theme={null}
      python cookbook/knowledge/basic_operations/09_remove_content.py
      bash Windows theme={null}
      python cookbook/knowledge/basic_operations/09_remove_content.py
      ```
    </CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Step title="Install libraries">
```

Example 2 (unknown):
```unknown
</Step>

  <Snippet file="run-pgvector-step.mdx" />

  <Step title="Run the example">
    <CodeGroup>
```

Example 3 (unknown):
```unknown

```

---

## Configuration for the Session page

**URL:** llms-txt#configuration-for-the-session-page

session:
  display_name: <DISPLAY_NAME>
  dbs:
    - <DB_ID>
      domain_config:
        display_name: <DISPLAY_NAME>
    ...

---

## Create a team for collaborative image-to-text processing

**URL:** llms-txt#create-a-team-for-collaborative-image-to-text-processing

**Contents:**
- Usage

image_team = Team(
    name="Image Story Team",
    model=OpenAIChat(id="gpt-5-mini"),
    members=[image_analyzer, creative_writer],
    instructions=[
        "Work together to create compelling fiction stories from images.",
        "Image Analyst: First analyze the image for visual details and context.",
        "Creative Writer: Transform the analysis into engaging fiction narratives.",
        "Ensure the story captures the essence and mood of the image.",
    ],
    markdown=True,
)

image_path = Path(__file__).parent.joinpath("sample.jpg")
image_team.print_response(
    "Write a 3 sentence fiction story about the image",
    images=[Image(filepath=image_path)],
)
bash  theme={null}
    pip install agno
    bash  theme={null}
    export OPENAI_API_KEY=****
    bash  theme={null}
    # Add a sample.jpg image file in the same directory as the script
    bash  theme={null}
    python cookbook/examples/teams/multimodal/image_to_text.py
    ```
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Install required libraries">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Set environment variables">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Add sample image">
```

Example 4 (unknown):
```unknown
</Step>

  <Step title="Run the agent">
```

---

## os.environ["OTEL_EXPORTER_OTLP_ENDPOINT"]="https://cloud.langfuse.com/api/public/otel" # 🇪🇺 EU data region

**URL:** llms-txt#os.environ["otel_exporter_otlp_endpoint"]="https://cloud.langfuse.com/api/public/otel"-#-🇪🇺-eu-data-region

---

## Audio Input (Local file)

**URL:** llms-txt#audio-input-(local-file)

**Contents:**
- Code

Source: https://docs.agno.com/examples/models/gemini/audio_input_local_file_upload

```python cookbook/models/google/gemini/audio_input_local_file_upload.py theme={null}
from pathlib import Path
from agno.agent import Agent
from agno.media import Audio
from agno.models.google import Gemini

agent = Agent(
    model=Gemini(id="gemini-2.0-flash-exp"),
    markdown=True,
)

---

## This metadata can include user IDs, document types, dates, or any other attributes

**URL:** llms-txt#this-metadata-can-include-user-ids,-document-types,-dates,-or-any-other-attributes

vector_db = Weaviate(
    collection="recipes",
    vector_index=VectorIndex.HNSW,
    distance=Distance.COSINE,
    local=False,  # Set to False if using Weaviate Cloud and True if using local instance
)

knowledge = Knowledge(
    name="Weaviate Knowledge Base",
    description="A knowledge base for Weaviate",
    vector_db=vector_db,
)

knowledge.add_contents(
    [
        {
            "path": downloaded_cv_paths[0],
            "metadata": {
                "user_id": "jordan_mitchell",
                "document_type": "cv",
                "year": 2025,
            },
        },
        {
            "path": downloaded_cv_paths[1],
            "metadata": {
                "user_id": "taylor_brooks",
                "document_type": "cv",
                "year": 2025,
            },
        },
        {
            "path": downloaded_cv_paths[2],
            "metadata": {
                "user_id": "morgan_lee",
                "document_type": "cv",
                "year": 2025,
            },
        },
        {
            "path": downloaded_cv_paths[3],
            "metadata": {
                "user_id": "casey_jordan",
                "document_type": "cv",
                "year": 2025,
            },
        },
        {
            "path": downloaded_cv_paths[4],
            "metadata": {
                "user_id": "alex_rivera",
                "document_type": "cv",
                "year": 2025,
            },
        },
    ]
)

---

## Print response EXAMPLE

**URL:** llms-txt#print-response-example

---

## Video Input (File Upload)

**URL:** llms-txt#video-input-(file-upload)

**Contents:**
- Code

Source: https://docs.agno.com/examples/models/gemini/video_input_file_upload

```python cookbook/models/google/gemini/video_input_file_upload.py theme={null}
import time
from pathlib import Path

from agno.agent import Agent
from agno.media import Video
from agno.models.google import Gemini
from agno.utils.log import logger

model = Gemini(id="gemini-2.0-flash-exp")
agent = Agent(
    model=model,
    markdown=True,
)

---

## Valyu Tools

**URL:** llms-txt#valyu-tools

**Contents:**
- Code
- Usage

Source: https://docs.agno.com/examples/concepts/tools/search/valyu

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Set your API keys">
    
  </Step>

<Step title="Install libraries">
    
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Set your API keys">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## ❌ Bad: All users share the same memories

**URL:** llms-txt#❌-bad:-all-users-share-the-same-memories

agent.print_response("I love pizza")
agent.print_response("I'm allergic to dairy")

---

## Define your workflow

**URL:** llms-txt#define-your-workflow

**Contents:**
  - Event Types
  - Storing Events

async def main():
    try:
        response: AsyncIterator[WorkflowRunOutputEvent] = basic_workflow.arun(
            message="Recent breakthroughs in quantum computing",
            stream=True,
            stream_events=True,
        )
        async for event in response:
            if event.event == WorkflowRunEvent.condition_execution_started.value:
                print(event)
                print()
            elif event.event == WorkflowRunEvent.condition_execution_completed.value:
                print(event)
                print()
            elif event.event == WorkflowRunEvent.workflow_started.value:
                print(event)
                print()
            elif event.event == WorkflowRunEvent.step_started.value:
                print(event)
                print()
            elif event.event == WorkflowRunEvent.step_completed.value:
                print(event)
                print()
            elif event.event == WorkflowRunEvent.workflow_completed.value:
                print(event)
                print()
    except Exception as e:
        print(f"❌ Error: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    asyncio.run(main())
python  theme={null}
from agno.run.workflow import WorkflowRunEvent

**Examples:**

Example 1 (unknown):
```unknown
See the [Async Streaming](/examples/concepts/workflows/01-basic-workflows/async_events_streaming) example for more details.

### Event Types

The following events are yielded by the `Workflow.run()` and `Workflow.arun()` functions depending on the workflow's configuration:

#### Core Events

| Event Type          | Description                                         |
| ------------------- | --------------------------------------------------- |
| `WorkflowStarted`   | Indicates the start of a workflow run               |
| `WorkflowCompleted` | Signals successful completion of the workflow run   |
| `WorkflowError`     | Indicates an error occurred during the workflow run |

#### Step Events

| Event Type      | Description                               |
| --------------- | ----------------------------------------- |
| `StepStarted`   | Indicates the start of a step             |
| `StepCompleted` | Signals successful completion of a step   |
| `StepError`     | Indicates an error occurred during a step |

#### Step Output Events (For custom functions)

| Event Type   | Description                    |
| ------------ | ------------------------------ |
| `StepOutput` | Indicates the output of a step |

#### Parallel Execution Events

| Event Type                   | Description                                      |
| ---------------------------- | ------------------------------------------------ |
| `ParallelExecutionStarted`   | Indicates the start of a parallel step           |
| `ParallelExecutionCompleted` | Signals successful completion of a parallel step |

#### Condition Execution Events

| Event Type                    | Description                                  |
| ----------------------------- | -------------------------------------------- |
| `ConditionExecutionStarted`   | Indicates the start of a condition           |
| `ConditionExecutionCompleted` | Signals successful completion of a condition |

#### Loop Execution Events

| Event Type                    | Description                                       |
| ----------------------------- | ------------------------------------------------- |
| `LoopExecutionStarted`        | Indicates the start of a loop                     |
| `LoopIterationStartedEvent`   | Indicates the start of a loop iteration           |
| `LoopIterationCompletedEvent` | Signals successful completion of a loop iteration |
| `LoopExecutionCompleted`      | Signals successful completion of a loop           |

#### Router Execution Events

| Event Type                 | Description                               |
| -------------------------- | ----------------------------------------- |
| `RouterExecutionStarted`   | Indicates the start of a router           |
| `RouterExecutionCompleted` | Signals successful completion of a router |

#### Steps Execution Events

| Event Type                | Description                                        |
| ------------------------- | -------------------------------------------------- |
| `StepsExecutionStarted`   | Indicates the start of `Steps` being executed      |
| `StepsExecutionCompleted` | Signals successful completion of `Steps` execution |

See detailed documentation in the [WorkflowRunOutputEvent](/reference/workflows/workflow_run_output) documentation.

### Storing Events

Workflows can automatically store all execution events for analysis, debugging, and audit purposes. Filter specific event types to reduce noise and storage overhead while maintaining essential execution records.

Access stored events via `workflow.run_response.events` and in the `runs` column of your workflow's session database (SQLite, PostgreSQL, etc.).

* `store_events=True`: Automatically stores all workflow events in the database
* `events_to_skip=[]`: Filter out specific event types to reduce storage and noise

Access all stored events via `workflow.run_response.events`

**Available Events to Skip:**
```

---

## Pubmed

**URL:** llms-txt#pubmed

**Contents:**
- Example
- Toolkit Params
- Toolkit Functions
- Developer Resources

Source: https://docs.agno.com/concepts/tools/toolkits/search/pubmed

**PubmedTools** enable an Agent to search for Pubmed for articles.

The following agent will search Pubmed for articles related to "ulcerative colitis".

| Parameter              | Type   | Default                    | Description                                                            |
| ---------------------- | ------ | -------------------------- | ---------------------------------------------------------------------- |
| `email`                | `str`  | `"your_email@example.com"` | Specifies the email address to use.                                    |
| `max_results`          | `int`  | `None`                     | Optional parameter to specify the maximum number of results to return. |
| `enable_search_pubmed` | `bool` | `True`                     | Enable the search\_pubmed functionality.                               |
| `all`                  | `bool` | `False`                    | Enable all functionality.                                              |

| Function        | Description                                                                                                                                                                                                                                                                                 |
| --------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| `search_pubmed` | Searches PubMed for articles based on a specified query. Parameters include `query` for the search term and `max_results` for the maximum number of results to return (default is 10). Returns a JSON string containing the search results, including publication date, title, and summary. |

## Developer Resources

* View [Tools](https://github.com/agno-agi/agno/blob/main/libs/agno/agno/tools/pubmed.py)
* View [Cookbook](https://github.com/agno-agi/agno/tree/main/cookbook/tools/pubmed_tools.py)

---

## enable_user_memories=True,

**URL:** llms-txt#enable_user_memories=true,

---

## Advanced Imagen Tool with Vertex AI

**URL:** llms-txt#advanced-imagen-tool-with-vertex-ai

**Contents:**
- Code
- Usage

Source: https://docs.agno.com/examples/models/gemini/imagen_tool_advanced

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Set up Vertex AI authentication">
    Follow the [authentication guide](https://cloud.google.com/sdk/docs/initializing) to set up Vertex AI credentials.

<Step title="Set your API keys">
    
  </Step>

<Step title="Install libraries">
    
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Set up Vertex AI authentication">
    Follow the [authentication guide](https://cloud.google.com/sdk/docs/initializing) to set up Vertex AI credentials.
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Set your API keys">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 4 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

---

## Configure the Phoenix tracer

**URL:** llms-txt#configure-the-phoenix-tracer

tracer_provider = register(
    project_name="agno-stock-price-agent",  # Default is 'default'
    auto_instrument=True,  # Automatically use the installed OpenInference instrumentation
)

---

## Building our Social Media Intelligence System

**URL:** llms-txt#building-our-social-media-intelligence-system

**Contents:**
- Step 1: Choose Your AI Model

## Step 1: Choose Your AI Model

**Which model should I use?**: You can choose any model from our supported providers. Normally models are chosen based on costs and performance. In this case, we will be using OpenAI's GPT-5 Mini.

* **Cost-effective**: Better price/performance ratio than other GPT models
* **Tool usage**: Excellent at deciding when and how to use tools
* **Complex reasoning**: Can follow detailed analysis methodologies
* **Structured output**: Reliable at generating formatted reports

Let's first create the file where we will define our agent:

Now let's add the basic imports and model setup:

```python  theme={null}
from pathlib import Path
from dotenv import load_dotenv
from agno.models.openai import OpenAIChat

**Examples:**

Example 1 (unknown):
```unknown
Now let's add the basic imports and model setup:
```

---

## Create the team

**URL:** llms-txt#create-the-team

team = Team(
    name="Web Research Team",
    model=OpenAIChat(id="gpt-5-mini"),
    members=[web_searcher],
    markdown=True,
    store_member_responses=True,
)

---

## Document Chunking

**URL:** llms-txt#document-chunking

Source: https://docs.agno.com/reference/knowledge/chunking/document

Document chunking is a method of splitting documents into smaller chunks based on document structure like paragraphs and sections.
It analyzes natural document boundaries rather than splitting at fixed character counts. This is useful when you want to process large documents while preserving semantic meaning and context.

<Snippet file="chunking-document.mdx" />

---

## Gemini with Reasoning Tools

**URL:** llms-txt#gemini-with-reasoning-tools

**Contents:**
- Code
- Usage

Source: https://docs.agno.com/examples/concepts/reasoning/tools/gemini-reasoning-tools

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Set your API key">
    
  </Step>

<Step title="Install libraries">
    
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Set your API key">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Example 2: Item consumption and removal

**URL:** llms-txt#example-2:-item-consumption-and-removal

print("Example 2: Item Consumption & Removal")
print("-" * 50)
shopping_team.print_response("I got bread from the store", stream=True)
print(f"Session state: {shopping_team.get_session_state()}")
print()

---

## Azure Cosmos DB MongoDB connection string

**URL:** llms-txt#azure-cosmos-db-mongodb-connection-string

"""
Example connection strings:
"mongodb+srv://<username>:<encoded_password>@cluster0.mongocluster.cosmos.azure.com/?tls=true&authMechanism=SCRAM-SHA-256&retrywrites=false&maxIdleTimeMS=120000"
"""
mdb_connection_string = f"mongodb+srv://<username>:<encoded_password>@cluster0.mongocluster.cosmos.azure.com/?tls=true&authMechanism=SCRAM-SHA-256&retrywrites=false&maxIdleTimeMS=120000"

knowledge_base = Knowledge(
    vector_db=MongoDb(
        collection_name="recipes",
        db_url=mdb_connection_string,
        search_index_name="recipes",
        cosmos_compatibility=True,
    ),
)

knowledge.add_content(
    url="https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf"
)

---

## Scenario Testing

**URL:** llms-txt#scenario-testing

**Contents:**
- Basic Scenario Testing

Source: https://docs.agno.com/integrations/testing/scenario-testing

This example demonstrates how to use the [Scenario](https://github.com/langwatch/scenario) framework for agentic simulation-based testing. Scenario enables you to simulate conversations between agents, user simulators, and judges, making it easy to test and evaluate agent behaviors in a controlled environment.

> **Tip:** For a more advanced scenario testing example, check out the [customer support scenario](https://github.com/langwatch/create-agent-app/tree/main/agno_example) for a more complex agent, including tool calls and advanced scenario features.

## Basic Scenario Testing

```python cookbook/agent_concepts/other/scenario_testing.py theme={null}
import pytest
import scenario
from agno.agent import Agent
from agno.models.openai import OpenAIChat

---

## Retrieve and display generated images

**URL:** llms-txt#retrieve-and-display-generated-images

run_response = image_agent.get_last_run_output()
if run_response and isinstance(run_response, RunOutput):
    for image_response in run_response.images:
        image_url = image_response.url
        print("image_url: ", image_url)
else:
    print("No images found or images is not a list")

---

## Branching Workflow

**URL:** llms-txt#branching-workflow

**Contents:**
- Example
- Developer Resources
- Reference

Source: https://docs.agno.com/concepts/workflows/workflow-patterns/branching-workflow

Complex decision trees requiring dynamic path selection based on content analysis

**Example Use-Cases**: Expert routing, content type detection, multi-path processing

Dynamic routing workflows provide intelligent path selection while maintaining predictable execution within each chosen branch.

<img className="block dark:hidden" src="https://mintcdn.com/agno-v2/6A2IKapU7R02zCpZ/images/workflows-router-steps-light.png?fit=max&auto=format&n=6A2IKapU7R02zCpZ&q=85&s=11256e6d5ebe78ee137ba56647bb732c" alt="Workflows router steps diagram" data-og-width="2493" width="2493" data-og-height="921" height="921" data-path="images/workflows-router-steps-light.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/agno-v2/6A2IKapU7R02zCpZ/images/workflows-router-steps-light.png?w=280&fit=max&auto=format&n=6A2IKapU7R02zCpZ&q=85&s=bbbf338fb349e3d6e9e66f92873ca74b 280w, https://mintcdn.com/agno-v2/6A2IKapU7R02zCpZ/images/workflows-router-steps-light.png?w=560&fit=max&auto=format&n=6A2IKapU7R02zCpZ&q=85&s=3c341c1b87faa0eca3092ea8f93c5d0b 560w, https://mintcdn.com/agno-v2/6A2IKapU7R02zCpZ/images/workflows-router-steps-light.png?w=840&fit=max&auto=format&n=6A2IKapU7R02zCpZ&q=85&s=75dabdfd37b0806915bd56520c176d0a 840w, https://mintcdn.com/agno-v2/6A2IKapU7R02zCpZ/images/workflows-router-steps-light.png?w=1100&fit=max&auto=format&n=6A2IKapU7R02zCpZ&q=85&s=8e91a1cb88327098c3420a0bd4994e69 1100w, https://mintcdn.com/agno-v2/6A2IKapU7R02zCpZ/images/workflows-router-steps-light.png?w=1650&fit=max&auto=format&n=6A2IKapU7R02zCpZ&q=85&s=e948b1e5b7f48fde2637265f2daef7f5 1650w, https://mintcdn.com/agno-v2/6A2IKapU7R02zCpZ/images/workflows-router-steps-light.png?w=2500&fit=max&auto=format&n=6A2IKapU7R02zCpZ&q=85&s=74139a11491c79a755974923831ad406 2500w" />

<img className="hidden dark:block" src="https://mintcdn.com/agno-v2/6A2IKapU7R02zCpZ/images/workflows-router-steps.png?fit=max&auto=format&n=6A2IKapU7R02zCpZ&q=85&s=593bc69b1647af6571151c051145e7c6" alt="Workflows router steps diagram" data-og-width="2493" width="2493" data-og-height="921" height="921" data-path="images/workflows-router-steps.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/agno-v2/6A2IKapU7R02zCpZ/images/workflows-router-steps.png?w=280&fit=max&auto=format&n=6A2IKapU7R02zCpZ&q=85&s=575bc73e719bb2ccf703278e5aaaa4b3 280w, https://mintcdn.com/agno-v2/6A2IKapU7R02zCpZ/images/workflows-router-steps.png?w=560&fit=max&auto=format&n=6A2IKapU7R02zCpZ&q=85&s=6886d723d73a9fc1ffec318b2fe33d3c 560w, https://mintcdn.com/agno-v2/6A2IKapU7R02zCpZ/images/workflows-router-steps.png?w=840&fit=max&auto=format&n=6A2IKapU7R02zCpZ&q=85&s=0c364bb81456fc509a64fcac0cb8373a 840w, https://mintcdn.com/agno-v2/6A2IKapU7R02zCpZ/images/workflows-router-steps.png?w=1100&fit=max&auto=format&n=6A2IKapU7R02zCpZ&q=85&s=d9ede7db094389fc173c590ea28aa21c 1100w, https://mintcdn.com/agno-v2/6A2IKapU7R02zCpZ/images/workflows-router-steps.png?w=1650&fit=max&auto=format&n=6A2IKapU7R02zCpZ&q=85&s=117488ab5bcecbf9e982474dd91580e8 1650w, https://mintcdn.com/agno-v2/6A2IKapU7R02zCpZ/images/workflows-router-steps.png?w=2500&fit=max&auto=format&n=6A2IKapU7R02zCpZ&q=85&s=e0f5a19e133076f0ba74ced4f21ba411 2500w" />

## Developer Resources

* [Router Steps Workflow](/examples/concepts/workflows/05_workflows_conditional_branching/router_steps_workflow)

For complete API documentation, see [Router Steps Reference](/reference/workflows/router-steps).

---

## Image Input URL

**URL:** llms-txt#image-input-url

**Contents:**
- Code
- Usage

Source: https://docs.agno.com/examples/models/vertexai/claude/image_input_url

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Set your environment variables">
    <CodeGroup>

</CodeGroup>
  </Step>

<Step title="Authenticate your CLI session">
    `gcloud auth application-default login `

<Note>You dont need to authenticate your CLI every time. </Note>
  </Step>

<Step title="Install libraries">`pip install -U anthropic agno `</Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Set your environment variables">
    <CodeGroup>
```

Example 2 (unknown):
```unknown

```

Example 3 (unknown):
```unknown
</CodeGroup>
  </Step>

  <Step title="Authenticate your CLI session">
    `gcloud auth application-default login `

    <Note>You dont need to authenticate your CLI every time. </Note>
  </Step>

  <Step title="Install libraries">`pip install -U anthropic agno `</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Audio Model Output

**URL:** llms-txt#audio-model-output

**Contents:**
- Audio response modality

Source: https://docs.agno.com/concepts/multimodal/audio/audio_output

Learn how to use audio from models as output with Agno agents.

Similar to providing audio inputs, you can also get audio outputs from an agent. Take a look at the [compatibility matrix](/concepts/models/compatibility#multimodal-support) to see which models support audio as output.

## Audio response modality

The following example demonstrates how some models can directly generate audio as part of their response.

```python audio_agent.py theme={null}
from agno.agent import Agent, RunOutput
from agno.models.openai import OpenAIChat
from agno.utils.audio import write_audio_to_file

agent = Agent(
    model=OpenAIChat(
        id="gpt-5-mini-audio-preview",
        modalities=["text", "audio"],
        audio={"voice": "alloy", "format": "wav"},
    ),
    markdown=True,
)
response: RunOutput = agent.run("Tell me a 5 second scary story")

---

## https://api.telegram.org/bot/<your-bot-token>/getUpdates

**URL:** llms-txt#https://api.telegram.org/bot/<your-bot-token>/getupdates

**Contents:**
- Toolkit Params
- Toolkit Functions
- Developer Resources

telegram_token = "<enter-your-bot-token>"
chat_id = "<enter-your-chat-id>"

agent = Agent(
    name="telegram",
    tools=[TelegramTools(token=telegram_token, chat_id=chat_id)],
)

agent.print_response("Send message to telegram chat a paragraph about the moon")
```

| Parameter             | Type              | Default | Description                                                                               |
| --------------------- | ----------------- | ------- | ----------------------------------------------------------------------------------------- |
| `token`               | `Optional[str]`   | `None`  | Telegram Bot API token. If not provided, will check TELEGRAM\_TOKEN environment variable. |
| `chat_id`             | `Union[str, int]` | -       | The ID of the chat to send messages to.                                                   |
| `enable_send_message` | `bool`            | `True`  | Enable the send\_message functionality.                                                   |
| `all`                 | `bool`            | `False` | Enable all functionality.                                                                 |

| Function       | Description                                                                                                                                                         |
| -------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| `send_message` | Sends a message to the specified Telegram chat. Takes a message string as input and returns the API response as text. If an error occurs, returns an error message. |

## Developer Resources

* View [Tools](https://github.com/agno-agi/agno/blob/main/libs/agno/agno/tools/telegram.py)
* View [Cookbook](https://github.com/agno-agi/agno/tree/main/cookbook/tools/telegram_tools.py)

---

## JSON for Workflows

**URL:** llms-txt#json-for-workflows

**Contents:**
- Usage

Source: https://docs.agno.com/examples/concepts/db/json/json_for_workflow

Agno supports using local JSON files as a storage backend for Workflows using the `JsonDb` class.

```python json_for_workflows.py theme={null}
from agno.agent import Agent
from agno.db.json import JsonDb
from agno.models.openai import OpenAIChat
from agno.team import Team
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.hackernews import HackerNewsTools
from agno.workflow.step import Step
from agno.workflow.workflow import Workflow

---

## ChromaDB Async

**URL:** llms-txt#chromadb-async

**Contents:**
- Code
- Usage

Source: https://docs.agno.com/examples/concepts/vectordb/chroma-db/async-chroma-db

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Install libraries">
    
  </Step>

<Step title="Set environment variables">
    
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Install libraries">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Set environment variables">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## - Create a new index in Upstash Console with the correct dimension

**URL:** llms-txt#--create-a-new-index-in-upstash-console-with-the-correct-dimension

---

## Define team-level tools

**URL:** llms-txt#define-team-level-tools

def list_items(session_state) -> str:
    """List all items in the shopping list."""
    # Access shared state (not private state)
    shopping_list = session_state["shopping_list"]

if not shopping_list:
        return "The shopping list is empty."

items_text = "\n".join([f"- {item}" for item in shopping_list])
    return f"Current shopping list:\n{items_text}"

def add_chore(session_state, chore: str) -> str:
    """Add a completed chore to the team's private log."""
    # Access team's private state
    if "chores" not in session_state:
        session_state["chores"] = []

session_state["chores"].append(chore)
    return f"Logged chore: {chore}"

---

## Use the sequence in a workflow

**URL:** llms-txt#use-the-sequence-in-a-workflow

**Contents:**
- Steps with Router

workflow = Workflow(
    name="Article Creation Workflow",
    steps=[article_creation_sequence]  # Single sequence
)

workflow.print_response("Write an article about renewable energy", markdown=True)
python  theme={null}
from agno.workflow import Steps, Router, Step, Workflow

**Examples:**

Example 1 (unknown):
```unknown
## Steps with Router

This is where `Steps` really shines - creating distinct sequences for different content types or workflows:
```

---

## Loop Steps

**URL:** llms-txt#loop-steps

Source: https://docs.agno.com/reference/workflows/loop-steps

| Parameter        | Type                                                                                                 | Default  | Description                                 |
| ---------------- | ---------------------------------------------------------------------------------------------------- | -------- | ------------------------------------------- |
| `steps`          | `WorkflowSteps`                                                                                      | Required | Steps to execute in each loop iteration     |
| `name`           | `Optional[str]`                                                                                      | `None`   | Name of the loop step                       |
| `description`    | `Optional[str]`                                                                                      | `None`   | Description of the loop step                |
| `max_iterations` | `int`                                                                                                | `3`      | Maximum number of iterations for the loop   |
| `end_condition`  | `Optional[Union[Callable[[List[StepOutput]], bool], Callable[[List[StepOutput]], Awaitable[bool]]]]` | `None`   | Function to evaluate if the loop should end |

---

## Input as Dictionary

**URL:** llms-txt#input-as-dictionary

**Contents:**
- Code

Source: https://docs.agno.com/examples/concepts/teams/other/input_as_dict

This example shows how to pass input to a team as a dictionary format, useful for multimodal inputs or structured data.

```python cookbook/examples/teams/basic/input_as_dict.py theme={null}
from agno.agent import Agent
from agno.team import Team

---

## Delete Evaluation Runs

**URL:** llms-txt#delete-evaluation-runs

Source: https://docs.agno.com/reference-api/schema/evals/delete-evaluation-runs

delete /eval-runs
Delete multiple evaluation runs by their IDs. This action cannot be undone.

---

## ASYNC EXAMPLE

**URL:** llms-txt#async-example

---

## Voyage AI Embedder

**URL:** llms-txt#voyage-ai-embedder

**Contents:**
- Usage

Source: https://docs.agno.com/concepts/knowledge/embedder/voyageai

The `VoyageAIEmbedder` class is used to embed text data into vectors using the Voyage AI API. Get your key from [here](https://dash.voyageai.com/api-keys).

```python voyageai_embedder.py theme={null}
from agno.knowledge.knowledge import Knowledge
from agno.vectordb.pgvector import PgVector
from agno.knowledge.embedder.voyageai import VoyageAIEmbedder

---

## "Please provide me with a personalized summary of today's priorities based on my profile and interests.",

**URL:** llms-txt#"please-provide-me-with-a-personalized-summary-of-today's-priorities-based-on-my-profile-and-interests.",

---

## Pydantic AI

**URL:** llms-txt#pydantic-ai

**Contents:**
  - Memory Usage
  - Results

python cookbook/evals/performance/comparison/pydantic_ai_instantiation.py
```

LangGraph is on the right, **let's start it first and give it a head start**. Then CrewAI and Pydantic AI follow, and finally Agno. Agno obviously finishes first, but let's see by how much.

<Frame>
  <video autoPlay muted loop playsInline style={{ borderRadius: "0.5rem", width: "100%", height: "auto" }}>
    <source src="https://mintcdn.com/agno-v2/Xc0-_OHxxYe_vtGw/videos/performance_benchmark.mp4?fit=max&auto=format&n=Xc0-_OHxxYe_vtGw&q=85&s=89288701c4cb61d9d2a551fd0b5630a6" type="video/mp4" data-path="videos/performance_benchmark.mp4" />
  </video>
</Frame>

To measure memory usage, we use the `tracemalloc` library. We first calculate a baseline memory usage by running an empty function, then run the Agent 1000x times and calculate the difference. This gives a (reasonably) isolated measurement of the memory usage of the Agent.

We recommend running the evaluation yourself on your own machine, and digging into the code to see how it works. If we've made a mistake, please let us know.

Taking Agno as the baseline, we can see that:

| Metric             | Agno | Langgraph   | PydanticAI | CrewAI     |
| ------------------ | ---- | ----------- | ---------- | ---------- |
| **Time (seconds)** | 1×   | 529× slower | 57× slower | 70× slower |
| **Memory (MiB)**   | 1×   | 24× higher  | 4× higher  | 10× higher |

Exact numbers from the benchmark:

| Metric             | Agno     | Langgraph | PydanticAI | CrewAI   |
| ------------------ | -------- | --------- | ---------- | -------- |
| **Time (seconds)** | 0.000003 | 0.001587  | 0.000170   | 0.000210 |
| **Memory (MiB)**   | 0.006642 | 0.161435  | 0.028712   | 0.065652 |

<Note>
  Agno agents are designed for performance and while we share benchmarks against other frameworks, we should be mindful that accuracy and reliability are more important than speed.
</Note>

---

## Anthropic Claude

**URL:** llms-txt#anthropic-claude

**Contents:**
- Authentication
- Example
- Params

Source: https://docs.agno.com/concepts/models/anthropic

Learn how to use Anthropic Claude models in Agno.

Claude is a family of foundational AI models by Anthropic that can be used in a variety of applications.
See their model comparisons [here](https://docs.anthropic.com/en/docs/about-claude/models#model-comparison-table).

We recommend experimenting to find the best-suited model for your use-case. Here are some general recommendations:

* `claude-sonnet-4-20250514` model is good for most use-cases and supports image input.
* `claude-opus-4-1-20250805` model is their best model.
* `claude-3-5-haiku-20241022` model is their fastest model.

Anthropic has rate limits on their APIs. See the [docs](https://docs.anthropic.com/en/api/rate-limits#response-headers) for more information.

<Note>
  Claude API expects a `max_tokens` param to be sent with each request. Unless
  set as a param, Agno will default to 8192. See the
  [docs](https://docs.claude.com/en/api/messages) for more information.
</Note>

Set your `ANTHROPIC_API_KEY` environment. You can get one [from Anthropic here](https://console.anthropic.com/settings/keys).

Use `Claude` with your `Agent`:

You can enable system prompt caching with our `Claude` model by setting `cache_system_prompt` to `True`:

Read more about prompt caching with Agno's `Claude` model [here](https://docs.agno.com/examples/models/anthropic/prompt_caching).
</CodeGroup>

<Note> View more examples [here](/examples/models/anthropic/basic). </Note>

| Parameter             | Type                                     | Default                        | Description                                                                                                                      |
| --------------------- | ---------------------------------------- | ------------------------------ | -------------------------------------------------------------------------------------------------------------------------------- |
| `id`                  | `str`                                    | `"claude-3-5-sonnet-20241022"` | The id of the Anthropic Claude model to use                                                                                      |
| `name`                | `str`                                    | `"Claude"`                     | The name of the model                                                                                                            |
| `provider`            | `str`                                    | `"Anthropic"`                  | The provider of the model                                                                                                        |
| `max_tokens`          | `Optional[int]`                          | `4096`                         | Maximum number of tokens to generate in the chat completion                                                                      |
| `thinking`            | `Optional[Dict[str, Any]]`               | `None`                         | Configuration for the thinking (reasoning) process (See [their docs](https://www.anthropic.com/news/visible-extended-thinking))) |
| `temperature`         | `Optional[float]`                        | `None`                         | Controls randomness in the model's output                                                                                        |
| `stop_sequences`      | `Optional[List[str]]`                    | `None`                         | A list of strings that the model should stop generating text at                                                                  |
| `top_p`               | `Optional[float]`                        | `None`                         | Controls diversity via nucleus sampling                                                                                          |
| `top_k`               | `Optional[int]`                          | `None`                         | Controls diversity via top-k sampling                                                                                            |
| `cache_system_prompt` | `Optional[bool]`                         | `False`                        | Whether to cache the system prompt for improved performance                                                                      |
| `extended_cache_time` | `Optional[bool]`                         | `False`                        | Whether to use extended cache time (1 hour instead of default)                                                                   |
| `request_params`      | `Optional[Dict[str, Any]]`               | `None`                         | Additional parameters to include in the request                                                                                  |
| `mcp_servers`         | `Optional[List[MCPServerConfiguration]]` | `None`                         | List of MCP (Model Context Protocol) server configurations                                                                       |
| `api_key`             | `Optional[str]`                          | `None`                         | The API key for authenticating with Anthropic                                                                                    |
| `default_headers`     | `Optional[Dict[str, Any]]`               | `None`                         | Default headers to include in all requests                                                                                       |
| `client_params`       | `Optional[Dict[str, Any]]`               | `None`                         | Additional parameters for client configuration                                                                                   |
| `client`              | `Optional[AnthropicClient]`              | `None`                         | A pre-configured instance of the Anthropic client                                                                                |
| `async_client`        | `Optional[AsyncAnthropicClient]`         | `None`                         | A pre-configured instance of the async Anthropic client                                                                          |

`Claude` is a subclass of the [Model](/reference/models/model) class and has access to the same params.

**Examples:**

Example 1 (unknown):
```unknown

```

Example 2 (unknown):
```unknown
</CodeGroup>

## Example

Use `Claude` with your `Agent`:

<CodeGroup>
```

Example 3 (unknown):
```unknown
## Prompt caching

  You can enable system prompt caching with our `Claude` model by setting `cache_system_prompt` to `True`:
```

---

## -*- Share personal information

**URL:** llms-txt#-*--share-personal-information

agent.print_response("I'm going to a concert tomorrow?", stream=True)

---

## for response in run_response_strem:

**URL:** llms-txt#for-response-in-run_response_strem:

---

## Replicate Tools

**URL:** llms-txt#replicate-tools

**Contents:**
- Code
- Usage

Source: https://docs.agno.com/examples/concepts/tools/others/replicate

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Set your API token">
    
  </Step>

<Step title="Install libraries">
    
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Set your API token">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Create cluster options with authentication

**URL:** llms-txt#create-cluster-options-with-authentication

auth = PasswordAuthenticator(username, password)
cluster_options = ClusterOptions(auth)
cluster_options.apply_profile(KnownConfigProfiles.WanDevelopment)

---

## ------------------------------------------------------------

**URL:** llms-txt#------------------------------------------------------------

---

## Generate unique user ID

**URL:** llms-txt#generate-unique-user-id

user_id = str(uuid4())

---

## Get all content with pagination

**URL:** llms-txt#get-all-content-with-pagination

contents, total_count = knowledge.get_content(
    limit=20,
    page=1,
    sort_by="created_at",
    sort_order="desc"
)

---

## Run the team

**URL:** llms-txt#run-the-team

run_response: TeamRunOutput = team.run(
    "What is going on in the world?"
)
pprint_run_response(run_response, markdown=True)

---

## Load the document

**URL:** llms-txt#load-the-document

raw_documents = TextLoader(str(state_of_the_union), encoding="utf-8").load()

---

## Ollama

**URL:** llms-txt#ollama

**Contents:**
- Key Features
- Parameters

Source: https://docs.agno.com/reference/models/ollama

The Ollama model provides access to open source models, both locally-hosted and via **Ollama Cloud**.

**Local Usage**: Run models on your own hardware using the Ollama client. Perfect for development, privacy-sensitive workloads, and when you want full control over your infrastructure.

**Cloud Usage**: Access cloud-hosted models via [Ollama Cloud](https://ollama.com) with an API key for scalable, production-ready deployments. No local setup required - simply set your `OLLAMA_API_KEY` and start using powerful models instantly.

* **Dual Deployment Options**: Choose between local hosting for privacy and control, or cloud hosting for scalability
* **Seamless Switching**: Easy transition between local and cloud deployments with minimal code changes
* **Auto-configuration**: When using an API key, the host automatically defaults to Ollama Cloud
* **Wide Model Support**: Access to extensive library of open-source models including GPT-OSS, Llama, Qwen, DeepSeek, and Phi models

| Parameter    | Type                          | Default                    | Description                                                  |
| ------------ | ----------------------------- | -------------------------- | ------------------------------------------------------------ |
| `id`         | `str`                         | `"llama3.2"`               | The name of the Ollama model to use                          |
| `name`       | `str`                         | `"Ollama"`                 | The name of the model                                        |
| `provider`   | `str`                         | `"Ollama"`                 | The provider of the model                                    |
| `host`       | `str`                         | `"http://localhost:11434"` | The host URL for the Ollama server                           |
| `timeout`    | `Optional[int]`               | `None`                     | Request timeout in seconds                                   |
| `format`     | `Optional[str]`               | `None`                     | The format to return the response in (e.g., "json")          |
| `options`    | `Optional[Dict[str, Any]]`    | `None`                     | Additional model options (temperature, top\_p, etc.)         |
| `keep_alive` | `Optional[Union[float, str]]` | `None`                     | How long to keep the model loaded (e.g., "5m", 3600 seconds) |
| `template`   | `Optional[str]`               | `None`                     | The prompt template to use                                   |
| `system`     | `Optional[str]`               | `None`                     | System message to use                                        |
| `raw`        | `Optional[bool]`              | `None`                     | Whether to return raw response without formatting            |
| `stream`     | `bool`                        | `True`                     | Whether to stream the response                               |

---

## Jira Tools

**URL:** llms-txt#jira-tools

**Contents:**
- Code
- Usage

Source: https://docs.agno.com/examples/concepts/tools/others/jira

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Set your Jira credentials">
    
  </Step>

<Step title="Install libraries">
    
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Set your Jira credentials">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## "Based on my profile, what should I focus on this week? Include specific recommendations.",

**URL:** llms-txt#"based-on-my-profile,-what-should-i-focus-on-this-week?-include-specific-recommendations.",

---

## Fixed Size Chunking

**URL:** llms-txt#fixed-size-chunking

Source: https://docs.agno.com/reference/knowledge/chunking/fixed-size

Fixed size chunking is a method of splitting documents into smaller chunks of a specified size, with optional overlap between chunks.
This is useful when you want to process large documents in smaller, manageable pieces.

<Snippet file="chunking-fixed-size.mdx" />

---

## YFinance Tools

**URL:** llms-txt#yfinance-tools

**Contents:**
- Code
- Usage

Source: https://docs.agno.com/examples/concepts/tools/others/yfinance

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Set your API key">
    
  </Step>

<Step title="Install libraries">
    
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Set your API key">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## "[Feature Request] Can you please update me on the above feature",

**URL:** llms-txt#"[feature-request]-can-you-please-update-me-on-the-above-feature",

---

## SerpAPI Tools

**URL:** llms-txt#serpapi-tools

**Contents:**
- Code
- Usage

Source: https://docs.agno.com/examples/concepts/tools/search/serpapi

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Set your API key">
    
  </Step>

<Step title="Install libraries">
    
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Set your API key">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Requesty

**URL:** llms-txt#requesty

**Contents:**
- Parameters

Source: https://docs.agno.com/reference/models/requesty

The Requesty model provides access to models through Requesty AI.

| Parameter    | Type            | Default                           | Description                                                       |
| ------------ | --------------- | --------------------------------- | ----------------------------------------------------------------- |
| `id`         | `str`           | `"openai/gpt-4.1"`                | The id of the model to use through Requesty                       |
| `name`       | `str`           | `"Requesty"`                      | The name of the model                                             |
| `provider`   | `str`           | `"Requesty"`                      | The provider of the model                                         |
| `api_key`    | `Optional[str]` | `None`                            | The API key for Requesty (defaults to REQUESTY\_API\_KEY env var) |
| `base_url`   | `str`           | `"https://router.requesty.ai/v1"` | The base URL for the Requesty API                                 |
| `max_tokens` | `int`           | `1024`                            | The maximum number of tokens to generate                          |

Requesty extends the OpenAI-compatible interface and supports most parameters from OpenAI.

---

## Telegram

**URL:** llms-txt#telegram

**Contents:**
- Prerequisites
- Example

Source: https://docs.agno.com/concepts/tools/toolkits/social/telegram

**TelegramTools** enable an Agent to send messages to a Telegram chat using the Telegram Bot API.

The following agent will send a message to a Telegram chat.

```python cookbook/tools/telegram_tools.py theme={null}
from agno.agent import Agent
from agno.tools.telegram import TelegramTools

**Examples:**

Example 1 (unknown):
```unknown

```

Example 2 (unknown):
```unknown
## Example

The following agent will send a message to a Telegram chat.
```

---

## Create a Steps sequence with a Condition containing Parallel steps

**URL:** llms-txt#create-a-steps-sequence-with-a-condition-containing-parallel-steps

article_creation_sequence = Steps(
    name="ArticleCreation",
    description="Complete article creation workflow from research to final edit",
    steps=[
        initial_research_step,
        # Condition with Parallel steps inside
        Condition(
            name="TechResearchCondition",
            description="If topic is tech-related, do specialized parallel research",
            evaluator=is_tech_topic,
            steps=[
                Parallel(
                    tech_research_step,
                    news_research_step,
                    name="SpecializedResearch",
                    description="Parallel tech and news research",
                ),
                content_prep_step,
            ],
        ),
        writing_step,
        editing_step,
    ],
)

---

## Test streaming response

**URL:** llms-txt#test-streaming-response

**Contents:**
- Usage

team.print_response(
    "What is the current stock price of NVDA?",
    stream=True,
    stream_events=True,
)
bash  theme={null}
    pip install agno exa_py
    bash  theme={null}
    export OPENAI_API_KEY=****
    export EXA_API_KEY=****
    bash  theme={null}
    python cookbook/examples/teams/streaming/01_team_streaming.py
    ```
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Install required libraries">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Set environment variables">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run the agent">
```

---

## Resend Tools

**URL:** llms-txt#resend-tools

**Contents:**
- Code
- Usage

Source: https://docs.agno.com/examples/concepts/tools/others/resend

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Set your API key">
    
  </Step>

<Step title="Install libraries">
    
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Set your API key">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Nebius Tools

**URL:** llms-txt#nebius-tools

**Contents:**
- Code
- Usage

Source: https://docs.agno.com/examples/concepts/tools/models/nebius

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Set your API keys">
    
  </Step>

<Step title="Install libraries">
    
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Set your API keys">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Parallel Workflow

**URL:** llms-txt#parallel-workflow

**Contents:**
- Example
- Developer Resources
- Reference

Source: https://docs.agno.com/concepts/workflows/workflow-patterns/parallel-workflow

Independent, concurrent tasks that can execute simultaneously for improved efficiency

**Example Use-Cases**: Multi-source research, parallel analysis, concurrent data processing

Parallel workflows maintain deterministic results while dramatically reducing execution time for independent operations.

<img className="block dark:hidden" src="https://mintcdn.com/agno-v2/JYIBgMrzFEujZh3_/images/workflows-parallel-steps-light.png?fit=max&auto=format&n=JYIBgMrzFEujZh3_&q=85&s=afda5268ab0637c6064ace8edd6f35e5" alt="Workflows parallel steps diagram" data-og-width="3441" width="3441" data-og-height="756" height="756" data-path="images/workflows-parallel-steps-light.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/agno-v2/JYIBgMrzFEujZh3_/images/workflows-parallel-steps-light.png?w=280&fit=max&auto=format&n=JYIBgMrzFEujZh3_&q=85&s=c153b9934fa98b2b886a9435022b020a 280w, https://mintcdn.com/agno-v2/JYIBgMrzFEujZh3_/images/workflows-parallel-steps-light.png?w=560&fit=max&auto=format&n=JYIBgMrzFEujZh3_&q=85&s=17253f58b485bb6180827516f7f947be 560w, https://mintcdn.com/agno-v2/JYIBgMrzFEujZh3_/images/workflows-parallel-steps-light.png?w=840&fit=max&auto=format&n=JYIBgMrzFEujZh3_&q=85&s=b067f30b291f2de8cb6a04e208ee61cc 840w, https://mintcdn.com/agno-v2/JYIBgMrzFEujZh3_/images/workflows-parallel-steps-light.png?w=1100&fit=max&auto=format&n=JYIBgMrzFEujZh3_&q=85&s=a814e829581fb1d7c64e49fa87ca847e 1100w, https://mintcdn.com/agno-v2/JYIBgMrzFEujZh3_/images/workflows-parallel-steps-light.png?w=1650&fit=max&auto=format&n=JYIBgMrzFEujZh3_&q=85&s=1cd456c3e949af82fd6325b3f3865f23 1650w, https://mintcdn.com/agno-v2/JYIBgMrzFEujZh3_/images/workflows-parallel-steps-light.png?w=2500&fit=max&auto=format&n=JYIBgMrzFEujZh3_&q=85&s=52fcc51f72ee6e1df2648612451cae70 2500w" />

<img className="hidden dark:block" src="https://mintcdn.com/agno-v2/JYIBgMrzFEujZh3_/images/workflows-parallel-steps.png?fit=max&auto=format&n=JYIBgMrzFEujZh3_&q=85&s=ec4f6c7c9a6ef76cec8f0866eb1acc5b" alt="Workflows parallel steps diagram" data-og-width="3441" width="3441" data-og-height="756" height="756" data-path="images/workflows-parallel-steps.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/agno-v2/JYIBgMrzFEujZh3_/images/workflows-parallel-steps.png?w=280&fit=max&auto=format&n=JYIBgMrzFEujZh3_&q=85&s=39624cca7177ba0064491bb64c645db2 280w, https://mintcdn.com/agno-v2/JYIBgMrzFEujZh3_/images/workflows-parallel-steps.png?w=560&fit=max&auto=format&n=JYIBgMrzFEujZh3_&q=85&s=c3e5cde671f164b7dd13eb417f5f74db 560w, https://mintcdn.com/agno-v2/JYIBgMrzFEujZh3_/images/workflows-parallel-steps.png?w=840&fit=max&auto=format&n=JYIBgMrzFEujZh3_&q=85&s=db6672401fdf35e0eb616e72016ec41c 840w, https://mintcdn.com/agno-v2/JYIBgMrzFEujZh3_/images/workflows-parallel-steps.png?w=1100&fit=max&auto=format&n=JYIBgMrzFEujZh3_&q=85&s=831053d087a3bf26e966f8f896ac9b61 1100w, https://mintcdn.com/agno-v2/JYIBgMrzFEujZh3_/images/workflows-parallel-steps.png?w=1650&fit=max&auto=format&n=JYIBgMrzFEujZh3_&q=85&s=445e07ea13a74913e67e2a2a9c8e9c5f 1650w, https://mintcdn.com/agno-v2/JYIBgMrzFEujZh3_/images/workflows-parallel-steps.png?w=2500&fit=max&auto=format&n=JYIBgMrzFEujZh3_&q=85&s=77083544c8387f660207dfaf645bdaf0 2500w" />

## Developer Resources

* [Parallel Steps Workflow](/examples/concepts/workflows/04-workflows-parallel-execution/parallel_steps_workflow)

For complete API documentation, see [Parallel Steps Reference](/reference/workflows/parallel-steps).

---

## Synchronous reading

**URL:** llms-txt#synchronous-reading

documents = reader.read("file.pdf")

---

## Newspaper4k

**URL:** llms-txt#newspaper4k

**Contents:**
- Prerequisites
- Example
- Toolkit Params
- Toolkit Functions
- Developer Resources

Source: https://docs.agno.com/concepts/tools/toolkits/web_scrape/newspaper4k

**Newspaper4k** enables an Agent to read news articles using the Newspaper4k library.

The following example requires the `newspaper4k` and `lxml_html_clean` libraries.

The following agent will summarize the article: [https://www.rockymountaineer.com/blog/experience-icefields-parkway-scenic-drive-lifetime](https://www.rockymountaineer.com/blog/experience-icefields-parkway-scenic-drive-lifetime).

| Parameter             | Type   | Default | Description                                                                        |
| --------------------- | ------ | ------- | ---------------------------------------------------------------------------------- |
| `enable_read_article` | `bool` | `True`  | Enables the functionality to read the full content of an article.                  |
| `include_summary`     | `bool` | `False` | Specifies whether to include a summary of the article along with the full content. |
| `article_length`      | `int`  | -       | The maximum length of the article or its summary to be processed or returned.      |

| Function           | Description                                                  |
| ------------------ | ------------------------------------------------------------ |
| `get_article_data` | This function reads the full content and data of an article. |
| `read_article`     | This function reads the full content of an article.          |

## Developer Resources

* View [Tools](https://github.com/agno-agi/agno/blob/main/libs/agno/agno/tools/newspaper4k.py)
* View [Cookbook](https://github.com/agno-agi/agno/tree/main/cookbook/tools/newspaper4k_tools.py)

**Examples:**

Example 1 (unknown):
```unknown
## Example

The following agent will summarize the article: [https://www.rockymountaineer.com/blog/experience-icefields-parkway-scenic-drive-lifetime](https://www.rockymountaineer.com/blog/experience-icefields-parkway-scenic-drive-lifetime).
```

---

## Weaviate Hybrid Search

**URL:** llms-txt#weaviate-hybrid-search

**Contents:**
- Code
- Usage

Source: https://docs.agno.com/examples/concepts/vectordb/weaviate-db/weaviate-db-hybrid-search

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Install libraries">
    
  </Step>

<Step title="Setup Weaviate">
    <CodeGroup>

</CodeGroup>
  </Step>

<Step title="Set environment variables">
    
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Install libraries">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Setup Weaviate">
    <CodeGroup>
```

Example 3 (unknown):
```unknown

```

Example 4 (unknown):
```unknown
</CodeGroup>
  </Step>

  <Step title="Set environment variables">
```

---

## Video Input (Bytes Content)

**URL:** llms-txt#video-input-(bytes-content)

**Contents:**
- Code

Source: https://docs.agno.com/examples/models/gemini/video_input_bytes_content

```python cookbook/models/google/gemini/video_input_bytes_content.py theme={null}
import requests
from agno.agent import Agent
from agno.media import Video
from agno.models.google import Gemini

agent = Agent(
    model=Gemini(id="gemini-2.0-flash-exp"),
    markdown=True,
)

url = "https://videos.pexels.com/video-files/5752729/5752729-uhd_2560_1440_30fps.mp4"

---

## Example usage

**URL:** llms-txt#example-usage

if __name__ == "__main__":
    # Generate a research report on a cutting-edge topic
    agent.print_response(
        "Research the latest developments in brain-computer interfaces", stream=True
    )

---

## --- Execution function ---

**URL:** llms-txt#----execution-function----

async def startup_validation_execution(
    workflow: Workflow,
    execution_input: WorkflowExecutionInput,
    startup_idea: str,
    **kwargs: Any,
) -> str:
    """Execute the complete startup idea validation workflow"""

# Get inputs
    message: str = execution_input.input
    idea: str = startup_idea

if not idea:
        return "❌ No startup idea provided"

print(f"🚀 Starting startup idea validation for: {idea}")
    print(f"💡 Validation request: {message}")

# Phase 1: Idea Clarification
    print("\n🎯 PHASE 1: IDEA CLARIFICATION & REFINEMENT")
    print("=" * 60)

clarification_prompt = f"""
    {message}

Please analyze and refine the following startup idea:

Evaluate:
    1. The originality of this idea compared to existing solutions
    2. Define a clear mission statement for this startup
    3. Outline specific, measurable objectives
    Provide insights on how to strengthen and focus the core concept.
    """

print("🔍 Analyzing and refining the startup concept...")

try:
        clarification_result = await idea_clarifier_agent.arun(clarification_prompt)
        idea_clarification = clarification_result.content

print("✅ Idea clarification completed")
        print(f"📝 Mission: {idea_clarification.mission[:100]}...")

except Exception as e:
        return f"❌ Failed to clarify idea: {str(e)}"

# Phase 2: Market Research
    print("\n📊 PHASE 2: MARKET RESEARCH & ANALYSIS")
    print("=" * 60)

market_research_prompt = f"""
    Based on the refined startup idea and clarification below, conduct comprehensive market research:
    STARTUP IDEA: {idea}
    ORIGINALITY: {idea_clarification.originality}
    MISSION: {idea_clarification.mission}
    OBJECTIVES: {idea_clarification.objectives}
    Please research and provide:
    1. Total Addressable Market (TAM) - overall market size
    2. Serviceable Available Market (SAM) - portion you could serve
    3. Serviceable Obtainable Market (SOM) - realistic market share
    4. Target customer segments with detailed characteristics
    Use web search to find current market data and trends.
    """

print("📈 Researching market size and customer segments...")

try:
        market_result = await market_research_agent.arun(market_research_prompt)
        market_research = market_result.content

print("✅ Market research completed")
        print(f"🎯 TAM: {market_research.total_addressable_market[:100]}...")

except Exception as e:
        return f"❌ Failed to complete market research: {str(e)}"

# Phase 3: Competitor Analysis
    print("\n🏢 PHASE 3: COMPETITIVE LANDSCAPE ANALYSIS")
    print("=" * 60)

competitor_prompt = f"""
    Based on the startup idea and market research below, analyze the competitive landscape:
    STARTUP IDEA: {idea}
    TAM: {market_research.total_addressable_market}
    SAM: {market_research.serviceable_available_market}
    SOM: {market_research.serviceable_obtainable_market}
    TARGET SEGMENTS: {market_research.target_customer_segments}
    Please research and provide:
    1. Identify direct and indirect competitors
    2. SWOT analysis for each major competitor
    3. Assessment of startup's potential competitive positioning
    4. Market gaps and opportunities
    Use web search to find current competitor information.
    """

print("🔎 Analyzing competitive landscape...")

try:
        competitor_result = await competitor_analysis_agent.arun(competitor_prompt)
        competitor_analysis = competitor_result.content

print("✅ Competitor analysis completed")
        print(f"🏆 Positioning: {competitor_analysis.positioning[:100]}...")

except Exception as e:
        return f"❌ Failed to complete competitor analysis: {str(e)}"

# Phase 4: Final Validation Report
    print("\n📋 PHASE 4: COMPREHENSIVE VALIDATION REPORT")
    print("=" * 60)

report_prompt = f"""
    Synthesize all the research and analysis into a comprehensive startup validation report:

IDEA CLARIFICATION:
    - Originality: {idea_clarification.originality}
    - Mission: {idea_clarification.mission}
    - Objectives: {idea_clarification.objectives}
    MARKET RESEARCH:
    - TAM: {market_research.total_addressable_market}
    - SAM: {market_research.serviceable_available_market}
    - SOM: {market_research.serviceable_obtainable_market}
    - Target Segments: {market_research.target_customer_segments}
    COMPETITOR ANALYSIS:
    - Competitors: {competitor_analysis.competitors}
    - SWOT: {competitor_analysis.swot_analysis}
    - Positioning: {competitor_analysis.positioning}
    Create a professional validation report with:
    1. Executive summary
    2. Idea assessment (strengths/weaknesses)
    3. Market opportunity analysis
    4. Competitive landscape overview
    5. Strategic recommendations
    6. Specific next steps for the entrepreneur
    """

print("📝 Generating comprehensive validation report...")

try:
        final_result = await report_agent.arun(report_prompt)
        validation_report = final_result.content

print("✅ Validation report completed")

except Exception as e:
        return f"❌ Failed to generate final report: {str(e)}"

# Final summary
    summary = f"""
    🎉 STARTUP IDEA VALIDATION COMPLETED!
    📊 Validation Summary:
    • Startup Idea: {idea}
    • Idea Clarification: ✅ Completed
    • Market Research: ✅ Completed
    • Competitor Analysis: ✅ Completed
    • Final Report: ✅ Generated

📈 Key Market Insights:
    • TAM: {market_research.total_addressable_market[:150]}...
    • Target Segments: {market_research.target_customer_segments[:150]}...

🏆 Competitive Positioning:
    {competitor_analysis.positioning[:200]}...

📋 COMPREHENSIVE VALIDATION REPORT:

## Executive Summary
    {validation_report.executive_summary}

## Idea Assessment
    {validation_report.idea_assessment}

## Market Opportunity
    {validation_report.market_opportunity}

## Competitive Landscape
    {validation_report.competitive_landscape}

## Strategic Recommendations
    {validation_report.recommendations}

## Next Steps
    {validation_report.next_steps}

⚠️ Disclaimer: This validation is for informational purposes only. Conduct additional due diligence before making investment decisions.
    """

---

## Async content loading for better performance

**URL:** llms-txt#async-content-loading-for-better-performance

await knowledge.add_content_async(path="large_dataset/")

---

## Example: File operations

**URL:** llms-txt#example:-file-operations

**Contents:**
- Usage

agent.print_response("Create a text file with the current date and time, then read it back")
bash  theme={null}
    pip install e2b_code_interpreter
    bash Mac/Linux theme={null}
      export E2B_API_KEY=your_api_key_here
      bash Windows (Command Prompt) theme={null}
      set E2B_API_KEY=your_api_key_here
      bash Windows (PowerShell) theme={null}
      $env:E2B_API_KEY="your_api_key_here"
      bash Mac/Linux theme={null}
      python cookbook/tools/e2b_tools.py
      bash Windows theme={null}
      python cookbook\tools\e2b_tools.py
      ```
    </CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Create an E2B account">
    Create an account at [E2B](https://e2b.dev/) and get your API key from the dashboard.
  </Step>

  <Step title="Install libraries">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Set your API Key">
    <CodeGroup>
```

Example 3 (unknown):
```unknown

```

Example 4 (unknown):
```unknown

```

---

## async_response = await team.arun(

**URL:** llms-txt#async_response-=-await-team.arun(

---

## FastEmbed

**URL:** llms-txt#fastembed

Source: https://docs.agno.com/reference/knowledge/embedder/fastembed

FastEmbed Embedder is a class that allows you to embed documents using FastEmbed's efficient embedding models, with BAAI/bge-small-en-v1.5 as the default model.

<Snippet file="embedder-fastembed-reference.mdx" />

---

## Execute team workflow

**URL:** llms-txt#execute-team-workflow

**Contents:**
- Sample Trace
- Advanced Features
  - LangDB Capabilities
- Notes
- Resources

reasoning_team.print_response("Analyze Apple (AAPL) investment potential")
```

View a complete example trace in the LangDB dashboard: [Finance Reasoning Team Trace](https://app.langdb.ai/sharing/threads/73c91c58-eab7-4c6b-afe1-5ab6324f1ada)

<Frame caption="LangDB Finance Team Thread">
  <img src="https://mintcdn.com/agno-v2/Y7twezR0wF2re1xh/images/langdb-finance-thread.png?fit=max&auto=format&n=Y7twezR0wF2re1xh&q=85&s=177481792ff1b6fcdc03d464b62f7711" style={{ borderRadius: '10px', width: '100%', maxWidth: '800px' }} alt="langdb-agno finance team observability" data-og-width="1241" width="1241" data-og-height="916" height="916" data-path="images/langdb-finance-thread.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/agno-v2/Y7twezR0wF2re1xh/images/langdb-finance-thread.png?w=280&fit=max&auto=format&n=Y7twezR0wF2re1xh&q=85&s=13536f1cb8949e0e540505a137e8c978 280w, https://mintcdn.com/agno-v2/Y7twezR0wF2re1xh/images/langdb-finance-thread.png?w=560&fit=max&auto=format&n=Y7twezR0wF2re1xh&q=85&s=2783572ef55c1d8645c2f9dc34c31809 560w, https://mintcdn.com/agno-v2/Y7twezR0wF2re1xh/images/langdb-finance-thread.png?w=840&fit=max&auto=format&n=Y7twezR0wF2re1xh&q=85&s=d08349da04673fd7f1f358f0201cfd55 840w, https://mintcdn.com/agno-v2/Y7twezR0wF2re1xh/images/langdb-finance-thread.png?w=1100&fit=max&auto=format&n=Y7twezR0wF2re1xh&q=85&s=b13d7216f599fef647e6f1f705198368 1100w, https://mintcdn.com/agno-v2/Y7twezR0wF2re1xh/images/langdb-finance-thread.png?w=1650&fit=max&auto=format&n=Y7twezR0wF2re1xh&q=85&s=0124f22d2915636b0086f19f628515f3 1650w, https://mintcdn.com/agno-v2/Y7twezR0wF2re1xh/images/langdb-finance-thread.png?w=2500&fit=max&auto=format&n=Y7twezR0wF2re1xh&q=85&s=0470d1883900d3fc2e0e4d2515417291 2500w" />
</Frame>

### LangDB Capabilities

* **Virtual Models**: Save, share, and reuse model configurations—combining prompts, parameters, tools, and routing logic into a single named unit for consistent behavior across apps
* **MCP Support**: Enhanced tool capabilities through Model Context Protocol servers
* **Multi-Provider**: Support for OpenAI, Anthropic, Google, xAI, and other providers

* **Initialization Order**: Always call `init()` before creating any Agno agents or teams
* **Environment Variables**: With `LANGDB_API_KEY` and `LANGDB_PROJECT_ID` set, you can create models with just `LangDB(id="model_name")`

* [LangDB Documentation](https://docs.langdb.ai/)
* [Building a Reasoning Finance Team Guide](https://docs.langdb.ai/guides/building-agents/building-a-reasoning-finance-team-with-agno)
* [LangDB GitHub Samples](https://github.com/langdb/langdb-samples/tree/main/examples/agno)
* [LangDB Dashboard](https://app.langdb.ai/)

By following these steps, you can effectively integrate Agno with LangDB, enabling comprehensive observability and monitoring of your AI agents.

---

## evaluation.run(print_results=True)

**URL:** llms-txt#evaluation.run(print_results=true)

---

## ✅ Good: Each user has isolated memories

**URL:** llms-txt#✅-good:-each-user-has-isolated-memories

**Contents:**
  - The Double-Enable Pitfall

agent.print_response("I love pizza", user_id="user_123")
agent.print_response("I'm allergic to dairy", user_id="user_456")
python  theme={null}

**Examples:**

Example 1 (unknown):
```unknown
**Best practice:** Always pass `user_id` explicitly, especially in multi-user applications.

### The Double-Enable Pitfall

**The Problem:** Using both `enable_user_memories=True` and `enable_agentic_memory=True` doesn't give you both—agentic mode overrides automatic mode.
```

---

## Clean workflow with clear branching

**URL:** llms-txt#clean-workflow-with-clear-branching

media_workflow = Workflow(
    name="AI Media Generation Workflow",
    description="Generate and analyze images or videos using AI agents",
    steps=[
        Router(
            name="Media Type Router",
            description="Routes to appropriate media generation pipeline",
            selector=media_sequence_selector,
            choices=[image_sequence, video_sequence],  # Clear choices
        )
    ],
)

---

## dependencies={

**URL:** llms-txt#dependencies={

---

## Example showing market analysis

**URL:** llms-txt#example-showing-market-analysis

agent.print_response(
    "What are the top gainers in the market today?"
)

---

## Example 7: Quick recipe check with new ingredients

**URL:** llms-txt#example-7:-quick-recipe-check-with-new-ingredients

**Contents:**
- Usage

print("Example 7: Quick Recipe Check with New Ingredients")
print("-" * 50)
shopping_team.print_response("What healthy breakfast can I make now?", stream=True)
print()

print(f"Team Session State: {shopping_team.get_session_state()}")
bash  theme={null}
    pip install agno
    bash  theme={null}
    export OPENAI_API_KEY=****
    bash  theme={null}
    python cookbook/examples/teams/state/team_with_nested_shared_state.py
    ```
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Install required libraries">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Set environment variables">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run the agent">
```

---

## Twilio

**URL:** llms-txt#twilio

**Contents:**
- Prerequisites
- Example
- Toolkit Params
- Toolkit Functions
- Developer Resources

Source: https://docs.agno.com/concepts/tools/toolkits/social/twilio

**TwilioTools** enables an Agent to interact with [Twilio](https://www.twilio.com/docs) services, such as sending SMS, retrieving call details, and listing messages.

The following examples require the `twilio` library and appropriate Twilio credentials, which can be obtained from [here](https://www.twilio.com/console).

Set the following environment variables:

The following agent will send an SMS message using Twilio:

| Name                      | Type            | Default | Description                                       |
| ------------------------- | --------------- | ------- | ------------------------------------------------- |
| `account_sid`             | `Optional[str]` | `None`  | Twilio Account SID for authentication.            |
| `auth_token`              | `Optional[str]` | `None`  | Twilio Auth Token for authentication.             |
| `api_key`                 | `Optional[str]` | `None`  | Twilio API Key for alternative authentication.    |
| `api_secret`              | `Optional[str]` | `None`  | Twilio API Secret for alternative authentication. |
| `region`                  | `Optional[str]` | `None`  | Optional Twilio region (e.g., `au1`).             |
| `edge`                    | `Optional[str]` | `None`  | Optional Twilio edge location (e.g., `sydney`).   |
| `debug`                   | `bool`          | `False` | Enable debug logging for troubleshooting.         |
| `enable_send_sms`         | `bool`          | `True`  | Enable the send\_sms functionality.               |
| `enable_get_call_details` | `bool`          | `True`  | Enable the get\_call\_details functionality.      |
| `enable_list_messages`    | `bool`          | `True`  | Enable the list\_messages functionality.          |
| `all`                     | `bool`          | `False` | Enable all functionality.                         |

| Function           | Description                                                                                                                                                                 |
| ------------------ | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| `send_sms`         | Sends an SMS to a recipient. Takes recipient phone number, sender number (Twilio), and message body. Returns message SID if successful or error message if failed.          |
| `get_call_details` | Retrieves details of a call using its SID. Takes the call SID and returns a dictionary with call details (e.g., status, duration).                                          |
| `list_messages`    | Lists recent SMS messages. Takes a limit for the number of messages to return (default 20). Returns a list of message details (e.g., SID, sender, recipient, body, status). |

## Developer Resources

* View [Tools](https://github.com/agno-agi/agno/blob/main/libs/agno/agno/tools/twilio.py)
* View [Cookbook](https://github.com/agno-agi/agno/tree/main/cookbook/tools/twilio_tools.py)

**Examples:**

Example 1 (unknown):
```unknown
Set the following environment variables:
```

Example 2 (unknown):
```unknown
## Example

The following agent will send an SMS message using Twilio:
```

---

## Not just: "send back", "item", "bought", "month"

**URL:** llms-txt#not-just:-"send-back",-"item",-"bought",-"month"

**Contents:**
  - Source Attribution

**Examples:**

Example 1 (unknown):
```unknown
### Source Attribution

Agents can provide references to where they found information, building trust and enabling verification.
```

---

## Set environment variables for Langfuse

**URL:** llms-txt#set-environment-variables-for-langfuse

LANGFUSE_AUTH = base64.b64encode(
    f"{os.getenv('LANGFUSE_PUBLIC_KEY')}:{os.getenv('LANGFUSE_SECRET_KEY')}".encode()
).decode()
os.environ["OTEL_EXPORTER_OTLP_ENDPOINT"] = "https://us.cloud.langfuse.com/api/public/otel"
os.environ["OTEL_EXPORTER_OTLP_HEADERS"] = f"Authorization=Basic {LANGFUSE_AUTH}"

---

## Custom Events

**URL:** llms-txt#custom-events

**Contents:**
  - Complete Example

Source: https://docs.agno.com/examples/concepts/teams/events/custom_events

Learn how to yield custom events from your own tools.

```python  theme={null}
import asyncio
from dataclasses import dataclass
from typing import Optional

from agno.agent import Agent
from agno.team import Team
from agno.models.openai import OpenAIChat
from agno.run.team import CustomEvent
from agno.tools import tool

---

## Get user timeline

**URL:** llms-txt#get-user-timeline

agent.print_response("Get my timeline", markdown=True)

---

## This is our tool, marked by the @tool decorator

**URL:** llms-txt#this-is-our-tool,-marked-by-the-@tool-decorator

@tool(stop_after_tool_call=True)
def get_weather(city: str) -> str:
    """Get the weather for the given city."""

# In a real implementation, this would call a weather API
    weather_conditions = ["sunny", "cloudy", "rainy", "snowy", "windy"]
    random_weather = random.choice(weather_conditions)

return f"The weather in {city} is {random_weather}."

---

## Prompt Caching

**URL:** llms-txt#prompt-caching

**Contents:**
- Usage
- Extended cache
- Working example

Source: https://docs.agno.com/examples/models/anthropic/prompt_caching

Learn how to use prompt caching with Anthropic models and Agno.

Prompt caching can help reducing processing time and costs. Consider it if you are using the same prompt multiple times in any flow.

You can read more about prompt caching with Anthropic models [here](https://docs.anthropic.com/en/docs/build-with-claude/prompt-caching).

To use prompt caching in your Agno setup, pass the `cache_system_prompt` argument when initializing the `Claude` model:

Notice that for prompt caching to work, the prompt needs to be of a certain length. You can read more about this on Anthropic's [docs](https://docs.anthropic.com/en/docs/build-with-claude/prompt-caching#cache-limitations).

You can also use Anthropic's extended cache beta feature. This updates the cache duration from 5 minutes to 1 hour. To activate it, pass the `extended_cache_time` argument and the following beta header:

```python cookbook/models/anthropic/prompt_caching_extended.py theme={null}
from pathlib import Path
from agno.agent import Agent
from agno.models.anthropic import Claude
from agno.utils.media import download_file

**Examples:**

Example 1 (unknown):
```unknown
Notice that for prompt caching to work, the prompt needs to be of a certain length. You can read more about this on Anthropic's [docs](https://docs.anthropic.com/en/docs/build-with-claude/prompt-caching#cache-limitations).

## Extended cache

You can also use Anthropic's extended cache beta feature. This updates the cache duration from 5 minutes to 1 hour. To activate it, pass the `extended_cache_time` argument and the following beta header:
```

Example 2 (unknown):
```unknown
## Working example
```

---

## Enable all ScrapeGraph functions

**URL:** llms-txt#enable-all-scrapegraph-functions

**Contents:**
- Toolkit Params
- Toolkit Functions
- Developer Resources

scrapegraph_all = Agent(
    tools=[
        ScrapeGraphTools(all=True, render_heavy_js=True)
    ],  # render_heavy_js=True scrapes all JavaScript
    model=agent_model,
    markdown=True,
    stream=True,
)

scrapegraph_all.print_response("""
Use any appropriate scraping method to extract comprehensive information from https://www.wired.com/category/science/:
- News articles and headlines
- Convert to markdown if needed  
- Search for specific information
""")
```

<Note>View the [Startup Analyst example](/examples/use-cases/agents/startup-analyst-agent) </Note>

| Parameter                | Type            | Default | Description                                                                                        |
| ------------------------ | --------------- | ------- | -------------------------------------------------------------------------------------------------- |
| `api_key`                | `Optional[str]` | `None`  | ScrapeGraph API key. If not provided, uses SGAI\_API\_KEY environment variable.                    |
| `enable_smartscraper`    | `bool`          | `True`  | Enable the smartscraper function for LLM-powered data extraction.                                  |
| `enable_markdownify`     | `bool`          | `False` | Enable the markdownify function for webpage to markdown conversion.                                |
| `enable_crawl`           | `bool`          | `False` | Enable the crawl function for website crawling and data extraction.                                |
| `enable_searchscraper`   | `bool`          | `False` | Enable the searchscraper function for web search and information extraction.                       |
| `enable_agentic_crawler` | `bool`          | `False` | Enable the agentic\_crawler function for automated browser actions and AI extraction.              |
| `enable_scrape`          | `bool`          | `False` | Enable the scrape function for retrieving raw HTML content from websites.                          |
| `render_heavy_js`        | `bool`          | `False` | Enable heavy JavaScript rendering for all scraping functions. Useful for SPAs and dynamic content. |
| `all`                    | `bool`          | `False` | Enable all available functions. When True, all enable flags are ignored.                           |

| Function          | Description                                                                                                                                                                                                            |
| ----------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| `smartscraper`    | Extract structured data from a webpage using LLM and natural language prompt. Parameters: url (str), prompt (str).                                                                                                     |
| `markdownify`     | Convert a webpage to markdown format. Parameters: url (str).                                                                                                                                                           |
| `crawl`           | Crawl a website and extract structured data. Parameters: url (str), prompt (str), data\_schema (dict), cache\_website (bool), depth (int), max\_pages (int), same\_domain\_only (bool), batch\_size (int).             |
| `searchscraper`   | Search the web and extract information. Parameters: user\_prompt (str).                                                                                                                                                |
| `agentic_crawler` | Perform automated browser actions with optional AI extraction. Parameters: url (str), steps (List\[str]), use\_session (bool), user\_prompt (Optional\[str]), output\_schema (Optional\[dict]), ai\_extraction (bool). |
| `scrape`          | Get raw HTML content from a website. Useful for complete source code retrieval and custom processing. Parameters: website\_url (str), headers (Optional\[dict]).                                                       |

## Developer Resources

* View [Tools](https://github.com/agno-agi/agno/blob/main/libs/agno/agno/tools/scrapegraph.py)
* View [Cookbook](https://github.com/agno-agi/agno/tree/main/cookbook/tools/scrapegraph_tools.py)
* View [Tests](https://github.com/agno-agi/agno/blob/main/libs/agno/tests/unit/tools/test_scrapegraph.py)

---

## Autonomous Startup Team

**URL:** llms-txt#autonomous-startup-team

**Contents:**
- Code
- Usage

Source: https://docs.agno.com/examples/use-cases/teams/autonomous_startup_team

This example shows how to create an autonomous startup team that can self-organize and drive innovative projects.

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Install required libraries">
    
  </Step>

<Step title="Set environment variables">
    
  </Step>

<Step title="Run the agent">
    
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Install required libraries">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Set environment variables">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run the agent">
```

---

## async def test_async():

**URL:** llms-txt#async-def-test_async():

---

## EVM (Ethereum Virtual Machine)

**URL:** llms-txt#evm-(ethereum-virtual-machine)

**Contents:**
- Example
- Toolkit Params
- Toolkit Functions
- Developer Resources

Source: https://docs.agno.com/concepts/tools/toolkits/others/evm

EvmTools enables agents to interact with Ethereum and EVM-compatible blockchains for transactions and smart contract operations.

The following agent can interact with Ethereum blockchain:

| Parameter                 | Type            | Default | Description                                                   |
| ------------------------- | --------------- | ------- | ------------------------------------------------------------- |
| `private_key`             | `Optional[str]` | `None`  | Private key for signing transactions. Uses EVM\_PRIVATE\_KEY. |
| `rpc_url`                 | `Optional[str]` | `None`  | RPC URL for blockchain connection. Uses EVM\_RPC\_URL.        |
| `enable_send_transaction` | `bool`          | `True`  | Enable transaction sending functionality.                     |

| Function           | Description                                                  |
| ------------------ | ------------------------------------------------------------ |
| `send_transaction` | Send ETH or interact with smart contracts on the blockchain. |
| `get_balance`      | Get ETH balance for an address.                              |
| `get_transaction`  | Get transaction details by hash.                             |
| `estimate_gas`     | Estimate gas cost for a transaction.                         |

## Developer Resources

* View [Tools Source](https://github.com/agno-agi/agno/blob/main/libs/agno/agno/tools/evm.py)
* [Web3.py Documentation](https://web3py.readthedocs.io/)
* [Ethereum Documentation](https://ethereum.org/developers/)

---

## Twilio Tools

**URL:** llms-txt#twilio-tools

**Contents:**
- Code
- Usage

Source: https://docs.agno.com/examples/concepts/tools/social/twilio

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Set your API key">
    
  </Step>

<Step title="Set your Twilio credentials">
    
  </Step>

<Step title="Install libraries">
    
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Set your API key">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Set your Twilio credentials">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 4 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

---

## Rename Session

**URL:** llms-txt#rename-session

Source: https://docs.agno.com/reference-api/schema/sessions/rename-session

post /sessions/{session_id}/rename
Update the name of an existing session. Useful for organizing and categorizing sessions with meaningful names for better identification and management.

---

## PostgresDb

**URL:** llms-txt#postgresdb

Source: https://docs.agno.com/reference/storage/postgres

`PostgresDb` is a class that implements the Db interface using PostgreSQL as the backend storage system. It provides robust, relational storage for agent sessions with support for JSONB data types, schema versioning, and efficient querying.

<Snippet file="db-postgres-params.mdx" />

<Snippet file="db-new-bulk-methods.mdx" />

---

## Stream Message

**URL:** llms-txt#stream-message

Source: https://docs.agno.com/reference-api/schema/a2a/stream-message

post /a2a/message/stream
Stream a message to an Agno Agent, Team, or Workflow.The Agent, Team or Workflow is identified via the 'agentId' field in params.message or X-Agent-ID header. Optional: Pass user ID via X-User-ID header (recommended) or 'userId' in params.message.metadata. Returns real-time updates as newline-delimited JSON (NDJSON).

---

## Initialize OpenLIT instrumentation

**URL:** llms-txt#initialize-openlit-instrumentation

import openlit
openlit.init(tracer=trace.get_tracer(__name__), disable_batch=True)

---

## Step 3: Define instructions

**URL:** llms-txt#step-3:-define-instructions

complete_instructions = dedent("""
    You are a Senior Social Media Intelligence Analyst specializing in cross-platform
    brand monitoring and strategic analysis.

DATA COLLECTION STRATEGY:
    - Use X Tools to gather direct social media mentions with full engagement metrics
    - Use Exa Tools to find broader web discussions, articles, and forum conversations
    - Cross-reference findings between social and web sources for comprehensive coverage

ANALYSIS FRAMEWORK:
    - Classify sentiment as Positive/Negative/Neutral/Mixed with detailed reasoning
    - Weight analysis by engagement volume and author influence (verified accounts = 1.5x)
    - Identify engagement patterns: viral advocacy, controversy, influence concentration
    - Extract cross-platform themes and recurring discussion points

INTELLIGENCE SYNTHESIS:
    - Detect crisis indicators through sentiment velocity and coordination patterns
    - Identify competitive positioning and feature gap discussions
    - Surface growth opportunities and advocacy moments
    - Generate strategic recommendations with clear priority levels

### Executive Dashboard
    - **Brand Health Score**: [1-10] with supporting evidence
    - **Net Sentiment**: [%positive - %negative] with trend analysis
    - **Key Drivers**: Top 3 positive and negative factors
    - **Alert Level**: Normal/Monitor/Crisis with threshold reasoning

### Quantitative Metrics
    | Sentiment | Posts | % | Avg Engagement | Influence Score |
    |-----------|-------|---|----------------|-----------------|
    [Detailed breakdown with engagement weighting]

### Strategic Recommendations
    **IMMEDIATE (≤48h)**: Crisis response, high-impact replies
    **SHORT-TERM (1-2 weeks)**: Content strategy, community engagement
    **LONG-TERM (1-3 months)**: Product positioning, market strategy

ANALYSIS PRINCIPLES:
    - Evidence-based conclusions with supporting metrics
    - Actionable insights that drive business decisions
    - Cross-platform correlation analysis
    - Influence-weighted sentiment scoring
    - Proactive risk and opportunity identification
""")

---

## Delete specific content

**URL:** llms-txt#delete-specific-content

knowledge.remove_content_by_id(content_id)

---

## print("John Doe's memories:")

**URL:** llms-txt#print("john-doe's-memories:")

---

## PIIDetectionGuardrail

**URL:** llms-txt#piidetectionguardrail

**Contents:**
- Parameters

Source: https://docs.agno.com/reference/hooks/pii-guardrail

| Parameter                  | Type   | Default | Description                                                                           |
| -------------------------- | ------ | ------- | ------------------------------------------------------------------------------------- |
| `mask_pii`                 | `bool` | `False` | Whether to mask the PII in the input, rather than raising an error.                   |
| `enable_ssn_check`         | `bool` | `True`  | Whether to check for Social Security Numbers.                                         |
| `enable_credit_card_check` | `bool` | `True`  | Whether to check for credit cards.                                                    |
| `enable_email_check`       | `bool` | `True`  | Whether to check for emails.                                                          |
| `enable_phone_check`       | `bool` | `True`  | Whether to check for phone numbers.                                                   |
| `custom_patterns`          | `dict` | `{}`    | A dictionary of custom PII patterns to detect. This is added to the default patterns. |

---

## Create multi-purpose reasoning team

**URL:** llms-txt#create-multi-purpose-reasoning-team

**Contents:**
- Usage

agent_team = Team(
    name="Multi-Purpose Agent Team",
    model=Claude(id="claude-3-5-sonnet-latest"),
    tools=[ReasoningTools()],  # Enable reasoning capabilities
    members=[
        web_agent,
        finance_agent,
        medical_agent,
        calculator_agent,
        agno_assist,
        code_agent,
    ],
    instructions=[
        "You are a team of agents that can answer a variety of questions.",
        "Use reasoning tools to analyze questions before delegating.",
        "You can answer directly or forward to appropriate specialist agents.",
        "For complex questions, reason about the best approach first.",
        "If the user is just being conversational, respond directly without tools.",
    ],
    markdown=True,
    show_members_responses=True,
    share_member_interactions=True,
)

async def main():
    """Main async function to demonstrate different team capabilities."""

# Add Agno documentation content
    await agno_assist_knowledge.add_contents_async(url="https://docs.agno.com/llms-full.txt")

# Example interactions:

# 1. General capability query
    await agent_team.aprint_response(input="Hi! What are you capable of doing?")

# 2. Technical code question
    # await agent_team.aprint_response(dedent("""
    #     Create a minimal Agno Agent that searches Hacker News for articles.
    #     Test it locally and save it as './python/hacker_news_agent.py'.
    #     Use real Agno documentation, don't mock anything.
    # """), stream=True)

# 3. Financial research
    # await agent_team.aprint_response(dedent("""
    #     What should I be investing in right now?
    #     Research current market trends and write a detailed report
    #     suitable for a financial advisor.
    # """), stream=True)

# 4. Medical analysis (using external medical history file)
    # txt_path = Path(__file__).parent.resolve() / "medical_history.txt"
    # if txt_path.exists():
    #     loaded_txt = open(txt_path, "r").read()
    #     await agent_team.aprint_response(
    #         f"Analyze this medical information and suggest a likely diagnosis:\n{loaded_txt}",
    #         stream=True,
    #     )

if __name__ == "__main__":
    asyncio.run(main())
bash  theme={null}
    pip install agno lancedb exa_py ddgs pubmed-parser e2b
    bash  theme={null}
    export OPENAI_API_KEY=****
    export ANTHROPIC_API_KEY=****
    export E2B_API_KEY=****
    export EXA_API_KEY=****
    bash  theme={null}
    python cookbook/examples/teams/reasoning/02_async_multi_purpose_reasoning_team.py
    ```
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Install required libraries">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Set environment variables">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run the agent">
```

---

## Run a keyword-based query

**URL:** llms-txt#run-a-keyword-based-query

**Contents:**
- Usage

results = keyword_db.search("chicken coconut soup", limit=5)
print("Keyword Search Results:", results)
bash  theme={null}
    pip install -U agno sqlalchemy psycopg pgvector
    bash Mac theme={null}
      python cookbook/knowledge/search_type/keyword_search.py
      bash Windows theme={null}
      python cookbook/knowledge/search_type/keyword_search.py
      ```
    </CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Step title="Install libraries">
```

Example 2 (unknown):
```unknown
</Step>

  <Snippet file="run-pgvector-step.mdx" />

  <Step title="Run the example">
    <CodeGroup>
```

Example 3 (unknown):
```unknown

```

---

## List Sessions

**URL:** llms-txt#list-sessions

Source: https://docs.agno.com/reference-api/schema/sessions/list-sessions

get /sessions
Retrieve paginated list of sessions with filtering and sorting options. Supports filtering by session type (agent, team, workflow), component, user, and name. Sessions represent conversation histories and execution contexts.

---

## You can also set the debug mode on a single run

**URL:** llms-txt#you-can-also-set-the-debug-mode-on-a-single-run

**Contents:**
- Usage

agent = Agent(
    model=OpenAIChat(id="gpt-5-mini"),
)
agent.print_response(input="Tell me a joke.", debug_mode=True)
bash  theme={null}
    pip install -U agno openai
    bash Mac/Linux theme={null}
        export OPENAI_API_KEY="your_openai_api_key_here"
      bash Windows theme={null}
        $Env:OPENAI_API_KEY="your_openai_api_key_here"
      bash  theme={null}
    touch debug.py
    bash Mac theme={null}
      python debug.py
      bash Windows theme={null}
      python debug.py
      ```
    </CodeGroup>
  </Step>

<Step title="Find All Cookbooks">
    Explore all the available cookbooks in the Agno repository. Click the link below to view the code on GitHub:

<Link href="https://github.com/agno-agi/agno/tree/main/cookbook/agents/other" target="_blank">
      Agno Cookbooks on GitHub
    </Link>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Install libraries">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Export your OpenAI API key">
    <CodeGroup>
```

Example 3 (unknown):
```unknown

```

Example 4 (unknown):
```unknown
</CodeGroup>
  </Step>

  <Step title="Create a Python file">
    Create a Python file and add the above code.
```

---

## Vertex AI Claude

**URL:** llms-txt#vertex-ai-claude

**Contents:**
- Authentication
- Example
- Parameters

Source: https://docs.agno.com/concepts/models/vertexai-claude

Learn how to use Vertex AI Claude models with Agno.

Use Claude models through Vertex AI. This provides a native Claude integration optimized for Vertex AI infrastructure.

We recommend experimenting to find the best-suited model for your use-case. Here are some general recommendations:

* `claude-sonnet-4@20250514` model is good for most use-cases.
* `claude-opus-4@20250805` model is their best model.

Set your `GOOGLE_CLOUD_PROJECT` and `CLOUD_ML_REGION` environment variables.

And then authenticate your CLI session:

Use `Claude` with your `Agent`:

<CodeGroup>
  
</CodeGroup>

<Note>View more examples [here](/examples/models/vertexai/claude/basic). </Note>

| Parameter    | Type  | Default                      | Description                                        |
| ------------ | ----- | ---------------------------- | -------------------------------------------------- |
| `id`         | `str` | `"claude-sonnet-4@20250514"` | The specific Vertex AI Claude model ID to use      |
| `name`       | `str` | `"Claude"`                   | The name identifier for the Vertex AI Claude model |
| `provider`   | `str` | `"VertexAI"`                 | The provider of the model                          |
| `region`     | `str` | `"None`                      | The region to use for the model                    |
| `project_id` | `str` | `None`                       | The project ID to use for the model                |
| `base_url`   | `str` | `None`                       | The base URL to use for the model                  |

`Claude` (Vertex AI) extends the [Anthropic Claude](/concepts/models/anthropic) model with Vertex AI integration and has access to most of the same parameters.

**Examples:**

Example 1 (unknown):
```unknown

```

Example 2 (unknown):
```unknown
</CodeGroup>

And then authenticate your CLI session:
```

Example 3 (unknown):
```unknown
## Example

Use `Claude` with your `Agent`:

<CodeGroup>
```

---

## Initialize the ZepTools

**URL:** llms-txt#initialize-the-zeptools

zep_tools = ZepTools(user_id="agno", session_id="agno-session", add_instructions=True)

---

## Example usage with diverse queries

**URL:** llms-txt#example-usage-with-diverse-queries

agent_team.print_response(
    input="Summarize analyst recommendations and share the latest news for NVDA",
    stream=True,
)
agent_team.print_response(
    input="What's the market outlook and financial performance of AI semiconductor companies?",
    stream=True,
)
agent_team.print_response(
    input="Analyze recent developments and financial performance of TSLA",
    stream=True,
)

---

## Cohere

**URL:** llms-txt#cohere

**Contents:**
- Parameters

Source: https://docs.agno.com/reference/models/cohere

The Cohere model provides access to Cohere's language models.

| Parameter           | Type                       | Default                    | Description                                                   |
| ------------------- | -------------------------- | -------------------------- | ------------------------------------------------------------- |
| `id`                | `str`                      | `"command-r-plus-08-2024"` | The id of the Cohere model to use                             |
| `name`              | `str`                      | `"CohereChat"`             | The name of the model                                         |
| `provider`          | `str`                      | `"Cohere"`                 | The provider of the model                                     |
| `api_key`           | `Optional[str]`            | `None`                     | The API key for Cohere (defaults to COHERE\_API\_KEY env var) |
| `max_tokens`        | `Optional[int]`            | `None`                     | Maximum number of tokens to generate                          |
| `temperature`       | `Optional[float]`          | `None`                     | Controls randomness in the model's output (0.0 to 1.0)        |
| `p`                 | `Optional[float]`          | `None`                     | Controls diversity via nucleus sampling (0.0 to 1.0)          |
| `k`                 | `Optional[int]`            | `None`                     | Controls diversity via top-k sampling                         |
| `seed`              | `Optional[int]`            | `None`                     | Random seed for deterministic sampling                        |
| `frequency_penalty` | `Optional[float]`          | `None`                     | Reduces repetition by penalizing frequent tokens (0.0 to 1.0) |
| `presence_penalty`  | `Optional[float]`          | `None`                     | Reduces repetition by penalizing present tokens (0.0 to 1.0)  |
| `stop_sequences`    | `Optional[List[str]]`      | `None`                     | List of strings that stop generation                          |
| `response_format`   | `Optional[Dict[str, Any]]` | `None`                     | Specifies the format of the response (e.g., JSON)             |
| `citation_options`  | `Optional[Dict[str, Any]]` | `None`                     | Options for citation generation                               |
| `request_params`    | `Optional[Dict[str, Any]]` | `None`                     | Additional parameters to include in the request               |
| `client_params`     | `Optional[Dict[str, Any]]` | `None`                     | Additional parameters for client configuration                |

---

## Configure SingleStore DB connection

**URL:** llms-txt#configure-singlestore-db-connection

USERNAME = getenv("SINGLESTORE_USERNAME")
PASSWORD = getenv("SINGLESTORE_PASSWORD")
HOST = getenv("SINGLESTORE_HOST")
PORT = getenv("SINGLESTORE_PORT")
DATABASE = getenv("SINGLESTORE_DATABASE")
SSL_CERT = getenv("SINGLESTORE_SSL_CERT", None)

db_url = (
    f"mysql+pymysql://{USERNAME}:{PASSWORD}@{HOST}:{PORT}/{DATABASE}?charset=utf8mb4"
)
db = SingleStoreDb(db_url=db_url)

---

## Groq

**URL:** llms-txt#groq

**Contents:**
- Parameters

Source: https://docs.agno.com/reference/models/groq

The Groq model provides access to Groq's high-performance language models.

| Parameter  | Type            | Default                            | Description                                               |
| ---------- | --------------- | ---------------------------------- | --------------------------------------------------------- |
| `id`       | `str`           | `"llama-3.3-70b-versatile"`        | The id of the Groq model to use                           |
| `name`     | `str`           | `"Groq"`                           | The name of the model                                     |
| `provider` | `str`           | `"Groq"`                           | The provider of the model                                 |
| `api_key`  | `Optional[str]` | `None`                             | The API key for Groq (defaults to GROQ\_API\_KEY env var) |
| `base_url` | `str`           | `"https://api.groq.com/openai/v1"` | The base URL for the Groq API                             |

Groq extends the OpenAI-compatible interface and supports most parameters from OpenAI.

---

## Example usage:

**URL:** llms-txt#example-usage:

analysis_prompt = """\
Analyze media trends for:
Keywords: ai agents
Sources: verge.com ,linkedin.com, x.com
"""

agent.print_response(analysis_prompt, stream=True)

---

## Initialize PgVector

**URL:** llms-txt#initialize-pgvector

db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"

vector_db = PgVector(table_name="recipes", db_url=db_url)

---

## ag infra restart

**URL:** llms-txt#ag-infra-restart

**Contents:**
- Params

Source: https://docs.agno.com/reference/agno-infra/cli/ws/restart

Restart resources for active infra

<ResponseField name="resources_filter" type="str">
  Resource filter. Format - ENV:INFRA:GROUP:NAME:TYPE
</ResponseField>

<ResponseField name="env_filter" type="str">
  Filter the environment to deploy `--env` `-e`
</ResponseField>

<ResponseField name="infra_filter" type="str">
  Filter the infra to deploy. `--infra` `-i`
</ResponseField>

<ResponseField name="group_filter" type="str">
  Filter resources using group name. `--group` `-g`
</ResponseField>

<ResponseField name="name_filter" type="str">
  Filter resource using name. `--name` `-n`
</ResponseField>

<ResponseField name="type_filter" type="str">
  Filter resource using type `--type` `-t`
</ResponseField>

<ResponseField name="dry_run" type="bool">
  Print resources and exit. `--dry-run` `-dr`
</ResponseField>

<ResponseField name="auto_confirm" type="bool">
  Skip the confirmation before deploying resources. `--yes` `-y`
</ResponseField>

<ResponseField name="print_debug_log" type="bool">
  Print debug logs. `--debug` `-d`
</ResponseField>

<ResponseField name="force" type="bool">
  Force `--force` `-f`
</ResponseField>

<ResponseField name="pull" type="bool">
  Pull `--pull` `-p`
</ResponseField>

---

## Run workflow and display events

**URL:** llms-txt#run-workflow-and-display-events

**Contents:**
  - Async Streaming

for event in workflow.run(
    "What is Python?",
    stream=True,
    stream_events=True,
):
    event_name = event.event if hasattr(event, "event") else type(event).__name__
    print(f"  → {event_name}")
Python  theme={null}

**Examples:**

Example 1 (unknown):
```unknown
### Async Streaming

The `Workflow.arun(stream=True)` returns an async iterator of `WorkflowRunOutputEvent` objects instead of a single response.
So for example, if you want to stream the response, you can do the following:
```

---

## Linkup Tools

**URL:** llms-txt#linkup-tools

**Contents:**
- Code
- Usage

Source: https://docs.agno.com/examples/concepts/tools/search/linkup

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Set your API keys">
    
  </Step>

<Step title="Install libraries">
    
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Set your API keys">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Sentence Transformer

**URL:** llms-txt#sentence-transformer

Source: https://docs.agno.com/reference/knowledge/embedder/sentence-transformer

Sentence Transformer Embedder is a class that allows you to embed documents using Hugging Face's sentence-transformers library, providing access to a wide range of open-source embedding models that can run locally.

<Snippet file="embedder-sentence-transformer-reference.mdx" />

---

## Will load the session state from the session with the id "user_2_session_1"

**URL:** llms-txt#will-load-the-session-state-from-the-session-with-the-id-"user_2_session_1"

**Contents:**
- Usage

team.print_response("How old am I?", session_id="user_2_session_1", user_id="user_2")
bash  theme={null}
    pip install agno openai
    bash  theme={null}
    export OPENAI_API_KEY=****
    bash  theme={null}
    python cookbook/examples/teams/state/change_state_on_run.py
    ```
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Install required libraries">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Set environment variables">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run the agent">
```

---

## SessionSummaryManager

**URL:** llms-txt#sessionsummarymanager

**Contents:**
- SessionSummaryManager Attributes
- SessionSummaryManager Methods
  - `create_session_summary(session: Union[AgentSession, TeamSession]) -> Optional[SessionSummary]`
  - `acreate_session_summary(session: Union[AgentSession, TeamSession]) -> Optional[SessionSummary]`
- SessionSummary Object

Source: https://docs.agno.com/reference/session/summary_manager

The `SessionSummaryManager` is responsible for generating and managing session summaries. It uses a model to analyze conversations and create concise summaries with optional topic extraction.

## SessionSummaryManager Attributes

| Parameter                 | Type              | Default                                      | Description                                                                        |
| ------------------------- | ----------------- | -------------------------------------------- | ---------------------------------------------------------------------------------- |
| `model`                   | `Optional[Model]` | `None`                                       | Model used for session summary generation                                          |
| `session_summary_prompt`  | `Optional[str]`   | `None`                                       | Custom prompt for session summary generation. If not provided, uses default prompt |
| `summary_request_message` | `str`             | `"Provide the summary of the conversation."` | User message prompt for requesting the summary                                     |
| `summaries_updated`       | `bool`            | `False`                                      | Whether session summaries were created in the last run                             |

## SessionSummaryManager Methods

### `create_session_summary(session: Union[AgentSession, TeamSession]) -> Optional[SessionSummary]`

Creates a summary of the session synchronously.

* `session`: The agent or team session to summarize

* `Optional[SessionSummary]`: A SessionSummary object containing the summary text, topics, and timestamp, or None if generation fails

### `acreate_session_summary(session: Union[AgentSession, TeamSession]) -> Optional[SessionSummary]`

Creates a summary of the session asynchronously.

* `session`: The agent or team session to summarize

* `Optional[SessionSummary]`: A SessionSummary object containing the summary text, topics, and timestamp, or None if generation fails

## SessionSummary Object

The `SessionSummary` object returned by the summary manager contains:

| Attribute    | Type                  | Description                                                      |
| ------------ | --------------------- | ---------------------------------------------------------------- |
| `summary`    | `str`                 | Concise summary of the session focusing on important information |
| `topics`     | `Optional[List[str]]` | List of topics discussed in the session                          |
| `updated_at` | `Optional[datetime]`  | Timestamp when the summary was created                           |

---

## Add PDF content synchronously

**URL:** llms-txt#add-pdf-content-synchronously

knowledge.add_content(
    path="cookbook/knowledge/testing_resources/cv_1.pdf",
    reader=PDFReader(),
)

---

## Automatic reader selection based on file extension

**URL:** llms-txt#automatic-reader-selection-based-on-file-extension

reader = ReaderFactory.get_reader_for_extension(".pdf")  # Returns PDFReader
reader = ReaderFactory.get_reader_for_extension(".csv")  # Returns CSVReader

---

## Initialize Upstash DB

**URL:** llms-txt#initialize-upstash-db

knowledge = Knowledge(
    name="Basic SDK Knowledge Base",
    description="Agno 2.0 Knowledge Implementation with Upstash Vector DB",
    vector_db=vector_db,
)

---

## E2B Code Execution

**URL:** llms-txt#e2b-code-execution

**Contents:**
- Code

Source: https://docs.agno.com/examples/concepts/tools/others/e2b

Learn to use Agno's E2B integration to run your Agent-generated code in a secure sandbox.

```python cookbook/tools/e2b_tools.py theme={null}
from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.e2b import E2BTools

e2b_tools = E2BTools(
    timeout=600,  # 10 minutes timeout (in seconds)
)

agent = Agent(
    name="Code Execution Sandbox",
    id="e2b-sandbox",
    model=OpenAIChat(id="gpt-5-mini"),
    tools=[e2b_tools],
    markdown=True,
        instructions=[
        "You are an expert at writing and validating Python code using a secure E2B sandbox environment.",
        "Your primary purpose is to:",
        "1. Write clear, efficient Python code based on user requests",
        "2. Execute and verify the code in the E2B sandbox",
        "3. Share the complete code with the user, as this is the main use case",
        "4. Provide thorough explanations of how the code works",
    ],
)

---

## Create team with metrics tracking enabled

**URL:** llms-txt#create-team-with-metrics-tracking-enabled

team = Team(
    name="Stock Research Team",
    model=OpenAIChat("gpt-5-mini"),
    members=[stock_searcher],
    db=db,  # Database required for session metrics
    session_id="team_metrics_demo",
    markdown=True,
    show_members_responses=True,
    store_member_responses=True,
)

---

## Create the company information gathering team

**URL:** llms-txt#create-the-company-information-gathering-team

**Contents:**
- Usage

company_info_team = Team(
    name="Company Info Team",
    id=id,
    model=Claude(id="claude-3-7-sonnet-latest"),
    tools=[AgentQLTools(agentql_query=custom_query)],
    members=[
        wikipedia_agent,
        website_agent,
    ],
    markdown=True,
    instructions=[
        "You are a team that finds information about a company.",
        "First search the web and wikipedia for information about the company.",
        "If you can find the company's website URL, then scrape the homepage and the about page.",
    ],
    show_members_responses=True,
)

if __name__ == "__main__":
    asyncio.run(
        company_info_team.aprint_response(
            "Write me a full report on everything you can find about Agno, the company building AI agent infrastructure.",
            stream=True,
            stream_events=True,
        )
    )
bash  theme={null}
    pip install agno wikipedia ddgs agentql
    bash  theme={null}
    export OPENAI_API_KEY=****
    export ANTHROPIC_API_KEY=****
    export MISTRAL_API_KEY=****
    export AGENTQL_API_KEY=****
    bash  theme={null}
    python cookbook/examples/teams/tools/03_async_team_with_tools.py
    ```
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Install required libraries">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Set environment variables">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run the agent">
```

---

## Step 1: Configure Atla

**URL:** llms-txt#step-1:-configure-atla

configure(token=getenv("ATLA_API_KEY"))

---

## Asynchronous reading - better for I/O intensive operations

**URL:** llms-txt#asynchronous-reading---better-for-i/o-intensive-operations

documents = await reader.async_read("file.pdf")

---

## And automatically recalled here

**URL:** llms-txt#and-automatically-recalled-here

**Contents:**
  - Agentic Memory (`enable_agentic_memory=True`)

agent.print_response("What's the best way to reach me?")
python  theme={null}
from agno.agent import Agent
from agno.db.sqlite import SqliteDb

**Examples:**

Example 1 (unknown):
```unknown
**Best for:** Customer support, personal assistants, conversational apps where you want consistent memory behavior.

### Agentic Memory (`enable_agentic_memory=True`)

The agent gets full control over memory management through built-in tools. It decides when to create, update, or delete memories based on the conversation context.
```

---

## Audio input requires specific audio-enabled models like gpt-5-mini-audio-preview

**URL:** llms-txt#audio-input-requires-specific-audio-enabled-models-like-gpt-5-mini-audio-preview

**Contents:**
- Usage

agent = Agent(
    model=LiteLLM(id="gpt-5-mini-audio-preview"),
    markdown=True,
)
agent.print_response(
    "What's the audio about?",
    audio=[Audio(content=mp3_data, format="mp3")],
    stream=True,
)

bash  theme={null}
    export LITELLM_API_KEY=xxx
    bash  theme={null}
    pip install -U litellm agno
    bash Mac theme={null}
      python cookbook/models/litellm/audio_input_agent.py
      bash Windows theme={null}
      python cookbook/models/litellm/audio_input_agent.py
      ```
    </CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Set your API key">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Skip files you've already processed

**URL:** llms-txt#skip-files-you've-already-processed

knowledge.add_content(
    path="large_document.pdf",
    skip_if_exists=True,  # Don't reprocess existing files
    upsert=False          # Don't update existing
)

---

## Custom Loggers

**URL:** llms-txt#custom-loggers

**Contents:**
- Example

Source: https://docs.agno.com/concepts/teams/custom-logger

Learn how to use custom loggers in your Agno setup.

You can provide your own loggers to Agno, to be used instead of the default ones.

This can be useful if you need your system to log in any specific format.

```python  theme={null}
import logging

from agno.agent import Agent
from agno.team import Team
from agno.utils.log import configure_agno_logging, log_info

---

## Configure X Tools for social media data

**URL:** llms-txt#configure-x-tools-for-social-media-data

**Contents:**
  - 2b. Add ExaTools for Web Intelligence

x_tools = XTools(
    include_post_metrics=True,    # Critical: gets likes, retweets, replies for influence analysis
    wait_on_rate_limit=True,      # Handles API limits gracefully
)

print("XTools configured with post metrics enabled")
python  theme={null}
from agno.tools.exa import ExaTools

**Examples:**

Example 1 (unknown):
```unknown
**What `include_post_metrics=True` gives you:**

* Like counts (engagement volume)
* Retweet counts (viral spread)
* Reply counts (conversation depth)
* Author verification status (influence weighting)

### 2b. Add ExaTools for Web Intelligence

**Why ExaTools?** Social media discussions often reference broader conversations happening across the web. ExaTools finds this context.
```

---

## Tavily Tools

**URL:** llms-txt#tavily-tools

**Contents:**
- Code
- Usage

Source: https://docs.agno.com/examples/concepts/tools/search/tavily

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Set your API keys">
    
  </Step>

<Step title="Install libraries">
    
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Set your API keys">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Protected route

**URL:** llms-txt#protected-route

@app.get("/protected", dependencies=[Depends(verify_token)])
async def protected_endpoint():
    return {"message": "Access granted"}

---

## Example 6: Complete list management

**URL:** llms-txt#example-6:-complete-list-management

print("Example 6: Complete List Reset & Restart")
print("-" * 50)
shopping_team.print_response(
    "Clear everything from my list and start over with just bananas and yogurt",
    stream=True,
)
print(f"Shared Session state: {shopping_team.get_session_state()}")
print()

---

## ag infra create

**URL:** llms-txt#ag-infra-create

**Contents:**
- Params

Source: https://docs.agno.com/reference/agno-infra/cli/ws/create

Create a new infra in the current directory.

<ResponseField name="name" type="str">
  Name of the new infra. `--name` `-n`
</ResponseField>

<ResponseField name="template" type="str">
  Starter template for the infra. `--template` `-t`
</ResponseField>

<ResponseField name="url" type="str">
  URL of the starter template. `--url` `-u`
</ResponseField>

<ResponseField name="print_debug_log" type="bool">
  Print debug logs. `--debug` `-d`
</ResponseField>

---

## Get Content Status

**URL:** llms-txt#get-content-status

Source: https://docs.agno.com/reference-api/schema/knowledge/get-content-status

get /knowledge/content/{content_id}/status
Retrieve the current processing status of a content item. Useful for monitoring asynchronous content processing progress and identifying any processing errors.

---

## Apple Metal (macOS)

**URL:** llms-txt#apple-metal-(macos)

---

## Slow: Search everything

**URL:** llms-txt#slow:-search-everything

results = knowledge.search("deployment process", max_results=10)

---

## input=ResearchTopic(

**URL:** llms-txt#input=researchtopic(

---

## Add content with metadata

**URL:** llms-txt#add-content-with-metadata

knowledge.add_content(
    name="Recipes",
    url="https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf",
    metadata={"doc_type": "recipe_book"},
)

---

## },

**URL:** llms-txt#},

---

## Background Workflow Execution

**URL:** llms-txt#background-workflow-execution

**Contents:**
- Example

Source: https://docs.agno.com/concepts/workflows/background-execution

How to execute workflows as non-blocking background tasks

Execute workflows as non-blocking background tasks by passing `background=True` to `Workflow.arun()`. This returns a `WorkflowRunOutput` object with a `run_id` for polling the workflow status until completion.

<Note>
  Background execution requires async workflows using `.arun()`. Poll for results using `workflow.get_run(run_id)` and check completion status with `.has_completed()`.

Ideal for long-running operations like large-scale data processing, multi-step research, or batch operations that shouldn't block your main application thread.
</Note>

```python  theme={null}
import asyncio

from agno.agent import Agent
from agno.db.sqlite import SqliteDb
from agno.models.openai import OpenAIChat
from agno.team import Team
from agno.tools.googlesearch import GoogleSearchTools
from agno.tools.hackernews import HackerNewsTools
from agno.utils.pprint import pprint_run_response
from agno.workflow.step import Step
from agno.workflow.workflow import Workflow

---

## OpenAI

**URL:** llms-txt#openai

**Contents:**
- Parameters

Source: https://docs.agno.com/reference/models/openai

The OpenAIChat model provides access to OpenAI models like GPT-4o.

| Parameter               | Type                                               | Default        | Description                                                                        |
| ----------------------- | -------------------------------------------------- | -------------- | ---------------------------------------------------------------------------------- |
| `id`                    | `str`                                              | `"gpt-4o"`     | The id of the OpenAI model to use                                                  |
| `name`                  | `str`                                              | `"OpenAIChat"` | The name of the model                                                              |
| `provider`              | `str`                                              | `"OpenAI"`     | The provider of the model                                                          |
| `store`                 | `Optional[bool]`                                   | `None`         | Whether to store the conversation for training purposes                            |
| `reasoning_effort`      | `Optional[str]`                                    | `None`         | The reasoning effort level for o1 models ("low", "medium", "high")                 |
| `verbosity`             | `Optional[Literal["low", "medium", "high"]]`       | `None`         | Controls verbosity level of reasoning models                                       |
| `metadata`              | `Optional[Dict[str, Any]]`                         | `None`         | Developer-defined metadata to associate with the completion                        |
| `frequency_penalty`     | `Optional[float]`                                  | `None`         | Penalizes new tokens based on their frequency in the text so far (-2.0 to 2.0)     |
| `logit_bias`            | `Optional[Any]`                                    | `None`         | Modifies the likelihood of specified tokens appearing in the completion            |
| `logprobs`              | `Optional[bool]`                                   | `None`         | Whether to return log probabilities of the output tokens                           |
| `top_logprobs`          | `Optional[int]`                                    | `None`         | Number of most likely tokens to return log probabilities for (0 to 20)             |
| `max_tokens`            | `Optional[int]`                                    | `None`         | Maximum number of tokens to generate (deprecated, use max\_completion\_tokens)     |
| `max_completion_tokens` | `Optional[int]`                                    | `None`         | Maximum number of completion tokens to generate                                    |
| `modalities`            | `Optional[List[str]]`                              | `None`         | List of modalities to use ("text" and/or "audio")                                  |
| `audio`                 | `Optional[Dict[str, Any]]`                         | `None`         | Audio configuration (e.g., `{"voice": "alloy", "format": "wav"}`)                  |
| `presence_penalty`      | `Optional[float]`                                  | `None`         | Penalizes new tokens based on whether they appear in the text so far (-2.0 to 2.0) |
| `seed`                  | `Optional[int]`                                    | `None`         | Random seed for deterministic sampling                                             |
| `stop`                  | `Optional[Union[str, List[str]]]`                  | `None`         | Up to 4 sequences where the API will stop generating further tokens                |
| `temperature`           | `Optional[float]`                                  | `None`         | Controls randomness in the model's output (0.0 to 2.0)                             |
| `user`                  | `Optional[str]`                                    | `None`         | A unique identifier representing your end-user                                     |
| `top_p`                 | `Optional[float]`                                  | `None`         | Controls diversity via nucleus sampling (0.0 to 1.0)                               |
| `service_tier`          | `Optional[str]`                                    | `None`         | Service tier to use ("auto", "default", "flex", "priority")                        |
| `extra_headers`         | `Optional[Any]`                                    | `None`         | Additional headers to include in requests                                          |
| `extra_query`           | `Optional[Any]`                                    | `None`         | Additional query parameters to include in requests                                 |
| `extra_body`            | `Optional[Any]`                                    | `None`         | Additional body parameters to include in requests                                  |
| `request_params`        | `Optional[Dict[str, Any]]`                         | `None`         | Additional parameters to include in the request                                    |
| `role_map`              | `Optional[Dict[str, str]]`                         | `None`         | Mapping of message roles to OpenAI roles                                           |
| `api_key`               | `Optional[str]`                                    | `None`         | The API key for authenticating with OpenAI (defaults to OPENAI\_API\_KEY env var)  |
| `organization`          | `Optional[str]`                                    | `None`         | The organization ID to use for requests                                            |
| `base_url`              | `Optional[Union[str, httpx.URL]]`                  | `None`         | The base URL for the OpenAI API                                                    |
| `timeout`               | `Optional[float]`                                  | `None`         | Request timeout in seconds                                                         |
| `max_retries`           | `Optional[int]`                                    | `None`         | Maximum number of retries for failed requests                                      |
| `default_headers`       | `Optional[Any]`                                    | `None`         | Default headers to include in all requests                                         |
| `default_query`         | `Optional[Any]`                                    | `None`         | Default query parameters to include in all requests                                |
| `http_client`           | `Optional[Union[httpx.Client, httpx.AsyncClient]]` | `None`         | HTTP client instance for making requests                                           |
| `client_params`         | `Optional[Dict[str, Any]]`                         | `None`         | Additional parameters for client configuration                                     |

---

## Start instrumenting agno

**URL:** llms-txt#start-instrumenting-agno

AgnoInstrumentor().instrument()

---

## Sleep Tools

**URL:** llms-txt#sleep-tools

**Contents:**
- Code
- Usage

Source: https://docs.agno.com/examples/concepts/tools/local/sleep

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Set your API key">
    
  </Step>

<Step title="Install libraries">
    
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Set your API key">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Couchbase connection settings

**URL:** llms-txt#couchbase-connection-settings

username = os.getenv("COUCHBASE_USER")
password = os.getenv("COUCHBASE_PASSWORD")
connection_string = os.getenv("COUCHBASE_CONNECTION_STRING")

---

## add_dependencies_to_context=True,

**URL:** llms-txt#add_dependencies_to_context=true,

---

## Create team with async streaming capabilities

**URL:** llms-txt#create-team-with-async-streaming-capabilities

**Contents:**
- Usage

team = Team(
    name="Stock Research Team",
    model=OpenAIChat("gpt-5-mini"),
    members=[stock_searcher, company_info_agent],
    markdown=True,
    show_members_responses=True,
)

async def streaming_with_arun():
    """Demonstrate async streaming using arun() method."""
    await apprint_run_response(
        team.arun(input="What is the current stock price of NVDA?", stream=True)
    )

async def streaming_with_aprint_response():
    """Demonstrate async streaming using aprint_response() method."""
    await team.aprint_response("What is the current stock price of NVDA?", stream=True)

if __name__ == "__main__":
    asyncio.run(streaming_with_arun())

# asyncio.run(streaming_with_aprint_response())
bash  theme={null}
    pip install agno exa_py
    bash  theme={null}
    export OPENAI_API_KEY=****
    export EXA_API_KEY=****
    bash  theme={null}
    python cookbook/examples/teams/streaming/04_async_team_streaming.py
    ```
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Install required libraries">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Set environment variables">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run the agent">
```

---

## Sambanova

**URL:** llms-txt#sambanova

**Contents:**
- Parameters

Source: https://docs.agno.com/reference/models/sambanova

The Sambanova model provides access to Sambanova's language models.

| Parameter  | Type            | Default                         | Description                                                         |
| ---------- | --------------- | ------------------------------- | ------------------------------------------------------------------- |
| `id`       | `str`           | `"Meta-Llama-3.1-8B-Instruct"`  | The id of the SambaNova model to use                                |
| `name`     | `str`           | `"SambaNova"`                   | The name of the model                                               |
| `provider` | `str`           | `"SambaNova"`                   | The provider of the model                                           |
| `api_key`  | `Optional[str]` | `None`                          | The API key for SambaNova (defaults to SAMBANOVA\_API\_KEY env var) |
| `base_url` | `str`           | `"https://api.sambanova.ai/v1"` | The base URL for the SambaNova API                                  |

SambaNova extends the OpenAI-compatible interface and supports most parameters from OpenAI.

---

## Zendesk

**URL:** llms-txt#zendesk

**Contents:**
- Prerequisites
- Example
- Toolkit Params
- Toolkit Functions
- Developer Resources

Source: https://docs.agno.com/concepts/tools/toolkits/others/zendesk

**ZendeskTools** enable an Agent to access Zendesk API to search for articles.

The following example requires the `requests` library and auth credentials.

The following agent will run seach Zendesk for "How do I login?" and print the response.

| Parameter               | Type   | Default | Description                                                             |
| ----------------------- | ------ | ------- | ----------------------------------------------------------------------- |
| `username`              | `str`  | -       | The username used for authentication or identification purposes.        |
| `password`              | `str`  | -       | The password associated with the username for authentication purposes.  |
| `company_name`          | `str`  | -       | The name of the company related to the user or the data being accessed. |
| `enable_search_zendesk` | `bool` | `True`  | Enable the search Zendesk functionality.                                |
| `all`                   | `bool` | `False` | Enable all functionality.                                               |

| Function         | Description                                                                                    |
| ---------------- | ---------------------------------------------------------------------------------------------- |
| `search_zendesk` | This function searches for articles in Zendesk Help Center that match the given search string. |

## Developer Resources

* View [Tools](https://github.com/agno-agi/agno/blob/main/libs/agno/agno/tools/zendesk.py)
* View [Cookbook](https://github.com/agno-agi/agno/tree/main/cookbook/tools/zendesk_tools.py)

**Examples:**

Example 1 (unknown):
```unknown

```

Example 2 (unknown):
```unknown
## Example

The following agent will run seach Zendesk for "How do I login?" and print the response.
```

---

## You can either specify the user_input_fields leave empty for all fields to be provided by the user

**URL:** llms-txt#you-can-either-specify-the-user_input_fields-leave-empty-for-all-fields-to-be-provided-by-the-user

**Contents:**
- Usage

@tool(requires_user_input=True, user_input_fields=["to_address"])
def send_email(subject: str, body: str, to_address: str) -> str:
    """
    Send an email.

Args:
        subject (str): The subject of the email.
        body (str): The body of the email.
        to_address (str): The address to send the email to.
    """
    return f"Sent email to {to_address} with subject {subject} and body {body}"

agent = Agent(
    model=OpenAIChat(id="gpt-5-mini"),
    tools=[send_email],
    markdown=True,
    db=SqliteDb(session_table="test_session", db_file="tmp/example.db"),
)

async def main():
    async for run_event in agent.arun(
        "Send an email with the subject 'Hello' and the body 'Hello, world!'",
        stream=True,
    ):
        if run_event.is_paused:  # Or agent.run_response.is_paused
            for tool in run_event.tools_requiring_user_input:  # type: ignore
                input_schema: List[UserInputField] = tool.user_input_schema  # type: ignore

for field in input_schema:
                    # Get user input for each field in the schema
                    field_type = field.field_type
                    field_description = field.description

# Display field information to the user
                    print(f"\nField: {field.name}")
                    print(f"Description: {field_description}")
                    print(f"Type: {field_type}")

# Get user input
                    if field.value is None:
                        user_value = input(f"Please enter a value for {field.name}: ")
                    else:
                        print(f"Value: {field.value}")
                        user_value = field.value

# Update the field value
                    field.value = user_value

async for resp in agent.acontinue_run(  # type: ignore
                run_id=run_event.run_id,
                updated_tools=run_event.tools,
                stream=True,
            ):
                print(resp.content, end="")

# Or for simple debug flow
    # agent.aprint_response("Send an email with the subject 'Hello' and the body 'Hello, world!'")

if __name__ == "__main__":
    asyncio.run(main())
bash  theme={null}
    pip install -U agno openai
    bash Mac/Linux theme={null}
        export OPENAI_API_KEY="your_openai_api_key_here"
      bash Windows theme={null}
        $Env:OPENAI_API_KEY="your_openai_api_key_here"
      bash  theme={null}
    touch user_input_required_stream_async.py
    bash Mac theme={null}
      python user_input_required_stream_async.py
      bash Windows   theme={null}
      python user_input_required_stream_async.py
      ```
    </CodeGroup>
  </Step>

<Step title="Find All Cookbooks">
    Explore all the available cookbooks in the Agno repository. Click the link below to view the code on GitHub:

<Link href="https://github.com/agno-agi/agno/tree/main/cookbook/agents/human_in_the_loop" target="_blank">
      Agno Cookbooks on GitHub
    </Link>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Install libraries">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Export your OpenAI API key">
    <CodeGroup>
```

Example 3 (unknown):
```unknown

```

Example 4 (unknown):
```unknown
</CodeGroup>
  </Step>

  <Step title="Create a Python file">
    Create a Python file and add the above code.
```

---

## Azure OpenAI o3

**URL:** llms-txt#azure-openai-o3

**Contents:**
- Code
- Usage

Source: https://docs.agno.com/examples/concepts/reasoning/models/azure-openai/o3-tools

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Set your API key">
    
  </Step>

<Step title="Install libraries">
    
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Set your API key">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## "My name is John Doe and I like to paint.", stream=True, user_id=john_doe_id

**URL:** llms-txt#"my-name-is-john-doe-and-i-like-to-paint.",-stream=true,-user_id=john_doe_id

---

## Time your searches

**URL:** llms-txt#time-your-searches

**Contents:**
- Next Steps

start = time.time()
results = knowledge.search("test query", max_results=5)
elapsed = time.time() - start
print(f"Search took {elapsed:.2f} seconds")
```

<CardGroup cols={2}>
  <Card title="Chunking Strategies" icon="scissors" href="/concepts/knowledge/chunking/overview">
    Learn how different chunking strategies affect performance
  </Card>

<Card title="Vector Databases" icon="database" href="/concepts/vectordb/overview">
    Compare vector database options for your scale
  </Card>

<Card title="Embedders" icon="vector-square" href="/concepts/knowledge/embedder/overview">
    Choose the right embedder for your use case
  </Card>

<Card title="Hybrid Search" icon="magnifying-glass" href="/concepts/knowledge/advanced/hybrid-search">
    Combine vector and keyword search for better results
  </Card>
</CardGroup>

<Tip>
  **Start simple, optimize when needed.** Agno's defaults work well for most use cases. Profile your application to find actual bottlenecks before spending time on optimization.
</Tip>

---

## Orchestrates the entire shopping and meal planning ecosystem

**URL:** llms-txt#orchestrates-the-entire-shopping-and-meal-planning-ecosystem

shopping_team = Team(
    id="shopping_list_team",
    name="Shopping List Team",
    role="Orchestrate shopping list management and meal planning",
    model=OpenAIChat(id="gpt-5-mini"),
    session_state={"shopping_list": [], "chores": []},
    tools=[list_items, add_chore],
    db=db,
    members=[
        shopping_mgmt_team,
        meal_planning_team,
    ],
    markdown=True,
    instructions=[
        "You are the orchestration layer for a comprehensive shopping and meal planning ecosystem",
        "If you need to add or remove items from the shopping list, forward the full request to the Shopping Management Team",
        "IMPORTANT: If the user asks about recipes or what they can make with ingredients, IMMEDIATELY forward the EXACT request to the meal_planning_team with NO additional questions",
        "Example: When user asks 'What can I make with these ingredients?', you should simply forward this exact request to meal_planning_team without asking for more information",
        "If you need to list the items in the shopping list, use the list_items tool",
        "If the user got something from the shopping list, it means it can be removed from the shopping list",
        "After each completed task, use the add_chore tool to log exactly what was done with high priority",
        "Provide a seamless experience by leveraging your specialized teams for their expertise",
    ],
    show_members_responses=True,
)

---

## Postgres

**URL:** llms-txt#postgres

**Contents:**
- Prerequisites
- Example

Source: https://docs.agno.com/concepts/tools/toolkits/database/postgres

**PostgresTools** enable an Agent to interact with a PostgreSQL database.

The following example requires the `psycopg2` library.

You will also need a database. The following example uses a Postgres database running in a Docker container.

The following agent will list all tables in the database.

```python cookbook/tools/postgres.py theme={null}
from agno.agent import Agent
from agno.tools.postgres import PostgresTools

**Examples:**

Example 1 (unknown):
```unknown
You will also need a database. The following example uses a Postgres database running in a Docker container.
```

Example 2 (unknown):
```unknown
## Example

The following agent will list all tables in the database.
```

---

## Optionally pass the dependencies to the print_response method

**URL:** llms-txt#optionally-pass-the-dependencies-to-the-print_response-method

---

## Setup infra for new users

**URL:** llms-txt#setup-infra-for-new-users

Source: https://docs.agno.com/templates/infra-management/new-users

Follow these steps to setup an existing infra:

<Steps>
  <Step title="Clone git repository">
    Clone the git repo and `cd` into the infra directory

</CodeGroup>
  </Step>

<Step title="Create and activate a virtual environment">
    <CodeGroup>

</CodeGroup>
  </Step>

<Step title="Install agno">
    <CodeGroup>

</CodeGroup>
  </Step>

<Step title="Copy secrets">
    Copy `infra/example_secrets` to `infra/secrets`

</CodeGroup>
  </Step>

<Step title="Start infra">
    <Note>
      Install [docker desktop](https://docs.docker.com/desktop/install/mac-install/) if needed.
    </Note>

</CodeGroup>
  </Step>

<Step title="Stop infra">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown

```

Example 2 (unknown):
```unknown
</CodeGroup>
  </Step>

  <Step title="Create and activate a virtual environment">
    <CodeGroup>
```

Example 3 (unknown):
```unknown

```

Example 4 (unknown):
```unknown
</CodeGroup>
  </Step>

  <Step title="Install agno">
    <CodeGroup>
```

---

## Request: C -> B -> A -> Your Route

**URL:** llms-txt#request:-c-->-b-->-a-->-your-route

---

## --- Simulation tools ---

**URL:** llms-txt#----simulation-tools----

def simulate_zoom_scheduling(
    agent: Agent, candidate_name: str, candidate_email: str
) -> str:
    """Simulate Zoom call scheduling"""
    # Generate a future time slot (1-7 days from now, between 10am-6pm IST)
    base_time = datetime.now() + timedelta(days=random.randint(1, 7))
    hour = random.randint(10, 17)  # 10am to 5pm
    scheduled_time = base_time.replace(hour=hour, minute=0, second=0, microsecond=0)

# Generate fake Zoom URL
    meeting_id = random.randint(100000000, 999999999)
    zoom_url = f"https://zoom.us/j/{meeting_id}"

result = "✅ Zoom call scheduled successfully!\n"
    result += f"📅 Time: {scheduled_time.strftime('%Y-%m-%d %H:%M')} IST\n"
    result += f"🔗 Meeting URL: {zoom_url}\n"
    result += f"👤 Participant: {candidate_name} ({candidate_email})"

def simulate_email_sending(agent: Agent, to_email: str, subject: str, body: str) -> str:
    """Simulate email sending"""
    result = "📧 Email sent successfully!\n"
    result += f"📮 To: {to_email}\n"
    result += f"📝 Subject: {subject}\n"
    result += f"✉️ Body length: {len(body)} characters\n"
    result += f"🕐 Sent at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}"

---

## User Control Flow Tools

**URL:** llms-txt#user-control-flow-tools

**Contents:**
- Code
- Usage

Source: https://docs.agno.com/examples/concepts/tools/others/user_control_flow

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Set your API key">
    
  </Step>

<Step title="Install libraries">
    
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Set your API key">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Arize

**URL:** llms-txt#arize

**Contents:**
- Integrating Agno with Arize Phoenix
- Prerequisites
- Sending Traces to Arize Phoenix

Source: https://docs.agno.com/integrations/observability/arize

Integrate Agno with Arize Phoenix to send traces and gain insights into your agent's performance.

## Integrating Agno with Arize Phoenix

[Arize Phoenix](https://phoenix.arize.com/) is a powerful platform for monitoring and analyzing AI models. By integrating Agno with Arize Phoenix, you can leverage OpenInference to send traces and gain insights into your agent's performance.

1. **Install Dependencies**

Ensure you have the necessary packages installed:

2. **Setup Arize Phoenix Account**

* Create an account at [Arize Phoenix](https://phoenix.arize.com/).
   * Obtain your API key from the Arize Phoenix dashboard.

3. **Set Environment Variables**

Configure your environment with the Arize Phoenix API key:

## Sending Traces to Arize Phoenix

* ### Example: Using Arize Phoenix with OpenInference

This example demonstrates how to instrument your Agno agent with OpenInference and send traces to Arize Phoenix.

```python  theme={null}
import asyncio
import os

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.yfinance import YFinanceTools
from phoenix.otel import register

**Examples:**

Example 1 (unknown):
```unknown
2. **Setup Arize Phoenix Account**

   * Create an account at [Arize Phoenix](https://phoenix.arize.com/).
   * Obtain your API key from the Arize Phoenix dashboard.

3. **Set Environment Variables**

   Configure your environment with the Arize Phoenix API key:
```

Example 2 (unknown):
```unknown
## Sending Traces to Arize Phoenix

* ### Example: Using Arize Phoenix with OpenInference

This example demonstrates how to instrument your Agno agent with OpenInference and send traces to Arize Phoenix.
```

---

## Async Tool Use

**URL:** llms-txt#async-tool-use

**Contents:**
- Code
- Usage

Source: https://docs.agno.com/examples/models/xai/async_tool_use

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Set your API key">
    
  </Step>

<Step title="Install libraries">
    
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Set your API key">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Pass a pydantic model that matches the input schema

**URL:** llms-txt#pass-a-pydantic-model-that-matches-the-input-schema

---

## Get Session Runs

**URL:** llms-txt#get-session-runs

Source: https://docs.agno.com/reference-api/schema/sessions/get-session-runs

get /sessions/{session_id}/runs
Retrieve all runs (executions) for a specific session. Runs represent individual interactions or executions within a session. Response schema varies based on session type.

---

## LanceDB Vector DB

**URL:** llms-txt#lancedb-vector-db

vector_db = LanceDb(
    table_name="recipes",
    uri="/tmp/lancedb",
    search_type=SearchType.keyword,
)

---

## Running Teams

**URL:** llms-txt#running-teams

**Contents:**
- Basic Execution

Source: https://docs.agno.com/concepts/teams/running-teams

Learn how to run Agno Teams.

Run your Team by calling `Team.run()` or `Team.arun()`. Here's how they work:

1. The team leader builds the context to send to the model (system message, user message, chat history, user memories, session state and other relevant inputs).
2. The team leader sends this context to the model.
3. The model processes the input and decides whether to use the `delegate_task_to_members` tool to delegate to team members, call other tools, or respond directly.
4. If delegation occurs, team members execute their tasks and return results to the team leader.
5. The team leader processes the updated context and provides a final response.
6. The team returns this final response to the caller.

The `Team.run()` function runs the team and returns the output — either as a `TeamRunOutput` object or as a stream of `TeamRunOutputEvent` and `RunOutputEvent` (for member agents) objects (when `stream=True`). For example:

```python  theme={null}
from agno.team import Team
from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.utils.pprint import pprint_run_response

news_agent = Agent(
    name="News Agent",
    model=OpenAIChat(id="gpt-4o"),
    role="Get the latest news",
    tools=[DuckDuckGoTools()]
)
weather_agent = Agent(
    name="Weather Agent",
    model=OpenAIChat(id="gpt-4o"),
    role="Get the weather for the next 7 days",
    tools=[DuckDuckGoTools()]
)

team = Team(
    name="News and Weather Team",
    members=[news_agent, weather_agent],
    model=OpenAIChat(id="gpt-4o")
)

---

## Run the example

**URL:** llms-txt#run-the-example

**Contents:**
- How MCPToolbox Works

asyncio.run(main())
python  theme={null}

**Examples:**

Example 1 (unknown):
```unknown
## How MCPToolbox Works

MCPToolbox solves the **tool overload problem**. Without filtering, your agent gets overwhelmed with too many database tools:

**Without MCPToolbox (50+ tools):**
```

---

## Get Content by ID

**URL:** llms-txt#get-content-by-id

Source: https://docs.agno.com/reference-api/schema/knowledge/get-content-by-id

get /knowledge/content/{content_id}
Retrieve detailed information about a specific content item including processing status and metadata.

---

## LightRAG

**URL:** llms-txt#lightrag

**Contents:**
- Code
- Usage

Source: https://docs.agno.com/examples/concepts/vectordb/lightrag/lightrag-db

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Install libraries">
    
  </Step>

<Step title="Set environment variables">
    
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Install libraries">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Set environment variables">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Optional: Define a TypedDict with optional fields

**URL:** llms-txt#optional:-define-a-typeddict-with-optional-fields

class ResearchTopicWithOptionals(TypedDict, total=False):
    topic: str
    focus_areas: List[str]
    target_audience: str
    sources_required: int
    priority: Optional[str]

---

## - `llama-3.3-70b-versatile` to generate the final response

**URL:** llms-txt#--`llama-3.3-70b-versatile`-to-generate-the-final-response

reasoning_agent = Agent(
    model=Groq(id="llama-3.3-70b-versatile"),
    reasoning_model=Groq(
        id="deepseek-r1-distill-llama-70b", temperature=0.6, max_tokens=1024, top_p=0.95
    ),
)

---

## Pipedream Slack

**URL:** llms-txt#pipedream-slack

**Contents:**
- Code

Source: https://docs.agno.com/examples/concepts/tools/mcp/pipedream_slack

This example shows how to use the Slack Pipedream MCP server with Agno Agents.

---

## ChromaDb

**URL:** llms-txt#chromadb

Source: https://docs.agno.com/reference/vector_db/chromadb

<Snippet file="vector-db-chromadb-reference.mdx" />

---

## List all spaces in Webex

**URL:** llms-txt#list-all-spaces-in-webex

agent.print_response("List all spaces on our Webex", markdown=True)

---

## 3. Add content with chunking strategy

**URL:** llms-txt#3.-add-content-with-chunking-strategy

knowledge.add_content(
    path="company_docs/employee_handbook.pdf",
    reader=PDFReader(
        chunking_strategy=SemanticChunking(  # Optional: defaults to FixedSizeChunking
            chunk_size=1000,
            similarity_threshold=0.5
        )
    ),
    metadata={"type": "policy", "department": "hr"}
)

---

## AWS SES

**URL:** llms-txt#aws-ses

**Contents:**
- Prerequisites

Source: https://docs.agno.com/concepts/tools/toolkits/others/aws_ses

**AWSSESTool** enables an Agent to send emails using Amazon Simple Email Service (SES).

The following example requires the `boto3` library and valid AWS credentials. You can install `boto3` via pip:

You must also configure your AWS credentials so that the SDK can authenticate to SES. The easiest way is via the AWS CLI:

```shell  theme={null}
aws configure

**Examples:**

Example 1 (unknown):
```unknown
You must also configure your AWS credentials so that the SDK can authenticate to SES. The easiest way is via the AWS CLI:
```

---

## Composio

**URL:** llms-txt#composio

**Contents:**
- Prerequisites
- Example
- Toolkit Params
- Toolkit Functions

Source: https://docs.agno.com/concepts/tools/toolkits/others/composio

[**ComposioTools**](https://docs.composio.dev/framework/phidata) enable an Agent to work with tools like Gmail, Salesforce, Github, etc.

The following example requires the `composio-agno` library.

The following agent will use Github Tool from Composio Toolkit to star a repo.

The following parameters are used when calling the GitHub star repository action:

| Parameter | Type  | Default | Description                          |
| --------- | ----- | ------- | ------------------------------------ |
| `owner`   | `str` | -       | The owner of the repository to star. |
| `repo`    | `str` | -       | The name of the repository to star.  |

Composio Toolkit provides 1000+ functions to connect to different software tools.
Open this [link](https://composio.dev/tools) to view the complete list of functions.

**Examples:**

Example 1 (unknown):
```unknown
## Example

The following agent will use Github Tool from Composio Toolkit to star a repo.
```

---

## Langfuse Via Openinference (With Structured Output)

**URL:** llms-txt#langfuse-via-openinference-(with-structured-output)

**Contents:**
- Code

Source: https://docs.agno.com/examples/concepts/integrations/observability/langfuse_via_openinference_response_model

```python cookbook/integrations/observability/langfuse_via_openinference_response_model.py theme={null}
import base64
import os
from enum import Enum

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.duckduckgo import DuckDuckGoTools
from openinference.instrumentation.agno import AgnoInstrumentor
from opentelemetry import trace as trace_api
from opentelemetry.exporter.otlp.proto.http.trace_exporter import OTLPSpanExporter
from opentelemetry.sdk.trace import TracerProvider
from opentelemetry.sdk.trace.export import SimpleSpanProcessor
from pydantic import BaseModel, Field

LANGFUSE_AUTH = base64.b64encode(
    f"{os.getenv('LANGFUSE_PUBLIC_KEY')}:{os.getenv('LANGFUSE_SECRET_KEY')}".encode()
).decode()
os.environ["OTEL_EXPORTER_OTLP_ENDPOINT"] = (
    "https://us.cloud.langfuse.com/api/public/otel"  # 🇺🇸 US data region
)

---

## Weave

**URL:** llms-txt#weave

**Contents:**
- Integrating Agno with Weave by WandB
- Prerequisites
- Logging Model Calls with Weave

Source: https://docs.agno.com/integrations/observability/weave

Integrate Agno with Weave by WandB to send traces and gain insights into your agent's performance.

## Integrating Agno with Weave by WandB

[Weave by Weights & Biases (WandB)](https://weave-docs.wandb.ai/) provides a powerful platform for logging and visualizing model calls. By integrating Agno with Weave, you can track and analyze your agent's performance and behavior.

Ensure you have the Weave package installed:

2. **Authentication**
   Go to [WandB](https://wandb.ai) and copy your API key

## Logging Model Calls with Weave

This example demonstrates how to use Weave to log model calls.

```python  theme={null}
import weave
from agno.agent import Agent
from agno.models.openai import OpenAIChat

**Examples:**

Example 1 (unknown):
```unknown
2. **Authentication**
   Go to [WandB](https://wandb.ai) and copy your API key
```

Example 2 (unknown):
```unknown
## Logging Model Calls with Weave

This example demonstrates how to use Weave to log model calls.
```

---

## Step-Based Workflows

**URL:** llms-txt#step-based-workflows

**Contents:**
- Example

Source: https://docs.agno.com/concepts/workflows/workflow-patterns/step-based-workflow

Named steps for better logging and support on the AgentOS chat page

**You can name your steps** for better logging and future support on the Agno platform.
This also changes the name of a step when accessing that step's output inside a `StepInput` object.

```python  theme={null}
from agno.workflow import Step, Workflow

---

## Docker Tools

**URL:** llms-txt#docker-tools

**Contents:**
- Code
- Usage

Source: https://docs.agno.com/examples/concepts/tools/local/docker

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Install Docker">
    Install Docker Desktop (for macOS/Windows) or Docker Engine (for Linux) from [Docker's official website](https://www.docker.com/products/docker-desktop).
  </Step>

<Step title="Install libraries">
    
  </Step>

<Step title="Start Docker">
    Make sure Docker is running on your system:

* **macOS/Windows**: Start Docker Desktop application
    * **Linux**: Run `sudo systemctl start docker`
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Install Docker">
    Install Docker Desktop (for macOS/Windows) or Docker Engine (for Linux) from [Docker's official website](https://www.docker.com/products/docker-desktop).
  </Step>

  <Step title="Install libraries">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Start Docker">
    Make sure Docker is running on your system:

    * **macOS/Windows**: Start Docker Desktop application
    * **Linux**: Run `sudo systemctl start docker`
  </Step>

  <Step title="Run Agent">
    <CodeGroup>
```

Example 3 (unknown):
```unknown

```

---

## MySQL for Team

**URL:** llms-txt#mysql-for-team

**Contents:**
- Usage
  - Run MySQL

Source: https://docs.agno.com/examples/concepts/db/mysql/mysql_for_team

Agno supports using MySQL as a storage backend for Teams using the `MySQLDb` class.

Install [docker desktop](https://docs.docker.com/desktop/install/mac-install/) and run **MySQL** on port **3306** using:

```python mysql_for_team.py theme={null}
from typing import List

from agno.agent import Agent
from agno.db.mysql import MySQLDb
from agno.models.openai import OpenAIChat
from agno.team import Team
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.hackernews import HackerNewsTools
from pydantic import BaseModel

**Examples:**

Example 1 (unknown):
```unknown

```

---

## Coordinated Team

**URL:** llms-txt#coordinated-team

reasoning_team = Team(
    name="Finance Reasoning Team",
    model=LangDB(id="xai/grok-4"),
    members=[web_agent, finance_agent],
    instructions=[
        "Collaborate to provide comprehensive financial insights",
        "Consider both fundamental analysis and market sentiment"
    ]
)

---

## Development: Fast, local, zero setup

**URL:** llms-txt#development:-fast,-local,-zero-setup

dev_db = LanceDb(
    table_name="dev_knowledge",
    uri="./local_db"
)

---

## New session, so new shopping list

**URL:** llms-txt#new-session,-so-new-shopping-list

**Contents:**
- Usage

agent.print_response(
    "Add chicken and soup to my list.",
    stream=True,
    user_id=user_id_2,
    session_id="user_3_session_2",
)

print(f"Final shopping lists: \n{json.dumps(shopping_list, indent=2)}")
bash  theme={null}
    pip install -U agno openai
    bash Mac/Linux theme={null}
        export OPENAI_API_KEY="your_openai_api_key_here"
      bash Windows theme={null}
        $Env:OPENAI_API_KEY="your_openai_api_key_here"
      bash  theme={null}
    touch session_state_multiple_users.py
    bash Mac theme={null}
      python session_state_multiple_users.py
      bash Windows theme={null}
      python session_state_multiple_users.py
      ```
    </CodeGroup>
  </Step>

<Step title="Find All Cookbooks">
    Explore all the available cookbooks in the Agno repository. Click the link below to view the code on GitHub:

<Link href="https://github.com/agno-agi/agno/tree/main/cookbook/agents/state" target="_blank">
      Agno Cookbooks on GitHub
    </Link>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Install libraries">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Export your OpenAI API key">
    <CodeGroup>
```

Example 3 (unknown):
```unknown

```

Example 4 (unknown):
```unknown
</CodeGroup>
  </Step>

  <Step title="Create a Python file">
    Create a Python file and add the above code.
```

---

## Clickhouse

**URL:** llms-txt#clickhouse

Source: https://docs.agno.com/reference/vector_db/clickhouse

<Snippet file="vector-db-clickhouse-reference.mdx" />

---

## Initialize hybrid vector DB

**URL:** llms-txt#initialize-hybrid-vector-db

hybrid_db = PgVector(
    table_name="recipes",
    db_url=db_url,
    search_type=SearchType.hybrid  # Hybrid Search
)

---

## Get Status

**URL:** llms-txt#get-status

Source: https://docs.agno.com/reference-api/schema/agui/get-status

---

## Web Tools

**URL:** llms-txt#web-tools

**Contents:**
- Code
- Usage

Source: https://docs.agno.com/examples/concepts/tools/others/webtools

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Set your API key">
    
  </Step>

<Step title="Install libraries">
    
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Set your API key">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Hugging Face

**URL:** llms-txt#hugging-face

Source: https://docs.agno.com/reference/knowledge/embedder/huggingface

Hugging Face Embedder is a class that allows you to embed documents using any embedding model hosted on HuggingFace's Inference API.

<Snippet file="embedder-huggingface-reference.mdx" />

---

## ag infra delete

**URL:** llms-txt#ag-infra-delete

**Contents:**
- Params

Source: https://docs.agno.com/reference/agno-infra/cli/ws/delete

<ResponseField name="infra_name" type="str">
  Name of the infra to delete `-infra`
</ResponseField>

<ResponseField name="print_debug_log" type="bool">
  Print debug logs. `--debug` `-d`
</ResponseField>

---

## Lumalabs

**URL:** llms-txt#lumalabs

**Contents:**
- Prerequisites
- Example
- Toolkit Params
- Toolkit Functions
- Developer Resources

Source: https://docs.agno.com/concepts/tools/toolkits/others/lumalabs

**LumaLabTools** enables an Agent to generate media using the [Lumalabs platform](https://lumalabs.ai/dream-machine).

The following example requires the `lumaai` library. To install the Lumalabs client, run the following command:

The following agent will use Lumalabs to generate any video requested by the user.

| Parameter               | Type   | Default | Description                                          |
| ----------------------- | ------ | ------- | ---------------------------------------------------- |
| `api_key`               | `str`  | `None`  | If you want to manually supply the Lumalabs API key. |
| `enable_generate_video` | `bool` | `True`  | Enable the generate\_video functionality.            |
| `enable_image_to_video` | `bool` | `True`  | Enable the image\_to\_video functionality.           |
| `all`                   | `bool` | `False` | Enable all functionality.                            |

| Function         | Description                                                           |
| ---------------- | --------------------------------------------------------------------- |
| `generate_video` | Generate a video from a prompt.                                       |
| `image_to_video` | Generate a video from a prompt, a starting image and an ending image. |

## Developer Resources

* View [Tools](https://github.com/agno-agi/agno/blob/main/libs/agno/agno/tools/lumalabs.py)
* View [Cookbook](https://github.com/agno-agi/agno/tree/main/cookbook/tools/lumalabs_tools.py)

**Examples:**

Example 1 (unknown):
```unknown
The following example requires the `lumaai` library. To install the Lumalabs client, run the following command:
```

Example 2 (unknown):
```unknown
## Example

The following agent will use Lumalabs to generate any video requested by the user.
```

---

## Configure the language model

**URL:** llms-txt#configure-the-language-model

model = Ollama(id="llama3.1:8b")

---

## -----------------------------------------------------------------------------

**URL:** llms-txt#-----------------------------------------------------------------------------

knowledge = Knowledge(
    name="CSV Knowledge Base",
    description="A knowledge base for CSV files",
    vector_db=vector_db,
)

---

## List Content

**URL:** llms-txt#list-content

Source: https://docs.agno.com/reference-api/schema/knowledge/list-content

get /knowledge/content
Retrieve paginated list of all content in the knowledge base with filtering and sorting options. Filter by status, content type, or metadata properties.

---

## -*- Print memories and summary

**URL:** llms-txt#-*--print-memories-and-summary

if agent.db:
    pprint(agent.get_user_memories(user_id="test_user"))
    pprint(
        agent.get_session(session_id="test_session").summary  # type: ignore
    )

---

## Audio Input (Upload the file)

**URL:** llms-txt#audio-input-(upload-the-file)

**Contents:**
- Code

Source: https://docs.agno.com/examples/models/gemini/audio_input_file_upload

```python cookbook/models/google/gemini/audio_input_file_upload.py theme={null}
from pathlib import Path

from agno.agent import Agent
from agno.media import Audio
from agno.models.google import Gemini

model = Gemini(id="gemini-2.0-flash-exp")
agent = Agent(
    model=model,
    markdown=True,
)

---

## Quick test to verify model works

**URL:** llms-txt#quick-test-to-verify-model-works

**Contents:**
- Step 2: Add Social Media Tools
  - 2a. Add XTools for Twitter/X Data

if __name__ == "__main__":
    test_response = model.invoke("Explain social media sentiment analysis in one sentence.")
    print(f"Model test: {test_response}")
python  theme={null}
from agno.tools.x import XTools

**Examples:**

Example 1 (unknown):
```unknown
This confirms your model is working, before we add more complexity.

## Step 2: Add Social Media Tools

**Which tools should I use?** We are adding XTools because we need direct Twitter/X data with engagement metrics, and ExaTools because we need broader web context that social media alone can't provide.

### 2a. Add XTools for Twitter/X Data

**Why XTools?** Direct access to Twitter/X with engagement metrics is crucial for understanding influence patterns and viral content.
```

---

## Todoist Tools

**URL:** llms-txt#todoist-tools

**Contents:**
- Code

Source: https://docs.agno.com/examples/concepts/tools/others/todoist

```python cookbook/tools/todoist_tools.py theme={null}
"""
Example showing how to use the Todoist Tools with Agno

Requirements:
- Sign up/login to Todoist and get a Todoist API Token (get from https://app.todoist.com/app/settings/integrations/developer)
- pip install todoist-api-python

Usage:
- Set the following environment variables:
    export TODOIST_API_TOKEN="your_api_token"

- Or provide them when creating the TodoistTools instance
"""

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.todoist import TodoistTools

todoist_agent = Agent(
    name="Todoist Agent",
    role="Manage your todoist tasks",
    instructions=[
        "When given a task, create a todoist task for it.",
        "When given a list of tasks, create a todoist task for each one.",
        "When given a task to update, update the todoist task.",
        "When given a task to delete, delete the todoist task.",
        "When given a task to get, get the todoist task.",
    ],
    id="todoist-agent",
    model=OpenAIChat(id="gpt-5-mini"),
    tools=[TodoistTools()],
    markdown=True,
    debug_mode=True,
    )

---

## Get information about a user

**URL:** llms-txt#get-information-about-a-user

agent.print_response("Can you retrieve information about this user https://x.com/AgnoAgi ", markdown=True)

---

## Video as input

**URL:** llms-txt#video-as-input

Source: https://docs.agno.com/concepts/multimodal/video/video_input

Learn how to use video as input with Agno agents.

Agno supports videos as input to agents and teams.  Take a look at the [compatibility matrix](/concepts/models/compatibility#multimodal-support) to see which models support videos as input.

Let's create an agent that can understand video input.

```python video_agent.py theme={null}
from pathlib import Path

from agno.agent import Agent
from agno.media import Video
from agno.models.google import Gemini

agent = Agent(
    model=Gemini(id="gemini-2.0-flash-001"),
    markdown=True,
)

---

## Google Calendar

**URL:** llms-txt#google-calendar

**Contents:**
- Prerequisites
  - Install dependencies
  - Setup Google Project and OAuth
- Example
- Toolkit Params
- Toolkit Functions
- Developer Resources

Source: https://docs.agno.com/concepts/tools/toolkits/others/googlecalendar

Enable an Agent to work with Google Calendar to view and schedule meetings.

### Install dependencies

### Setup Google Project and OAuth

Reference: [https://developers.google.com/calendar/api/quickstart/python](https://developers.google.com/calendar/api/quickstart/python)

1. Enable Google Calender API

* Go to [Google Cloud Console](https://console.cloud.google.com/apis/enableflow?apiid=calendar-json.googleapis.com).
   * Select Project and Enable.

2. Go To API & Service -> OAuth Consent Screen

* If you are a Google Workspace user, select Internal.
   * Otherwise, select External.

4. Fill in the app details (App name, logo, support email, etc).

* Click on Add or Remove Scope.
   * Search for Google Calender API (Make sure you've enabled Google calender API otherwise scopes wont be visible).
   * Select scopes accordingly
     * From the dropdown check on `/auth/calendar` scope
   * Save and continue.

* Click Add Users and enter the email addresses of the users you want to allow during testing.
   * NOTE : Only these users can access the app's OAuth functionality when the app is in "Testing" mode.
     Any other users will receive access denied errors.
   * To make the app available to all users, you'll need to move the app's status to "In Production".
     Before doing so, ensure the app is fully verified by Google if it uses sensitive or restricted scopes.
   * Click on Go back to Dashboard.

7. Generate OAuth 2.0 Client ID

* Go to Credentials.
   * Click on Create Credentials -> OAuth Client ID
   * Select Application Type as Desktop app.
   * Download JSON.

8. Using Google Calender Tool
   * Pass the path of downloaded credentials as credentials\_path to Google Calender tool.
   * Optional: Set the `token_path` parameter to specify where the tool should create the `token.json` file.
   * The `token.json` file is used to store the user's access and refresh tokens and is automatically created during the authorization flow if it doesn't already exist.
   * If `token_path` is not explicitly provided, the file will be created in the default location which is your current working directory.
   * If you choose to specify `token_path`, please ensure that the directory you provide has write access, as the application needs to create or update this file during the authentication process.

The following agent will use GoogleCalendarTools to find today's events.

| Parameter          | Type        | Default      | Description                                                                   |
| ------------------ | ----------- | ------------ | ----------------------------------------------------------------------------- |
| `scopes`           | `List[str]` | `None`       | List of OAuth scopes for Google Calendar API access                           |
| `credentials_path` | `str`       | `None`       | Path of the file credentials.json file which contains OAuth 2.0 Client ID     |
| `token_path`       | `str`       | `token.json` | Path of the file token.json which stores the user's access and refresh tokens |
| `access_token`     | `str`       | `None`       | Direct access token for authentication (alternative to OAuth flow)            |
| `calendar_id`      | `str`       | `primary`    | The calendar ID to use for operations                                         |
| `oauth_port`       | `int`       | `8080`       | Port number for OAuth callback server                                         |
| `allow_update`     | `bool`      | `False`      | Whether to allow write operations (create/update/delete events)               |

| Function       | Description                                        |
| -------------- | -------------------------------------------------- |
| `list_events`  | List events from the user's primary calendar.      |
| `create_event` | Create a new event in the user's primary calendar. |

You can use `include_tools` or `exclude_tools` to modify the list of tools the agent has access to. Learn more about [selecting tools](/concepts/tools/selecting-tools).

## Developer Resources

* View [Tools](https://github.com/agno-agi/agno/blob/main/libs/agno/agno/tools/googlecalendar.py)
* View [Cookbook](https://github.com/agno-agi/agno/tree/main/cookbook/tools/googlecalendar_tools.py)

**Examples:**

Example 1 (unknown):
```unknown
### Setup Google Project and OAuth

Reference: [https://developers.google.com/calendar/api/quickstart/python](https://developers.google.com/calendar/api/quickstart/python)

1. Enable Google Calender API

   * Go to [Google Cloud Console](https://console.cloud.google.com/apis/enableflow?apiid=calendar-json.googleapis.com).
   * Select Project and Enable.

2. Go To API & Service -> OAuth Consent Screen

3. Select User Type

   * If you are a Google Workspace user, select Internal.
   * Otherwise, select External.

4. Fill in the app details (App name, logo, support email, etc).

5. Select Scope

   * Click on Add or Remove Scope.
   * Search for Google Calender API (Make sure you've enabled Google calender API otherwise scopes wont be visible).
   * Select scopes accordingly
     * From the dropdown check on `/auth/calendar` scope
   * Save and continue.

6. Adding Test User

   * Click Add Users and enter the email addresses of the users you want to allow during testing.
   * NOTE : Only these users can access the app's OAuth functionality when the app is in "Testing" mode.
     Any other users will receive access denied errors.
   * To make the app available to all users, you'll need to move the app's status to "In Production".
     Before doing so, ensure the app is fully verified by Google if it uses sensitive or restricted scopes.
   * Click on Go back to Dashboard.

7. Generate OAuth 2.0 Client ID

   * Go to Credentials.
   * Click on Create Credentials -> OAuth Client ID
   * Select Application Type as Desktop app.
   * Download JSON.

8. Using Google Calender Tool
   * Pass the path of downloaded credentials as credentials\_path to Google Calender tool.
   * Optional: Set the `token_path` parameter to specify where the tool should create the `token.json` file.
   * The `token.json` file is used to store the user's access and refresh tokens and is automatically created during the authorization flow if it doesn't already exist.
   * If `token_path` is not explicitly provided, the file will be created in the default location which is your current working directory.
   * If you choose to specify `token_path`, please ensure that the directory you provide has write access, as the application needs to create or update this file during the authentication process.

## Example

The following agent will use GoogleCalendarTools to find today's events.
```

---

## LangGraph

**URL:** llms-txt#langgraph

python cookbook/evals/performance/comparison/langgraph_instantiation.py

---

## MySQL connection settings

**URL:** llms-txt#mysql-connection-settings

**Contents:**
- Params
- Developer Resources

db_url = "mysql+pymysql://ai:ai@localhost:3306/ai"
db = MySQLDb(db_url=db_url)

class Article(BaseModel):
    title: str
    summary: str
    reference_links: List[str]

hn_researcher = Agent(
    name="HackerNews Researcher",
    model=OpenAIChat("gpt-5-mini"),
    role="Gets top stories from hackernews.",
    tools=[HackerNewsTools()],
)

web_searcher = Agent(
    name="Web Searcher",
    model=OpenAIChat("gpt-5-mini"),
    role="Searches the web for information on a topic",
    tools=[DuckDuckGoTools()],
    add_datetime_to_context=True,
)

hn_team = Team(
    name="HackerNews Team",
    model=OpenAIChat("gpt-5-mini"),
    members=[hn_researcher, web_searcher],
    db=db,
    instructions=[
        "First, search hackernews for what the user is asking about.",
        "Then, ask the web searcher to search for each story to get more information.",
        "Finally, provide a thoughtful and engaging summary.",
    ],
    output_schema=Article,
    markdown=True,
    show_members_responses=True,
    add_member_tools_to_context=False,
)

hn_team.print_response("Write an article about the top 2 stories on hackernews")
```

<Snippet file="db-mysql-params.mdx" />

## Developer Resources

* View [Cookbook](https://github.com/agno-agi/agno/blob/main/cookbook/db/mysql/mysql_storage_for_team.py)

---

## Slack Events

**URL:** llms-txt#slack-events

Source: https://docs.agno.com/reference-api/schema/slack/slack-events

post /slack/events
Process incoming Slack events

---

## Shopping management team - new layer for handling all shopping list modifications

**URL:** llms-txt#shopping-management-team---new-layer-for-handling-all-shopping-list-modifications

shopping_mgmt_team = Team(
    name="Shopping Management Team",
    role="Execute shopping list operations",
    id="shopping_management",
    model=OpenAIChat(id="gpt-5-mini"),
    members=[shopping_list_agent],
    instructions=[
        "Manage adding and removing items from the shopping list using the Shopping List Agent",
        "Forward requests to add or remove items to the Shopping List Agent",
    ],
)

def get_ingredients(session_state) -> str:
    """Retrieve ingredients from the shopping list to use for recipe suggestions."""
    shopping_list = session_state["shopping_list"]

if not shopping_list:
        return "The shopping list is empty. Add some ingredients first to get recipe suggestions."

# Just return the ingredients - the agent will create recipes
    return f"Available ingredients from shopping list: {', '.join(shopping_list)}"

recipe_agent = Agent(
    name="Recipe Suggester",
    id="recipe_suggester",
    role="Suggest recipes based on available ingredients",
    model=OpenAIChat(id="gpt-5-mini"),
    tools=[get_ingredients],
    instructions=[
        "First, use the get_ingredients tool to get the current ingredients from the shopping list",
        "After getting the ingredients, create detailed recipe suggestions based on those ingredients",
        "Create at least 3 different recipe ideas using the available ingredients",
        "For each recipe, include: name, ingredients needed (highlighting which ones are from the shopping list), and brief preparation steps",
        "Be creative but practical with recipe suggestions",
        "Consider common pantry items that people usually have available in addition to shopping list items",
        "Consider dietary preferences if mentioned by the user",
        "If no meal type is specified, suggest a variety of options (breakfast, lunch, dinner, snacks)",
    ],
)

def list_items(session_state) -> str:
    """List all items in the shopping list."""
    shopping_list = session_state["shopping_list"]

if not shopping_list:
        return "The shopping list is empty."

items_text = "\n".join([f"- {item}" for item in shopping_list])
    return f"Current shopping list:\n{items_text}"

---

## === RESEARCH STEPS ===

**URL:** llms-txt#===-research-steps-===

research_hackernews_step = Step(
    name="ResearchHackerNews",
    description="Research tech news from Hacker News",
    agent=hackernews_agent,
)

research_exa_step = Step(
    name="ResearchExa",
    description="Research using Exa search",
    agent=exa_agent,
)

---

## Image As Input

**URL:** llms-txt#image-as-input

**Contents:**
- Developer Resources

Source: https://docs.agno.com/concepts/multimodal/images/image_input

Learn how to use image as input with Agno agents.

Agno supports images as input to agents and teams.  Take a look at the [compatibility matrix](/concepts/models/compatibility#multimodal-support) to see which models support images as input.

Let's create an agent that can understand images and make tool calls as needed

## Developer Resources

* View more [Examples](/examples/concepts/multimodal/image-to-text)

---

## Define executor using a class.

**URL:** llms-txt#define-executor-using-a-class.

---

## Each content object includes:

**URL:** llms-txt#each-content-object-includes:

**Contents:**
- Management Features
  - Content Deletion with Vector Cleanup

print(content.name)         # Content name
print(content.description)  # Description
print(content.metadata)     # Custom metadata
print(content.file_type)    # File type (.pdf, .txt, etc.)
print(content.size)         # File size in bytes
print(content.status)       # Processing status
print(content.created_at)   # When it was added
print(content.updated_at)   # Last modification
python  theme={null}

**Examples:**

Example 1 (unknown):
```unknown
## Management Features

### Content Deletion with Vector Cleanup

Delete content and automatically clean up associated vectors:

This automatically:

1. Removes the content metadata from ContentsDB
2. Deletes associated vectors from the vector database
3. Maintains consistency between both databases
```

---

## team.print_response("I also like to play basketball.")

**URL:** llms-txt#team.print_response("i-also-like-to-play-basketball.")

**Contents:**
- Usage

bash  theme={null}
    pip install agno psycopg2-binary
    bash  theme={null}
    export OPENAI_API_KEY=****
    bash  theme={null}
    cookbook/run_pgvector.sh
    bash  theme={null}
    python cookbook/examples/teams/session/04_session_summary_references.py
    ```
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Install required libraries">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Set environment variables">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Start PostgreSQL database">
```

Example 4 (unknown):
```unknown
</Step>

  <Step title="Run the agent">
```

---

## =============================================================================

**URL:** llms-txt#=============================================================================

---

## Print stored events in a nice format

**URL:** llms-txt#print-stored-events-in-a-nice-format

print_stored_events(run_response, "Simple Step Workflow")

---

## BrightData

**URL:** llms-txt#brightdata

**Contents:**
- Prerequisites
- Example

Source: https://docs.agno.com/concepts/tools/toolkits/web_scrape/brightdata

**BrightDataTools** provide comprehensive web scraping capabilities including markdown conversion, screenshots, search engine results, and structured data feeds from various platforms like LinkedIn, Amazon, Instagram, and more.

The following examples require the `requests` library:

You'll also need a BrightData API key. Set the `BRIGHT_DATA_API_KEY` environment variable:

Optionally, you can configure zone settings:

Extract structured data from platforms like LinkedIn, Amazon, etc.:

```python  theme={null}
from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.brightdata import BrightDataTools
from agno.utils.media import save_base64_data

agent = Agent(
    model=OpenAIChat(id="gpt-5-mini"),
    tools=[
        BrightDataTools(
            get_screenshot=True,
        )
    ],
    markdown=True,
    )

**Examples:**

Example 1 (unknown):
```unknown
You'll also need a BrightData API key. Set the `BRIGHT_DATA_API_KEY` environment variable:
```

Example 2 (unknown):
```unknown
Optionally, you can configure zone settings:
```

Example 3 (unknown):
```unknown
## Example

Extract structured data from platforms like LinkedIn, Amazon, etc.:
```

---

## LiteLLM

**URL:** llms-txt#litellm

**Contents:**
- Prerequisites

Source: https://docs.agno.com/concepts/models/litellm

Integrate LiteLLM with Agno for a unified LLM experience.

[LiteLLM](https://docs.litellm.ai/docs/) provides a unified interface for various LLM providers, allowing you to use different models with the same code.

Agno integrates with LiteLLM in two ways:

1. **Direct SDK integration** - Using the LiteLLM Python SDK
2. **Proxy Server integration** - Using LiteLLM as an OpenAI-compatible proxy

For both integration methods, you'll need:

```shell  theme={null}

---

## Custom API

**URL:** llms-txt#custom-api

**Contents:**
- Prerequisites
- Example
- Toolkit Params
- Toolkit Functions
- Developer Resources

Source: https://docs.agno.com/concepts/tools/toolkits/others/custom_api

**CustomApiTools** enable an Agent to make HTTP requests to any external API with customizable authentication and parameters.

The following example requires the `requests` library.

The following agent will use CustomApiTools to make API calls to the Dog CEO API.

| Parameter             | Type             | Default | Description                                 |
| --------------------- | ---------------- | ------- | ------------------------------------------- |
| `base_url`            | `str`            | `None`  | Base URL for API calls                      |
| `username`            | `str`            | `None`  | Username for basic authentication           |
| `password`            | `str`            | `None`  | Password for basic authentication           |
| `api_key`             | `str`            | `None`  | API key for bearer token authentication     |
| `headers`             | `Dict[str, str]` | `None`  | Default headers to include in requests      |
| `verify_ssl`          | `bool`           | `True`  | Whether to verify SSL certificates          |
| `timeout`             | `int`            | `30`    | Request timeout in seconds                  |
| `enable_make_request` | `bool`           | `True`  | Enables functionality to make HTTP requests |
| `all`                 | `bool`           | `False` | Enables all functionality when set to True  |

| Function       | Description                                                                                                                                |
| -------------- | ------------------------------------------------------------------------------------------------------------------------------------------ |
| `make_request` | Makes an HTTP request to the API. Takes method (GET, POST, etc.), endpoint, and optional params, data, headers, and json\_data parameters. |

## Developer Resources

* View [Tools](https://github.com/agno-agi/agno/blob/main/libs/agno/agno/tools/api.py)
* View [Cookbook](https://github.com/agno-agi/agno/tree/main/cookbook/tools/custom_api_tools.py)

**Examples:**

Example 1 (unknown):
```unknown
## Example

The following agent will use CustomApiTools to make API calls to the Dog CEO API.
```

Example 2 (unknown):
```unknown
## Toolkit Params

| Parameter             | Type             | Default | Description                                 |
| --------------------- | ---------------- | ------- | ------------------------------------------- |
| `base_url`            | `str`            | `None`  | Base URL for API calls                      |
| `username`            | `str`            | `None`  | Username for basic authentication           |
| `password`            | `str`            | `None`  | Password for basic authentication           |
| `api_key`             | `str`            | `None`  | API key for bearer token authentication     |
| `headers`             | `Dict[str, str]` | `None`  | Default headers to include in requests      |
| `verify_ssl`          | `bool`           | `True`  | Whether to verify SSL certificates          |
| `timeout`             | `int`            | `30`    | Request timeout in seconds                  |
| `enable_make_request` | `bool`           | `True`  | Enables functionality to make HTTP requests |
| `all`                 | `bool`           | `False` | Enables all functionality when set to True  |

## Toolkit Functions

| Function       | Description                                                                                                                                |
| -------------- | ------------------------------------------------------------------------------------------------------------------------------------------ |
| `make_request` | Makes an HTTP request to the API. Takes method (GET, POST, etc.), endpoint, and optional params, data, headers, and json\_data parameters. |

## Developer Resources

* View [Tools](https://github.com/agno-agi/agno/blob/main/libs/agno/agno/tools/api.py)
* View [Cookbook](https://github.com/agno-agi/agno/tree/main/cookbook/tools/custom_api_tools.py)
```

---

## OpenCL

**URL:** llms-txt#opencl

**Contents:**
  - Model Quantization
- Troubleshooting
  - Server Connection Issues
  - Model Loading Problems
  - Performance Issues

make LLAMA_CLBLAST=1
bash check server theme={null}
curl http://127.0.0.1:8080/v1/models
```

### Model Loading Problems

* Verify the model file exists and is in GGML format
* Check available memory for large models
* Ensure the model is compatible with your LlamaCpp version

### Performance Issues

* Adjust batch sizes (`-b`, `-ub`) based on your hardware
* Use GPU acceleration if available
* Consider using quantized models for faster inference

**Examples:**

Example 1 (unknown):
```unknown
### Model Quantization

Use quantized models for better performance:

* `Q4_K_M`: Balanced size and quality
* `Q8_0`: Higher quality, larger size
* `Q2_K`: Smallest size, lower quality

## Troubleshooting

### Server Connection Issues

Ensure the LlamaCpp server is running and accessible:
```

---

## Cohere Embedder

**URL:** llms-txt#cohere-embedder

**Contents:**
- Code

Source: https://docs.agno.com/examples/concepts/knowledge/embedders/cohere-embedder

```python  theme={null}
import asyncio
from agno.knowledge.embedder.cohere import CohereEmbedder
from agno.knowledge.knowledge import Knowledge
from agno.vectordb.pgvector import PgVector

embeddings = CohereEmbedder().get_embedding(
    "The quick brown fox jumps over the lazy dog."
)

---

## ArXiv Tools

**URL:** llms-txt#arxiv-tools

**Contents:**
- Code
- Usage

Source: https://docs.agno.com/examples/concepts/tools/search/arxiv

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Set your API key">
    
  </Step>

<Step title="Install libraries">
    
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Set your API key">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## PPTX Reader

**URL:** llms-txt#pptx-reader

Source: https://docs.agno.com/reference/knowledge/reader/pptx

PPTXReader is a reader class that allows you to read data from PowerPoint (.pptx) files.

<Snippet file="pptx-reader-reference.mdx" />

---

## Sentence Transformer Embedder

**URL:** llms-txt#sentence-transformer-embedder

**Contents:**
- Code

Source: https://docs.agno.com/examples/concepts/knowledge/embedders/sentence-transformer-embedder

```python  theme={null}
from agno.knowledge.embedder.sentence_transformer import SentenceTransformerEmbedder
from agno.knowledge.knowledge import Knowledge
from agno.vectordb.pgvector import PgVector

embeddings = SentenceTransformerEmbedder().get_embedding(
    "The quick brown fox jumps over the lazy dog."
)

---

## -*- Create a run

**URL:** llms-txt#-*--create-a-run

team.print_response("Share a 2 sentence horror story", stream=True)

---

## store everything

**URL:** llms-txt#store-everything

debug_workflow = Workflow(
    name="Debug Workflow",
    store_events=True,
    steps=[...]
)

---

## Azure OpenAI

**URL:** llms-txt#azure-openai

**Contents:**
- Parameters

Source: https://docs.agno.com/reference/models/azure_open_ai

The AzureOpenAI model provides access to Azure-hosted OpenAI models.

| Parameter                 | Type                              | Default                | Description                                                                        |
| ------------------------- | --------------------------------- | ---------------------- | ---------------------------------------------------------------------------------- |
| `id`                      | `str`                             | `"gpt-4o"`             | The id of the Azure OpenAI model to use                                            |
| `name`                    | `str`                             | `"AzureOpenAI"`        | The name of the model                                                              |
| `provider`                | `str`                             | `"Azure"`              | The provider of the model                                                          |
| `temperature`             | `Optional[float]`                 | `None`                 | Controls randomness in the model's output (0.0 to 2.0)                             |
| `max_tokens`              | `Optional[int]`                   | `None`                 | Maximum number of tokens to generate in the response                               |
| `max_completion_tokens`   | `Optional[int]`                   | `None`                 | Maximum number of completion tokens to generate                                    |
| `frequency_penalty`       | `Optional[float]`                 | `None`                 | Penalizes new tokens based on their frequency in the text so far (-2.0 to 2.0)     |
| `presence_penalty`        | `Optional[float]`                 | `None`                 | Penalizes new tokens based on whether they appear in the text so far (-2.0 to 2.0) |
| `top_p`                   | `Optional[float]`                 | `None`                 | Controls diversity via nucleus sampling (0.0 to 1.0)                               |
| `stop`                    | `Optional[Union[str, List[str]]]` | `None`                 | Up to 4 sequences where the API will stop generating further tokens                |
| `seed`                    | `Optional[int]`                   | `None`                 | Random seed for deterministic sampling                                             |
| `logprobs`                | `Optional[bool]`                  | `None`                 | Whether to return log probabilities of the output tokens                           |
| `top_logprobs`            | `Optional[int]`                   | `None`                 | Number of most likely tokens to return log probabilities for (0 to 20)             |
| `user`                    | `Optional[str]`                   | `None`                 | A unique identifier representing your end-user                                     |
| `request_params`          | `Optional[Dict[str, Any]]`        | `None`                 | Additional parameters to include in the request                                    |
| `azure_endpoint`          | `Optional[str]`                   | `None`                 | The Azure endpoint URL (defaults to AZURE\_OPENAI\_ENDPOINT env var)               |
| `api_key`                 | `Optional[str]`                   | `None`                 | The API key for Azure OpenAI (defaults to AZURE\_OPENAI\_API\_KEY env var)         |
| `api_version`             | `str`                             | `"2024-12-01-preview"` | The API version to use                                                             |
| `azure_ad_token`          | `Optional[str]`                   | `None`                 | Azure AD token for authentication                                                  |
| `azure_ad_token_provider` | `Optional[Any]`                   | `None`                 | Azure AD token provider for authentication                                         |
| `timeout`                 | `Optional[float]`                 | `None`                 | Request timeout in seconds                                                         |
| `max_retries`             | `Optional[int]`                   | `None`                 | Maximum number of retries for failed requests                                      |
| `client_params`           | `Optional[Dict[str, Any]]`        | `None`                 | Additional parameters for client configuration                                     |

---

## Define structured models for each step

**URL:** llms-txt#define-structured-models-for-each-step

class ResearchFindings(BaseModel):
    """Structured research findings with key insights"""

topic: str = Field(description="The research topic")
    key_insights: List[str] = Field(description="Main insights discovered", min_items=3)
    trending_technologies: List[str] = Field(
        description="Technologies that are trending", min_items=2
    )
    market_impact: str = Field(description="Potential market impact analysis")
    sources_count: int = Field(description="Number of sources researched")
    confidence_score: float = Field(
        description="Confidence in findings (0.0-1.0)", ge=0.0, le=1.0
    )

class ContentStrategy(BaseModel):
    """Structured content strategy based on research"""

target_audience: str = Field(description="Primary target audience")
    content_pillars: List[str] = Field(description="Main content themes", min_items=3)
    posting_schedule: List[str] = Field(description="Recommended posting schedule")
    key_messages: List[str] = Field(
        description="Core messages to communicate", min_items=3
    )
    hashtags: List[str] = Field(description="Recommended hashtags", min_items=5)
    engagement_tactics: List[str] = Field(
        description="Ways to increase engagement", min_items=2
    )

class FinalContentPlan(BaseModel):
    """Final content plan with specific deliverables"""

campaign_name: str = Field(description="Name for the content campaign")
    content_calendar: List[str] = Field(
        description="Specific content pieces planned", min_items=6
    )
    success_metrics: List[str] = Field(
        description="How to measure success", min_items=3
    )
    budget_estimate: str = Field(description="Estimated budget range")
    timeline: str = Field(description="Implementation timeline")
    risk_factors: List[str] = Field(
        description="Potential risks and mitigation", min_items=2
    )

---

## Selecting Custom Table Names

**URL:** llms-txt#selecting-custom-table-names

**Contents:**
- Usage

Source: https://docs.agno.com/examples/concepts/db/miscellaneous/selecting_tables

Agno allows you to customize table names when using databases, providing flexibility in organizing your data storage.

Specify custom table names when initializing your database connection.

```python selecting_tables.py theme={null}
from agno.agent import Agent
from agno.db.sqlite import SqliteDb

---

## Neon

**URL:** llms-txt#neon

**Contents:**
- Usage

Source: https://docs.agno.com/concepts/db/neon

Learn to use Neon as a database provider for your Agents

Agno supports using [Neon](https://neon.com/) with the `PostgresDb` class.

You can get started with Neon following their [Get Started guide](https://neon.com/docs/get-started/signing-up).

You can also read more about the [`PostgresDb` class](/concepts/db/postgres) in its section.

```python neon_for_agent.py theme={null}
from agno.agent import Agent
from agno.db.postgres import PostgresDb
from os import getenv

---

## Async Postgres for Workflows

**URL:** llms-txt#async-postgres-for-workflows

**Contents:**
- Usage
  - Run PgVector

Source: https://docs.agno.com/examples/concepts/db/async_postgres/async_postgres_for_workflow

Agno supports using [PostgreSQL](https://www.postgresql.org/) asynchronously, with the `AsyncPostgresDb` class.

Install [docker desktop](https://docs.docker.com/desktop/install/mac-install/) and run **PgVector** on port **5532** using:

```python async_postgres_for_workflow.py theme={null}
import asyncio

from agno.agent import Agent
from agno.db.postgres import AsyncPostgresDb
from agno.models.openai import OpenAIChat
from agno.team import Team
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.hackernews import HackerNewsTools
from agno.workflow.step import Step
from agno.workflow.workflow import Workflow

db_url = "postgresql+psycopg_async://ai:ai@localhost:5532/ai"
db = AsyncPostgresDb(db_url=db_url)

**Examples:**

Example 1 (unknown):
```unknown

```

---

## DeepSeek

**URL:** llms-txt#deepseek

**Contents:**
- Parameters

Source: https://docs.agno.com/reference/models/deepseek

The DeepSeek model provides access to DeepSeek's language models.

| Parameter  | Type            | Default                      | Description                                                       |
| ---------- | --------------- | ---------------------------- | ----------------------------------------------------------------- |
| `id`       | `str`           | `"deepseek-chat"`            | The id of the DeepSeek model to use                               |
| `name`     | `str`           | `"DeepSeek"`                 | The name of the model                                             |
| `provider` | `str`           | `"DeepSeek"`                 | The provider of the model                                         |
| `api_key`  | `Optional[str]` | `None`                       | The API key for DeepSeek (defaults to DEEPSEEK\_API\_KEY env var) |
| `base_url` | `str`           | `"https://api.deepseek.com"` | The base URL for the DeepSeek API                                 |

DeepSeek extends the OpenAI-compatible interface and supports most parameters from OpenAI.

---

## Define a tool that adds an item to the shopping list

**URL:** llms-txt#define-a-tool-that-adds-an-item-to-the-shopping-list

def add_item(session_state, item: str) -> str:
    """Add an item to the shopping list."""
    session_state["shopping_list"].append(item)
    return f"The shopping list is now {session_state['shopping_list']}"

---

## Iterative Workflow

**URL:** llms-txt#iterative-workflow

**Contents:**
- Example
- Developer Resources
- Reference

Source: https://docs.agno.com/concepts/workflows/workflow-patterns/iterative-workflow

Quality-driven processes requiring repetition until specific conditions are met

**Example Use-Cases**: Quality improvement loops, retry mechanisms, iterative refinement

Iterative workflows provide controlled repetition with deterministic exit conditions, ensuring consistent quality standards.

<img className="block dark:hidden" src="https://mintcdn.com/agno-v2/JYIBgMrzFEujZh3_/images/workflows-loop-steps-light.png?fit=max&auto=format&n=JYIBgMrzFEujZh3_&q=85&s=edba198de555846a2ea8b2e5b65c6d8e" alt="Workflows loop steps diagram" data-og-width="3441" width="3441" data-og-height="756" height="756" data-path="images/workflows-loop-steps-light.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/agno-v2/JYIBgMrzFEujZh3_/images/workflows-loop-steps-light.png?w=280&fit=max&auto=format&n=JYIBgMrzFEujZh3_&q=85&s=96d954f1d665b4b01e0fb030c0544504 280w, https://mintcdn.com/agno-v2/JYIBgMrzFEujZh3_/images/workflows-loop-steps-light.png?w=560&fit=max&auto=format&n=JYIBgMrzFEujZh3_&q=85&s=f4ce273ba17af6b0b5b417ec2e73385c 560w, https://mintcdn.com/agno-v2/JYIBgMrzFEujZh3_/images/workflows-loop-steps-light.png?w=840&fit=max&auto=format&n=JYIBgMrzFEujZh3_&q=85&s=15eb9ff0487ff79dccaa1a046ad7c32d 840w, https://mintcdn.com/agno-v2/JYIBgMrzFEujZh3_/images/workflows-loop-steps-light.png?w=1100&fit=max&auto=format&n=JYIBgMrzFEujZh3_&q=85&s=ac3e8fb6db4326f2e78948c467924f7c 1100w, https://mintcdn.com/agno-v2/JYIBgMrzFEujZh3_/images/workflows-loop-steps-light.png?w=1650&fit=max&auto=format&n=JYIBgMrzFEujZh3_&q=85&s=45c09671d2bb3190bb14f23ecab28f85 1650w, https://mintcdn.com/agno-v2/JYIBgMrzFEujZh3_/images/workflows-loop-steps-light.png?w=2500&fit=max&auto=format&n=JYIBgMrzFEujZh3_&q=85&s=05a4004d7c8228caefbacbc21a2efd2f 2500w" />

<img className="hidden dark:block" src="https://mintcdn.com/agno-v2/JYIBgMrzFEujZh3_/images/workflows-loop-steps.png?fit=max&auto=format&n=JYIBgMrzFEujZh3_&q=85&s=30027b401899598a38a73c6038d1d988" alt="Workflows loop steps diagram" data-og-width="3441" width="3441" data-og-height="756" height="756" data-path="images/workflows-loop-steps.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/agno-v2/JYIBgMrzFEujZh3_/images/workflows-loop-steps.png?w=280&fit=max&auto=format&n=JYIBgMrzFEujZh3_&q=85&s=aa15df4e2e353012150474b5fa26d5c5 280w, https://mintcdn.com/agno-v2/JYIBgMrzFEujZh3_/images/workflows-loop-steps.png?w=560&fit=max&auto=format&n=JYIBgMrzFEujZh3_&q=85&s=f411273ea9e60981989e16324940fbc0 560w, https://mintcdn.com/agno-v2/JYIBgMrzFEujZh3_/images/workflows-loop-steps.png?w=840&fit=max&auto=format&n=JYIBgMrzFEujZh3_&q=85&s=a71354c338a1520f27797ca6e53dc378 840w, https://mintcdn.com/agno-v2/JYIBgMrzFEujZh3_/images/workflows-loop-steps.png?w=1100&fit=max&auto=format&n=JYIBgMrzFEujZh3_&q=85&s=cf1495378fc757cd5995b96c734cda71 1100w, https://mintcdn.com/agno-v2/JYIBgMrzFEujZh3_/images/workflows-loop-steps.png?w=1650&fit=max&auto=format&n=JYIBgMrzFEujZh3_&q=85&s=3d32977904938d0cc22407d61c6efd8e 1650w, https://mintcdn.com/agno-v2/JYIBgMrzFEujZh3_/images/workflows-loop-steps.png?w=2500&fit=max&auto=format&n=JYIBgMrzFEujZh3_&q=85&s=e3e1c672c2953455defceb971b803bce 2500w" />

## Developer Resources

* [Loop Steps Workflow](/examples/concepts/workflows/03_workflows_loop_execution/loop_steps_workflow)

For complete API documentation, see [Loop Steps Reference](/reference/workflows/loop-steps).

---

## Print the aggregated metrics for the whole run

**URL:** llms-txt#print-the-aggregated-metrics-for-the-whole-run

print("---" * 5, "Run Metrics", "---" * 5)
pprint(run_response.metrics.to_dict())

---

## "I don't pain anymore, i draw instead.", stream=True, user_id=john_doe_id

**URL:** llms-txt#"i-don't-pain-anymore,-i-draw-instead.",-stream=true,-user_id=john_doe_id

---

## "Get the user profile for the user with ID 123 and tell me about their experience level.",

**URL:** llms-txt#"get-the-user-profile-for-the-user-with-id-123-and-tell-me-about-their-experience-level.",

---

## Reasoning O3 Mini

**URL:** llms-txt#reasoning-o3-mini

**Contents:**
- Code

Source: https://docs.agno.com/examples/models/openai/responses/reasoning_o3_mini

```python cookbook/models/openai/responses/reasoning_o3_mini.py theme={null}
from agno.agent import Agent
from agno.models.openai import OpenAIResponses
from agno.tools.duckduckgo import DuckDuckGoTools

agent = Agent(
    model=OpenAIResponses(id="gpt-5-mini"),
    tools=[DuckDuckGoTools()],
    markdown=True,
)

---

## add_session_summary_to_context=False,

**URL:** llms-txt#add_session_summary_to_context=false,

---

## NVIDIA GPU (CUDA)

**URL:** llms-txt#nvidia-gpu-(cuda)

---

## Download the file using the download_file function

**URL:** llms-txt#download-the-file-using-the-download_file-function

**Contents:**
- Usage

download_file(
    "https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf", str(pdf_path)
)

agent = Agent(
    model=Claude(id="claude-sonnet-4@20250514"),
    markdown=True,
)

agent.print_response(
    "Summarize the contents of the attached file.",
    files=[
        File(
            filepath=pdf_path,
        ),
    ],
)
bash Mac theme={null}
      export CLOUD_ML_REGION=xxx
      export GOOGLE_CLOUD_PROJECT=xxx
      bash Windows theme={null}
        setx CLOUD_ML_REGION xxx
        setx GOOGLE_CLOUD_PROJECT xxx
      bash Mac theme={null}
      python cookbook/models/vertexai/claude/pdf_input_local.py
      bash Windows theme={null}
      python cookbook/models/vertexai/claude/pdf_input_local.py
      ```
    </CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Set your environment variables">
    <CodeGroup>
```

Example 2 (unknown):
```unknown

```

Example 3 (unknown):
```unknown
</CodeGroup>
  </Step>

  <Step title="Authenticate your CLI session">
    `gcloud auth application-default login `

    <Note>You dont need to authenticate your CLI every time. </Note>
  </Step>

  <Step title="Install libraries">`pip install -U anthropic agno `</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Configuration for the Metrics page

**URL:** llms-txt#configuration-for-the-metrics-page

metrics:
  display_name: <DISPLAY_NAME>
  dbs:
    - <DB_ID>
      domain_config:
        display_name: <DISPLAY_NAME>
    ...
```

---

## Initialize Anthropic client

**URL:** llms-txt#initialize-anthropic-client

---

## Fal Tools

**URL:** llms-txt#fal-tools

**Contents:**
- Code
- Usage

Source: https://docs.agno.com/examples/concepts/tools/others/fal

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Set your API keys">
    
  </Step>

<Step title="Install libraries">
    
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Set your API keys">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Execute research with structured input

**URL:** llms-txt#execute-research-with-structured-input

team.print_response(input=research_request)

---

## List Evaluation Runs

**URL:** llms-txt#list-evaluation-runs

Source: https://docs.agno.com/reference-api/schema/evals/list-evaluation-runs

get /eval-runs
Retrieve paginated evaluation runs with filtering and sorting options. Filter by agent, team, workflow, model, or evaluation type.

---

## Get current weather for a location

**URL:** llms-txt#get-current-weather-for-a-location

**Contents:**
- Toolkit Params
- Toolkit Functions
- Developer Resources

agent.print_response("What's the current weather in Tokyo?", markdown=True)
```

| Parameter                | Type   | Default  | Description                                                                  |
| ------------------------ | ------ | -------- | ---------------------------------------------------------------------------- |
| `api_key`                | `str`  | `None`   | OpenWeatherMap API key. If not provided, uses OPENWEATHER\_API\_KEY env var. |
| `units`                  | `str`  | `metric` | Units of measurement. Options: 'standard', 'metric', 'imperial'.             |
| `enable_current_weather` | `bool` | `True`   | Enable current weather function.                                             |
| `enable_forecast`        | `bool` | `True`   | Enable forecast function.                                                    |
| `enable_air_pollution`   | `bool` | `True`   | Enable air pollution function.                                               |
| `enable_geocoding`       | `bool` | `True`   | Enable geocoding function.                                                   |

| Function              | Description                                                                                          |
| --------------------- | ---------------------------------------------------------------------------------------------------- |
| `get_current_weather` | Gets current weather data for a location. Takes a location name (e.g., "London").                    |
| `get_forecast`        | Gets weather forecast for a location. Takes a location name and optional number of days (default 5). |
| `get_air_pollution`   | Gets current air pollution data for a location. Takes a location name.                               |
| `geocode_location`    | Converts a location name to geographic coordinates. Takes a location name and optional result limit. |

## Developer Resources

* View [Tools](https://github.com/agno-agi/agno/blob/main/libs/agno/agno/tools/openweather.py)
* View [Cookbook](https://github.com/agno-agi/agno/tree/main/cookbook/tools/openweather_tools.py)

---

## File Tools

**URL:** llms-txt#file-tools

**Contents:**
- Code
- Usage

Source: https://docs.agno.com/examples/concepts/tools/local/file

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Set your API key">
    
  </Step>

<Step title="Install libraries">
    
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Set your API key">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## WhatsApp

**URL:** llms-txt#whatsapp

**Contents:**
- Prerequisites
- Example

Source: https://docs.agno.com/concepts/tools/toolkits/social/whatsapp

**WhatsAppTools** enable an Agent to interact with the WhatsApp Business API, allowing it to send text and template messages.

This cookbook demonstrates how to use WhatsApp integration with Agno. Before running this example,
you'''ll need to complete these setup steps:

1. Create Meta Developer Account
   * Go to [Meta Developer Portal](https://developers.facebook.com/) and create a new account
   * Create a new app at [Meta Apps Dashboard](https://developers.facebook.com/apps/)
   * Enable WhatsApp integration for your app [here](https://developers.facebook.com/docs/whatsapp/cloud-api/get-started)

2. Set Up WhatsApp Business API
   You can get your WhatsApp Business Account ID from [Business Settings](https://developers.facebook.com/docs/whatsapp/cloud-api/get-started)

3. Configure Environment
   * Set these environment variables:

* For first-time outreach, you must use pre-approved message templates
  [here](https://developers.facebook.com/docs/whatsapp/cloud-api/guides/send-message-templates)
* Test messages can only be sent to numbers that are registered in your test environment

The example below shows how to send a template message using Agno'''s WhatsApp tools.
For more complex use cases, check out the WhatsApp Cloud API documentation:
[here](https://developers.facebook.com/docs/whatsapp/cloud-api/overview)

The following agent will send a template message using WhatsApp:

```python cookbook/tools/whatsapp_tool.py theme={null}
from agno.agent import Agent
from agno.models.google import Gemini
from agno.tools.whatsapp import WhatsAppTools

agent = Agent(
    name="whatsapp",
    model=Gemini(id="gemini-2.0-flash"),
    tools=[WhatsAppTools()]
)

**Examples:**

Example 1 (unknown):
```unknown
Important Notes:

* For first-time outreach, you must use pre-approved message templates
  [here](https://developers.facebook.com/docs/whatsapp/cloud-api/guides/send-message-templates)
* Test messages can only be sent to numbers that are registered in your test environment

The example below shows how to send a template message using Agno'''s WhatsApp tools.
For more complex use cases, check out the WhatsApp Cloud API documentation:
[here](https://developers.facebook.com/docs/whatsapp/cloud-api/overview)

## Example

The following agent will send a template message using WhatsApp:
```

---

## This should route to the stock_searcher

**URL:** llms-txt#this-should-route-to-the-stock_searcher

**Contents:**
- Usage

response = team.run("What is the current stock price of NVDA?")
assert isinstance(response.content, StockReport)
pprint_run_response(response)
bash  theme={null}
    pip install agno openai ddgs
    bash  theme={null}
    export OPENAI_API_KEY=****
    bash  theme={null}
    python cookbook/examples/teams/structured_input_output/00_pydantic_model_output.py
    ```
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Install required libraries">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Set environment variables">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run the agent">
```

---

## Delete all content

**URL:** llms-txt#delete-all-content

**Contents:**
  - Filtering and Search

knowledge.remove_all_content()
python  theme={null}

**Examples:**

Example 1 (unknown):
```unknown
### Filtering and Search

ContentsDB enables powerful filtering capabilities:
```

---

## Run a vector-based query

**URL:** llms-txt#run-a-vector-based-query

**Contents:**
- Usage

results = vector_db.search("chicken coconut soup", limit=5)
print("Vector Search Results:", results)
bash  theme={null}
    pip install -U agno sqlalchemy psycopg pgvector
    bash Mac theme={null}
      python cookbook/knowledge/search_type/vector_search.py
      bash Windows theme={null}
      python cookbook/knowledge/search_type/vector_search.py
      ```
    </CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Step title="Install libraries">
```

Example 2 (unknown):
```unknown
</Step>

  <Snippet file="run-pgvector-step.mdx" />

  <Step title="Run the example">
    <CodeGroup>
```

Example 3 (unknown):
```unknown

```

---

## Initialize Milvus vector db

**URL:** llms-txt#initialize-milvus-vector-db

vector_db = Milvus(
    collection="recipes",
    uri="tmp/milvus.db",
)

---

## Youtube

**URL:** llms-txt#youtube

**Contents:**
- Prerequisites
- Example
- Toolkit Params
- Toolkit Functions
- Developer Resources

Source: https://docs.agno.com/concepts/tools/toolkits/others/youtube

**YouTubeTools** enable an Agent to access captions and metadata of YouTube videos, when provided with a video URL.

The following example requires the `youtube_transcript_api` library.

The following agent will provide a summary of a YouTube video.

| Param                         | Type        | Default | Description                                                                        |
| ----------------------------- | ----------- | ------- | ---------------------------------------------------------------------------------- |
| `get_video_captions`          | `bool`      | `True`  | Enables the functionality to retrieve video captions.                              |
| `get_video_data`              | `bool`      | `True`  | Enables the functionality to retrieve video metadata and other related data.       |
| `languages`                   | `List[str]` | -       | Specifies the list of languages for which data should be retrieved, if applicable. |
| `enable_get_video_captions`   | `bool`      | `True`  | Enable the get\_video\_captions functionality.                                     |
| `enable_get_video_data`       | `bool`      | `True`  | Enable the get\_video\_data functionality.                                         |
| `enable_get_video_timestamps` | `bool`      | `True`  | Enable the get\_video\_timestamps functionality.                                   |
| `all`                         | `bool`      | `False` | Enable all functionality.                                                          |

| Function                       | Description                                                |
| ------------------------------ | ---------------------------------------------------------- |
| `get_youtube_video_captions`   | This function retrieves the captions of a YouTube video.   |
| `get_youtube_video_data`       | This function retrieves the metadata of a YouTube video.   |
| `get_youtube_video_timestamps` | This function retrieves the timestamps of a YouTube video. |

## Developer Resources

* View [Tools](https://github.com/agno-agi/agno/blob/main/libs/agno/agno/tools/youtube.py)
* View [Cookbook](https://github.com/agno-agi/agno/tree/main/cookbook/tools/youtube_tools.py)

**Examples:**

Example 1 (unknown):
```unknown
## Example

The following agent will provide a summary of a YouTube video.
```

---

## Slack Tools

**URL:** llms-txt#slack-tools

**Contents:**
- Code
- Usage

Source: https://docs.agno.com/examples/concepts/tools/social/slack

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Set your API key">
    
  </Step>

<Step title="Set your Slack token">
    
  </Step>

<Step title="Install libraries">
    
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Set your API key">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Set your Slack token">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 4 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

---

## Google Calendar Tools

**URL:** llms-txt#google-calendar-tools

**Contents:**
- Code
- Usage

Source: https://docs.agno.com/examples/concepts/tools/others/google_calendar

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Set your API key">
    
  </Step>

<Step title="Set up Google Calendar credentials">
    
  </Step>

<Step title="Install libraries">
    
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Set your API key">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Set up Google Calendar credentials">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 4 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

---

## Early Stopping

**URL:** llms-txt#early-stopping

**Contents:**
- Example

Source: https://docs.agno.com/concepts/workflows/early-stop

How to early stop workflows

Workflows support early termination when specific conditions are met, preventing unnecessary processing and implementing safety gates. Any step can trigger early termination by returning `StepOutput(stop=True)`, immediately halting the entire workflow execution.

<img className="block dark:hidden" src="https://mintcdn.com/agno-v2/JYIBgMrzFEujZh3_/images/workflows-early-stop-light.png?fit=max&auto=format&n=JYIBgMrzFEujZh3_&q=85&s=8d2d411a2853e475b18e4a195a9d65df" alt="Workflows early stop diagram" data-og-width="7281" width="7281" data-og-height="1179" height="1179" data-path="images/workflows-early-stop-light.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/agno-v2/JYIBgMrzFEujZh3_/images/workflows-early-stop-light.png?w=280&fit=max&auto=format&n=JYIBgMrzFEujZh3_&q=85&s=c0dcd798df8113cf9c146b8d6946f2e4 280w, https://mintcdn.com/agno-v2/JYIBgMrzFEujZh3_/images/workflows-early-stop-light.png?w=560&fit=max&auto=format&n=JYIBgMrzFEujZh3_&q=85&s=230326e06f4d0d607e4fff253910b7d0 560w, https://mintcdn.com/agno-v2/JYIBgMrzFEujZh3_/images/workflows-early-stop-light.png?w=840&fit=max&auto=format&n=JYIBgMrzFEujZh3_&q=85&s=7ea8b5e50c4303af888b0bab7fcdb6f9 840w, https://mintcdn.com/agno-v2/JYIBgMrzFEujZh3_/images/workflows-early-stop-light.png?w=1100&fit=max&auto=format&n=JYIBgMrzFEujZh3_&q=85&s=28702a9fcd250dd3f678f84b40b4f207 1100w, https://mintcdn.com/agno-v2/JYIBgMrzFEujZh3_/images/workflows-early-stop-light.png?w=1650&fit=max&auto=format&n=JYIBgMrzFEujZh3_&q=85&s=9cddb518531d6dc748a4d7a9da2308a4 1650w, https://mintcdn.com/agno-v2/JYIBgMrzFEujZh3_/images/workflows-early-stop-light.png?w=2500&fit=max&auto=format&n=JYIBgMrzFEujZh3_&q=85&s=1c1b977a426432c9e08df94a524a4d2f 2500w" />

<img className="hidden dark:block" src="https://mintcdn.com/agno-v2/JYIBgMrzFEujZh3_/images/workflows-early-stop.png?fit=max&auto=format&n=JYIBgMrzFEujZh3_&q=85&s=6c2609a237ba9a60fde24fb6ef3c25dc" alt="Workflows early stop diagram" data-og-width="7281" width="7281" data-og-height="1179" height="1179" data-path="images/workflows-early-stop.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/agno-v2/JYIBgMrzFEujZh3_/images/workflows-early-stop.png?w=280&fit=max&auto=format&n=JYIBgMrzFEujZh3_&q=85&s=8ab6aca3447a781d230858aefc525613 280w, https://mintcdn.com/agno-v2/JYIBgMrzFEujZh3_/images/workflows-early-stop.png?w=560&fit=max&auto=format&n=JYIBgMrzFEujZh3_&q=85&s=5ad070c303d4203799af900d5bf15dae 560w, https://mintcdn.com/agno-v2/JYIBgMrzFEujZh3_/images/workflows-early-stop.png?w=840&fit=max&auto=format&n=JYIBgMrzFEujZh3_&q=85&s=c29866bad79a0fa0215962c59605e93f 840w, https://mintcdn.com/agno-v2/JYIBgMrzFEujZh3_/images/workflows-early-stop.png?w=1100&fit=max&auto=format&n=JYIBgMrzFEujZh3_&q=85&s=5b498d3d0faacc133474650fb236e29a 1100w, https://mintcdn.com/agno-v2/JYIBgMrzFEujZh3_/images/workflows-early-stop.png?w=1650&fit=max&auto=format&n=JYIBgMrzFEujZh3_&q=85&s=8b8e0f5a7070635380fdf23217e5fda3 1650w, https://mintcdn.com/agno-v2/JYIBgMrzFEujZh3_/images/workflows-early-stop.png?w=2500&fit=max&auto=format&n=JYIBgMrzFEujZh3_&q=85&s=97cf31c8418685c1b28185e594c11e44 2500w" />

```python  theme={null}
from agno.workflow import Step, Workflow, StepInput, StepOutput

def security_gate(step_input: StepInput) -> StepOutput:
    """Security gate that stops deployment if vulnerabilities found"""
    security_result = step_input.previous_step_content or ""
    
    if "VULNERABLE" in security_result.upper():
        return StepOutput(
            content="🚨 SECURITY ALERT: Critical vulnerabilities detected. Deployment blocked.",
            stop=True  # Stop the entire workflow
        )
    else:
        return StepOutput(
            content="✅ Security check passed. Proceeding with deployment...",
            stop=False
        )

---

## Ask questions that benefit from real-time information

**URL:** llms-txt#ask-questions-that-benefit-from-real-time-information

**Contents:**
- Usage

agent.print_response(
    "What are the current market trends in renewable energy?",
    stream=True,
    markdown=True,
)
bash  theme={null}
    export GOOGLE_API_KEY=xxx
    bash  theme={null}
    pip install -U google-genai agno
    bash Mac theme={null}
      python cookbook/models/google/gemini/grounding.py
      bash Windows theme={null}
      python cookbook/models/google/gemini/grounding.py
      ```
    </CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Set your API key">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## -*- Ask a follow up question that continues the conversation

**URL:** llms-txt#-*--ask-a-follow-up-question-that-continues-the-conversation

team.print_response("What was my first message?", stream=True)

---

## Keep executing externally-required tools until the run completes

**URL:** llms-txt#keep-executing-externally-required-tools-until-the-run-completes

while (
    run_response.is_paused and len(run_response.tools_awaiting_external_execution) > 0
):
    for external_tool in run_response.tools_awaiting_external_execution:
        if external_tool.tool_name == execute_shell_command.name:
            print(
                f"Executing {external_tool.tool_name} with args {external_tool.tool_args} externally"
            )
            result = execute_shell_command.entrypoint(**external_tool.tool_args)
            external_tool.result = result
        else:
            print(f"Skipping unsupported external tool: {external_tool.tool_name}")

run_response = asyncio.run(agent.acontinue_run(run_response=run_response))

pprint.pprint_run_response(run_response)

---

## Get the response in a variable

**URL:** llms-txt#get-the-response-in-a-variable

---

## Create team with structured output streaming

**URL:** llms-txt#create-team-with-structured-output-streaming

**Contents:**
- Usage

team = Team(
    name="Stock Research Team",
    model=OpenAIChat("gpt-5-mini"),
    members=[stock_searcher, company_info_agent],
    output_schema=StockReport,
    markdown=True,
    show_members_responses=True,
)

async def test_structured_streaming():
    """Test async structured output streaming."""
    # Run with streaming and consume the async generator to get the final response
    async_stream = team.arun(
        "Give me a stock report for NVDA", stream=True, stream_events=True
    )

# Consume the async streaming events and get the final response
    run_response = None
    async for event_or_response in async_stream:
        # The last item in the stream is the final TeamRunOutput
        run_response = event_or_response

assert isinstance(run_response.content, StockReport)
    print(f"✅ Stock Symbol: {run_response.content.symbol}")
    print(f"✅ Company Name: {run_response.content.company_name}")

async def test_structured_streaming_with_arun():
    """Test async structured output streaming using arun() method."""
    await apprint_run_response(
        team.arun(
            input="Give me a stock report for AAPL",
            stream=True,
            stream_events=True,
        )
    )

if __name__ == "__main__":
    asyncio.run(test_structured_streaming())

asyncio.run(test_structured_streaming_with_arun())
bash  theme={null}
    pip install agno exa_py pydantic
    bash  theme={null}
    export OPENAI_API_KEY=****
    export EXA_API_KEY=****
    bash  theme={null}
    python cookbook/examples/teams/structured_input_output/05_async_structured_output_streaming.py
    ```
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Install required libraries">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Set environment variables">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run the agent">
```

---

## Tool Use O3

**URL:** llms-txt#tool-use-o3

**Contents:**
- Code
- Usage

Source: https://docs.agno.com/examples/models/openai/responses/tool_use_o3

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Set your API key">
    
  </Step>

<Step title="Install libraries">
    
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Set your API key">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Older searches will be filtered from context

**URL:** llms-txt#older-searches-will-be-filtered-from-context

**Contents:**
- Additional input

team.print_response("Search for startups") 
team.print_response("Search for weather in Mumbai")
team.print_response("Search for latest startup fundraises")
team.print_response("What topics did I search for recently?")
python  theme={null}
from agno.team import Team
from agno.models.message import Message
from agno.models.openai.chat import OpenAIChat

**Examples:**

Example 1 (unknown):
```unknown
In this example:

* **Run 1-3:** Team sees tool calls \[1], \[1,2], \[1,2,3,4,5]
* **Run 4:** Team sees tool calls from runs that fit within the last 5 tool calls (older tool calls filtered out)
* **Run 5:** Team sees tool calls from runs that fit within the last 5 tool calls (earlier tool calls filtered out)

<Note>
  **Important:** `max_tool_calls_from_history` filters tool calls from the runs loaded by `num_history_runs`. Your database always contains the complete history.
</Note>

See the [full example](/examples/concepts/teams/context_management/filter_tool_calls_from_history) for a complete demonstration.

## Additional input

You can add entire additional messages to your team's context using the `additional_input` parameter.
These messages are added to the context as if they were part of the conversation history.

You can give your team examples of how it should respond (also called "few-shot prompting"):
```

---

## Verbosity Control

**URL:** llms-txt#verbosity-control

**Contents:**
- Code
- Usage

Source: https://docs.agno.com/examples/models/openai/responses/verbosity_control

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Set your API key">
    
  </Step>

<Step title="Install libraries">
    
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Set your API key">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Reliability with Teams

**URL:** llms-txt#reliability-with-teams

**Contents:**
- Code

Source: https://docs.agno.com/examples/concepts/evals/reliability/reliability_with_teams

Learn how to assert an Agno Team is making the expected tool calls.

---

## ✅ Choose one approach

**URL:** llms-txt#✅-choose-one-approach

agent = Agent(db=db, enable_user_memories=True)  # Automatic

---

## xAI

**URL:** llms-txt#xai

**Contents:**
- Parameters

Source: https://docs.agno.com/reference/models/xai

The xAI model provides access to xAI's language models.

| Parameter  | Type            | Default                 | Description                                             |
| ---------- | --------------- | ----------------------- | ------------------------------------------------------- |
| `id`       | `str`           | `"grok-beta"`           | The id of the xAI model to use                          |
| `name`     | `str`           | `"xAI"`                 | The name of the model                                   |
| `provider` | `str`           | `"xAI"`                 | The provider of the model                               |
| `api_key`  | `Optional[str]` | `None`                  | The API key for xAI (defaults to XAI\_API\_KEY env var) |
| `base_url` | `str`           | `"https://api.x.ai/v1"` | The base URL for the xAI API                            |

xAI extends the OpenAI-compatible interface and supports most parameters from OpenAI.

---

## Performance with Teams

**URL:** llms-txt#performance-with-teams

**Contents:**
- Code

Source: https://docs.agno.com/examples/concepts/evals/performance/performance_team_instantiation

Learn how to analyze the runtime and memory usage of an Agno Team.

---

## Get audio files from storage/audio directory

**URL:** llms-txt#get-audio-files-from-storage/audio-directory

**Contents:**
- Toolkit Params
- Toolkit Functions
- Developer Resources

agno_root_dir = Path(__file__).parent.parent.parent.resolve()
audio_storage_dir = agno_root_dir.joinpath("storage/audio")
if not audio_storage_dir.exists():
    audio_storage_dir.mkdir(exist_ok=True, parents=True)

agent = Agent(
    name="Transcription Agent",
    model=OpenAIChat(id="gpt-5-mini"),
    tools=[MLXTranscribeTools(base_dir=audio_storage_dir)],
    instructions=[
        "To transcribe an audio file, use the `transcribe` tool with the name of the audio file as the argument.",
        "You can find all available audio files using the `read_files` tool.",
    ],
    markdown=True,
)

agent.print_response("Summarize the reid hoffman ted talk, split into sections", stream=True)
```

| Parameter                         | Type                           | Default                                  | Description                                  |
| --------------------------------- | ------------------------------ | ---------------------------------------- | -------------------------------------------- |
| `base_dir`                        | `Path`                         | `Path.cwd()`                             | Base directory for audio files               |
| `enable_read_files_in_base_dir`   | `bool`                         | `True`                                   | Whether to register the read\_files function |
| `path_or_hf_repo`                 | `str`                          | `"mlx-community/whisper-large-v3-turbo"` | Path or HuggingFace repo for the model       |
| `verbose`                         | `bool`                         | `None`                                   | Enable verbose output                        |
| `temperature`                     | `float` or `Tuple[float, ...]` | `None`                                   | Temperature for sampling                     |
| `compression_ratio_threshold`     | `float`                        | `None`                                   | Compression ratio threshold                  |
| `logprob_threshold`               | `float`                        | `None`                                   | Log probability threshold                    |
| `no_speech_threshold`             | `float`                        | `None`                                   | No speech threshold                          |
| `condition_on_previous_text`      | `bool`                         | `None`                                   | Whether to condition on previous text        |
| `initial_prompt`                  | `str`                          | `None`                                   | Initial prompt for transcription             |
| `word_timestamps`                 | `bool`                         | `None`                                   | Enable word-level timestamps                 |
| `prepend_punctuations`            | `str`                          | `None`                                   | Punctuations to prepend                      |
| `append_punctuations`             | `str`                          | `None`                                   | Punctuations to append                       |
| `clip_timestamps`                 | `str` or `List[float]`         | `None`                                   | Clip timestamps                              |
| `hallucination_silence_threshold` | `float`                        | `None`                                   | Hallucination silence threshold              |
| `decode_options`                  | `dict`                         | `None`                                   | Additional decoding options                  |

| Function     | Description                                 |
| ------------ | ------------------------------------------- |
| `transcribe` | Transcribes an audio file using MLX Whisper |
| `read_files` | Lists all audio files in the base directory |

## Developer Resources

* View [Tools](https://github.com/agno-agi/agno/blob/main/libs/agno/agno/tools/mlx_transcribe.py)
* View [Cookbook](https://github.com/agno-agi/agno/tree/main/cookbook/tools/mlx_transcribe_tools.py)

---

## CI/CD

**URL:** llms-txt#ci/cd

**Contents:**
- Test and Validate on every PR
- Build Docker Images with Github Releases
- Build ECR Images with Github Releases

Source: https://docs.agno.com/templates/infra-management/ci-cd

Agno templates come pre-configured with [Github Actions](https://docs.github.com/en/actions) for CI/CD. We can

1. [Test and Validate on every PR](#test-and-validate-on-every-pr)
2. [Build Docker Images with Github Releases](#build-docker-images-with-github-releases)
3. [Build ECR Images with Github Releases](#build-ecr-images-with-github-releases)

## Test and Validate on every PR

Whenever a PR is opened against the `main` branch, a validate script runs that ensures

1. The changes are formatted using ruff
2. All unit-tests pass
3. The changes don't have any typing or linting errors.

Checkout the `.github/workflows/validate.yml` file for more information.

<img src="https://mintcdn.com/agno-v2/yeT29TzCG5roT0hQ/images/validate-cicd.png?fit=max&auto=format&n=yeT29TzCG5roT0hQ&q=85&s=01d9c697e3f87a8248fa8daa6fac3922" alt="validate-cicd" data-og-width="940" width="940" data-og-height="353" height="353" data-path="images/validate-cicd.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/agno-v2/yeT29TzCG5roT0hQ/images/validate-cicd.png?w=280&fit=max&auto=format&n=yeT29TzCG5roT0hQ&q=85&s=e8bf732e3895a4377b65766816624fc5 280w, https://mintcdn.com/agno-v2/yeT29TzCG5roT0hQ/images/validate-cicd.png?w=560&fit=max&auto=format&n=yeT29TzCG5roT0hQ&q=85&s=5dd89160c72ebf2f088d75bd8dfe52dc 560w, https://mintcdn.com/agno-v2/yeT29TzCG5roT0hQ/images/validate-cicd.png?w=840&fit=max&auto=format&n=yeT29TzCG5roT0hQ&q=85&s=8a27817ed129343cad278184145eb5ee 840w, https://mintcdn.com/agno-v2/yeT29TzCG5roT0hQ/images/validate-cicd.png?w=1100&fit=max&auto=format&n=yeT29TzCG5roT0hQ&q=85&s=eb51747558d308ee09444057ba4f7f85 1100w, https://mintcdn.com/agno-v2/yeT29TzCG5roT0hQ/images/validate-cicd.png?w=1650&fit=max&auto=format&n=yeT29TzCG5roT0hQ&q=85&s=57ed9ba5331f3170a62cdfe304d9c05d 1650w, https://mintcdn.com/agno-v2/yeT29TzCG5roT0hQ/images/validate-cicd.png?w=2500&fit=max&auto=format&n=yeT29TzCG5roT0hQ&q=85&s=6b995305ffe38e39b14bbe7ef18ce4ba 2500w" />

## Build Docker Images with Github Releases

If you're using [Dockerhub](https://hub.docker.com/) for images, you can buld and push the images throug a Github Release. This action is defined in the `.github/workflows/docker-images.yml` file.

1. Create a [Docker Access Token](https://hub.docker.com/settings/security) for Github Actions

<img src="https://mintcdn.com/agno-v2/Y7twezR0wF2re1xh/images/docker-access-token.png?fit=max&auto=format&n=Y7twezR0wF2re1xh&q=85&s=870118e1906c093108643eceaaf577e6" alt="docker-access-token" data-og-width="742" width="742" data-og-height="568" height="568" data-path="images/docker-access-token.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/agno-v2/Y7twezR0wF2re1xh/images/docker-access-token.png?w=280&fit=max&auto=format&n=Y7twezR0wF2re1xh&q=85&s=44ecd6f45c24f63b65c47d63d5dda04e 280w, https://mintcdn.com/agno-v2/Y7twezR0wF2re1xh/images/docker-access-token.png?w=560&fit=max&auto=format&n=Y7twezR0wF2re1xh&q=85&s=7010dc82d6907e7a0e4c9b727a5b1f14 560w, https://mintcdn.com/agno-v2/Y7twezR0wF2re1xh/images/docker-access-token.png?w=840&fit=max&auto=format&n=Y7twezR0wF2re1xh&q=85&s=f35b57f4d86867cd143d00861e9a188d 840w, https://mintcdn.com/agno-v2/Y7twezR0wF2re1xh/images/docker-access-token.png?w=1100&fit=max&auto=format&n=Y7twezR0wF2re1xh&q=85&s=ae91c75b8692c79f3f67b7c949a87305 1100w, https://mintcdn.com/agno-v2/Y7twezR0wF2re1xh/images/docker-access-token.png?w=1650&fit=max&auto=format&n=Y7twezR0wF2re1xh&q=85&s=ce6d88dc5073bcbb6ec943c2ff9f1750 1650w, https://mintcdn.com/agno-v2/Y7twezR0wF2re1xh/images/docker-access-token.png?w=2500&fit=max&auto=format&n=Y7twezR0wF2re1xh&q=85&s=c730143dfa3232d2daffbe8f04b77eb1 2500w" />

2. Create secret variables `DOCKERHUB_REPO`, `DOCKERHUB_TOKEN` and `DOCKERHUB_USERNAME` in your github repo. These variables are used by the action in `.github/workflows/docker-images.yml`

<img src="https://mintcdn.com/agno-v2/Y7twezR0wF2re1xh/images/github-actions-docker-secrets.png?fit=max&auto=format&n=Y7twezR0wF2re1xh&q=85&s=70015ffb61a3816c45806f85c8c44877" alt="github-actions-docker-secrets" data-og-width="1143" width="1143" data-og-height="822" height="822" data-path="images/github-actions-docker-secrets.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/agno-v2/Y7twezR0wF2re1xh/images/github-actions-docker-secrets.png?w=280&fit=max&auto=format&n=Y7twezR0wF2re1xh&q=85&s=98a34ba0df0ee5fa4ad3ed51852978d1 280w, https://mintcdn.com/agno-v2/Y7twezR0wF2re1xh/images/github-actions-docker-secrets.png?w=560&fit=max&auto=format&n=Y7twezR0wF2re1xh&q=85&s=025950971a2df62fd409c74a6bf265df 560w, https://mintcdn.com/agno-v2/Y7twezR0wF2re1xh/images/github-actions-docker-secrets.png?w=840&fit=max&auto=format&n=Y7twezR0wF2re1xh&q=85&s=fdb266b2ef9b01200bbf0704a607af87 840w, https://mintcdn.com/agno-v2/Y7twezR0wF2re1xh/images/github-actions-docker-secrets.png?w=1100&fit=max&auto=format&n=Y7twezR0wF2re1xh&q=85&s=3a78c11b5d5fe0e1294bdea54fd03004 1100w, https://mintcdn.com/agno-v2/Y7twezR0wF2re1xh/images/github-actions-docker-secrets.png?w=1650&fit=max&auto=format&n=Y7twezR0wF2re1xh&q=85&s=e37288626fd82e5fa083c0bc48cf00c9 1650w, https://mintcdn.com/agno-v2/Y7twezR0wF2re1xh/images/github-actions-docker-secrets.png?w=2500&fit=max&auto=format&n=Y7twezR0wF2re1xh&q=85&s=f854830e5040e368d1154389162e173e 2500w" />

3. Run workflow using a Github Release

This workflow is configured to run when a release is created. Create a new release using:

<Note>
  Confirm the image name in the `.github/workflows/docker-images.yml` file before running
</Note>

<img src="https://mintcdn.com/agno-v2/Y7twezR0wF2re1xh/images/github-actions-build-docker.png?fit=max&auto=format&n=Y7twezR0wF2re1xh&q=85&s=2f96812f7b12f8e1f9d831152556c5d7" alt="github-actions-build-docker" data-og-width="1042" width="1042" data-og-height="732" height="732" data-path="images/github-actions-build-docker.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/agno-v2/Y7twezR0wF2re1xh/images/github-actions-build-docker.png?w=280&fit=max&auto=format&n=Y7twezR0wF2re1xh&q=85&s=ec6400a97ff55a3a44da52d9e53473ca 280w, https://mintcdn.com/agno-v2/Y7twezR0wF2re1xh/images/github-actions-build-docker.png?w=560&fit=max&auto=format&n=Y7twezR0wF2re1xh&q=85&s=6568fab440d3f4794aecce651f2a3a0e 560w, https://mintcdn.com/agno-v2/Y7twezR0wF2re1xh/images/github-actions-build-docker.png?w=840&fit=max&auto=format&n=Y7twezR0wF2re1xh&q=85&s=c88fbcfbeb5e647b45e56d064c8066b6 840w, https://mintcdn.com/agno-v2/Y7twezR0wF2re1xh/images/github-actions-build-docker.png?w=1100&fit=max&auto=format&n=Y7twezR0wF2re1xh&q=85&s=c7d8bc7f1d25a8167dcbe7920bfca3e6 1100w, https://mintcdn.com/agno-v2/Y7twezR0wF2re1xh/images/github-actions-build-docker.png?w=1650&fit=max&auto=format&n=Y7twezR0wF2re1xh&q=85&s=ab0a319474f8e5380c2dc9740715b916 1650w, https://mintcdn.com/agno-v2/Y7twezR0wF2re1xh/images/github-actions-build-docker.png?w=2500&fit=max&auto=format&n=Y7twezR0wF2re1xh&q=85&s=0a29ea2ec1f152a8142a2988768316da 2500w" />

<Note>
  You can also run the workflow using `gh workflow run`
</Note>

## Build ECR Images with Github Releases

If you're using ECR for images, you can buld and push the images through a Github Release. This action is defined in the `.github/workflows/ecr-images.yml` file and uses the new OpenID Connect (OIDC) approach to request the access token, without using IAM access keys.

We will follow this [guide](https://aws.amazon.com/blogs/security/use-iam-roles-to-connect-github-actions-to-actions-in-aws/) to create an IAM role which will be used by the github action.

1. Open the IAM console.
2. In the left navigation menu, choose Identity providers.
3. In the Identity providers pane, choose Add provider.
4. For Provider type, choose OpenID Connect.
5. For Provider URL, enter the URL of the GitHub OIDC IdP: [https://token.actions.githubusercontent.com](https://token.actions.githubusercontent.com)
6. Get thumbprint to verify the server certificate
7. For Audience, enter sts.amazonaws.com.

Verify the information matches the screenshot below and Add provider

<img src="https://mintcdn.com/agno-v2/Y7twezR0wF2re1xh/images/github-oidc-provider.png?fit=max&auto=format&n=Y7twezR0wF2re1xh&q=85&s=3eda54501351859a9afc8a041dc82139" alt="github-oidc-provider" data-og-width="1125" width="1125" data-og-height="799" height="799" data-path="images/github-oidc-provider.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/agno-v2/Y7twezR0wF2re1xh/images/github-oidc-provider.png?w=280&fit=max&auto=format&n=Y7twezR0wF2re1xh&q=85&s=4c8f4ac0f7afae5f6c02d105fb827306 280w, https://mintcdn.com/agno-v2/Y7twezR0wF2re1xh/images/github-oidc-provider.png?w=560&fit=max&auto=format&n=Y7twezR0wF2re1xh&q=85&s=2e714e3b3ef8993d0d0db50b9975eb12 560w, https://mintcdn.com/agno-v2/Y7twezR0wF2re1xh/images/github-oidc-provider.png?w=840&fit=max&auto=format&n=Y7twezR0wF2re1xh&q=85&s=96bbb52cb5fd2bd262fcb1d2eb2caf66 840w, https://mintcdn.com/agno-v2/Y7twezR0wF2re1xh/images/github-oidc-provider.png?w=1100&fit=max&auto=format&n=Y7twezR0wF2re1xh&q=85&s=69a06894592bebeefa2170cdf776f424 1100w, https://mintcdn.com/agno-v2/Y7twezR0wF2re1xh/images/github-oidc-provider.png?w=1650&fit=max&auto=format&n=Y7twezR0wF2re1xh&q=85&s=7cf6268657b4073faa5671f6710dcbff 1650w, https://mintcdn.com/agno-v2/Y7twezR0wF2re1xh/images/github-oidc-provider.png?w=2500&fit=max&auto=format&n=Y7twezR0wF2re1xh&q=85&s=ff74cc7bd14c9412ff7f9ef72d069e15 2500w" />

8. Assign a Role to the provider.

<img src="https://mintcdn.com/agno-v2/Y7twezR0wF2re1xh/images/github-oidc-provider-assign-role.png?fit=max&auto=format&n=Y7twezR0wF2re1xh&q=85&s=dbd84c74dc15c2dd74311e69afd6a6cd" alt="github-oidc-provider-assign-role" data-og-width="1347" width="1347" data-og-height="587" height="587" data-path="images/github-oidc-provider-assign-role.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/agno-v2/Y7twezR0wF2re1xh/images/github-oidc-provider-assign-role.png?w=280&fit=max&auto=format&n=Y7twezR0wF2re1xh&q=85&s=fc0720d26b0176b03192881e0d00d4c7 280w, https://mintcdn.com/agno-v2/Y7twezR0wF2re1xh/images/github-oidc-provider-assign-role.png?w=560&fit=max&auto=format&n=Y7twezR0wF2re1xh&q=85&s=305275efad18dd80e182f7442bfcb292 560w, https://mintcdn.com/agno-v2/Y7twezR0wF2re1xh/images/github-oidc-provider-assign-role.png?w=840&fit=max&auto=format&n=Y7twezR0wF2re1xh&q=85&s=0dc5facedf7d84a8e35ad5faa29409e7 840w, https://mintcdn.com/agno-v2/Y7twezR0wF2re1xh/images/github-oidc-provider-assign-role.png?w=1100&fit=max&auto=format&n=Y7twezR0wF2re1xh&q=85&s=7723cea354fe2bc26fed0bccdf406853 1100w, https://mintcdn.com/agno-v2/Y7twezR0wF2re1xh/images/github-oidc-provider-assign-role.png?w=1650&fit=max&auto=format&n=Y7twezR0wF2re1xh&q=85&s=9038f7189b3752d863fdb6772d34ceae 1650w, https://mintcdn.com/agno-v2/Y7twezR0wF2re1xh/images/github-oidc-provider-assign-role.png?w=2500&fit=max&auto=format&n=Y7twezR0wF2re1xh&q=85&s=e0e20507f59fa7b57970bdd1b187072e 2500w" />

9. Create a new role.

<img src="https://mintcdn.com/agno-v2/Y7twezR0wF2re1xh/images/github-oidc-provider-create-new-role.png?fit=max&auto=format&n=Y7twezR0wF2re1xh&q=85&s=e7b3d4a069f97ba3dbfe8bc08e8a534f" alt="github-oidc-provider-create-new-role" data-og-width="604" width="604" data-og-height="278" height="278" data-path="images/github-oidc-provider-create-new-role.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/agno-v2/Y7twezR0wF2re1xh/images/github-oidc-provider-create-new-role.png?w=280&fit=max&auto=format&n=Y7twezR0wF2re1xh&q=85&s=0b0bbe7da72790aeaa23eca25e846e12 280w, https://mintcdn.com/agno-v2/Y7twezR0wF2re1xh/images/github-oidc-provider-create-new-role.png?w=560&fit=max&auto=format&n=Y7twezR0wF2re1xh&q=85&s=7536bf15de67ff8826aaaaa336d3b2ff 560w, https://mintcdn.com/agno-v2/Y7twezR0wF2re1xh/images/github-oidc-provider-create-new-role.png?w=840&fit=max&auto=format&n=Y7twezR0wF2re1xh&q=85&s=65907ad9152fa24dcd1fe791d6a1980d 840w, https://mintcdn.com/agno-v2/Y7twezR0wF2re1xh/images/github-oidc-provider-create-new-role.png?w=1100&fit=max&auto=format&n=Y7twezR0wF2re1xh&q=85&s=7396e3e8f50462a000d5cd3131cf7e94 1100w, https://mintcdn.com/agno-v2/Y7twezR0wF2re1xh/images/github-oidc-provider-create-new-role.png?w=1650&fit=max&auto=format&n=Y7twezR0wF2re1xh&q=85&s=cd64cc43cbf45bf06a66f40db3976251 1650w, https://mintcdn.com/agno-v2/Y7twezR0wF2re1xh/images/github-oidc-provider-create-new-role.png?w=2500&fit=max&auto=format&n=Y7twezR0wF2re1xh&q=85&s=c8037ba2ea7bfc3e8f03dcb19ed66c9f 2500w" />

10. Confirm that Web identity is already selected as the trusted entity and the Identity provider field is populated with the IdP. In the Audience list, select sts.amazonaws.com, and then select Next.

<img src="https://mintcdn.com/agno-v2/Y7twezR0wF2re1xh/images/github-oidc-provider-trusted-entity.png?fit=max&auto=format&n=Y7twezR0wF2re1xh&q=85&s=3fe69db526ec7276382189d8d063561f" alt="github-oidc-provider-trusted-entity" data-og-width="1300" width="1300" data-og-height="934" height="934" data-path="images/github-oidc-provider-trusted-entity.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/agno-v2/Y7twezR0wF2re1xh/images/github-oidc-provider-trusted-entity.png?w=280&fit=max&auto=format&n=Y7twezR0wF2re1xh&q=85&s=1eb0d8ae46efdbb4f0ce072de01a4287 280w, https://mintcdn.com/agno-v2/Y7twezR0wF2re1xh/images/github-oidc-provider-trusted-entity.png?w=560&fit=max&auto=format&n=Y7twezR0wF2re1xh&q=85&s=a54123b0b191d9587345115f28a5c2e2 560w, https://mintcdn.com/agno-v2/Y7twezR0wF2re1xh/images/github-oidc-provider-trusted-entity.png?w=840&fit=max&auto=format&n=Y7twezR0wF2re1xh&q=85&s=74e9c6b7764f1ee331fc692808e898d0 840w, https://mintcdn.com/agno-v2/Y7twezR0wF2re1xh/images/github-oidc-provider-trusted-entity.png?w=1100&fit=max&auto=format&n=Y7twezR0wF2re1xh&q=85&s=a8d61a562ca252b89940f25c67e94c4c 1100w, https://mintcdn.com/agno-v2/Y7twezR0wF2re1xh/images/github-oidc-provider-trusted-entity.png?w=1650&fit=max&auto=format&n=Y7twezR0wF2re1xh&q=85&s=c2904811b03b257358ba3578dc0a4c8e 1650w, https://mintcdn.com/agno-v2/Y7twezR0wF2re1xh/images/github-oidc-provider-trusted-entity.png?w=2500&fit=max&auto=format&n=Y7twezR0wF2re1xh&q=85&s=cd5780393d9adde78a009499ef3ba6bf 2500w" />

11. Add the `AmazonEC2ContainerRegistryPowerUser` permission to this role.

12. Create the role with the name `GithubActionsRole`.

13. Find the role `GithubActionsRole` and copy the ARN.

<img src="https://mintcdn.com/agno-v2/Y7twezR0wF2re1xh/images/github-oidc-role.png?fit=max&auto=format&n=Y7twezR0wF2re1xh&q=85&s=ff1efeba61931aa435c13062d91a8f0b" alt="github-oidc-role" data-og-width="1389" width="1389" data-og-height="710" height="710" data-path="images/github-oidc-role.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/agno-v2/Y7twezR0wF2re1xh/images/github-oidc-role.png?w=280&fit=max&auto=format&n=Y7twezR0wF2re1xh&q=85&s=1c1afadf6e661558e3cc861e2353a38d 280w, https://mintcdn.com/agno-v2/Y7twezR0wF2re1xh/images/github-oidc-role.png?w=560&fit=max&auto=format&n=Y7twezR0wF2re1xh&q=85&s=2231af7fff49341e8393eb7b49b610b1 560w, https://mintcdn.com/agno-v2/Y7twezR0wF2re1xh/images/github-oidc-role.png?w=840&fit=max&auto=format&n=Y7twezR0wF2re1xh&q=85&s=43d85fdcd8f72dbe0ed7948a95793d38 840w, https://mintcdn.com/agno-v2/Y7twezR0wF2re1xh/images/github-oidc-role.png?w=1100&fit=max&auto=format&n=Y7twezR0wF2re1xh&q=85&s=f05dc967abe03969118e755451c43a4a 1100w, https://mintcdn.com/agno-v2/Y7twezR0wF2re1xh/images/github-oidc-role.png?w=1650&fit=max&auto=format&n=Y7twezR0wF2re1xh&q=85&s=b5e1b433d11d461a5fc24c8b14e6bc91 1650w, https://mintcdn.com/agno-v2/Y7twezR0wF2re1xh/images/github-oidc-role.png?w=2500&fit=max&auto=format&n=Y7twezR0wF2re1xh&q=85&s=8b7072639f56d42f00d00b8b735cb375 2500w" />

14. Create the ECR Repositories: `llm` and `jupyter-llm` which are built by the workflow.

<img src="https://mintcdn.com/agno-v2/Y7twezR0wF2re1xh/images/create-ecr-image.png?fit=max&auto=format&n=Y7twezR0wF2re1xh&q=85&s=c68ceb3a9b6784fd519cc04b0e38caf1" alt="create-ecr-image" data-og-width="1389" width="1389" data-og-height="408" height="408" data-path="images/create-ecr-image.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/agno-v2/Y7twezR0wF2re1xh/images/create-ecr-image.png?w=280&fit=max&auto=format&n=Y7twezR0wF2re1xh&q=85&s=2ce02d48da7e53a6c335736a17ebec6e 280w, https://mintcdn.com/agno-v2/Y7twezR0wF2re1xh/images/create-ecr-image.png?w=560&fit=max&auto=format&n=Y7twezR0wF2re1xh&q=85&s=f0b4d1687849a637c0a595c4a8d0690a 560w, https://mintcdn.com/agno-v2/Y7twezR0wF2re1xh/images/create-ecr-image.png?w=840&fit=max&auto=format&n=Y7twezR0wF2re1xh&q=85&s=26b1f13eb8b6f9b09a06b9e6bb1eeb27 840w, https://mintcdn.com/agno-v2/Y7twezR0wF2re1xh/images/create-ecr-image.png?w=1100&fit=max&auto=format&n=Y7twezR0wF2re1xh&q=85&s=e53f084201341a7c92738fa62efdb64c 1100w, https://mintcdn.com/agno-v2/Y7twezR0wF2re1xh/images/create-ecr-image.png?w=1650&fit=max&auto=format&n=Y7twezR0wF2re1xh&q=85&s=c9e2477e1befaf12f81d4d345dac5a26 1650w, https://mintcdn.com/agno-v2/Y7twezR0wF2re1xh/images/create-ecr-image.png?w=2500&fit=max&auto=format&n=Y7twezR0wF2re1xh&q=85&s=a22af7830053ba9139cfb6d0d4017d4a 2500w" />

15. Update the workflow with the `GithubActionsRole` ARN and ECR Repository.

16. Update the `docker-images` workflow to **NOT** run on a release

17. Run workflow using a Github Release

<img src="https://mintcdn.com/agno-v2/Y7twezR0wF2re1xh/images/github-actions-build-ecr.png?fit=max&auto=format&n=Y7twezR0wF2re1xh&q=85&s=aff1bdde8baea8591770b0f6b5ac036b" alt="github-actions-build-ecr" data-og-width="1389" width="1389" data-og-height="710" height="710" data-path="images/github-actions-build-ecr.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/agno-v2/Y7twezR0wF2re1xh/images/github-actions-build-ecr.png?w=280&fit=max&auto=format&n=Y7twezR0wF2re1xh&q=85&s=7440b9e93662a242501bdf7eb37f0620 280w, https://mintcdn.com/agno-v2/Y7twezR0wF2re1xh/images/github-actions-build-ecr.png?w=560&fit=max&auto=format&n=Y7twezR0wF2re1xh&q=85&s=a62d3057b07926a447999d1b75a092dd 560w, https://mintcdn.com/agno-v2/Y7twezR0wF2re1xh/images/github-actions-build-ecr.png?w=840&fit=max&auto=format&n=Y7twezR0wF2re1xh&q=85&s=494393ce11efce791ab0d62f19b1b1fb 840w, https://mintcdn.com/agno-v2/Y7twezR0wF2re1xh/images/github-actions-build-ecr.png?w=1100&fit=max&auto=format&n=Y7twezR0wF2re1xh&q=85&s=2a85f548629bf8dc605f9964f99329fc 1100w, https://mintcdn.com/agno-v2/Y7twezR0wF2re1xh/images/github-actions-build-ecr.png?w=1650&fit=max&auto=format&n=Y7twezR0wF2re1xh&q=85&s=27878c2ccf71bb63426dc120b9233cd3 1650w, https://mintcdn.com/agno-v2/Y7twezR0wF2re1xh/images/github-actions-build-ecr.png?w=2500&fit=max&auto=format&n=Y7twezR0wF2re1xh&q=85&s=9116b3c2eb2bf07b4e8f4c5070e3c1fa 2500w" />

<Note>
  You can also run the workflow using `gh workflow run`
</Note>

**Examples:**

Example 1 (unknown):
```unknown

```

Example 2 (unknown):
```unknown
</CodeGroup>

<img src="https://mintcdn.com/agno-v2/Y7twezR0wF2re1xh/images/github-actions-build-docker.png?fit=max&auto=format&n=Y7twezR0wF2re1xh&q=85&s=2f96812f7b12f8e1f9d831152556c5d7" alt="github-actions-build-docker" data-og-width="1042" width="1042" data-og-height="732" height="732" data-path="images/github-actions-build-docker.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/agno-v2/Y7twezR0wF2re1xh/images/github-actions-build-docker.png?w=280&fit=max&auto=format&n=Y7twezR0wF2re1xh&q=85&s=ec6400a97ff55a3a44da52d9e53473ca 280w, https://mintcdn.com/agno-v2/Y7twezR0wF2re1xh/images/github-actions-build-docker.png?w=560&fit=max&auto=format&n=Y7twezR0wF2re1xh&q=85&s=6568fab440d3f4794aecce651f2a3a0e 560w, https://mintcdn.com/agno-v2/Y7twezR0wF2re1xh/images/github-actions-build-docker.png?w=840&fit=max&auto=format&n=Y7twezR0wF2re1xh&q=85&s=c88fbcfbeb5e647b45e56d064c8066b6 840w, https://mintcdn.com/agno-v2/Y7twezR0wF2re1xh/images/github-actions-build-docker.png?w=1100&fit=max&auto=format&n=Y7twezR0wF2re1xh&q=85&s=c7d8bc7f1d25a8167dcbe7920bfca3e6 1100w, https://mintcdn.com/agno-v2/Y7twezR0wF2re1xh/images/github-actions-build-docker.png?w=1650&fit=max&auto=format&n=Y7twezR0wF2re1xh&q=85&s=ab0a319474f8e5380c2dc9740715b916 1650w, https://mintcdn.com/agno-v2/Y7twezR0wF2re1xh/images/github-actions-build-docker.png?w=2500&fit=max&auto=format&n=Y7twezR0wF2re1xh&q=85&s=0a29ea2ec1f152a8142a2988768316da 2500w" />

<Note>
  You can also run the workflow using `gh workflow run`
</Note>

## Build ECR Images with Github Releases

If you're using ECR for images, you can buld and push the images through a Github Release. This action is defined in the `.github/workflows/ecr-images.yml` file and uses the new OpenID Connect (OIDC) approach to request the access token, without using IAM access keys.

We will follow this [guide](https://aws.amazon.com/blogs/security/use-iam-roles-to-connect-github-actions-to-actions-in-aws/) to create an IAM role which will be used by the github action.

1. Open the IAM console.
2. In the left navigation menu, choose Identity providers.
3. In the Identity providers pane, choose Add provider.
4. For Provider type, choose OpenID Connect.
5. For Provider URL, enter the URL of the GitHub OIDC IdP: [https://token.actions.githubusercontent.com](https://token.actions.githubusercontent.com)
6. Get thumbprint to verify the server certificate
7. For Audience, enter sts.amazonaws.com.

Verify the information matches the screenshot below and Add provider

<img src="https://mintcdn.com/agno-v2/Y7twezR0wF2re1xh/images/github-oidc-provider.png?fit=max&auto=format&n=Y7twezR0wF2re1xh&q=85&s=3eda54501351859a9afc8a041dc82139" alt="github-oidc-provider" data-og-width="1125" width="1125" data-og-height="799" height="799" data-path="images/github-oidc-provider.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/agno-v2/Y7twezR0wF2re1xh/images/github-oidc-provider.png?w=280&fit=max&auto=format&n=Y7twezR0wF2re1xh&q=85&s=4c8f4ac0f7afae5f6c02d105fb827306 280w, https://mintcdn.com/agno-v2/Y7twezR0wF2re1xh/images/github-oidc-provider.png?w=560&fit=max&auto=format&n=Y7twezR0wF2re1xh&q=85&s=2e714e3b3ef8993d0d0db50b9975eb12 560w, https://mintcdn.com/agno-v2/Y7twezR0wF2re1xh/images/github-oidc-provider.png?w=840&fit=max&auto=format&n=Y7twezR0wF2re1xh&q=85&s=96bbb52cb5fd2bd262fcb1d2eb2caf66 840w, https://mintcdn.com/agno-v2/Y7twezR0wF2re1xh/images/github-oidc-provider.png?w=1100&fit=max&auto=format&n=Y7twezR0wF2re1xh&q=85&s=69a06894592bebeefa2170cdf776f424 1100w, https://mintcdn.com/agno-v2/Y7twezR0wF2re1xh/images/github-oidc-provider.png?w=1650&fit=max&auto=format&n=Y7twezR0wF2re1xh&q=85&s=7cf6268657b4073faa5671f6710dcbff 1650w, https://mintcdn.com/agno-v2/Y7twezR0wF2re1xh/images/github-oidc-provider.png?w=2500&fit=max&auto=format&n=Y7twezR0wF2re1xh&q=85&s=ff74cc7bd14c9412ff7f9ef72d069e15 2500w" />

8. Assign a Role to the provider.

<img src="https://mintcdn.com/agno-v2/Y7twezR0wF2re1xh/images/github-oidc-provider-assign-role.png?fit=max&auto=format&n=Y7twezR0wF2re1xh&q=85&s=dbd84c74dc15c2dd74311e69afd6a6cd" alt="github-oidc-provider-assign-role" data-og-width="1347" width="1347" data-og-height="587" height="587" data-path="images/github-oidc-provider-assign-role.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/agno-v2/Y7twezR0wF2re1xh/images/github-oidc-provider-assign-role.png?w=280&fit=max&auto=format&n=Y7twezR0wF2re1xh&q=85&s=fc0720d26b0176b03192881e0d00d4c7 280w, https://mintcdn.com/agno-v2/Y7twezR0wF2re1xh/images/github-oidc-provider-assign-role.png?w=560&fit=max&auto=format&n=Y7twezR0wF2re1xh&q=85&s=305275efad18dd80e182f7442bfcb292 560w, https://mintcdn.com/agno-v2/Y7twezR0wF2re1xh/images/github-oidc-provider-assign-role.png?w=840&fit=max&auto=format&n=Y7twezR0wF2re1xh&q=85&s=0dc5facedf7d84a8e35ad5faa29409e7 840w, https://mintcdn.com/agno-v2/Y7twezR0wF2re1xh/images/github-oidc-provider-assign-role.png?w=1100&fit=max&auto=format&n=Y7twezR0wF2re1xh&q=85&s=7723cea354fe2bc26fed0bccdf406853 1100w, https://mintcdn.com/agno-v2/Y7twezR0wF2re1xh/images/github-oidc-provider-assign-role.png?w=1650&fit=max&auto=format&n=Y7twezR0wF2re1xh&q=85&s=9038f7189b3752d863fdb6772d34ceae 1650w, https://mintcdn.com/agno-v2/Y7twezR0wF2re1xh/images/github-oidc-provider-assign-role.png?w=2500&fit=max&auto=format&n=Y7twezR0wF2re1xh&q=85&s=e0e20507f59fa7b57970bdd1b187072e 2500w" />

9. Create a new role.

<img src="https://mintcdn.com/agno-v2/Y7twezR0wF2re1xh/images/github-oidc-provider-create-new-role.png?fit=max&auto=format&n=Y7twezR0wF2re1xh&q=85&s=e7b3d4a069f97ba3dbfe8bc08e8a534f" alt="github-oidc-provider-create-new-role" data-og-width="604" width="604" data-og-height="278" height="278" data-path="images/github-oidc-provider-create-new-role.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/agno-v2/Y7twezR0wF2re1xh/images/github-oidc-provider-create-new-role.png?w=280&fit=max&auto=format&n=Y7twezR0wF2re1xh&q=85&s=0b0bbe7da72790aeaa23eca25e846e12 280w, https://mintcdn.com/agno-v2/Y7twezR0wF2re1xh/images/github-oidc-provider-create-new-role.png?w=560&fit=max&auto=format&n=Y7twezR0wF2re1xh&q=85&s=7536bf15de67ff8826aaaaa336d3b2ff 560w, https://mintcdn.com/agno-v2/Y7twezR0wF2re1xh/images/github-oidc-provider-create-new-role.png?w=840&fit=max&auto=format&n=Y7twezR0wF2re1xh&q=85&s=65907ad9152fa24dcd1fe791d6a1980d 840w, https://mintcdn.com/agno-v2/Y7twezR0wF2re1xh/images/github-oidc-provider-create-new-role.png?w=1100&fit=max&auto=format&n=Y7twezR0wF2re1xh&q=85&s=7396e3e8f50462a000d5cd3131cf7e94 1100w, https://mintcdn.com/agno-v2/Y7twezR0wF2re1xh/images/github-oidc-provider-create-new-role.png?w=1650&fit=max&auto=format&n=Y7twezR0wF2re1xh&q=85&s=cd64cc43cbf45bf06a66f40db3976251 1650w, https://mintcdn.com/agno-v2/Y7twezR0wF2re1xh/images/github-oidc-provider-create-new-role.png?w=2500&fit=max&auto=format&n=Y7twezR0wF2re1xh&q=85&s=c8037ba2ea7bfc3e8f03dcb19ed66c9f 2500w" />

10. Confirm that Web identity is already selected as the trusted entity and the Identity provider field is populated with the IdP. In the Audience list, select sts.amazonaws.com, and then select Next.

<img src="https://mintcdn.com/agno-v2/Y7twezR0wF2re1xh/images/github-oidc-provider-trusted-entity.png?fit=max&auto=format&n=Y7twezR0wF2re1xh&q=85&s=3fe69db526ec7276382189d8d063561f" alt="github-oidc-provider-trusted-entity" data-og-width="1300" width="1300" data-og-height="934" height="934" data-path="images/github-oidc-provider-trusted-entity.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/agno-v2/Y7twezR0wF2re1xh/images/github-oidc-provider-trusted-entity.png?w=280&fit=max&auto=format&n=Y7twezR0wF2re1xh&q=85&s=1eb0d8ae46efdbb4f0ce072de01a4287 280w, https://mintcdn.com/agno-v2/Y7twezR0wF2re1xh/images/github-oidc-provider-trusted-entity.png?w=560&fit=max&auto=format&n=Y7twezR0wF2re1xh&q=85&s=a54123b0b191d9587345115f28a5c2e2 560w, https://mintcdn.com/agno-v2/Y7twezR0wF2re1xh/images/github-oidc-provider-trusted-entity.png?w=840&fit=max&auto=format&n=Y7twezR0wF2re1xh&q=85&s=74e9c6b7764f1ee331fc692808e898d0 840w, https://mintcdn.com/agno-v2/Y7twezR0wF2re1xh/images/github-oidc-provider-trusted-entity.png?w=1100&fit=max&auto=format&n=Y7twezR0wF2re1xh&q=85&s=a8d61a562ca252b89940f25c67e94c4c 1100w, https://mintcdn.com/agno-v2/Y7twezR0wF2re1xh/images/github-oidc-provider-trusted-entity.png?w=1650&fit=max&auto=format&n=Y7twezR0wF2re1xh&q=85&s=c2904811b03b257358ba3578dc0a4c8e 1650w, https://mintcdn.com/agno-v2/Y7twezR0wF2re1xh/images/github-oidc-provider-trusted-entity.png?w=2500&fit=max&auto=format&n=Y7twezR0wF2re1xh&q=85&s=cd5780393d9adde78a009499ef3ba6bf 2500w" />

11. Add the `AmazonEC2ContainerRegistryPowerUser` permission to this role.

12. Create the role with the name `GithubActionsRole`.

13. Find the role `GithubActionsRole` and copy the ARN.

<img src="https://mintcdn.com/agno-v2/Y7twezR0wF2re1xh/images/github-oidc-role.png?fit=max&auto=format&n=Y7twezR0wF2re1xh&q=85&s=ff1efeba61931aa435c13062d91a8f0b" alt="github-oidc-role" data-og-width="1389" width="1389" data-og-height="710" height="710" data-path="images/github-oidc-role.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/agno-v2/Y7twezR0wF2re1xh/images/github-oidc-role.png?w=280&fit=max&auto=format&n=Y7twezR0wF2re1xh&q=85&s=1c1afadf6e661558e3cc861e2353a38d 280w, https://mintcdn.com/agno-v2/Y7twezR0wF2re1xh/images/github-oidc-role.png?w=560&fit=max&auto=format&n=Y7twezR0wF2re1xh&q=85&s=2231af7fff49341e8393eb7b49b610b1 560w, https://mintcdn.com/agno-v2/Y7twezR0wF2re1xh/images/github-oidc-role.png?w=840&fit=max&auto=format&n=Y7twezR0wF2re1xh&q=85&s=43d85fdcd8f72dbe0ed7948a95793d38 840w, https://mintcdn.com/agno-v2/Y7twezR0wF2re1xh/images/github-oidc-role.png?w=1100&fit=max&auto=format&n=Y7twezR0wF2re1xh&q=85&s=f05dc967abe03969118e755451c43a4a 1100w, https://mintcdn.com/agno-v2/Y7twezR0wF2re1xh/images/github-oidc-role.png?w=1650&fit=max&auto=format&n=Y7twezR0wF2re1xh&q=85&s=b5e1b433d11d461a5fc24c8b14e6bc91 1650w, https://mintcdn.com/agno-v2/Y7twezR0wF2re1xh/images/github-oidc-role.png?w=2500&fit=max&auto=format&n=Y7twezR0wF2re1xh&q=85&s=8b7072639f56d42f00d00b8b735cb375 2500w" />

14. Create the ECR Repositories: `llm` and `jupyter-llm` which are built by the workflow.

<img src="https://mintcdn.com/agno-v2/Y7twezR0wF2re1xh/images/create-ecr-image.png?fit=max&auto=format&n=Y7twezR0wF2re1xh&q=85&s=c68ceb3a9b6784fd519cc04b0e38caf1" alt="create-ecr-image" data-og-width="1389" width="1389" data-og-height="408" height="408" data-path="images/create-ecr-image.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/agno-v2/Y7twezR0wF2re1xh/images/create-ecr-image.png?w=280&fit=max&auto=format&n=Y7twezR0wF2re1xh&q=85&s=2ce02d48da7e53a6c335736a17ebec6e 280w, https://mintcdn.com/agno-v2/Y7twezR0wF2re1xh/images/create-ecr-image.png?w=560&fit=max&auto=format&n=Y7twezR0wF2re1xh&q=85&s=f0b4d1687849a637c0a595c4a8d0690a 560w, https://mintcdn.com/agno-v2/Y7twezR0wF2re1xh/images/create-ecr-image.png?w=840&fit=max&auto=format&n=Y7twezR0wF2re1xh&q=85&s=26b1f13eb8b6f9b09a06b9e6bb1eeb27 840w, https://mintcdn.com/agno-v2/Y7twezR0wF2re1xh/images/create-ecr-image.png?w=1100&fit=max&auto=format&n=Y7twezR0wF2re1xh&q=85&s=e53f084201341a7c92738fa62efdb64c 1100w, https://mintcdn.com/agno-v2/Y7twezR0wF2re1xh/images/create-ecr-image.png?w=1650&fit=max&auto=format&n=Y7twezR0wF2re1xh&q=85&s=c9e2477e1befaf12f81d4d345dac5a26 1650w, https://mintcdn.com/agno-v2/Y7twezR0wF2re1xh/images/create-ecr-image.png?w=2500&fit=max&auto=format&n=Y7twezR0wF2re1xh&q=85&s=a22af7830053ba9139cfb6d0d4017d4a 2500w" />

15. Update the workflow with the `GithubActionsRole` ARN and ECR Repository.
```

Example 3 (unknown):
```unknown
16. Update the `docker-images` workflow to **NOT** run on a release
```

Example 4 (unknown):
```unknown
17. Run workflow using a Github Release

<CodeGroup>
```

---

## 7. Print results

**URL:** llms-txt#7.-print-results

**Contents:**
- Usage

print("\n--- Generated Shorts ---")
for short in shorts:
    print(f"Short at {short['path']}")
    print(f"Description: {short['description']}")
    print(f"Engagement Score: {short['score']}/10\n")
bash  theme={null}
    pip install -U agno opencv-python sqlalchemy
    bash Mac theme={null}
      brew install ffmpeg
      bash Windows   theme={null}
      # Install ffmpeg from https://ffmpeg.org/download.html
      bash Mac/Linux theme={null}
       export GOOGLE_API_KEY="your_google_api_key_here"
      bash Windows theme={null}
        $Env:GOOGLE_API_KEY="your_google_api_key_here"
      bash  theme={null}
    touch video_to_shorts.py
    bash Mac theme={null}
      python video_to_shorts.py
      bash Windows   theme={null}
      python video_to_shorts.py
      ```
    </CodeGroup>
  </Step>

<Step title="Find All Cookbooks">
    Explore all the available cookbooks in the Agno repository. Click the link below to view the code on GitHub:

<Link href="https://github.com/agno-agi/agno/tree/main/cookbook/agents/multimodal" target="_blank">
      Agno Cookbooks on GitHub
    </Link>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Install libraries">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Install ffmpeg">
    <CodeGroup>
```

Example 3 (unknown):
```unknown

```

Example 4 (unknown):
```unknown
</CodeGroup>
  </Step>

  <Step title="Export your GOOGLE API key">
    <CodeGroup>
```

---

## Hooks

**URL:** llms-txt#hooks

**Contents:**
- Tool Hooks
  - Multiple Tool Hooks
- Pre and Post Hooks
- Example: Human in the loop using tool hooks

Source: https://docs.agno.com/concepts/tools/hooks

Learn how to use tool hooks to modify the behavior of a tool.

You can use tool hooks to perform validation, logging, or any other logic before or after a tool is called.

A tool hook is a function that takes a function name, function call, and arguments. Optionally, you can access the `Agent` or `Team` object as well.  Inside the tool hook, you have to call the function call and return the result.

<Note>
  It is important to use exact parameter names when defining a tool hook. `agent`, `team`, `function_name`, `function_call`, and `arguments` are available parameters.
</Note>

You can assign tool hooks on agents and teams.  The tool hooks will be applied to all tool calls made by the agent or team.

You can also get access to the `Agent` or `Team` object in the tool hook.

### Multiple Tool Hooks

You can also assign multiple tool hooks at once. They will be applied in the order they are assigned.

You can also assign tool hooks to specific custom tools.

## Pre and Post Hooks

Pre and post hooks let's you modify what happens before and after a tool is called. It is an alternative to tool hooks.

Set the `pre_hook` in the `@tool` decorator to run a function before the tool call.

Set the `post_hook` in the `@tool` decorator to run a function after the tool call.

Here's a demo example of using a `pre_hook`, `post_hook` along with Agent Context.

## Example: Human in the loop using tool hooks

This example shows how to:

* Add hooks to tools for user confirmation
* Handle user input during tool execution
* Gracefully cancel operations based on user choice

<Steps>
  <Step title="Create the example">
    
  </Step>

<Step title="Run the example">
    Install libraries

**Examples:**

Example 1 (unknown):
```unknown
or
```

Example 2 (unknown):
```unknown
You can assign tool hooks on agents and teams.  The tool hooks will be applied to all tool calls made by the agent or team.

For example:
```

Example 3 (unknown):
```unknown
You can also get access to the `Agent` or `Team` object in the tool hook.
```

Example 4 (unknown):
```unknown
### Multiple Tool Hooks

You can also assign multiple tool hooks at once. They will be applied in the order they are assigned.
```

---

## Generate Music using Models Lab

**URL:** llms-txt#generate-music-using-models-lab

**Contents:**
- Code
- Usage

Source: https://docs.agno.com/examples/concepts/multimodal/generate-music-agent

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Set your API key">
    
  </Step>

<Step title="Install libraries">
    
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Set your API key">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Set Client

**URL:** llms-txt#set-client

**Contents:**
- Code

Source: https://docs.agno.com/examples/models/ollama/set_client

```python cookbook/models/ollama/set_client.py theme={null}
from agno.agent import Agent, RunOutput  # noqa
from agno.models.ollama import Ollama
from ollama import Client as OllamaClient

agent = Agent(
    model=Ollama(id="llama3.1:8b", client=OllamaClient()),
    markdown=True,
)

---

## Mistral

**URL:** llms-txt#mistral

**Contents:**
- Parameters

Source: https://docs.agno.com/reference/models/mistral

The Mistral model provides access to Mistral's language models.

| Parameter  | Type            | Default                       | Description                                                     |
| ---------- | --------------- | ----------------------------- | --------------------------------------------------------------- |
| `id`       | `str`           | `"mistral-large-latest"`      | The id of the Mistral model to use                              |
| `name`     | `str`           | `"Mistral"`                   | The name of the model                                           |
| `provider` | `str`           | `"Mistral"`                   | The provider of the model                                       |
| `api_key`  | `Optional[str]` | `None`                        | The API key for Mistral (defaults to MISTRAL\_API\_KEY env var) |
| `base_url` | `str`           | `"https://api.mistral.ai/v1"` | The base URL for the Mistral API                                |

Mistral extends the OpenAI-compatible interface and supports most parameters from OpenAI.

---

## Add a simple endpoint to set the JWT authentication cookie

**URL:** llms-txt#add-a-simple-endpoint-to-set-the-jwt-authentication-cookie

@app.get("/set-auth-cookie")
async def set_auth_cookie(response: Response):
    """
    Endpoint to set the JWT authentication cookie.
    In a real application, this would be done after successful login.
    """
    # Create a test JWT token
    payload = {
        "sub": "cookie_user_789",
        "session_id": "cookie_session_123",
        "name": "Jane Smith",
        "email": "jane.smith@example.com",
        "roles": ["user", "premium"],
        "org": "Example Corp",
        "exp": datetime.now(UTC) + timedelta(hours=24),
        "iat": datetime.now(UTC),
    }

token = jwt.encode(payload, JWT_SECRET, algorithm="HS256")

# Set HTTP-only cookie (more secure than localStorage for JWT storage)
    response.set_cookie(
        key="auth_token",
        value=token,
        httponly=True,  # Prevents access from JavaScript (XSS protection)
        secure=True,  # Only send over HTTPS in production
        samesite="strict",  # CSRF protection
        max_age=24 * 60 * 60,  # 24 hours
    )

return {
        "message": "Authentication cookie set successfully",
        "cookie_name": "auth_token",
        "expires_in": "24 hours",
        "security_features": ["httponly", "secure", "samesite=strict"],
        "instructions": "Now you can make authenticated requests without Authorization headers",
    }

---

## Reasoning Tools

**URL:** llms-txt#reasoning-tools

**Contents:**
- Code
- Usage

Source: https://docs.agno.com/examples/concepts/tools/others/reasoning

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Set your API key">
    
  </Step>

<Step title="Install libraries">
    
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Set your API key">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Process the audio and get a response

**URL:** llms-txt#process-the-audio-and-get-a-response

run_response = agent.run(
    "What's in this recording? Please analyze the content and tone.",
    audio=[Audio(content=response.content, format="wav")],
)

---

## Vercel with Reasoning Tools

**URL:** llms-txt#vercel-with-reasoning-tools

**Contents:**
- Code
- Usage

Source: https://docs.agno.com/examples/concepts/reasoning/tools/vercel-reasoning-tools

This example shows how to use `ReasoningTools` with a Vercel model.

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Set your API key">
    
  </Step>

<Step title="Install libraries">
    
  </Step>

<Step title="Run Example">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Set your API key">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Example">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Configure the tracer provider

**URL:** llms-txt#configure-the-tracer-provider

tracer_provider = TracerProvider()
tracer_provider.add_span_processor(
    SimpleSpanProcessor(OTLPSpanExporter(endpoint=endpoint, headers=headers))
)
trace_api.set_tracer_provider(tracer_provider=tracer_provider)

---

## Advanced Workflow Patterns

**URL:** llms-txt#advanced-workflow-patterns

Source: https://docs.agno.com/concepts/workflows/workflow-patterns/advanced-workflow-patterns

Combine multiple workflow patterns to build sophisticated, production-ready automation systems

**Pattern Combination**: Conditional Logic + Parallel Execution + Iterative Loops + Custom Processing + Dynamic Routing

This example demonstrates how deterministic patterns can be composed to create complex yet predictable workflows.

```python advanced_workflow_patterns.py theme={null}
from agno.workflow import Condition, Loop, Parallel, Router, Step, Workflow

def research_post_processor(step_input) -> StepOutput:
    """Post-process and consolidate research data from parallel conditions"""
    research_data = step_input.previous_step_content or ""

try:
        # Analyze research quality and completeness
        word_count = len(research_data.split())
        has_tech_content = any(keyword in research_data.lower()
                              for keyword in ["technology", "ai", "software", "tech"])
        has_business_content = any(keyword in research_data.lower()
                                  for keyword in ["market", "business", "revenue", "strategy"])

# Create enhanced research summary
        enhanced_summary = f"""
            ## Research Analysis Report

**Data Quality:** {"✓ High-quality" if word_count > 200 else "⚠ Limited data"}

**Content Coverage:**
            - Technical Analysis: {"✓ Completed" if has_tech_content else "✗ Not available"}
            - Business Analysis: {"✓ Completed" if has_business_content else "✗ Not available"}

**Research Findings:**
            {research_data}
        """.strip()

return StepOutput(
            content=enhanced_summary,
            success=True,
        )

except Exception as e:
        return StepOutput(
            content=f"Research post-processing failed: {str(e)}",
            success=False,
            error=str(e)
        )

---

## DuckDuckGo

**URL:** llms-txt#duckduckgo

**Contents:**
- Prerequisites
- Example
- Toolkit Params
- Toolkit Functions
- Developer Resources

Source: https://docs.agno.com/concepts/tools/toolkits/search/duckduckgo

**DuckDuckGo** enables an Agent to search the web for information.

The following example requires the `ddgs` library. To install DuckDuckGo, run the following command:

| Parameter           | Type            | Default | Description                                           |
| ------------------- | --------------- | ------- | ----------------------------------------------------- |
| `enable_search`     | `bool`          | `True`  | Enable DuckDuckGo search function.                    |
| `enable_news`       | `bool`          | `True`  | Enable DuckDuckGo news function.                      |
| `all`               | `bool`          | `False` | Enable all available functions in the toolkit.        |
| `modifier`          | `Optional[str]` | `None`  | A modifier to be used in the search request.          |
| `fixed_max_results` | `Optional[int]` | `None`  | A fixed number of maximum results.                    |
| `proxy`             | `Optional[str]` | `None`  | Proxy to be used in the search request.               |
| `timeout`           | `Optional[int]` | `10`    | The maximum number of seconds to wait for a response. |
| `verify_ssl`        | `bool`          | `True`  | Whether to verify SSL certificates.                   |

| Function            | Description                                                                                                                                                                             |
| ------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| `duckduckgo_search` | Search DuckDuckGo for a query. Parameters include `query` (str) for the search query and `max_results` (int, default=5) for maximum results. Returns JSON formatted search results.     |
| `duckduckgo_news`   | Get the latest news from DuckDuckGo. Parameters include `query` (str) for the search query and `max_results` (int, default=5) for maximum results. Returns JSON formatted news results. |

## Developer Resources

* View [Tools](https://github.com/agno-agi/agno/blob/main/libs/agno/agno/tools/duckduckgo.py)
* View [Cookbook](https://github.com/agno-agi/agno/tree/main/cookbook/tools/duckduckgo_tools.py)

**Examples:**

Example 1 (unknown):
```unknown
## Example
```

---

## CrewAI

**URL:** llms-txt#crewai

python cookbook/evals/performance/comparison/crewai_instantiation.py

---

## Upload and process video

**URL:** llms-txt#upload-and-process-video

video_file = upload_file(video_path)
while video_file.state.name == "PROCESSING":
    time.sleep(2)
    video_file = get_file(video_file.name)

---

## Model

**URL:** llms-txt#model

**Contents:**
- Parameters

Source: https://docs.agno.com/reference/models/model

The Model class is the base class for all models in Agno. It provides common functionality and parameters that are inherited by specific model implementations like OpenAIChat, Claude, etc.

| Parameter           | Type                              | Default  | Description                                                          |
| ------------------- | --------------------------------- | -------- | -------------------------------------------------------------------- |
| `id`                | `str`                             | Required | The id/name of the model to use                                      |
| `name`              | `Optional[str]`                   | `None`   | The display name of the model                                        |
| `provider`          | `Optional[str]`                   | `None`   | The provider of the model                                            |
| `frequency_penalty` | `Optional[float]`                 | `None`   | Penalizes new tokens based on their frequency in the text so far     |
| `presence_penalty`  | `Optional[float]`                 | `None`   | Penalizes new tokens based on whether they appear in the text so far |
| `response_format`   | `Optional[str]`                   | `None`   | The format of the response                                           |
| `seed`              | `Optional[int]`                   | `None`   | Random seed for deterministic sampling                               |
| `stop`              | `Optional[Union[str, List[str]]]` | `None`   | Up to 4 sequences where the API will stop generating further tokens  |
| `stream`            | `bool`                            | `True`   | Whether to stream the response                                       |
| `temperature`       | `Optional[float]`                 | `None`   | Controls randomness in the model's output                            |
| `top_p`             | `Optional[float]`                 | `None`   | Controls diversity via nucleus sampling                              |
| `max_tokens`        | `Optional[int]`                   | `None`   | Maximum number of tokens to generate                                 |
| `request_params`    | `Optional[Dict[str, Any]]`        | `None`   | Additional parameters to include in the request                      |

---

## Webex Tools

**URL:** llms-txt#webex-tools

**Contents:**
- Code

Source: https://docs.agno.com/examples/concepts/tools/social/webex

```python cookbook/tools/webex_tools.py theme={null}
from agno.agent import Agent
from agno.tools.webex import WebexTools

agent = Agent(
    name="Webex Assistant",
    tools=[WebexTools()],
    description="You are a Webex assistant that can send messages and manage spaces.",
    instructions=[
        "You can help users by:",
        "- Listing available Webex spaces",
        "- Sending messages to spaces",
        "Always confirm the space exists before sending messages.",
    ],
        markdown=True,
)

---

## Now switch to Google Gemini using same session

**URL:** llms-txt#now-switch-to-google-gemini-using-same-session

gemini_agent = Agent(
    model=Gemini(id="gemini-2.0-flash-exp"),
    instructions="You are a helpful assistant.",
    db=db,
    add_history_to_context=True,
)

---

## Airflow Tools

**URL:** llms-txt#airflow-tools

**Contents:**
- Code

Source: https://docs.agno.com/examples/concepts/tools/others/airflow

```python cookbook/tools/airflow_tools.py theme={null}
from agno.agent import Agent
from agno.tools.airflow import AirflowTools

agent = Agent(
    tools=[AirflowTools(dags_dir="tmp/dags", enable_save_dag=True, enable_read_dag=True)],
        markdown=True,
)

dag_content = """
from airflow import DAG
from airflow.operators.python import PythonOperator
from datetime import datetime, timedelta

default_args = {
    'owner': 'airflow',
    'depends_on_past': False,
    'start_date': datetime(2024, 1, 1),
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 1,
    'retry_delay': timedelta(minutes=5),
}

---

## LangDB

**URL:** llms-txt#langdb

**Contents:**
- Integrating Agno with LangDB
- Prerequisites
- Sending Traces to LangDB
  - Example: Basic Agent Setup

Source: https://docs.agno.com/integrations/observability/langdb

Integrate Agno with LangDB to trace agent execution, tool calls, and gain comprehensive observability into your agent's performance.

## Integrating Agno with LangDB

[LangDB](https://langdb.ai/) provides an AI Gateway platform for comprehensive observability and tracing of AI agents and LLM interactions. By integrating Agno with LangDB, you can gain full visibility into your agent's operations, including agent runs, tool calls, team interactions, and performance metrics.

For detailed integration instructions, see the [LangDB Agno documentation](https://docs.langdb.ai/getting-started/working-with-agent-frameworks/working-with-agno).

<Frame caption="LangDB Finance Team Trace">
  <img src="https://mintcdn.com/agno-v2/Y7twezR0wF2re1xh/images/langdb-finance-trace.png?fit=max&auto=format&n=Y7twezR0wF2re1xh&q=85&s=12aaf8fd6e3e9ce0dcca4e7bd0da9c43" style={{ borderRadius: '10px', width: '100%', maxWidth: '800px' }} alt="langdb-agno finance team observability" data-og-width="1623" width="1623" data-og-height="900" height="900" data-path="images/langdb-finance-trace.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/agno-v2/Y7twezR0wF2re1xh/images/langdb-finance-trace.png?w=280&fit=max&auto=format&n=Y7twezR0wF2re1xh&q=85&s=44bb9cf4c423a327b5459917cd3562cb 280w, https://mintcdn.com/agno-v2/Y7twezR0wF2re1xh/images/langdb-finance-trace.png?w=560&fit=max&auto=format&n=Y7twezR0wF2re1xh&q=85&s=a2589678cacdac15c3b5c8dc21903189 560w, https://mintcdn.com/agno-v2/Y7twezR0wF2re1xh/images/langdb-finance-trace.png?w=840&fit=max&auto=format&n=Y7twezR0wF2re1xh&q=85&s=b3aeeed6a9e129f4465d41d2e9e75929 840w, https://mintcdn.com/agno-v2/Y7twezR0wF2re1xh/images/langdb-finance-trace.png?w=1100&fit=max&auto=format&n=Y7twezR0wF2re1xh&q=85&s=d803a20ad4a20d1871212d0c23156624 1100w, https://mintcdn.com/agno-v2/Y7twezR0wF2re1xh/images/langdb-finance-trace.png?w=1650&fit=max&auto=format&n=Y7twezR0wF2re1xh&q=85&s=3f5eb5fa7780f3c740331550747b190b 1650w, https://mintcdn.com/agno-v2/Y7twezR0wF2re1xh/images/langdb-finance-trace.png?w=2500&fit=max&auto=format&n=Y7twezR0wF2re1xh&q=85&s=a397f1a8bb1d2e1ef366866ec6484a04 2500w" />
</Frame>

1. **Install Dependencies**

Ensure you have the necessary packages installed:

2. **Setup LangDB Account**

* Sign up for an account at [LangDB](https://app.langdb.ai/signup)
   * Create a new project in the LangDB dashboard
   * Obtain your API key and Project ID from the project settings

3. **Set Environment Variables**

Configure your environment with the LangDB credentials:

## Sending Traces to LangDB

### Example: Basic Agent Setup

This example demonstrates how to instrument your Agno agent with LangDB tracing.

```python  theme={null}
from pylangdb.agno import init

**Examples:**

Example 1 (unknown):
```unknown
2. **Setup LangDB Account**

   * Sign up for an account at [LangDB](https://app.langdb.ai/signup)
   * Create a new project in the LangDB dashboard
   * Obtain your API key and Project ID from the project settings

3. **Set Environment Variables**

   Configure your environment with the LangDB credentials:
```

Example 2 (unknown):
```unknown
## Sending Traces to LangDB

### Example: Basic Agent Setup

This example demonstrates how to instrument your Agno agent with LangDB tracing.
```

---

## Example 3: Get all tasks

**URL:** llms-txt#example-3:-get-all-tasks

**Contents:**
- Usage

print("\n=== Get all tasks ===")
todoist_agent.print_response("Get all the todoist tasks")
bash  theme={null}
    export TODOIST_API_TOKEN=xxx
    export OPENAI_API_KEY=xxx
    bash  theme={null}
    pip install -U todoist-api-python openai agno
    bash Mac theme={null}
      python cookbook/tools/todoist_tools.py
      bash Windows theme={null}
      python cookbook/tools/todoist_tools.py
      ```
    </CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Set your API Token">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## wget https://storage.googleapis.com/generativeai-downloads/images/GreatRedSpot.mp4

**URL:** llms-txt#wget-https://storage.googleapis.com/generativeai-downloads/images/greatredspot.mp4

**Contents:**
- Developer Resources

video_path = Path(__file__).parent.joinpath("GreatRedSpot.mp4")

agent.print_response("Tell me about this video", videos=[Video(filepath=video_path)])
```

## Developer Resources

View more [Examples](/examples/concepts/multimodal/video-caption)

---

## Serpapi

**URL:** llms-txt#serpapi

**Contents:**
- Prerequisites
- Example
- Toolkit Params
- Toolkit Functions
- Developer Resources

Source: https://docs.agno.com/concepts/tools/toolkits/search/serpapi

**SerpApiTools** enable an Agent to search Google and YouTube for a query.

The following example requires the `google-search-results` library and an API key from [SerpApi](https://serpapi.com/).

The following agent will search Google for the query: "Whats happening in the USA" and share results.

| Parameter               | Type            | Default | Description                                                                     |
| ----------------------- | --------------- | ------- | ------------------------------------------------------------------------------- |
| `api_key`               | `Optional[str]` | `None`  | SerpApi API key. If not provided, will use SERP\_API\_KEY environment variable. |
| `enable_search_google`  | `bool`          | `True`  | Enable Google search functionality.                                             |
| `enable_search_youtube` | `bool`          | `False` | Enable YouTube search functionality.                                            |
| `all`                   | `bool`          | `False` | Enable all available functions in the toolkit.                                  |

| Function         | Description                                                                                                                                                                                                                                                                              |
| ---------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| `search_google`  | Search Google using the Serpapi API. Parameters include `query` (str) for the search query and `num_results` (int, default=10) for the number of results. Returns JSON formatted search results with organic results, recipes, shopping results, knowledge graph, and related questions. |
| `search_youtube` | Search YouTube using the Serpapi API. Parameters include `query` (str) for the search query. Returns JSON formatted search results with video results, movie results, and channel results.                                                                                               |

## Developer Resources

* View [Tools](https://github.com/agno-agi/agno/blob/main/libs/agno/agno/tools/serpapi.py)
* View [Cookbook](https://github.com/agno-agi/agno/tree/main/cookbook/tools/serpapi_tools.py)

**Examples:**

Example 1 (unknown):
```unknown

```

Example 2 (unknown):
```unknown
## Example

The following agent will search Google for the query: "Whats happening in the USA" and share results.
```

---

## JWT Secret (use environment variable in production)

**URL:** llms-txt#jwt-secret-(use-environment-variable-in-production)

JWT_SECRET = "a-string-secret-at-least-256-bits-long"

---

## LightRAG Async

**URL:** llms-txt#lightrag-async

**Contents:**
- Code
- Usage

Source: https://docs.agno.com/examples/concepts/vectordb/lightrag/async-lightrag-db

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Install libraries">
    
  </Step>

<Step title="Set environment variables">
    
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Install libraries">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Set environment variables">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Claude with Reasoning Tools

**URL:** llms-txt#claude-with-reasoning-tools

**Contents:**
- Code

Source: https://docs.agno.com/examples/concepts/reasoning/tools/claude-reasoning-tools

This example shows how to use `ReasoningTools` with a Claude model.

```python cookbook/reasoning/tools/claude_reasoning_tools.py theme={null}
from agno.agent import Agent
from agno.models.anthropic import Claude
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.reasoning import ReasoningTools

reasoning_agent = Agent(
    model=Claude(id="claude-sonnet-4-20250514"),
    tools=[
        ReasoningTools(add_instructions=True),
        DuckDuckGoTools(search=True),
    ],
    instructions="Use tables to display data.",
    markdown=True,
)

---

## Pdf Input Local

**URL:** llms-txt#pdf-input-local

**Contents:**
- Code

Source: https://docs.agno.com/examples/models/openai/responses/pdf_input_local

```python cookbook/models/openai/responses/pdf_input_local.py theme={null}
from pathlib import Path

from agno.agent import Agent
from agno.media import File
from agno.models.openai.responses import OpenAIResponses
from agno.utils.media import download_file

pdf_path = Path(__file__).parent.joinpath("ThaiRecipes.pdf")

---

## Response: Your Route -> A -> B -> C

**URL:** llms-txt#response:-your-route-->-a-->-b-->-c

**Contents:**
- Examples

**Best Practice:** Add middleware in logical order:

1. **Security middleware first** (CORS, security headers)
2. **Authentication middleware** (JWT, session validation)
3. **Monitoring middleware** (logging, metrics)
4. **Business logic middleware** (rate limiting, custom logic)

<CardGroup cols={2}>
  <Card title="JWT with Headers" icon="shield" href="/examples/agent-os/middleware/jwt-middleware">
    JWT authentication using Authorization headers for API clients.
  </Card>

<Card title="JWT with Cookies" icon="cookie" href="/examples/agent-os/middleware/jwt-cookies">
    JWT authentication using HTTP-only cookies for web applications.
  </Card>

<Card title="Custom Middleware" icon="gear" href="/examples/agent-os/middleware/custom-middleware">
    Rate limiting and request logging middleware implementation.
  </Card>

<Card title="Custom FastAPI + JWT" icon="code" href="/examples/agent-os/middleware/custom-fastapi-jwt">
    Custom FastAPI app with JWT middleware and AgentOS integration.
  </Card>
</CardGroup>

---

## Initialize Weave with your project name

**URL:** llms-txt#initialize-weave-with-your-project-name

---

## Create workflow with loop

**URL:** llms-txt#create-workflow-with-loop

workflow = Workflow(
    name="Research and Content Workflow",
    description="Research topics in a loop until conditions are met, then create content",
    steps=[
        Loop(
            name="Research Loop",
            steps=[research_hackernews_step, research_web_step],
            end_condition=research_evaluator,
            max_iterations=3,  # Maximum 3 iterations
        ),
        content_step,
    ],
)

if __name__ == "__main__":
    # Test the workflow
    workflow.print_response(
        input="Research the latest trends in AI and machine learning, then create a summary",
    )
```

This was a synchronous non-streaming example of this pattern. To checkout async and streaming versions, see the cookbooks-

* [Loop Steps Workflow (sync streaming)](https://github.com/agno-agi/agno/tree/main/cookbook/workflows/_03_workflows_loop_execution/sync/loop_steps_workflow_stream.py)
* [Loop Steps Workflow (async non-streaming)](https://github.com/agno-agi/agno/tree/main/cookbook/workflows/_03_workflows_loop_execution/async/loop_steps_workflow.py)
* [Loop Steps Workflow (async streaming)](https://github.com/agno-agi/agno/tree/main/cookbook/workflows/_03_workflows_loop_execution/async/loop_steps_workflow_stream.py)

---

## Example 5: Location Analysis

**URL:** llms-txt#example-5:-location-analysis

print("\n=== Location Analysis Example ===")
agent.print_response(
    """Analyze this location in Phoenix:
    Address: '2301 N Central Ave, Phoenix, AZ 85004'
    Please provide:
    1. Exact coordinates
    2. Nearby landmarks
    3. Elevation data
    4. Local timezone""",
    markdown=True,
    stream=True,
)

---

## for event in run_events:

**URL:** llms-txt#for-event-in-run_events:

---

## Instrument and run

**URL:** llms-txt#instrument-and-run

**Contents:**
- Usage

with instrument_agno("openai"):
    agent.print_response("What are the latest developments in artificial intelligence?")

bash  theme={null}
    export ATLA_API_KEY=<your-key>
    bash  theme={null}
    pip install -U agno atla-insights openai
    bash Mac theme={null}
      python cookbook/integrations/observability/atla_op.py
      bash Windows theme={null}
      python cookbook/integrations/observability/atla_op.py
      ```
    </CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Set your API key">
    Sign up for an account at [https://app.atla-ai.com](https://app.atla-ai.com)
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Define steps for image pipeline

**URL:** llms-txt#define-steps-for-image-pipeline

generate_image_step = Step(
    name="generate_image",
    agent=image_generator,
    description="Generate a detailed image creation prompt based on the user's request",
)

describe_image_step = Step(
    name="describe_image",
    agent=image_describer,
    description="Analyze and describe the generated image concept in vivid detail",
)

---

## Local File System

**URL:** llms-txt#local-file-system

**Contents:**
- Example
- Toolkit Params
- Toolkit Functions
- Developer Resources

Source: https://docs.agno.com/concepts/tools/toolkits/local/local_file_system

LocalFileSystemTools enables agents to write files to the local file system with automatic directory management.

The following agent can write content to local files:

| Parameter           | Type            | Default | Description                                                  |
| ------------------- | --------------- | ------- | ------------------------------------------------------------ |
| `target_directory`  | `Optional[str]` | `None`  | Default directory to write files to. Uses current directory. |
| `default_extension` | `str`           | `"txt"` | Default file extension to use if none specified.             |
| `enable_write_file` | `bool`          | `True`  | Enable file writing functionality.                           |

| Function     | Description                                              |
| ------------ | -------------------------------------------------------- |
| `write_file` | Write content to a local file with customizable options. |

## Developer Resources

* View [Tools Source](https://github.com/agno-agi/agno/blob/main/libs/agno/agno/tools/local_file_system.py)
* [Python pathlib Documentation](https://docs.python.org/3/library/pathlib.html)
* [File I/O Best Practices](https://docs.python.org/3/tutorial/inputoutput.html)

---

## "My name is John Doe and I like to hike in the mountains on weekends.",

**URL:** llms-txt#"my-name-is-john-doe-and-i-like-to-hike-in-the-mountains-on-weekends.",

---

## Valyu

**URL:** llms-txt#valyu

**Contents:**
- Example
- Toolkit Params
- Toolkit Functions
- Developer Resources

Source: https://docs.agno.com/concepts/tools/toolkits/search/valyu

ValyuTools provides academic and web search capabilities with advanced filtering and relevance scoring.

The following agent can perform academic and web searches:

| Parameter                | Type                  | Default | Description                                     |
| ------------------------ | --------------------- | ------- | ----------------------------------------------- |
| `api_key`                | `Optional[str]`       | `None`  | Valyu API key. Uses VALYU\_API\_KEY if not set. |
| `enable_academic_search` | `bool`                | `True`  | Enable academic sources search functionality.   |
| `enable_web_search`      | `bool`                | `True`  | Enable web search functionality.                |
| `enable_paper_search`    | `bool`                | `True`  | Enable search within paper functionality.       |
| `text_length`            | `int`                 | `1000`  | Maximum length of text content per result.      |
| `max_results`            | `int`                 | `10`    | Maximum number of results to return.            |
| `relevance_threshold`    | `float`               | `0.5`   | Minimum relevance score for results.            |
| `content_category`       | `Optional[str]`       | `None`  | Content category for filtering.                 |
| `search_start_date`      | `Optional[str]`       | `None`  | Start date for search filtering (YYYY-MM-DD).   |
| `search_end_date`        | `Optional[str]`       | `None`  | End date for search filtering (YYYY-MM-DD).     |
| `search_domains`         | `Optional[List[str]]` | `None`  | List of domains to search within.               |
| `sources`                | `Optional[List[str]]` | `None`  | List of specific sources to search.             |
| `max_price`              | `float`               | `30.0`  | Maximum price for API calls.                    |

| Function          | Description                                                   |
| ----------------- | ------------------------------------------------------------- |
| `academic_search` | Search academic sources for research papers and publications. |
| `web_search`      | Search web sources for general information and content.       |
| `paper_search`    | Search within specific papers for detailed information.       |

## Developer Resources

* View [Tools Source](https://github.com/agno-agi/agno/blob/main/libs/agno/agno/tools/valyu.py)
* [Valyu API Documentation](https://valyu.ai/docs)
* [Academic Search Best Practices](https://valyu.ai/academic-search)

---

## db=db,

**URL:** llms-txt#db=db,

---

## Portkey

**URL:** llms-txt#portkey

**Contents:**
- Authentication
- Example
- Advanced Configuration
- Params

Source: https://docs.agno.com/concepts/models/portkey

Learn how to use models through the Portkey AI Gateway in Agno.

Portkey is an AI Gateway that provides a unified interface to access multiple AI providers with advanced features like routing, load balancing, retries, and observability. Use Portkey to build production-ready AI applications with better reliability and cost optimization.

With Portkey, you can:

* Route requests across multiple AI providers
* Implement fallback mechanisms for better reliability
* Monitor and analyze your AI usage
* Cache responses for cost optimization
* Apply rate limiting and usage controls

You need both a Portkey API key and a virtual key for model routing. Get them [from Portkey here](https://app.portkey.ai/).

Use `Portkey` with your `Agent`:

<CodeGroup>
  
</CodeGroup>

## Advanced Configuration

You can configure Portkey with custom routing and retry policies:

<Note> View more examples [here](/examples/models/portkey/basic). </Note>

| Parameter     | Type            | Default                       | Description                                                     |
| ------------- | --------------- | ----------------------------- | --------------------------------------------------------------- |
| `id`          | `str`           | `"gpt-4o-mini"`               | The id of the model to use through Portkey                      |
| `name`        | `str`           | `"Portkey"`                   | The name of the model                                           |
| `provider`    | `str`           | `"Portkey"`                   | The provider of the model                                       |
| `api_key`     | `Optional[str]` | `None`                        | The API key for Portkey (defaults to PORTKEY\_API\_KEY env var) |
| `base_url`    | `str`           | `"https://api.portkey.ai/v1"` | The base URL for the Portkey API                                |
| `virtual_key` | `Optional[str]` | `None`                        | The virtual key for the underlying provider                     |
| `trace_id`    | `Optional[str]` | `None`                        | Custom trace ID for request tracking                            |
| `config_id`   | `Optional[str]` | `None`                        | Configuration ID for Portkey routing                            |

`Portkey` also supports the params of [OpenAI](/reference/models/openai).

**Examples:**

Example 1 (unknown):
```unknown

```

Example 2 (unknown):
```unknown
</CodeGroup>

## Example

Use `Portkey` with your `Agent`:

<CodeGroup>
```

Example 3 (unknown):
```unknown
</CodeGroup>

## Advanced Configuration

You can configure Portkey with custom routing and retry policies:
```

---

## Upload the video file if it doesn't exist

**URL:** llms-txt#upload-the-video-file-if-it-doesn't-exist

**Contents:**
- Usage

if not video_file:
    try:
        logger.info(f"Uploading video: {video_path}")
        video_file = model.get_client().files.upload(
            file=video_path,
            config=dict(name=video_path.stem, display_name=video_path.stem),
        )

# Check whether the file is ready to be used.
        while video_file.state.name == "PROCESSING":
            time.sleep(2)
            video_file = model.get_client().files.get(name=video_file.name)

logger.info(f"Uploaded video: {video_file}")
    except Exception as e:
        logger.error(f"Error uploading video: {e}")

if __name__ == "__main__":
    agent.print_response(
        "Tell me about this video",
        videos=[Video(content=video_file)],
        stream=True,
    )
bash  theme={null}
    export GOOGLE_API_KEY=xxx
    bash  theme={null}
    pip install -U google-genai agno
    bash Mac theme={null}
      python cookbook/models/google/gemini/video_input_file_upload.py
      bash Windows theme={null}
      python cookbook/models/google/gemini/video_input_file_upload.py
      ```
    </CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Set your API key">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Content Team

**URL:** llms-txt#content-team

**Contents:**
- Code

Source: https://docs.agno.com/examples/use-cases/teams/content_team

This example shows how to create a content creation team with specialized researchers and writers using the `coordinate` mode.

```python cookbook/examples/teams/coordinate_mode/content_team.py theme={null}
from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.team import Team
from agno.tools.duckduckgo import DuckDuckGoTools

---

## Contributing to Agno

**URL:** llms-txt#contributing-to-agno

**Contents:**
- 👩‍💻 How to contribute
- Development setup
- Formatting and validation

Source: https://docs.agno.com/how-to/contribute

Learn how to contribute to Agno through our fork and pull request workflow.

Agno is an open-source project and we welcome contributions.

## 👩‍💻 How to contribute

Please follow the [fork and pull request](https://docs.github.com/en/get-started/quickstart/contributing-to-projects) workflow:

* Fork the repository.
* Create a new branch for your feature.
* Add your feature or improvement.
* Send a pull request.
* We appreciate your support & input!

1. Clone the repository.
2. Create a virtual environment:
   * For Unix, use `./scripts/dev_setup.sh`.
   * This setup will:
     * Create a `.venv` virtual environment in the current directory.
     * Install the required packages.
     * Install the `agno` package in editable mode.
3. Activate the virtual environment:
   * On Unix: `source .venv/bin/activate`

> From here on you have to use `uv pip install` to install missing packages

## Formatting and validation

Ensure your code meets our quality standards by running the appropriate formatting and validation script before submitting a pull request:

* For Unix:
  * `./scripts/format.sh`
  * `./scripts/validate.sh`

These scripts will perform code formatting with `ruff` and static type checks with `mypy`.

Read more about the guidelines [here](https://github.com/agno-agi/agno/tree/main/CONTRIBUTING.md)

Message us on [Discord](https://discord.gg/4MtYHHrgA8) or post on [Discourse](https://community.agno.com/) if you have any questions or need help with credits.

---

## Visualization Tools

**URL:** llms-txt#visualization-tools

**Contents:**
- Code
- Usage

Source: https://docs.agno.com/examples/concepts/tools/others/visualization

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Set your API key">
    
  </Step>

<Step title="Install libraries">
    
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Set your API key">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Now let's setup an example Team and run it.

**URL:** llms-txt#now-let's-setup-an-example-team-and-run-it.

---

## Make sure you define the `__call__` method that receives

**URL:** llms-txt#make-sure-you-define-the-`__call__`-method-that-receives

---

## MLX Transcribe Tools

**URL:** llms-txt#mlx-transcribe-tools

**Contents:**
- Code
- Usage

Source: https://docs.agno.com/examples/concepts/tools/others/mlx_transcribe

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Set your API key">
    
  </Step>

<Step title="Install libraries">
    
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Set your API key">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Run the workflow if the script is executed directly

**URL:** llms-txt#run-the-workflow-if-the-script-is-executed-directly

if __name__ == "__main__":
    import random

from rich.prompt import Prompt

# Fun example prompts to showcase the generator's versatility
    example_prompts = [
        "Why Cats Secretly Run the Internet",
        "The Science Behind Why Pizza Tastes Better at 2 AM",
        "Time Travelers' Guide to Modern Social Media",
        "How Rubber Ducks Revolutionized Software Development",
        "The Secret Society of Office Plants: A Survival Guide",
        "Why Dogs Think We're Bad at Smelling Things",
        "The Underground Economy of Coffee Shop WiFi Passwords",
        "A Historical Analysis of Dad Jokes Through the Ages",
    ]

# Get topic from user
    topic = Prompt.ask(
        "[bold]Enter a blog post topic[/bold] (or press Enter for a random example)\n✨",
        default=random.choice(example_prompts),
    )

# Convert the topic to a URL-safe string for use in session_id
    url_safe_topic = topic.lower().replace(" ", "-")

# Initialize the blog post generator workflow
    # - Creates a unique session ID based on the topic
    # - Sets up SQLite storage for caching results
    generate_blog_post = BlogPostGenerator(
        session_id=f"generate-blog-post-on-{url_safe_topic}",
        db=SqliteDb(
            db_file="tmp/agno_workflows.db",
        ),
        debug_mode=True,
    )

# Execute the workflow with caching enabled
    # Returns an iterator of RunOutput objects containing the generated content
    blog_post: Iterator[RunOutputEvent] = generate_blog_post.run(
        topic=topic,
        use_search_cache=True,
        use_scrape_cache=True,
        use_cached_report=True,
    )

# Print the response
    pprint_run_response(blog_post, markdown=True)
python  theme={null}
import asyncio
import json
from textwrap import dedent
from typing import Dict, Optional

from agno.agent import Agent
from agno.db.sqlite import SqliteDb
from agno.models.openai import OpenAIChat
from agno.tools.googlesearch import GoogleSearchTools
from agno.tools.newspaper4k import Newspaper4kTools
from agno.utils.log import logger
from agno.utils.pprint import pprint_run_response
from agno.workflow.workflow import Workflow
from pydantic import BaseModel, Field

**Examples:**

Example 1 (unknown):
```unknown
To convert this into **Workflows 2.0** structure, either we can break down the workflow into smaller steps and follow the [development guide](/concepts/workflows/workflow-patterns/overview).
Or for simplicity we can directly replace the run method to a single custom function executor as mentioned [here](/concepts/workflows/workflow-patterns/fully-python-workflow).

It will look like this:
```

---

## OpenBB Tools

**URL:** llms-txt#openbb-tools

**Contents:**
- Code
- Usage

Source: https://docs.agno.com/examples/concepts/tools/others/openbb

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Set your API key">
    
  </Step>

<Step title="Install libraries">
    
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Set your API key">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## store only important events

**URL:** llms-txt#store-only-important-events

production_workflow = Workflow(
    name="Production Workflow",
    store_events=True,
    events_to_skip=[
        WorkflowRunEvent.step_started,
        WorkflowRunEvent.parallel_execution_started,
        # keep step_completed and workflow_completed
    ],
    steps=[...]
)

---

## Text Reader

**URL:** llms-txt#text-reader

Source: https://docs.agno.com/reference/knowledge/reader/text

TextReader is a reader class that allows you to read data from text files.

<Snippet file="text-reader-reference.mdx" />

---

## Add Secrets

**URL:** llms-txt#add-secrets

**Contents:**
- Development Secrets
- Production Secrets

Source: https://docs.agno.com/templates/infra-management/secrets

Secret management is a critical part of your application security and should be taken seriously.

Local secrets are defined in the `infra/secrets` directory which is excluded from version control (see `.gitignore`). Its contents should be handled with the same security as passwords.

Production secrets are managed by [AWS Secrets Manager](https://docs.aws.amazon.com/secretsmanager/latest/userguide/intro.html).

<Note>
  Incase you're missing the secrets dir, copy `infra/example_secrets`
</Note>

## Development Secrets

Apps running locally can read secrets using a `yaml` file, for example:

## Production Secrets

`AWS Secrets` are used to manage production secrets, which are read by the production apps.

```python prd_resources.py theme={null}

**Examples:**

Example 1 (unknown):
```unknown
## Production Secrets

`AWS Secrets` are used to manage production secrets, which are read by the production apps.
```

---

## MongoDB for Workflow

**URL:** llms-txt#mongodb-for-workflow

**Contents:**
- Usage
  - Run MongoDB

Source: https://docs.agno.com/examples/concepts/db/mongodb/mongodb_for_workflow

Agno supports using MongoDB as a storage backend for Workflows using the `MongoDBDb` class.

Install [docker desktop](https://docs.docker.com/desktop/install/mac-install/) and run **MongoDB** on port **27017** using:

```python mongodb_for_workflow.py theme={null}
from agno.agent import Agent
from agno.db.mongodb import MongoDBDb
from agno.models.openai import OpenAIChat
from agno.team import Team
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.hackernews import HackerNewsTools
from agno.workflow.step import Step
from agno.workflow.workflow import Workflow

db_url = "mongodb://localhost:27017"

db = MongoDb(db_url=db_url)

**Examples:**

Example 1 (unknown):
```unknown

```

---

## Initialize Langtrace

**URL:** llms-txt#initialize-langtrace

---

## Example 2: Delete a task

**URL:** llms-txt#example-2:-delete-a-task

print("\n=== Delete a task ===")
todoist_agent.print_response(
    "Delete the todoist task to buy groceries tomorrow at 10am"
)

---

## Semantic Chunking

**URL:** llms-txt#semantic-chunking

Source: https://docs.agno.com/reference/knowledge/chunking/semantic

Semantic chunking is a method of splitting documents into smaller chunks by analyzing semantic similarity between text segments using embeddings.
It uses the chonkie library to identify natural breakpoints where the semantic meaning changes significantly, based on a configurable similarity threshold.
This helps preserve context and meaning better than fixed-size chunking by ensuring semantically related content stays together in the same chunk, while splitting occurs at meaningful topic transitions.

<Snippet file="chunking-semantic.mdx" />

---

## Agno Assist

**URL:** llms-txt#agno-assist

**Contents:**
- Code
- Usage

Source: https://docs.agno.com/examples/use-cases/agents/agno_assist

This example shows how to create a specialized AI assistant that helps users understand and work with the Agno framework. Learn how to build domain-specific agents that provide expert guidance, answer technical questions, and help users navigate complex systems. Perfect for creating help desk agents, technical support systems, and educational AI assistants.

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Set your API key">
    
  </Step>

<Step title="Install libraries">
    
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Set your API key">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## JsonDb

**URL:** llms-txt#jsondb

Source: https://docs.agno.com/reference/storage/json

`JsonDb` is a class that implements the Db interface using JSON files as the backend storage system. It provides a simple, file-based storage solution for agent sessions with each session stored in a separate JSON file.

<Snippet file="db-json-params.mdx" />

<Snippet file="db-new-bulk-methods.mdx" />

---

## Input and Output

**URL:** llms-txt#input-and-output

**Contents:**
- Structured Inputs with Pydantic
  - Validating the input
  - Developer Resources
- Structured Input/Output at Step Level
  - Data Flow Between Steps

Source: https://docs.agno.com/concepts/workflows/input-and-output

Learn how to use structured input and output with Workflows for reliable, production-ready systems.

Workflows support multiple input types for maximum flexibility:

| Input Type         | Example                                           | Use Case                   |
| ------------------ | ------------------------------------------------- | -------------------------- |
| **String**         | `"Analyze AI trends"`                             | Simple text prompts        |
| **Pydantic Model** | `ResearchRequest(topic="AI", depth=5)`            | Type-safe structured input |
| **List**           | `["AI", "ML", "LLMs"]`                            | Multiple items to process  |
| **Dictionary**     | `{"query": "AI", "sources": ["web", "academic"]}` | Key-value pairs            |

<Note>
  When this input is passed to an `Agent` or `Team`, it will be serialized to a
  string before being passed to the agent or team.
</Note>

See more on Pydantic as input in the [Advanced Workflows](/concepts/workflows/input-and-output#structured-inputs-with-pydantic) documentation.

## Structured Inputs with Pydantic

Leverage Pydantic models for type-safe, validated workflow inputs:

### Validating the input

You can set `input_schema` on the Workflow to validate the input. If you then pass the input as a dictionary, it will be automatically validated against the schema.

### Developer Resources

* [Pydantic Model as Input](/examples/concepts/teams/structured_input_output/pydantic_model_as_input)
* [Workflow with Input Schema Validation](/examples/concepts/workflows/06_workflows_advanced_concepts/workflow_with_input_schema)

## Structured Input/Output at Step Level

Workflows feature a powerful type-safe data flow system enabling each step to:

1. **Receive** structured input (Pydantic models, lists, dicts, or raw strings)
2. **Produce** structured output (validated Pydantic models)
3. **Maintain** type safety throughout entire workflow execution

### Data Flow Between Steps

* First step receives the workflow's input message
* Subsequent steps receive the previous step's structured output

**Output Generation**

* Each Agent processes input using its configured `output_schema`
* Output is automatically validated against the defined model

```python  theme={null}

**Examples:**

Example 1 (unknown):
```unknown
### Validating the input

You can set `input_schema` on the Workflow to validate the input. If you then pass the input as a dictionary, it will be automatically validated against the schema.
```

Example 2 (unknown):
```unknown
### Developer Resources

* [Pydantic Model as Input](/examples/concepts/teams/structured_input_output/pydantic_model_as_input)
* [Workflow with Input Schema Validation](/examples/concepts/workflows/06_workflows_advanced_concepts/workflow_with_input_schema)

## Structured Input/Output at Step Level

Workflows feature a powerful type-safe data flow system enabling each step to:

1. **Receive** structured input (Pydantic models, lists, dicts, or raw strings)
2. **Produce** structured output (validated Pydantic models)
3. **Maintain** type safety throughout entire workflow execution

### Data Flow Between Steps

**Input Processing**

* First step receives the workflow's input message
* Subsequent steps receive the previous step's structured output

**Output Generation**

* Each Agent processes input using its configured `output_schema`
* Output is automatically validated against the defined model
```

---

## Simple function we will use as a post-hook

**URL:** llms-txt#simple-function-we-will-use-as-a-post-hook

**Contents:**
- Post-hooks Parameters
- Guardrails
- Developer Resources

def validate_output_length(run_output: RunOutput) -> None:
    """Post-hook to validate output length."""
    max_length = 1000
    if len(run_output.content) > max_length:
        raise OutputCheckError(
            f"Output too long. Max {max_length} characters allowed",
            check_trigger=CheckTrigger.INPUT_NOT_ALLOWED,
        )

team = Team(
    name="My Team",
    model=OpenAIChat(id="gpt-5-mini"),
    # Provide the post-hook to the Team using the post_hooks parameter
    post_hooks=[validate_output_length],
)
````

You can see some complete examples of post-hooks in the [Examples](/examples/concepts/teams/hooks) section.

## Post-hooks Parameters

Post-hooks run automatically during the Team run. Some parameters will be injected automatically.

You can learn more about the parameters in the [Post-hooks](/reference/hooks/post-hooks) reference.

A popular use case for pre-hooks are Guardrails: built-in safeguards for your Teams.

You can learn more about them in the [Guardrails](/concepts/teams/guardrails) section.

## Developer Resources

* View [Examples](/examples/concepts/teams/hooks)
* View [Cookbook](https://github.com/agno-agi/agno/tree/main/cookbook/teams/hooks)

---

## OpenAI gpt-5-mini

**URL:** llms-txt#openai-gpt-5-mini

**Contents:**
- Code
- Usage

Source: https://docs.agno.com/examples/concepts/reasoning/models/openai/o3-mini

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Set your API key">
    
  </Step>

<Step title="Install libraries">
    
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Set your API key">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Condition evaluator function

**URL:** llms-txt#condition-evaluator-function

def is_tech_topic(step_input) -> bool:
    """Check if the topic is tech-related and needs specialized research"""
    message = step_input.input.lower() if step_input.input else ""
    tech_keywords = [
        "ai",
        "machine learning",
        "technology",
        "software",
        "programming",
        "tech",
        "startup",
        "blockchain",
    ]
    return any(keyword in message for keyword in tech_keywords)

---

## Agno v2.0 Changelog

**URL:** llms-txt#agno-v2.0-changelog

**Contents:**
- General Changes
- Session & Run State
- Storage
- Memory
- Knowledge
- Tools updates
- Media
- Logging
- Agent updates
- Team updates

Source: https://docs.agno.com/how-to/v2-changelog

<img height="200" src="https://mintcdn.com/agno-v2/HVmF95GvYttNRl-5/images/changelogs/agent_os_stack.png?fit=max&auto=format&n=HVmF95GvYttNRl-5&q=85&s=a2223b0ee329d32688b43f6ebe1b7174" data-og-width="323" data-og-height="320" data-path="images/changelogs/agent_os_stack.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/agno-v2/HVmF95GvYttNRl-5/images/changelogs/agent_os_stack.png?w=280&fit=max&auto=format&n=HVmF95GvYttNRl-5&q=85&s=ebcc3630f1f4c8482631fb06a7d29c91 280w, https://mintcdn.com/agno-v2/HVmF95GvYttNRl-5/images/changelogs/agent_os_stack.png?w=560&fit=max&auto=format&n=HVmF95GvYttNRl-5&q=85&s=ee6bca0ef5c4acf434bd96854d698456 560w, https://mintcdn.com/agno-v2/HVmF95GvYttNRl-5/images/changelogs/agent_os_stack.png?w=840&fit=max&auto=format&n=HVmF95GvYttNRl-5&q=85&s=b12920d3ed0141b7eae5364aeefb795f 840w, https://mintcdn.com/agno-v2/HVmF95GvYttNRl-5/images/changelogs/agent_os_stack.png?w=1100&fit=max&auto=format&n=HVmF95GvYttNRl-5&q=85&s=7d8e536bad2a2e737c907296c9b550c3 1100w, https://mintcdn.com/agno-v2/HVmF95GvYttNRl-5/images/changelogs/agent_os_stack.png?w=1650&fit=max&auto=format&n=HVmF95GvYttNRl-5&q=85&s=141013bcc891f080fe2e63188ce33e70 1650w, https://mintcdn.com/agno-v2/HVmF95GvYttNRl-5/images/changelogs/agent_os_stack.png?w=2500&fit=max&auto=format&n=HVmF95GvYttNRl-5&q=85&s=5424dcf79c186f18ec276e514bd0e932 2500w" />

This is a major release that introduces a completely new approach to building multi-agent systems. It also introduces the AgentOS, a runtime for agents.

This is a major rewrite of the Agno library and introduces various new concepts and updates to the existing ones.

Some of the major changes are:

* Agents, Teams and Workflows are now fully stateless.
* Knowledge is now a single solution that supports many forms of content.
* Storage of sessions, memories, evals, etc. has been simplified

<Accordion title="Repo Updates">
  * `/libs/agno` has been restructured to fit the new concepts in Agno and for better organization.
  * All code related to managing workspaces and agent deployment in Agno has been moved to a new package called `agno-infra`. This is a combination of the previous `agno-aws` and `agno-docker` packages, as well as the CLI and other tools.
  * `agno-aws` and `agno-docker` packages have been deprecated and will no-longer be maintained.
  * All code related to the Agno CLI (`ag`) has been moved to this new `agno-infra` package.
  * Added `AgentOS` to `agno` as a comprehensive API solution for building multi-agent systems. This also replaces `Playground` and other Apps. See details below.
  * Cookbook has been completely restructured, with new and more valuable READMEs, better coverage of concepts, and more examples.
</Accordion>

<Accordion title="AgentOS">
  * Introducing `AgentOS`, a system for hosting agents, teams and workflows as a production-ready API. See full details in the [AgentOS](/agent-os/introduction) section.
  * This adds routes for session management, memory management, knowledge management, evals management, and metrics.
  * This enables you to host agents, teams and workflows, and use the [Agent OS UI](https://os.agno.com) to manage them.
</Accordion>

<Accordion title="Apps Deprecations">
  * Removed `Playground`. Its functionality has been replaced by `AgentOS`.
  * Removed `AGUIApp` and replace with `AGUI` interface on `AgentOS`.
  * Removed `SlackApi` and replace with `Slack` interface on `AgentOS`.
  * Removed `WhatsappApi` and replace with `Whatsapp` interface on `AgentOS`.
  * Removed `FastAPIApp`. Its functionality has been replaced by `AgentOS`.
  * `DiscordClient` has been moved to `/integrations/discord`.
</Accordion>

## Session & Run State

* We have made significant changes to the innerworkings of `Agent`, `Team` and `Workflow` to make them completely stateless.
* This means that `agent_session`, `session_metrics`, `session_state`, etc. should not be seen as stateful variables that would be updated during the course of a run, but rather as "defaults" for the agent if they can be set on initialisation.
* `CustomEvent` is now supported and you can inherit from it to create your own custom events that can be yielded from your own tools. See the [documentation](/concepts/agents/running-agents#custom-events) for more details.

<Accordion title="Updates to Run Objects">
  For agents:

* `RunResponse` -> `RunOutput`
  * `RunResponseStartedEvent` -> `RunStartedEvent`
  * `RunResponseContentEvent` -> `RunContentEvent`
  * `RunResponseCompletedEvent` -> `RunCompletedEvent`
  * `IntermediateRunResponseContentEvent` -> `IntermediateRunContentEvent`
  * `RunResponseErrorEvent` -> `RunErrorEvent`
  * `RunResponseCancelledEvent` -> `RunCancelledEvent`

* `TeamRunResponse` -> `TeamRunOutput`
  * `RunResponseStartedEvent` -> `RunStartedEvent`
  * `RunResponseContentEvent` -> `RunContentEvent`
  * `RunResponseCompletedEvent` -> `RunCompletedEvent`
  * `IntermediateRunResponseContentEvent` -> `IntermediateRunContentEvent`
  * `RunResponseErrorEvent` -> `RunErrorEvent`
  * `RunResponseCancelledEvent` -> `RunCancelledEvent`

* `WorkflowRunResponse` -> `WorkflowRunOutput`

* `WorkflowRunResponseStartedEvent` -> `WorkflowRunStartedEvent`

* `WorkflowRunResponseContentEvent` -> `WorkflowRunContentEvent`

* `WorkflowRunResponseCompletedEvent` -> `WorkflowRunCompletedEvent`

* `WorkflowRunResponseErrorEvent` -> `WorkflowRunErrorEvent`

* `WorkflowRunResponseCancelledEvent` -> `WorkflowRunCancelledEvent`

* The import location for `RunOutput` (and events) has been moved to `agno.run.agent`.

* For `RunOutput`, `TeamRunOutput` and `WorkflowRunOutput` the `extra_data` attribute has been removed and the internal attributes are now top-level. This is `references`, `additional_input`, `reasoning_steps`, and `reasoning_messages`.

* `metadata` added to `RunOutput`, `TeamRunOutput` and `WorkflowRunOutput`. This represents all the set metadata for the run.
</Accordion>

<Accordion title="Updates to Session Objects">
  * Session storage now stores `AgentSession`, `TeamSession` and `WorkflowSession` with new schemas. See full details in the [Session](/concepts/agents/sessions) section.
  * Session objects now have `runs` directly on it.
  * Session objects support new convenience methods:
    * `get_run` -> Get a specific run by ID.
    * `get_session_summary` -> Get the session summary.
    * `get_chat_history` -> Get an aggregated view of all messages for all runs in the session.
</Accordion>

<Accordion title="Updates to Metrics">
  * `SessionMetrics` and `MessageMetrics` have been unified as a single `Metrics` class.
  * `audio_tokens` has been renamed to `audio_total_tokens`.
  * `input_audio_tokens` has been renamed to `audio_input_tokens`.
  * `output_audio_tokens` has been renamed to `audio_output_tokens`.
  * `cached_tokens` has been renamed to `cache_read_tokens`.
  * `prompt_tokens` and `completion_tokens` have been removed (only `input_tokens` and `output_tokens` should be used)
  * `prompt_tokens_details` and `completion_tokens_details` have been removed. Instead `provider_metrics` captures any provider-specific metrics.
  * `time` has been renamed to `duration`.
</Accordion>

<Accordion title="Cancelling Runs">
  * You can now cancel a run by calling `cancel_run` on the `Agent`, `Team` or `Workflow`.
  * This will cancel the run and return a `RunCancelledEvent` during streaming, or set the `RunOutput.status` to `"cancelled"`.
</Accordion>

* `Agent`, `Team`, `Workflow` and the various evals now all support a single `db` parameter. This is to enable storage for the instance of that class. This is required for persistence of sessions, memories, metrics, etc.
* `storage` and `memory` have been removed from `Agent`, `Team` and `Workflow`.

<Accordion title="Updates to Storage Classes">
  - This means all previous storage providers have been reworked. Also session storage, memory storage and eval storage are all a single solution now referred to as a "DB".
  - `PostgresStorage` -> `PostgresDb`
  - `SqliteStorage` -> `SqliteDb`
  - `MysqlStorage` -> `MysqlDb`
  - `RedisStorage` -> `RedisDb`
  - `MongoStorage` -> `MongoDb`
  - `DynamoDBStorage` -> `DynamoDb`
  - `SingleStoreStorage` -> `SingleStoreDb`
  - `InMemoryStorage` -> `InMemoryDb`
  - `JsonStorage` -> `JsonDb`
  - `GCSJsonStorage` -> `GCSJsonDb`
</Accordion>

* With the above changes to storage, memory has been simplified.
* `memory` has been removed from `Agent` and `Team`. Instead memory is enabled with `enable_user_memories: bool` (like before) and persisted in the `db` instance.
* Changes to how memories are created can still be done by overriding the `MemoryManager` class on `Agent` or `Team`. E.g. `Agent(memory_manager=MyMemoryManager())`.
* `AgentMemory` and `TeamMemory` have been removed.

* Knowledge has been completely reworked. See full details in the [Knowledge](/concepts/knowledge/) section.
* You now define a single `Knowledge` instance for all types of content. Files (PDF, CSV, etc.), URLs, and other.
* The agent can still use your knowledge base to search for information at runtime. All existing RAG implementations are still supported.
* Added **full `async` support** for embedding models and vector DBs. This has a significant impact on performance and is a major speed improvement when adding content to the knowledge base using `knowledge.add_content_async(...)`.
* `AgentKnowledge` and all other knowledge base classes have been removed.
* Import locations for `embedder`, `document`, `chunking`, `reranker` and `reader` have been moved to `agno.knowledge`. See [examples](/examples/concepts/knowledge) for more details.

* General:
  * Since Agents and Teams are now stateless, using attributes from the agent/team object inside a function will give you access to the attributes set on initialisation of that agent/team. E.g. `agent.session_state` should not be used, instead `session_state` can now be directly accessed and would have the "current" state of the session.
  * A new flow allows images, audio and video files generated during tool execution to be passed back in a `FunctionExecutionResult` object and this will ensure these artifacts are made available to the model and agent as needed.
* All tools that handle media (e.g. image generation tools) now correctly add this generated media to the `RunOutput`, but also make it available for subsequent model calls.
* The interface of almost all the toolkits have been updated for a more consistent experience around switching specific tools on and off. The list of changes is too long to list here. We suggest you take a look at the toolkits you use specifically and how they have been updated.
* `show_results` is now `True` by default for all tools. If you just set `stop_after_tool_call=True` then `show_results` will be automatically set to `True`.
* `images`, `videos`, `audio` and `files` are now available as parameters to tools. See the [documentation](/concepts/tools/overview) for more details.

* **Removed legacy artifact classes**: `ImageArtifact`, `VideoArtifact`, `AudioArtifact`, and `AudioResponse` classes have been completely removed in favor of unified media classes.

#### New Unified Media Architecture

* **Unified `Image` class**: Now serves all use cases (input, output, artifacts) with standardized `content: Optional[bytes]` field for raw image data
* **Unified `Audio` class**: Replaces both `AudioArtifact` and `AudioResponse` with consistent byte-based content storage and additional fields like `transcript`, `expires_at`, `sample_rate`, and `channels`
* **Unified `Video` class**: Updated to handle all video use cases with standardized content handling and metadata fields
* **Enhanced `File` class**: Updated to work seamlessly across agent, team, workflow, and toolkit contexts

#### New Methods and Features

* **`from_base64()` class method**: Added to `Image`, `Audio`, and `Video` classes for creating instances from base64-encoded content (automatically converts to raw bytes)
* **`get_content_bytes()` method**: Retrieves content as raw bytes, handling loading from URLs or file paths
* **`to_base64()` method**: Converts content to base64 string for transmission/storage
* **`to_dict()` method**: Enhanced serialization with optional base64 content inclusion

#### Content Standardization

* **Byte-based storage**: All media content is now stored as raw bytes (`Optional[bytes]`) instead of mixed string/bytes formats
* **Automatic validation**: Model validators ensure exactly one content source (`url`, `filepath`, or `content`) is provided
* **Auto-generated IDs**: Media objects automatically generate UUIDs when not provided

* Added support for custom loggers. See the [documentation](/concepts/agents/custom-logger) for more details.

<Accordion title="Updates to Agent Class">
  * `agent_id` -> `id` -> If `id` is not set, it is autogenerated using the `name` of the agent, or a random UUID if the `name` is not set.
  * `search_previous_sessions_history` -> `search_session_history`
  * `context` -> `dependencies`
  * `add_context` -> `add_dependencies_to_context`
  * `add_history_to_messages` -> `add_history_to_context`
  * `add_name_to_instructions` -> `add_name_to_context`
  * `add_datetime_to_instructions` -> `add_datetime_to_context`
  * `add_location_to_instructions` -> `add_location_to_context`
  * `add_messages` -> `additional_input`
  * `extra_data` -> `metadata`
  * `create_default_system_message` -> `build_context`
  * `create_default_user_message` -> `build_user_context`
  * Added `send_media_to_model` -> `True` by default. Set to False if you don't want to send media (image, audio, video, files) to the model.  This is useful if you only want media for tools.
  * Added `store_media` -> `True` by default. Set to False if you don't want to store any media in the `RunOutput` that is persisted with sessions.
  * `num_history_responses` -> `num_history_runs`
  * Removed `success_criteria` and `goal`
  * Removed `team_session_id` and `workflow_session_id`.
  * Removed `introduction`
  * Removed `show_tool_calls` -> This is now just always enabled.
  * Removed `team` and `team_data`
  * Removed `respond_directly`, `add_transfer_instructions`, `team_response_separator` and `team_session_id` (since team has been removed from `Agent`)
  * Removed all `team` functionality from inside `Agent` (i.e. the deprecated teams implementation has been removed)
  * Removed all `monitoring` from `Agent`. With the new AgentOS platform, monitoring is done using your own data. Go to [os.agno.com](https://os.agno.com) to get started.
</Accordion>

<Accordion title="Updates to Input & Output">
  * `response_model` -> `output_schema`
  * Added `input_schema` (a pydantic model) to validate the input to the agent.
  * Changed `message` to `input` (which also replaces `messages`). `input` can be of type `str`, `list`, `dict`, `Message`, `BaseModel`, or `list[Message]`.
  * If a `dict` and `input_schema` is provided, the dict will be validated against the schema.
  * If a `BaseModel` and `input_schema` is provided, the model will be validated against the schema.
  * `arun` and `acontinue_run` with `stream=True` now return an async iterator of `RunOutputEvent` directly and is not a coroutine anymore.
  * `debug_mode: bool` added to `run`, `arun`, `print_response` and `aprint_response` to enable debug mode for a specific run.
  * `add_history_to_context` added to `run`, `arun`, `print_response` and `aprint_response` to add the chat history to the context for the current run.
  * `dependencies` added to `run`, `arun`, `print_response` and `aprint_response` to add dependencies to the context for the current run.
  * `metadata` added to `run`, `arun`, `print_response` and `aprint_response` to set the metadata for the current run. This is merged with the metadata set on the `Team` object.
  * Added `get_run_output` and `get_last_run_output` to `Agent` to retrieve a run output by ID.
</Accordion>

<Accordion title="Updates to Metrics">
  * Metrics have been simplified and cleaned up.
  * There are now 3 levels of metrics:
    * `Message.metrics` -> Metrics for each message (assistant, tool, etc.).
    * `RunOutput.metrics` -> Aggregated metrics for the whole run.
    * `AgentSession.metrics` -> Aggregated metrics for the whole session.
</Accordion>

<Accordion title="Updates to Knowledge">
  * `knowledge` is now an instance of `Knowledge` instead of `AgentKnowledge`.
  * `retriever` -> `knowledge_retriever` -> For a custom retriever.
  * `add_references` -> `add_knowledge_to_context` -> To enable traditional RAG.
</Accordion>

<Accordion title="Updates to Memory">
  * `add_memory_references` -> `add_memories_to_context`
  * You can set a custom `memory_manager` to use when creating memories.
  * Added `get_user_memories` to retrieve the user memories.
</Accordion>

<Accordion title="Updates to Sessions">
  * `add_session_summary_references` -> `add_session_summary_to_context`
  * You can set a custom `session_summary_manager` to use when creating session summaries.
  * Removed `session_name` and replace with functions `get_session_name` and `rename_session`.
  * Added `get_session` to retrieve a session by ID.
  * Added `get_chat_history` to retrieve the chat history from a session.
  * Added `get_session_metrics` to retrieve the metrics for a session.
  * Added `get_session_state` to retrieve the session state from a session.
  * Added `get_session_summary` to retrieve the session summary from a session.
  * Because `Agent` is now stateless, `agent_session`, `session_metrics`, `run_id`, `run_input`, `run_messages` and `run_response` as "sticky" agent attributes have been removed.
  * Because `Agent` is now stateless, `images`, `videos`, `audio` are no longer available as agent attributes. Instead these can be accessed on the `RunOutput` for a particular run.
  * Removed `team_session_state` and `workflow_session_state`. Only `session_state` is used.
  * Added `enable_agentic_state` to `Agent` and `Team` to allow the agent to update the session state with a tool call.
</Accordion>

<Accordion title="Updates to Team Class">
  * Removed `mode` from `Team`. Instead there are attributes that can be used to control the behavior of the team:
    * `respond_directly` -> If True, the team leader won't process responses from the members and instead will return them directly
    * `delegate_task_to_all_members` -> If True, the team leader will delegate tasks to all members simultaneously, instead of one by one. When running async (using `arun`) members will run concurrently.
    * `determine_input_for_members` -> `True` by default. Set to False if you want to send the run input directly to the member agents without the team leader synthesizing its own input.
  * `team_id` -> `id` -> If `id` is not set, it is autogenerated using the `name` of the team, or a random UUID if the `name` is not set.
  * `search_previous_sessions_history` -> `search_session_history`
  * `context` -> `dependencies`
  * `add_context` -> `add_dependencies_to_context`
  * `add_history_to_messages` -> `add_history_to_context`
  * `add_name_to_instructions` -> `add_name_to_context`
  * `add_datetime_to_instructions` -> `add_datetime_to_context`
  * `add_location_to_instructions` -> `add_location_to_context`
  * `add_member_tools_to_system_message` -> `add_member_tools_to_context`
  * `extra_data` -> `metadata`
  * Added `additional_input` (works the same as for `Agent`)
  * Added `store_member_responses: bool` to optionally store the member responses on the team run output object.
  * Added `acli_app` to `Team` to enable the CLI app for the team in async mode.
  * Added `send_media_to_model` -> `True` by default. Set to False if you don't want to send media (image, audio, video, files) to the model.  This is useful if you only want media for tools.
  * Added `store_media` -> `True` by default. Set to False if you don't want to store any media in the `RunOutput` that is persisted with sessions.
  * `num_history_responses` -> `num_history_runs`
  * Removed `success_criteria`
  * Removed `team_session_id` and `workflow_session_id`.
  * Removed `enable_team_history`
  * Removed `num_of_interactions_from_history`
  * Removed `show_tool_calls` -> This is now just always enabled.
  * Removed `enable_agentic_context`. `session_state` and `enable_agentic_state` should rather be used to manage state shared between the team and the members.
  * Removed all `monitoring` from `Team`. With the new AgentOS platform, monitoring is done using your own data. Go to [os.agno.com](https://os.agno.com) to get started.
</Accordion>

<Accordion title="Updates to Input & Output">
  * `response_model` -> `output_schema`
  * Added `input_schema` (a pydantic model) to validate the input to the agent.
  * Changed `message` to `input` (which also replaces `messages`). `input` can be of type `str`, `list`, `dict`, `Message`, `BaseModel`, or `list[Message]`.
  * If a `dict` and `input_schema` is provided, the dict will be validated against the schema.
  * If a `BaseModel` and `input_schema` is provided, the model will be validated against the schema.
  * `arun` with `stream=True` now return an async iterator of `TeamRunOutputEvent` directly and is not a coroutine anymore.
  * `debug_mode: bool` added to `run`, `arun`, `print_response` and `aprint_response` to enable debug mode for a specific run.
  * `add_history_to_context` added to `run`, `arun`, `print_response` and `aprint_response` to add the chat history to the context for the current run.
  * `dependencies` added to `run`, `arun`, `print_response` and `aprint_response` to add dependencies to the context for the current run.
  * `metadata` added to `run`, `arun`, `print_response` and `aprint_response` to set the metadata for the current run. This is merged with the metadata set on the `Team` object.
  * Added `get_run_output` and `get_last_run_output` to `Team` to retrieve a run output by ID.
</Accordion>

<Accordion title="Updates to Metrics">
  * Metrics have been simplified and cleaned up.
  * There are now 3 levels of metrics:
    * `Message.metrics` -> Metrics for each message (assistant, tool, etc.).
    * `RunOutput.metrics` -> Aggregated metrics for the whole run.
    * `TeamSession.metrics` -> Aggregated metrics for the whole session.
</Accordion>

<Accordion title="Updates to Knowledge">
  * `knowledge` is now an instance of `Knowledge` instead of `AgentKnowledge`.
  * `retriever` -> `knowledge_retriever` -> For a custom retriever.
  * `add_references` -> `add_knowledge_to_context` -> To enable traditional RAG.
  * Added `update_knowledge` tool to update the knowledge base. Works the same as for `Agent`.
</Accordion>

<Accordion title="Updates to Memory">
  * `add_memory_references` -> `add_memories_to_context`
  * You can set a custom `memory_manager` to use when creating memories.
  * Added `get_user_memories` to retrieve the user memories.
</Accordion>

<Accordion title="Updates to Sessions">
  * `add_session_summary_references` -> `add_session_summary_to_context`
  * You can set a custom `session_summary_manager` to use when creating session summaries.
  * Removed `session_name` and replace with functions `get_session_name` and `rename_session`.
  * Added `get_session` to retrieve a session by ID.
  * Added `get_chat_history` to retrieve the chat history from a session.
  * Added `get_session_metrics` to retrieve the metrics for a session.
  * Added `get_session_state` to retrieve the session state from a session.
  * Added `get_session_summary` to retrieve the session summary from a session.
  * Because `Team` is now stateless, `team_session`, `session_metrics`, `run_id`, `run_input`, `run_messages` and `run_response` as "sticky" team attributes have been removed.
  * Because `Team` is now stateless, `images`, `videos`, `audio` are no longer available as team attributes. Instead these can be accessed on the `TeamRunOutput` for a particular run.
  * Removed `team_session_state` and `workflow_session_state`. Only `session_state` is used.
  * Added `enable_agentic_state` to `Team` to allow the agent to update the session state with a tool call.
</Accordion>

<Accordion title="Updates to Workflow Class">
  * `workflow_id` -> `id` -> If `id` is not set, it is autogenerated using the `name` of the workflow, or a random UUID if the `name` is not set.
  * Workflows "v1" has been completely removed and replaced with `Workflows v2`. See full details in the [Workflows](/concepts/workflows) section.
  * This means the import locations for "Workflows v2" is now `agno.workflows`.
  * `extra_data` -> `metadata`
  * Added `store_events` to `Workflow` to optionally store the events on the workflow run output object. Also added `events_to_skip` to skip certain events from being stored. This works the same as for `Agent` and `Team`.
  * Added `store_executor_outputs` to `Workflow` to optionally store the agent/team responses on the workflow run output object.
  * Added `input_schema` to `Workflow` to validate the input to the workflow.
  * Added support for websocket streaming of the workflow. This is appropriate for long-running workflows that need to be streamed to a client. This is only available for `arun`.
  * Removed all `monitoring` from `Workflow`. With the new AgentOS platform, monitoring is done using your own data. Go to [os.agno.com](https://os.agno.com) to get started.
</Accordion>

<Accordion title="Updates to Input & Output">
  * Changed `message` to `input` (which also replaces `messages`). `input` can be of type `str`, `list`, `dict`, or `BaseModel`.
  * If a `dict` and `input_schema` is provided, the dict will be validated against the schema.
  * If a `BaseModel` and `input_schema` is provided, the model will be validated against the schema.
  * `arun` with `stream=True` now return an async iterator of `WorkflowRunOutputEvent` directly and is not a coroutine anymore.
  * `debug_mode: bool` added to `run`, `arun`, `print_response` and `aprint_response` to enable debug mode for a specific run.
  * Added `get_run_output` and `get_last_run_output` to `Workflow` to retrieve a run output by ID.
</Accordion>

<Accordion title="Updates to Sessions">
  * Removed `session_name` and replace with functions `get_session_name` and `rename_session`.
  * Because `Workflow` is now stateless, `workflow_session`, `session_metrics`, `run_id`, `run_input`, `run_messages` and `run_response` as "sticky" workflow attributes have been removed.
  * Because `Workflow` is now stateless, `images`, `videos`, `audio` are no longer available as workflow attributes. Instead these can be accessed on the `WorkflowRunOutput` for a particular run.
  * Added `get_session` to retrieve a session by ID.
  * Added `get_session_metrics` to retrieve the metrics for a session.
  * Added `get_session_state` to retrieve the session state from a session.
  * Added `get_session_summary` to retrieve the session summary from a session.
</Accordion>

---

## customer_support_team.print_response(

**URL:** llms-txt#customer_support_team.print_response(

---

## Generate Images

**URL:** llms-txt#generate-images

**Contents:**
- Code
- Usage

Source: https://docs.agno.com/examples/models/openai/chat/generate_images

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Set your API key">
    
  </Step>

<Step title="Install libraries">
    
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Set your API key">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Create workflow with loop containing parallel steps

**URL:** llms-txt#create-workflow-with-loop-containing-parallel-steps

workflow = Workflow(
    name="Advanced Research and Content Workflow",
    description="Research topics with parallel execution in a loop until conditions are met, then create content",
    steps=[
        Loop(
            name="Research Loop with Parallel Execution",
            steps=[
                Parallel(
                    research_hackernews_step,
                    research_web_step,
                    trend_analysis_step,
                    name="Parallel Research & Analysis",
                    description="Execute research and analysis in parallel for efficiency",
                ),
                sentiment_analysis_step,
            ],
            end_condition=research_evaluator,
            max_iterations=3,  # Maximum 3 iterations
        ),
        content_step,
    ],
)

if __name__ == "__main__":
    workflow.print_response(
        input="Research the latest trends in AI and machine learning, then create a summary",
        stream=True,
        stream_events=True,
    )
```

This was a synchronous streaming example of this pattern. To checkout async and non-streaming versions, see the cookbooks-

* [Loop with Parallel Steps Workflow (sync)](https://github.com/agno-agi/agno/tree/main/cookbook/workflows/_03_workflows_loop_execution/sync/loop_with_parallel_steps.py)
* [Loop with Parallel Steps Workflow (async non-streaming)](https://github.com/agno-agi/agno/blob/main/cookbook/workflows/_03_workflows_loop_execution/sync/loop_with_parallel_steps.py)
* [Loop with Parallel Steps Workflow (async streaming)](https://github.com/agno-agi/agno/tree/main/cookbook/workflows/_03_workflows_loop_execution/async/loop_with_parallel_steps_stream.py)

---

## Async Tools

**URL:** llms-txt#async-tools

**Contents:**
- Example

Source: https://docs.agno.com/concepts/tools/async-tools

Learn how to use async tools in Agno.

Agno Agents can execute multiple tools concurrently, allowing you to process function calls that the model makes efficiently. This is especially valuable when the functions involve time-consuming operations. It improves responsiveness and reduces overall execution time.

<Check>
  When you call `arun` or `aprint_response`, your tools will execute concurrently. If you provide synchronous functions as tools, they will execute concurrently on separate threads.
</Check>

1. Provide your Agent with a list of tools, preferably asynchronous for optimal performance. However, synchronous functions can also be used since they will execute concurrently on separate threads.
2. Run the Agent using either the `arun` or `aprint_response` method, enabling concurrent execution of tool calls.

<Note>
  Concurrent execution of tools requires a model that supports parallel function
  calling. For example, OpenAI models have a `parallel_tool_calls` parameter
  (enabled by default) that allows multiple tool calls to be requested and
  executed simultaneously.
</Note>

In this example, `gpt-5-mini` makes three simultaneous tool calls to `atask1`, `atask2` and `atask3`. Normally these tool calls would execute sequentially, but using the `aprint_response` function, they run concurrently, improving execution time.

<img height="200" src="https://mintcdn.com/agno-v2/Y7twezR0wF2re1xh/images/async-tools.png?fit=max&auto=format&n=Y7twezR0wF2re1xh&q=85&s=4ec6216f4c1dafa6c7a675bd345c6ad2" style={{ borderRadius: "8px" }} data-og-width="344" data-og-height="463" data-path="images/async-tools.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/agno-v2/Y7twezR0wF2re1xh/images/async-tools.png?w=280&fit=max&auto=format&n=Y7twezR0wF2re1xh&q=85&s=2ef332604d86c02722791a3beffa7589 280w, https://mintcdn.com/agno-v2/Y7twezR0wF2re1xh/images/async-tools.png?w=560&fit=max&auto=format&n=Y7twezR0wF2re1xh&q=85&s=948e641806ce6a2224074a78a5dd9521 560w, https://mintcdn.com/agno-v2/Y7twezR0wF2re1xh/images/async-tools.png?w=840&fit=max&auto=format&n=Y7twezR0wF2re1xh&q=85&s=9bbc9234b871fdec04c5229f69637c4a 840w, https://mintcdn.com/agno-v2/Y7twezR0wF2re1xh/images/async-tools.png?w=1100&fit=max&auto=format&n=Y7twezR0wF2re1xh&q=85&s=7a8dcfeb62df06475d15981339375437 1100w, https://mintcdn.com/agno-v2/Y7twezR0wF2re1xh/images/async-tools.png?w=1650&fit=max&auto=format&n=Y7twezR0wF2re1xh&q=85&s=7078423de7061f62251c3f6209dbb38e 1650w, https://mintcdn.com/agno-v2/Y7twezR0wF2re1xh/images/async-tools.png?w=2500&fit=max&auto=format&n=Y7twezR0wF2re1xh&q=85&s=512ee4e5ccf4e4bfa33edac026b3a472 2500w" />

**Examples:**

Example 1 (unknown):
```unknown
Run the Agent:
```

---

## To get the response in a variable:

**URL:** llms-txt#to-get-the-response-in-a-variable:

---

## Neo4j Tools

**URL:** llms-txt#neo4j-tools

**Contents:**
- Code
- Usage

Source: https://docs.agno.com/examples/concepts/tools/database/neo4j

Neo4jTools enables agents to interact with Neo4j graph databases for querying and managing graph data.

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Set your credentials">
    
  </Step>

<Step title="Install libraries">
    
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Set your credentials">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Cerebras Llama with Reasoning Tools

**URL:** llms-txt#cerebras-llama-with-reasoning-tools

**Contents:**
- Code

Source: https://docs.agno.com/examples/concepts/reasoning/tools/cerebras-llama-reasoning-tools

```python cookbook/reasoning/tools/cerebras_llama_reasoning_tools.py theme={null}
from textwrap import dedent

from agno.agent import Agent
from agno.models.cerebras import Cerebras
from agno.tools.reasoning import ReasoningTools

reasoning_agent = Agent(
    model=Cerebras(id="llama-3.3-70b"),
    tools=[ReasoningTools(add_instructions=True)],
    instructions=dedent("""\
        You are an expert problem-solving assistant with strong analytical skills! 🧠

Your approach to problems:
        1. First, break down complex questions into component parts
        2. Clearly state your assumptions
        3. Develop a structured reasoning path
        4. Consider multiple perspectives
        5. Evaluate evidence and counter-arguments
        6. Draw well-justified conclusions

When solving problems:
        - Use explicit step-by-step reasoning
        - Identify key variables and constraints
        - Explore alternative scenarios
        - Highlight areas of uncertainty
        - Explain your thought process clearly
        - Consider both short and long-term implications
        - Evaluate trade-offs explicitly

For quantitative problems:
        - Show your calculations
        - Explain the significance of numbers
        - Consider confidence intervals when appropriate
        - Identify source data reliability

For qualitative reasoning:
        - Assess how different factors interact
        - Consider psychological and social dynamics
        - Evaluate practical constraints
        - Address value considerations
        \
    """),
    add_datetime_to_context=True,
    stream_events=True,
    markdown=True,
)

---

## Huggingface Embedder

**URL:** llms-txt#huggingface-embedder

**Contents:**
- Code

Source: https://docs.agno.com/examples/concepts/knowledge/embedders/huggingface-embedder

```python  theme={null}
from agno.knowledge.embedder.huggingface import HuggingfaceCustomEmbedder
from agno.knowledge.knowledge import Knowledge
from agno.vectordb.pgvector import PgVector

embeddings = HuggingfaceCustomEmbedder().get_embedding(
    "The quick brown fox jumps over the lazy dog."
)

---

## Open the file once in append-binary mode

**URL:** llms-txt#open-the-file-once-in-append-binary-mode

**Contents:**
- Usage

with wave.open(str(filename), "wb") as wav_file:
    wav_file.setnchannels(CHANNELS)
    wav_file.setsampwidth(SAMPLE_WIDTH)
    wav_file.setframerate(SAMPLE_RATE)

# Iterate over generated audio
    for response in output_stream:
        response_audio = response.response_audio  # type: ignore
        if response_audio:
            if response_audio.transcript:
                print(response_audio.transcript, end="", flush=True)
            if response_audio.content:
                try:
                    pcm_bytes = base64.b64decode(response_audio.content)
                    wav_file.writeframes(pcm_bytes)
                except Exception as e:
                    print(f"Error decoding audio: {e}")
print()
bash  theme={null}
    export OPENAI_API_KEY=xxx
    bash  theme={null}
    pip install -U openai agno
    bash Mac theme={null}
      python cookbook/agent_concepts/multimodal/audio_streaming.py
      bash Windows theme={null}
      python cookbook/agent_concepts/multimodal/audio_streaming.py
      ```
    </CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Set your API key">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## "product development. They want to maximize growth and user acquisition within 12 months. "

**URL:** llms-txt#"product-development.-they-want-to-maximize-growth-and-user-acquisition-within-12-months.-"

---

## Email

**URL:** llms-txt#email

**Contents:**
- Example
- Toolkit Params
- Toolkit Functions
- Developer Resources

Source: https://docs.agno.com/concepts/tools/toolkits/social/email

**EmailTools** enable an Agent to send an email to a user. The Agent can send an email to a user with a specific subject and body.

| Parameter           | Type            | Default | Description                         |
| ------------------- | --------------- | ------- | ----------------------------------- |
| `receiver_email`    | `Optional[str]` | `None`  | The email address of the receiver.  |
| `sender_name`       | `Optional[str]` | `None`  | The name of the sender.             |
| `sender_email`      | `Optional[str]` | `None`  | The email address of the sender.    |
| `sender_passkey`    | `Optional[str]` | `None`  | The passkey for the sender's email. |
| `enable_email_user` | `bool`          | `True`  | Enable the email\_user function.    |
| `all`               | `bool`          | `False` | Enable all available functions.     |

| Function     | Description                                                                                                                                                                                                                       |
| ------------ | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| `email_user` | Emails the user with the given subject and body. Parameters include `subject` (str) for the email subject and `body` (str) for the email content. Currently works with Gmail. Returns "email sent successfully" or error message. |

## Developer Resources

* View [Tools](https://github.com/agno-agi/agno/blob/main/libs/agno/agno/tools/email.py)
* View [Cookbook](https://github.com/agno-agi/agno/tree/main/cookbook/tools/email_tools.py)

---

## ScrapeGraph

**URL:** llms-txt#scrapegraph

**Contents:**
- Prerequisites
- Example
  - Raw HTML Scraping

Source: https://docs.agno.com/concepts/tools/toolkits/web_scrape/scrapegraph

ScrapeGraphTools enable an Agent to extract structured data from webpages, convert content to markdown, and retrieve raw HTML content.

**ScrapeGraphTools** enable an Agent to extract structured data from webpages, convert content to markdown, and retrieve raw HTML content using the ScrapeGraphAI API.

The toolkit provides 5 core capabilities:

1. **smartscraper**: Extract structured data using natural language prompts
2. **markdownify**: Convert web pages to markdown format
3. **searchscraper**: Search the web and extract information
4. **crawl**: Crawl websites with structured data extraction
5. **scrape**: Get raw HTML content from websites *(NEW!)*

The scrape method is particularly useful when you need:

* Complete HTML source code
* Raw content for further processing
* HTML structure analysis
* Content that needs to be parsed differently

All methods support heavy JavaScript rendering when needed.

The following examples require the `scrapegraph-py` library.

Optionally, if your ScrapeGraph configuration or specific models require an API key, set the `SGAI_API_KEY` environment variable:

The following agent will extract structured data from a website using the smartscraper tool:

### Raw HTML Scraping

Get complete HTML content from websites for custom processing:

```python cookbook/tools/scrapegraph_tools.py theme={null}

**Examples:**

Example 1 (unknown):
```unknown
Optionally, if your ScrapeGraph configuration or specific models require an API key, set the `SGAI_API_KEY` environment variable:
```

Example 2 (unknown):
```unknown
## Example

The following agent will extract structured data from a website using the smartscraper tool:
```

Example 3 (unknown):
```unknown
### Raw HTML Scraping

Get complete HTML content from websites for custom processing:
```

---

## AI/ML API

**URL:** llms-txt#ai/ml-api

**Contents:**
- Parameters

Source: https://docs.agno.com/reference/models/aimlapi

The **AI/ML API** provider gives unified access to over **300+ AI models**, including **Deepseek**, **Gemini**, **ChatGPT**, and others, via a single standardized interface.

The models run with **enterprise-grade rate limits and uptime**, and are ideal for production use.

You can sign up at [aimlapi.com](https://aimlapi.com/?utm_source=agno\&utm_medium=integration\&utm_campaign=aimlapi) and view full provider documentation at [docs.aimlapi.com](https://docs.aimlapi.com/?utm_source=agno\&utm_medium=github\&utm_campaign=integration).

| Parameter    | Type            | Default                        | Description                                                       |
| ------------ | --------------- | ------------------------------ | ----------------------------------------------------------------- |
| `id`         | `str`           | `"gpt-4o-mini"`                | The id of the model to use                                        |
| `name`       | `str`           | `"AIMLAPI"`                    | The name of the model                                             |
| `provider`   | `str`           | `"AIMLAPI"`                    | The provider of the model                                         |
| `api_key`    | `Optional[str]` | `None`                         | The API key for AI/ML API (defaults to AIMLAPI\_API\_KEY env var) |
| `base_url`   | `str`           | `"https://api.aimlapi.com/v1"` | The base URL for the AI/ML API                                    |
| `max_tokens` | `int`           | `4096`                         | Maximum number of tokens to generate                              |

AIMLAPI extends the OpenAI-compatible interface and supports most parameters from OpenAI.

---

## --- Main Execution Function ---

**URL:** llms-txt#----main-execution-function----

async def blog_generation_execution(
    session_state,
    topic: str = None,
    use_search_cache: bool = True,
    use_scrape_cache: bool = True,
    use_blog_cache: bool = True,
) -> str:
    """
    Blog post generation workflow execution function.

Args:
        session_state: The shared session state
        topic: Blog post topic (if not provided, uses execution_input.input)
        use_search_cache: Whether to use cached search results
        use_scrape_cache: Whether to use cached scraped articles
        use_blog_cache: Whether to use cached blog posts
    """

if not blog_topic:
        return "❌ No blog topic provided. Please specify a topic."

print(f"🎨 Generating blog post about: {blog_topic}")
    print("=" * 60)

# Check for cached blog post first
    if use_blog_cache:
        cached_blog = get_cached_blog_post(session_state, blog_topic)
        if cached_blog:
            print("📋 Found cached blog post!")
            return cached_blog

# Phase 1: Research and gather sources
    print("\n🔍 PHASE 1: RESEARCH & SOURCE GATHERING")
    print("=" * 50)

search_results = await get_search_results(
        session_state, blog_topic, use_search_cache
    )

if not search_results or len(search_results.articles) == 0:
        return f"❌ Sorry, could not find any articles on the topic: {blog_topic}"

print(f"📊 Found {len(search_results.articles)} relevant sources:")
    for i, article in enumerate(search_results.articles, 1):
        print(f"   {i}. {article.title[:60]}...")

# Phase 2: Content extraction
    print("\n📄 PHASE 2: CONTENT EXTRACTION")
    print("=" * 50)

scraped_articles = await scrape_articles(
        session_state, blog_topic, search_results, use_scrape_cache
    )

if not scraped_articles:
        return f"❌ Could not extract content from any articles for topic: {blog_topic}"

print(f"📖 Successfully extracted content from {len(scraped_articles)} articles")

# Phase 3: Blog post writing
    print("\n✍️ PHASE 3: BLOG POST CREATION")
    print("=" * 50)

# Prepare input for the writer
    writer_input = {
        "topic": blog_topic,
        "articles": [article.model_dump() for article in scraped_articles.values()],
    }

print("🤖 AI is crafting your blog post...")
    writer_response = await blog_writer_agent.arun(json.dumps(writer_input, indent=2))

if not writer_response or not writer_response.content:
        return f"❌ Failed to generate blog post for topic: {blog_topic}"

blog_post = writer_response.content

# Cache the blog post
    cache_blog_post(session_state, blog_topic, blog_post)

print("✅ Blog post generated successfully!")
    print(f"📝 Length: {len(blog_post)} characters")
    print(f"📚 Sources: {len(scraped_articles)} articles")

---

## Create and post a tweet

**URL:** llms-txt#create-and-post-a-tweet

agent.print_response("Create a post about AI ethics", markdown=True)

---

## Have a conversation

**URL:** llms-txt#have-a-conversation

expensive_agent.print_response(
    "Explain quantum computing basics", session_id=session_id, user_id=user_id
)
expensive_agent.print_response(
    "What are the main applications?", session_id=session_id, user_id=user_id
)

---

## S3 Content

**URL:** llms-txt#s3-content

Source: https://docs.agno.com/reference/knowledge/remote-content/s3-content

S3Content is a class that allows you to add content from a S3 bucket to the knowledge base.

<Snippet file="s3-remote-content-params.mdx" />

---

## === MULTI-STEP CONDITION STEPS ===

**URL:** llms-txt#===-multi-step-condition-steps-===

deep_exa_analysis_step = Step(
    name="DeepExaAnalysis",
    description="Conduct deep analysis using Exa search capabilities",
    agent=exa_agent,
)

trend_analysis_step = Step(
    name="TrendAnalysis",
    description="Analyze trends and patterns from the research data",
    agent=trend_analyzer_agent,
)

fact_verification_step = Step(
    name="FactVerification",
    description="Verify facts and cross-reference information",
    agent=fact_checker_agent,
)

---

## === FINAL STEPS ===

**URL:** llms-txt#===-final-steps-===

write_step = Step(
    name="WriteContent",
    description="Write the final content based on research",
    agent=content_agent,
)

---

## CSV Reader

**URL:** llms-txt#csv-reader

Source: https://docs.agno.com/reference/knowledge/reader/csv

CSVReader is a reader class that allows you to read data from CSV files.

<Snippet file="csv-reader-reference.mdx" />

---

## Print results

**URL:** llms-txt#print-results

**Contents:**
- Usage

print("\n--- Generated Shorts ---")
for short in shorts:
    print(f"Short at {short['path']}")
    print(f"Description: {short['description']}")
    print(f"Engagement Score: {short['score']}/10\n")
bash  theme={null}
    export GOOGLE_API_KEY=xxx
    bash  theme={null}
    pip install -U opencv-python google-generativeai sqlalchemy ffmpeg-python agno
    bash Mac theme={null}
      brew install ffmpeg
      bash Windows theme={null}
      # Install ffmpeg using chocolatey or download from https://ffmpeg.org/download.html
      choco install ffmpeg
      bash Mac theme={null}
      python cookbook/agent_concepts/multimodal/video_to_shorts.py
      bash Windows theme={null}
      python cookbook/agent_concepts/multimodal/video_to_shorts.py
      ```
    </CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Set your API key">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Install ffmpeg">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## BAAI/bge-reranker-large

**URL:** llms-txt#baai/bge-reranker-large

---

## Get the most recent income statement for Apple

**URL:** llms-txt#get-the-most-recent-income-statement-for-apple

**Contents:**
- Toolkit Params
- Toolkit Functions
- Rate Limits and Usage
- Developer Resources

agent.print_response("Get the most recent income statement for AAPL and highlight key metrics")
```

For more examples, see the [Financial Datasets Examples](/examples/concepts/tools/others/financial_datasets).

\| Parameter | Type            | Default | Description                                                                             |
\| --------- | --------------- | ------- | --------------------------------------------------------------------------------------- | --- |
\| `api_key` | `Optional[str]` | `None`  | Optional API key. If not provided, uses FINANCIAL\_DATASETS\_API\_KEY environment variable |     |

| Function                      | Description                                                                                                     |
| ----------------------------- | --------------------------------------------------------------------------------------------------------------- |
| `get_income_statements`       | Get income statements for a company with options for annual, quarterly, or trailing twelve months (ttm) periods |
| `get_balance_sheets`          | Get balance sheets for a company with period options                                                            |
| `get_cash_flow_statements`    | Get cash flow statements for a company                                                                          |
| `get_company_info`            | Get company information including business description, sector, and industry                                    |
| `get_crypto_prices`           | Get cryptocurrency prices with configurable time intervals                                                      |
| `get_earnings`                | Get earnings reports with EPS estimates, actuals, and revenue data                                              |
| `get_financial_metrics`       | Get key financial metrics and ratios for a company                                                              |
| `get_insider_trades`          | Get data on insider buying and selling activity                                                                 |
| `get_institutional_ownership` | Get information about institutional investors and their positions                                               |
| `get_news`                    | Get market news, optionally filtered by company                                                                 |
| `get_stock_prices`            | Get historical stock prices with configurable time intervals                                                    |
| `search_tickers`              | Search for stock tickers based on a query string                                                                |
| `get_sec_filings`             | Get SEC filings with optional filtering by form type (10-K, 10-Q, etc.)                                         |
| `get_segmented_financials`    | Get segmented financial data by product category and geographic region                                          |

You can use `include_tools` or `exclude_tools` to modify the list of tools the agent has access to. Learn more about [selecting tools](/concepts/tools/selecting-tools).

## Rate Limits and Usage

The Financial Datasets API may have usage limits based on your subscription tier. Please refer to their documentation for specific rate limit information.

## Developer Resources

* View [Tools](https://github.com/agno-agi/agno/blob/main/libs/agno/agno/tools/financial_datasets.py)
* View [Cookbook](https://github.com/agno-agi/agno/tree/main/cookbook/tools/financial_datasets_tools.py)

---

## Create a reusable content creation sequence

**URL:** llms-txt#create-a-reusable-content-creation-sequence

article_creation_sequence = Steps(
    name="ArticleCreation",
    description="Complete article creation workflow from research to final edit",
    steps=[
        Step(name="research", agent=researcher),
        Step(name="writing", agent=writer),
        Step(name="editing", agent=editor),
    ],
)

---

## Complex workflow combining multiple patterns

**URL:** llms-txt#complex-workflow-combining-multiple-patterns

workflow = Workflow(
    name="Advanced Multi-Pattern Workflow",
    steps=[
        Parallel(
            Condition(
                name="Tech Check",
                evaluator=is_tech_topic,
                steps=[Step(name="Tech Research", agent=tech_researcher)]
            ),
            Condition(
                name="Business Check",
                evaluator=is_business_topic,
                steps=[
                    Loop(
                        name="Deep Business Research",
                        steps=[Step(name="Market Research", agent=market_researcher)],
                        end_condition=research_quality_check,
                        max_iterations=3
                    )
                ]
            ),
            name="Conditional Research Phase"
        ),
        Step(
            name="Research Post-Processing",
            executor=research_post_processor,
            description="Consolidate and analyze research findings with quality metrics"
        ),
        Router(
            name="Content Type Router",
            selector=content_type_selector,
            choices=[blog_post_step, social_media_step, report_step]
        ),
        Step(name="Final Review", agent=reviewer),
    ]
)

workflow.print_response("Create a comprehensive analysis of sustainable technology trends and their business impact for 2024", markdown=True)
```

* [Condition and Parallel Steps (Streaming Example)](/examples/concepts/workflows/02-workflows-conditional-execution/condition_and_parallel_steps_stream)
* [Loop with Parallel Steps (Streaming Example)](/examples/concepts/workflows/03_workflows_loop_execution/loop_with_parallel_steps_stream)
* [Router with Loop Steps](/examples/concepts/workflows/05_workflows_conditional_branching/router_with_loop_steps)

---

## Email Tools

**URL:** llms-txt#email-tools

**Contents:**
- Code
- Usage

Source: https://docs.agno.com/examples/concepts/tools/social/email

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Set your API key">
    
  </Step>

<Step title="Set your email credentials">
    
  </Step>

<Step title="Install libraries">
    
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Set your API key">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Set your email credentials">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 4 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

---

## Cassandra

**URL:** llms-txt#cassandra

Source: https://docs.agno.com/reference/vector_db/cassandra

<Snippet file="vector-db-cassandra-reference.mdx" />

---

## Reliability with Single Tool

**URL:** llms-txt#reliability-with-single-tool

**Contents:**
- Code

Source: https://docs.agno.com/examples/concepts/evals/reliability/basic

Evaluation to assert an Agent is making the expected tool calls.

---

## Example 3: Get the message history of a specific channel by channel ID

**URL:** llms-txt#example-3:-get-the-message-history-of-a-specific-channel-by-channel-id

**Contents:**
- Toolkit Params
- Toolkit Functions
- Developer Resources

agent.print_response("Get the last 10 messages from the channel 1231241", markdown=True)

| Parameter                    | Type   | Default | Description                                                     |
| ---------------------------- | ------ | ------- | --------------------------------------------------------------- |
| `token`                      | `str`  | `None`  | Slack API token for authentication                              |
| `enable_send_message`        | `bool` | `True`  | Enables functionality to send messages to Slack channels        |
| `enable_send_message_thread` | `bool` | `True`  | Enables functionality to send threaded messages                 |
| `enable_list_channels`       | `bool` | `True`  | Enables functionality to list available Slack channels          |
| `enable_get_channel_history` | `bool` | `True`  | Enables functionality to retrieve message history from channels |
| `all`                        | `bool` | `False` | Enables all functionality when set to True                      |

| Function              | Description                                         |
| --------------------- | --------------------------------------------------- |
| `send_message`        | Sends a message to a specified Slack channel        |
| `list_channels`       | Lists all available channels in the Slack workspace |
| `get_channel_history` | Retrieves message history from a specified channel  |

## Developer Resources

* View [Tools](https://github.com/agno-agi/agno/blob/main/libs/agno/agno/tools/slack.py)
* View [Cookbook](https://github.com/agno-agi/agno/tree/main/cookbook/tools/slack_tools.py)

---

## Pipedream LinkedIn

**URL:** llms-txt#pipedream-linkedin

**Contents:**
- Code

Source: https://docs.agno.com/examples/concepts/tools/mcp/pipedream_linkedin

This example shows how to use the LinkedIn Pipedream MCP server with Agno Agents.

---

## Every use of the logging function in agno.utils.log will now use our custom logger.

**URL:** llms-txt#every-use-of-the-logging-function-in-agno.utils.log-will-now-use-our-custom-logger.

log_info("This is using our custom logger!")

---

## Response includes source information

**URL:** llms-txt#response-includes-source-information

**Contents:**
- Knowledge Base Architecture
- Benefits of Knowledge-Powered Agents
  - Accuracy and Reliability
  - Scalability and Maintenance
  - Context Awareness
- Getting Started with Knowledge Bases
- Learn More

"Based on section 3.2 of our Return Policy document,
items can be returned within 30 days of purchase..."
python Basic Setup theme={null}
  from agno.knowledge.knowledge import Knowledge
  from agno.vectordb.lancedb import LanceDb

# Create a knowledge base
  knowledge = Knowledge(
      vector_db=LanceDb(table_name="my_knowledge")
  )

# Add your content
  knowledge.add_content(path="documents/")
  python With Custom Configuration theme={null}
  from agno.knowledge.knowledge import Knowledge
  from agno.vectordb.pgvector import PgVector
  from agno.embedder.openai import OpenAIEmbedder

# Customized knowledge base
  knowledge = Knowledge(
      vector_db=PgVector(
          table_name="company_knowledge",
          embedder=OpenAIEmbedder(model="text-embedding-3-large")
      )
  )
  ```
</CodeGroup>

<CardGroup cols={2}>
  <Card title="Content Types" icon="file-lines" href="/concepts/knowledge/content_types">
    Explore different ways to add information to your knowledge base
  </Card>

<Card title="Search & Retrieval" icon="magnifying-glass" href="/concepts/knowledge/core-concepts/search-retrieval">
    Learn how agents search and find relevant information
  </Card>

<Card title="Vector Databases" icon="database" href="/concepts/vectordb/overview">
    Choose the right storage solution for your knowledge base
  </Card>

<Card title="Performance Tips" icon="gauge" href="/concepts/knowledge/advanced/performance-tips">
    Optimize your knowledge base for speed and accuracy
  </Card>
</CardGroup>

**Examples:**

Example 1 (unknown):
```unknown
## Knowledge Base Architecture

Here's how the different pieces work together:

<Steps>
  <Step title="Content Ingestion">
    Raw content is processed through **readers** that understand different file formats (PDF, websites, databases, etc.) and extract meaningful text.
  </Step>

  <Step title="Intelligent Chunking">
    Large documents are broken down into smaller, meaningful pieces using **chunking strategies** that preserve context while enabling precise retrieval.
  </Step>

  <Step title="Embedding Generation">
    Each chunk is converted into a vector embedding that captures its semantic meaning using **embedders** powered by language models.
  </Step>

  <Step title="Vector Storage">
    Embeddings are stored in **vector databases** optimized for similarity search, often with support for hybrid search combining semantic and keyword matching.
  </Step>

  <Step title="Intelligent Retrieval">
    When agents need information, they generate search queries, find similar embeddings, and retrieve the most relevant content chunks.
  </Step>
</Steps>

## Benefits of Knowledge-Powered Agents

### Accuracy and Reliability

* Responses are grounded in your specific information, not generic training data
* Reduced hallucinations because agents reference actual sources
* Up-to-date information that reflects your current state

### Scalability and Maintenance

* Add new information without retraining or modifying code
* Handle unlimited amounts of information without performance degradation
* Easy updates by simply adding new content to the knowledge base

### Context Awareness

* Agents understand your specific domain, terminology, and processes
* Responses are tailored to your organization's context and needs
* Consistent information across all agent interactions

## Getting Started with Knowledge Bases

Ready to build your own knowledge base? The process is straightforward:

<CodeGroup>
```

Example 2 (unknown):
```unknown

```

---

## Nebius

**URL:** llms-txt#nebius

**Contents:**
- Parameters

Source: https://docs.agno.com/reference/models/nebius

The Nebius model provides access to Nebius's text and image models.

| Parameter  | Type            | Default                                    | Description                                                   |
| ---------- | --------------- | ------------------------------------------ | ------------------------------------------------------------- |
| `id`       | `str`           | `"meta-llama/Meta-Llama-3.1-70B-Instruct"` | The id of the Nebius model to use                             |
| `name`     | `str`           | `"Nebius"`                                 | The name of the model                                         |
| `provider` | `str`           | `"Nebius"`                                 | The provider of the model                                     |
| `api_key`  | `Optional[str]` | `None`                                     | The API key for Nebius (defaults to NEBIUS\_API\_KEY env var) |
| `base_url` | `str`           | `"https://api.studio.nebius.ai/v1"`        | The base URL for the Nebius API                               |

Nebius extends the OpenAI-compatible interface and supports most parameters from OpenAI.

---

## "I'd like to start with a soup, then im thinking a thai curry for the main course and finish with a dessert",

**URL:** llms-txt#"i'd-like-to-start-with-a-soup,-then-im-thinking-a-thai-curry-for-the-main-course-and-finish-with-a-dessert",

---

## Sequential Workflows

**URL:** llms-txt#sequential-workflows

Source: https://docs.agno.com/concepts/workflows/workflow-patterns/sequential

Linear, deterministic processes where each step depends on the output of the previous step.

Sequential workflows ensure predictable execution order and clear data flow between steps.

**Example Flow**: Research → Data Processing → Content Creation → Final Review

Sequential workflows ensure predictable execution order and clear data flow between steps.

<Note>
  For more information on how to use custom functions, refer to the
  [Workflow with custom function step](/concepts/workflows/workflow-patterns/custom-function-step-workflow) page.
</Note>

* [Sequence of Functions and Agents](/examples/concepts/workflows/01-basic-workflows/sequence_of_functions_and_agents) - Complete workflow with functions and agents

<Note>
  `StepInput` and `StepOutput` provides standardized interfaces for data flow between steps:
  So if you make a custom function as an executor for a step, make sure that the input and output types are compatible with the `StepInput` and `StepOutput` interfaces.
  This will ensure that your custom function can seamlessly integrate into the workflow system.

Take a look at the schemas for [`StepInput`](/reference/workflows/step_input) and [`StepOutput`](/reference/workflows/step_output).
</Note>

---

## InternLM

**URL:** llms-txt#internlm

**Contents:**
- Parameters

Source: https://docs.agno.com/reference/models/internlm

The InternLM model provides access to the InternLM model.

| Parameter  | Type            | Default                                                | Description                                                       |
| ---------- | --------------- | ------------------------------------------------------ | ----------------------------------------------------------------- |
| `id`       | `str`           | `"internlm/internlm2_5-7b-chat"`                       | The id of the InternLM model to use                               |
| `name`     | `str`           | `"InternLM"`                                           | The name of the model                                             |
| `provider` | `str`           | `"InternLM"`                                           | The provider of the model                                         |
| `api_key`  | `Optional[str]` | `None`                                                 | The API key for InternLM (defaults to INTERNLM\_API\_KEY env var) |
| `base_url` | `str`           | `"https://internlm-chat.intern-ai.org.cn/puyu/api/v1"` | The base URL for the InternLM API                                 |

InternLM extends the OpenAI-compatible interface and supports most parameters from OpenAI.

---

## Or

**URL:** llms-txt#or

---

## OpenTelemetry

**URL:** llms-txt#opentelemetry

**Contents:**
- Key Benefits
- OpenTelemetry Support
- Developer Resources

Source: https://docs.agno.com/integrations/observability/overview

Agno supports observability through OpenTelemetry, integrating seamlessly with popular tracing and monitoring platforms.

Observability helps us understand, debug, and improve AI agents. Agno supports observability through [OpenTelemetry](https://opentelemetry.io/), integrating seamlessly with popular tracing and monitoring platforms.

* **Trace**: Visualize and analyze agent execution flows.
* **Monitor**: Track performance, errors, and usage.
* **Debug**: Quickly identify and resolve issues.

## OpenTelemetry Support

Agno offers first-class support for OpenTelemetry, the industry standard for distributed tracing and observability.

* **Auto-Instrumentation**: Automatically instrument your agents and tools.
* **Flexible Export**: Send traces to any OpenTelemetry-compatible backend.
* **Custom Tracing**: Extend or customize tracing as needed.

<Note>
  OpenTelemetry-compatible backends including Arize Phoenix, Langfuse, Langsmith, Langtrace, Logfire, Maxim and Weave are supported by Agno out of the box.
</Note>

## Developer Resources

* [Cookbooks](https://github.com/agno-agi/agno/tree/main/cookbook/integrations/observability)

---

## Define the expected output structure

**URL:** llms-txt#define-the-expected-output-structure

**Contents:**
  - 3e. Define Analysis Principles

report_format = dedent("""
    REPORT FORMAT:

### Executive Dashboard
    - **Brand Health Score**: [1-10] with supporting evidence
    - **Net Sentiment**: [%positive - %negative] with trend analysis
    - **Key Drivers**: Top 3 positive and negative factors
    - **Alert Level**: Normal/Monitor/Crisis with threshold reasoning

### Quantitative Metrics
    | Sentiment | Posts | % | Avg Engagement | Influence Score |
    |-----------|-------|---|----------------|-----------------|
    [Detailed breakdown with engagement weighting]

### Strategic Recommendations
    **IMMEDIATE (≤48h)**: Crisis response, high-impact replies
    **SHORT-TERM (1-2 weeks)**: Content strategy, community engagement
    **LONG-TERM (1-3 months)**: Product positioning, market strategy
""")

print("Report format defined")
python  theme={null}

**Examples:**

Example 1 (unknown):
```unknown
### 3e. Define Analysis Principles
```

---

## Configure chunking strategy with a reader

**URL:** llms-txt#configure-chunking-strategy-with-a-reader

reader = PDFReader(
    chunking_strategy=SemanticChunking(similarity_threshold=0.7)
)

---

## SiliconFlow

**URL:** llms-txt#siliconflow

**Contents:**
- Authentication
- Example
- Params

Source: https://docs.agno.com/concepts/models/siliconflow

Learn how to use Siliconflow models in Agno.

Siliconflow is a platform for providing endpoints for Large Language models.

Explore Siliconflow’s models [here](https://siliconflow.ai/models).

Set your `SILICONFLOW_API_KEY` environment variable. Get your key [from Siliconflow here](https://siliconflow.ai).

Use `Siliconflow` with your `Agent`:

<CodeGroup>
  
</CodeGroup>

<Note> View more examples [here](/examples/models/siliconflow/basic). </Note>

| Parameter  | Type            | Default                                   | Description                                                             |
| ---------- | --------------- | ----------------------------------------- | ----------------------------------------------------------------------- |
| `id`       | `str`           | `"meta-llama/Meta-Llama-3.1-8B-Instruct"` | The id of the SiliconFlow model to use                                  |
| `name`     | `str`           | `"SiliconFlow"`                           | The name of the model                                                   |
| `provider` | `str`           | `"SiliconFlow"`                           | The provider of the model                                               |
| `api_key`  | `Optional[str]` | `None`                                    | The API key for SiliconFlow (defaults to SILICONFLOW\_API\_KEY env var) |
| `base_url` | `str`           | `"https://api.siliconflow.cn/v1"`         | The base URL for the SiliconFlow API                                    |

`Siliconflow` also supports the params of [OpenAI](/reference/models/openai).

**Examples:**

Example 1 (unknown):
```unknown

```

Example 2 (unknown):
```unknown
</CodeGroup>

## Example

Use `Siliconflow` with your `Agent`:

<CodeGroup>
```

---

## asyncio.run(test_async())

**URL:** llms-txt#asyncio.run(test_async())

**Contents:**
- Usage

bash  theme={null}
    pip install agno openai
    bash  theme={null}
    export OPENAI_API_KEY=****
    bash  theme={null}
    python cookbook/examples/teams/dependencies/add_dependencies_to_context.py
    ```
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Install required libraries">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Set environment variables">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run the agent">
```

---

## Validate your filters to catch typos

**URL:** llms-txt#validate-your-filters-to-catch-typos

**Contents:**
  - 4. Match Chunking Strategy to Your Content

valid_filters, invalid_keys = knowledge.validate_filters({
    "department": "engineering",
    "invalid_key": "value"  # This gets flagged
})
python  theme={null}
from agno.knowledge.chunking.fixed import FixedSizeChunking
from agno.knowledge.chunking.semantic import SemanticChunking

**Examples:**

Example 1 (unknown):
```unknown
### 4. Match Chunking Strategy to Your Content

Different strategies have different performance characteristics:

| Strategy       | Speed  | Quality | Best For                            |
| -------------- | ------ | ------- | ----------------------------------- |
| **Fixed Size** | Fast   | Good    | Uniform content, when speed matters |
| **Semantic**   | Slower | Best    | Complex docs, when quality matters  |
| **Recursive**  | Fast   | Good    | Structured docs, good balance       |
```

---

## Example 2: Balance Sheet Analysis

**URL:** llms-txt#example-2:-balance-sheet-analysis

**Contents:**
- Usage

print("\n=== Balance Sheet Analysis Example ===")
agent.print_response(
    "Analyze the balance sheets for MSFT over the last 3 years. Focus on debt-to-equity ratio and cash position.",
    stream=True,
)

bash  theme={null}
    export FINANCIAL_DATASETS_API_KEY=xxx
    bash  theme={null}
    pip install -U agno
    bash Mac theme={null}
        python cookbook/tools/financial_datasets_tools.py
      bash Windows theme={null}
        python cookbook/tools/financial_datasets_tools.py
      ```
    </CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Set your API credentials">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## pprint(json_mode_response.content)

**URL:** llms-txt#pprint(json_mode_response.content)

---

## Load an example large system message from S3. A large prompt like this would benefit from caching.

**URL:** llms-txt#load-an-example-large-system-message-from-s3.-a-large-prompt-like-this-would-benefit-from-caching.

txt_path = Path(__file__).parent.joinpath("system_prompt.txt")
download_file(
    "https://agno-public.s3.amazonaws.com/prompts/system_promt.txt",
    str(txt_path),
)
system_message = txt_path.read_text()

agent = Agent(
    model=Claude(
        id="claude-sonnet-4-20250514",
        cache_system_prompt=True,  # Activate prompt caching for Anthropic to cache the system prompt
    ),
    system_message=system_message,
    markdown=True,
)

---

## Create the research steps

**URL:** llms-txt#create-the-research-steps

research_hackernews = Step(
    name="research_hackernews",
    agent=hackernews_agent,
    description="Research latest tech trends from Hacker News",
)

research_web = Step(
    name="research_web",
    agent=web_agent,
    description="Comprehensive web research on the topic",
)

---

## Calculator

**URL:** llms-txt#calculator

**Contents:**
- Code
- Usage

Source: https://docs.agno.com/examples/concepts/tools/local/calculator

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Set your API key">
    
  </Step>

<Step title="Install libraries">
    
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Set your API key">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Example usage with detailed research request

**URL:** llms-txt#example-usage-with-detailed-research-request

if __name__ == "__main__":
    research_agent.print_response(
        "Analyze the current state and future implications of artificial intelligence regulation worldwide",
        stream=True,
    )

---

## for chunk in run_response:

**URL:** llms-txt#for-chunk-in-run_response:

---

## target_audience="Developers",

**URL:** llms-txt#target_audience="developers",

---

## -*- Secrets for production application

**URL:** llms-txt#-*--secrets-for-production-application

prd_secret = SecretsManager(
    ...
    # Create secret from workspace/secrets/prd_app_secrets.yml
    secret_files=[
        infra_settings.infra_root.joinpath("infra/secrets/prd_app_secrets.yml")
    ],
)

---

## Split the document into chunks

**URL:** llms-txt#split-the-document-into-chunks

text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)
documents = text_splitter.split_documents(raw_documents)

---

## Example showing economic indicators

**URL:** llms-txt#example-showing-economic-indicators

**Contents:**
- Toolkit Params
- Toolkit Functions
- Developer Resources

agent.print_response(
    "Show me the latest GDP growth rate and inflation numbers for the US"
)
```

| Parameter                      | Type   | Default      | Description                                                                                                              |
| ------------------------------ | ------ | ------------ | ------------------------------------------------------------------------------------------------------------------------ |
| `obb`                          | `Any`  | `None`       | OpenBB app instance. If not provided, uses default.                                                                      |
| `openbb_pat`                   | `str`  | `None`       | Personal Access Token for OpenBB API authentication.                                                                     |
| `provider`                     | `str`  | `"yfinance"` | Data provider for financial information. Options: "benzinga", "fmp", "intrinio", "polygon", "tiingo", "tmx", "yfinance". |
| `enable_get_stock_price`       | `bool` | `True`       | Enable the stock price retrieval function.                                                                               |
| `enable_search_company_symbol` | `bool` | `False`      | Enable the company symbol search function.                                                                               |
| `enable_get_company_news`      | `bool` | `False`      | Enable the company news retrieval function.                                                                              |
| `enable_get_company_profile`   | `bool` | `False`      | Enable the company profile retrieval function.                                                                           |
| `enable_get_price_targets`     | `bool` | `False`      | Enable the price targets retrieval function.                                                                             |
| `all`                          | `bool` | `False`      | Enable all available functions. When True, all enable flags are ignored.                                                 |

| Function                | Description                                                                       |
| ----------------------- | --------------------------------------------------------------------------------- |
| `get_stock_price`       | This function gets the current stock price for a stock symbol or list of symbols. |
| `search_company_symbol` | This function searches for the stock symbol of a company.                         |
| `get_price_targets`     | This function gets the price targets for a stock symbol or list of symbols.       |
| `get_company_news`      | This function gets the latest news for a stock symbol or list of symbols.         |
| `get_company_profile`   | This function gets the company profile for a stock symbol or list of symbols.     |

## Developer Resources

* View [Tools](https://github.com/agno-agi/agno/blob/main/libs/agno/agno/tools/openbb.py)
* View [Cookbook](https://github.com/agno-agi/agno/tree/main/cookbook/tools/openbb_tools.py)

---

## Langfuse Via Openlit

**URL:** llms-txt#langfuse-via-openlit

**Contents:**
- Code

Source: https://docs.agno.com/examples/concepts/integrations/observability/langfuse_via_openlit

```python cookbook/integrations/observability/langfuse_via_openlit.py theme={null}
import base64
import os

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.duckduckgo import DuckDuckGoTools

LANGFUSE_AUTH = base64.b64encode(
    f"{os.getenv('LANGFUSE_PUBLIC_KEY')}:{os.getenv('LANGFUSE_SECRET_KEY')}".encode()
).decode()

os.environ["OTEL_EXPORTER_OTLP_ENDPOINT"] = (
    "https://us.cloud.langfuse.com/api/public/otel"  # 🇺🇸 US data region
)

---

## Add a simple endpoint to clear the JWT authentication cookie

**URL:** llms-txt#add-a-simple-endpoint-to-clear-the-jwt-authentication-cookie

@app.get("/clear-auth-cookie")
async def clear_auth_cookie(response: Response):
    """Endpoint to clear the JWT authentication cookie (logout)."""
    response.delete_cookie(key="auth_token")
    return {"message": "Authentication cookie cleared successfully"}

---

## JSON for Team

**URL:** llms-txt#json-for-team

**Contents:**
- Usage

Source: https://docs.agno.com/examples/concepts/db/json/json_for_team

Agno supports using local JSON files as a storage backend for Teams using the `JsonDb` class.

```python json_for_team.py theme={null}
"""
Run: `pip install openai ddgs newspaper4k lxml_html_clean agno` to install the dependencies
"""

from typing import List

from agno.agent import Agent
from agno.db.json import JsonDb
from agno.models.openai import OpenAIChat
from agno.team import Team
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.hackernews import HackerNewsTools
from pydantic import BaseModel

---

## Collaboration Team

**URL:** llms-txt#collaboration-team

**Contents:**
- Code
- Usage

Source: https://docs.agno.com/examples/use-cases/teams/discussion_team

This example shows how to create a collaboration team that allows multiple agents to work together on research topics using the `collaborate` mode. In Collaborate Mode, all team members are given the same task and the team leader synthesizes their outputs into a cohesive response.

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Install required libraries">
    
  </Step>

<Step title="Set environment variables">
    
  </Step>

<Step title="Run the agent">
    
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Install required libraries">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Set environment variables">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run the agent">
```

---

## Search for posts

**URL:** llms-txt#search-for-posts

agent.print_response("Search for recent posts about AI agents", markdown=True)

---

## Create your workflow

**URL:** llms-txt#create-your-workflow

---

## with optional fields

**URL:** llms-txt#with-optional-fields

optional_agent = Agent(
    name="Hackernews Agent with Optional Fields",
    model=OpenAIChat(id="gpt-5-mini"),
    tools=[HackerNewsTools()],
    input_schema=ResearchTopicWithOptionals,
)

print("\n=== Testing TypedDict with Optional Fields ===")
optional_agent.print_response(
    input={
        "topic": "Blockchain",
        "focus_areas": ["DeFi", "NFTs"],
        "target_audience": "Investors",
        # sources_required is optional, omitting it
        "priority": "high",
    }
)

---

## Stream all events

**URL:** llms-txt#stream-all-events

**Contents:**
  - Handling Events
  - Storing Events
  - Event Types
  - Custom Events
- Specify Run User and Session
- Passing Images / Audio / Video / Files
- Cancelling a Run
- Developer Resources

response_stream = team.run(
    "What is the weather in Tokyo?",
    stream=True,
    stream_events=True
)
python  theme={null}
response_stream = team.run("Your prompt", stream=True, stream_events=True)

for event in response_stream:
    if event.event == "TeamRunContent":
        print(f"Content: {event.content}")
    elif event.event == "TeamToolCallStarted":
        print(f"Tool call started: {event.tool}")
    elif event.event == "ToolCallStarted":
        print(f"Member tool call started: {event.tool}")
    elif event.event == "ToolCallCompleted":
        print(f"Member tool call completed: {event.tool}")
    elif event.event == "TeamReasoningStep":
        print(f"Reasoning step: {event.content}")
    ...
python  theme={null}
from agno.team import Team
from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.utils.pprint import pprint_run_response

team = Team(
    name="Story Team",
    members=[],
    model=OpenAIChat(id="gpt-4o"),
    store_events=True
)

response = team.run("Tell me a 5 second short story about a lion", stream=True, stream_events=True)
pprint_run_response(response)

for event in response.events:
    print(event.event)
python  theme={null}
team = Team(
    name="Story Team",
    members=[],
    model=OpenAIChat(id="gpt-4o"),
    store_events=True,
    events_to_skip=["TeamRunStarted"]
)
python  theme={null}
from dataclasses import dataclass
from agno.run.team import CustomEvent

@dataclass
class CustomerProfileEvent(CustomEvent):
    """CustomEvent for customer profile."""

customer_name: Optional[str] = None
    customer_email: Optional[str] = None
    customer_phone: Optional[str] = None
python  theme={null}
from agno.tools import tool

@tool()
async def get_customer_profile():
    """Example custom tool that simply yields a custom event."""

yield CustomerProfileEvent(
        customer_name="John Doe",
        customer_email="john.doe@example.com",
        customer_phone="1234567890",
    )
python  theme={null}
team.run("Get me my monthly report", user_id="john@example.com", session_id="session_123")
python  theme={null}
team.run("Tell me a 5 second short story about this image", images=[Image(url="https://example.com/image.jpg")])
```

For more information see the [Multimodal](/concepts/multimodal) documentation.

A run can be cancelled by calling the `Team.cancel_run()` method.

See more details in the [Cancelling a Run](/concepts/teams/run-cancel) documentation.

## Developer Resources

* View the [Team reference](/reference/teams/team)
* View the [TeamRunOutput schema](/reference/teams/team-response)
* View [Team Cookbook](https://github.com/agno-agi/agno/tree/main/cookbook/teams/README.md)

**Examples:**

Example 1 (unknown):
```unknown
### Handling Events

You can process events as they arrive by iterating over the response stream:
```

Example 2 (unknown):
```unknown
<Note>
  Team member events are yielded during team execution when a team member is
  being executed. You can disable this by setting `stream_member_events=False`.
</Note>

### Storing Events

You can store all the events that happened during a run on the `RunOutput` object.
```

Example 3 (unknown):
```unknown
By default the `TeamRunContentEvent` and `RunContentEvent` events are not stored. You can modify which events are skipped by setting the `events_to_skip` parameter.

For example:
```

Example 4 (unknown):
```unknown
### Event Types

The following events are sent by the `Team.run()` and `Team.arun()` functions depending on team's configuration:

#### Core Events

| Event Type                   | Description                                                                                                    |
| ---------------------------- | -------------------------------------------------------------------------------------------------------------- |
| `TeamRunStarted`             | Indicates the start of a run                                                                                   |
| `TeamRunContent`             | Contains the model's response text as individual chunks                                                        |
| `TeamRunContentCompleted`    | Signals completion of content streaming                                                                        |
| `TeamRunIntermediateContent` | Contains the model's intermediate response text as individual chunks. This is used when `output_model` is set. |
| `TeamRunCompleted`           | Signals successful completion of the run                                                                       |
| `TeamRunError`               | Indicates an error occurred during the run                                                                     |
| `TeamRunCancelled`           | Signals that the run was cancelled                                                                             |

#### Tool Events

| Event Type              | Description                                                    |
| ----------------------- | -------------------------------------------------------------- |
| `TeamToolCallStarted`   | Indicates the start of a tool call                             |
| `TeamToolCallCompleted` | Signals completion of a tool call, including tool call results |

#### Reasoning Events

| Event Type               | Description                                         |
| ------------------------ | --------------------------------------------------- |
| `TeamReasoningStarted`   | Indicates the start of the team's reasoning process |
| `TeamReasoningStep`      | Contains a single step in the reasoning process     |
| `TeamReasoningCompleted` | Signals completion of the reasoning process         |

#### Memory Events

| Event Type                  | Description                                    |
| --------------------------- | ---------------------------------------------- |
| `TeamMemoryUpdateStarted`   | Indicates that the team is updating its memory |
| `TeamMemoryUpdateCompleted` | Signals completion of a memory update          |

#### Session Summary Events

| Event Type                    | Description                                       |
| ----------------------------- | ------------------------------------------------- |
| `TeamSessionSummaryStarted`   | Indicates the start of session summary generation |
| `TeamSessionSummaryCompleted` | Signals completion of session summary generation  |

#### Pre-Hook Events

| Event Type             | Description                                    |
| ---------------------- | ---------------------------------------------- |
| `TeamPreHookStarted`   | Indicates the start of a pre-run hook          |
| `TeamPreHookCompleted` | Signals completion of a pre-run hook execution |

#### Post-Hook Events

| Event Type              | Description                                     |
| ----------------------- | ----------------------------------------------- |
| `TeamPostHookStarted`   | Indicates the start of a post-run hook          |
| `TeamPostHookCompleted` | Signals completion of a post-run hook execution |

#### Parser Model events

| Event Type                         | Description                                      |
| ---------------------------------- | ------------------------------------------------ |
| `TeamParserModelResponseStarted`   | Indicates the start of the parser model response |
| `TeamParserModelResponseCompleted` | Signals completion of the parser model response  |

#### Output Model events

| Event Type                         | Description                                      |
| ---------------------------------- | ------------------------------------------------ |
| `TeamOutputModelResponseStarted`   | Indicates the start of the output model response |
| `TeamOutputModelResponseCompleted` | Signals completion of the output model response  |

See detailed documentation in the [TeamRunOutput](/reference/teams/team-response) documentation.

### Custom Events

If you are using your own custom tools, it will often be useful to be able to yield custom events. Your custom events will be yielded together with the rest of the expected Agno events.

We recommend creating your custom event class extending the built-in `CustomEvent` class:
```

---

## SentenceTransformers Embedder

**URL:** llms-txt#sentencetransformers-embedder

**Contents:**
- Usage

Source: https://docs.agno.com/concepts/knowledge/embedder/sentencetransformers

The `SentenceTransformerEmbedder` class is used to embed text data into vectors using the [SentenceTransformers](https://www.sbert.net/) library.

```python sentence_transformer_embedder.py theme={null}
from agno.knowledge.knowledge import Knowledge
from agno.vectordb.pgvector import PgVector
from agno.knowledge.embedder.sentence_transformer import SentenceTransformerEmbedder

---

## Analyze aggregated team metrics

**URL:** llms-txt#analyze-aggregated-team-metrics

print("=" * 50)
print("AGGREGATED TEAM METRICS")
print("=" * 50)
pprint(run_output.metrics)

---

## Cassandra Async

**URL:** llms-txt#cassandra-async

**Contents:**
- Code
- Usage

Source: https://docs.agno.com/examples/concepts/vectordb/cassandra-db/async-cassandra-db

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Install libraries">
    
  </Step>

<Step title="Run Cassandra">
    
  </Step>

<Step title="Set environment variables">
    
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Install libraries">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Run Cassandra">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Set environment variables">
```

Example 4 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

---

## CometAPI

**URL:** llms-txt#cometapi

**Contents:**
- Authentication
- Example
- Parameters
- Available Models

Source: https://docs.agno.com/concepts/models/cometapi

Learn how to use CometAPI models in Agno.

CometAPI is a platform for providing endpoints for Large Language models.

See all CometAPI supported models and pricing [here](https://api.cometapi.com/pricing).

Set your `COMETAPI_KEY` environment variable. Get your API key from [here](https://api.cometapi.com/console/token).

Use `CometAPI` with your `Agent`:

<CodeGroup>
  
</CodeGroup>

<Note> View more examples [here](/examples/models/cometapi/basic). </Note>

| Parameter  | Type            | Default                         | Description                                                  |
| ---------- | --------------- | ------------------------------- | ------------------------------------------------------------ |
| `id`       | `str`           | `"gpt-5-mini"`                  | The id of the model to use                                   |
| `name`     | `str`           | `"CometAPI"`                    | The name of the model                                        |
| `api_key`  | `Optional[str]` | `None`                          | The API key for CometAPI (defaults to COMETAPI\_KEY env var) |
| `base_url` | `str`           | `"https://api.cometapi.com/v1"` | The base URL for the CometAPI                                |

`CometAPI` extends the OpenAI-compatible interface and supports most parameters from the [OpenAI model](/concepts/models/openai).

CometAPI provides access to 300+ AI models. You can fetch the available models programmatically:

**Examples:**

Example 1 (unknown):
```unknown

```

Example 2 (unknown):
```unknown
</CodeGroup>

## Example

Use `CometAPI` with your `Agent`:

<CodeGroup>
```

Example 3 (unknown):
```unknown
</CodeGroup>

<Note> View more examples [here](/examples/models/cometapi/basic). </Note>

## Parameters

| Parameter  | Type            | Default                         | Description                                                  |
| ---------- | --------------- | ------------------------------- | ------------------------------------------------------------ |
| `id`       | `str`           | `"gpt-5-mini"`                  | The id of the model to use                                   |
| `name`     | `str`           | `"CometAPI"`                    | The name of the model                                        |
| `api_key`  | `Optional[str]` | `None`                          | The API key for CometAPI (defaults to COMETAPI\_KEY env var) |
| `base_url` | `str`           | `"https://api.cometapi.com/v1"` | The base URL for the CometAPI                                |

`CometAPI` extends the OpenAI-compatible interface and supports most parameters from the [OpenAI model](/concepts/models/openai).

## Available Models

CometAPI provides access to 300+ AI models. You can fetch the available models programmatically:
```

---

## Gemini

**URL:** llms-txt#gemini

**Contents:**
- Parameters

Source: https://docs.agno.com/reference/models/gemini

The Gemini model provides access to Google's Gemini models.

| Parameter            | Type                       | Default              | Description                                                      |
| -------------------- | -------------------------- | -------------------- | ---------------------------------------------------------------- |
| `id`                 | `str`                      | `"gemini-1.5-flash"` | The id of the Gemini model to use                                |
| `name`               | `str`                      | `"Gemini"`           | The name of the model                                            |
| `provider`           | `str`                      | `"Google"`           | The provider of the model                                        |
| `api_key`            | `Optional[str]`            | `None`               | The API key for Google AI (defaults to GOOGLE\_API\_KEY env var) |
| `generation_config`  | `Optional[Dict[str, Any]]` | `None`               | Generation configuration parameters for the model                |
| `safety_settings`    | `Optional[List[Dict]]`     | `None`               | Safety settings to filter content                                |
| `tools`              | `Optional[List[Dict]]`     | `None`               | Tools available to the model                                     |
| `tool_config`        | `Optional[Dict[str, Any]]` | `None`               | Configuration for tool use                                       |
| `system_instruction` | `Optional[str]`            | `None`               | System instruction for the model                                 |
| `cached_content`     | `Optional[str]`            | `None`               | Cached content identifier for context caching                    |
| `request_params`     | `Optional[Dict[str, Any]]` | `None`               | Additional parameters to include in the request                  |
| `client_params`      | `Optional[Dict[str, Any]]` | `None`               | Additional parameters for client configuration                   |
| `thinking_enabled`   | `Optional[bool]`           | `None`               | Whether to enable thinking mode for supported models             |

---

## Jina Embedder

**URL:** llms-txt#jina-embedder

**Contents:**
- Code

Source: https://docs.agno.com/examples/concepts/knowledge/embedders/jina-embedder

```python  theme={null}

from agno.knowledge.embedder.jina import JinaEmbedder
from agno.knowledge.knowledge import Knowledge
from agno.vectordb.pgvector import PgVector

---

## print(run.content)

**URL:** llms-txt#print(run.content)

---

## Get specific content by ID

**URL:** llms-txt#get-specific-content-by-id

content = knowledge.get_content_by_id(content_id)

---

## Comparison Accuracy Evaluation

**URL:** llms-txt#comparison-accuracy-evaluation

**Contents:**
- Code

Source: https://docs.agno.com/examples/concepts/evals/accuracy/accuracy_comparison

Learn how to evaluate agent accuracy on comparison tasks.

This example shows how to evaluate an agent's ability to correctly compare numbers using calculator tools.

---

## Get the run response directly from the non-streaming call

**URL:** llms-txt#get-the-run-response-directly-from-the-non-streaming-call

run_response = agent.run("What is the stock price of NVDA")
print("Tool execution completed successfully!")

---

## Give a transcript of this audio conversation. Use speaker A, speaker B to identify speakers.

**URL:** llms-txt#give-a-transcript-of-this-audio-conversation.-use-speaker-a,-speaker-b-to-identify-speakers.

**Contents:**
- Usage

agent.print_response(
    "Give a transcript of this audio conversation. Use speaker A, speaker B to identify speakers.",
    audio=[Audio(content=audio_content)],
    stream=True,
)
bash  theme={null}
    pip install -U agno google-generativeai requests
    bash Mac/Linux theme={null}
        export GOOGLE_API_KEY="your_google_api_key_here"
      bash Windows theme={null}
        $Env:GOOGLE_API_KEY="your_google_api_key_here"
      bash  theme={null}
    touch audio_to_text.py
    bash Mac theme={null}
      python audio_to_text.py
      bash Windows theme={null}
      python audio_to_text.py
      ```
    </CodeGroup>
  </Step>

<Step title="Find All Cookbooks">
    Explore all the available cookbooks in the Agno repository. Click the link below to view the code on GitHub:

<Link href="https://github.com/agno-agi/agno/tree/main/cookbook/agents/multimodal" target="_blank">
      Agno Cookbooks on GitHub
    </Link>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Install libraries">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Export your GOOGLE API key">
    <CodeGroup>
```

Example 3 (unknown):
```unknown

```

Example 4 (unknown):
```unknown
</CodeGroup>
  </Step>

  <Step title="Create a Python file">
    Create a Python file and add the above code.
```

---

## RedisDb

**URL:** llms-txt#redisdb

Source: https://docs.agno.com/reference/storage/redis

`RedisDb` is a class that implements the Db interface using Redis as the backend storage system. It provides high-performance, distributed storage for agent sessions with support for JSON data types and schema versioning.

<Snippet file="db-redis-params.mdx" />

<Snippet file="db-new-bulk-methods.mdx" />

---

## 6. Process segments

**URL:** llms-txt#6.-process-segments

shorts = extract_segments(response.content)

---

## Exa Tools

**URL:** llms-txt#exa-tools

**Contents:**
- Code
- Usage

Source: https://docs.agno.com/examples/concepts/tools/search/exa

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Set your API key">
    
  </Step>

<Step title="Install libraries">
    
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Set your API key">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## sources_required=5,

**URL:** llms-txt#sources_required=5,

---

## 3. Send a message to the bot.

**URL:** llms-txt#3.-send-a-message-to-the-bot.

---

## Parallel Steps

**URL:** llms-txt#parallel-steps

Source: https://docs.agno.com/reference/workflows/parallel-steps

| Parameter     | Type             | Default  | Description                                     |
| ------------- | ---------------- | -------- | ----------------------------------------------- |
| `*steps`      | `*WorkflowSteps` | Required | Variable number of steps to execute in parallel |
| `name`        | `Optional[str]`  | `None`   | Name of the parallel execution block            |
| `description` | `Optional[str]`  | `None`   | Description of the parallel execution           |

---

## Download the audio file from the URL as bytes

**URL:** llms-txt#download-the-audio-file-from-the-url-as-bytes

**Contents:**
- Usage

response = requests.get(url)
audio_content = response.content

agent.print_response(
    "Tell me about this audio",
    audio=[Audio(content=audio_content)],
)
bash  theme={null}
    export GOOGLE_API_KEY=xxx
    bash  theme={null}
    pip install -U google-genai requests agno
    bash Mac theme={null}
      python cookbook/models/google/gemini/audio_input_bytes_content.py
      bash Windows theme={null}
      python cookbook/models/google/gemini/audio_input_bytes_content.py
      ```
    </CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Set your API key">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Define the custom retriever

**URL:** llms-txt#define-the-custom-retriever

---

## Add memories for Jane Doe

**URL:** llms-txt#add-memories-for-jane-doe

jane_doe_id = "jane_doe@example.com"
print(f"\nUser: {jane_doe_id}")
memory_id_1 = memory.add_user_memory(
    memory=UserMemory(memory="The user's name is Jane Doe", topics=["name"]),
    user_id=jane_doe_id,
)
memory_id_2 = memory.add_user_memory(
    memory=UserMemory(memory="She likes to play tennis", topics=["hobbies"]),
    user_id=jane_doe_id,
)
memories = memory.get_user_memories(user_id=jane_doe_id)
print("Memories:")
pprint(memories)

---

## Async Basic

**URL:** llms-txt#async-basic

**Contents:**
- Code

Source: https://docs.agno.com/examples/models/openai/responses/async_basic

```python cookbook/models/openai/responses/async_basic.py theme={null}
import asyncio

from agno.agent import Agent, RunOutput  # noqa
from agno.models.openai import OpenAIResponses

agent = Agent(model=OpenAIResponses(id="gpt-5-mini"), markdown=True)

---

## with valid input

**URL:** llms-txt#with-valid-input

print("=== Testing TypedDict Input Schema ===")
hackernews_agent.print_response(
    input={
        "topic": "AI",
        "focus_areas": ["Machine Learning", "LLMs", "Neural Networks"],
        "target_audience": "Developers",
        "sources_required": 5,
    }
)

---

## Milvus Hybrid Search

**URL:** llms-txt#milvus-hybrid-search

**Contents:**
- Code
- Usage

Source: https://docs.agno.com/examples/concepts/vectordb/milvus-db/milvus-db-hybrid-search

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Install libraries">
    
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Install libraries">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

Example 3 (unknown):
```unknown

```

---

## This will create a new session summary

**URL:** llms-txt#this-will-create-a-new-session-summary

team.print_response(
    "My name is John Doe and I like to hike in the mountains on weekends.",
)

---

## stream=True,

**URL:** llms-txt#stream=true,

---

## OR set environment variables manually

**URL:** llms-txt#or-set-environment-variables-manually

**Contents:**
- Example

export AWS_ACCESS_KEY_ID=****
export AWS_SECRET_ACCESS_KEY=****
export AWS_DEFAULT_REGION=us-east-1
python aws_ses_tools.py theme={null}
from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.aws_ses import AWSSESTool
from agno.tools.duckduckgo import DuckDuckGoTools

**Examples:**

Example 1 (unknown):
```unknown
<Note>
  Make sure to add the domain or email address you want to send FROM (and, if
  still in sandbox mode, the TO address) to the verified emails in the [SES
  Console](https://console.aws.amazon.com/ses/home).
</Note>

## Example

The following agent researches the latest AI news and then emails a summary via AWS SES:
```

---

## Setup virtual environment

**URL:** llms-txt#setup-virtual-environment

./scripts/perf_setup.sh
source .venvs/perfenv/bin/activate

---

## Create your workflows...

**URL:** llms-txt#create-your-workflows...

workflow_tools = WorkflowTools(
    workflow=blog_post_workflow,
)

agent = Agent(
    model=OpenAIChat(id="gpt-5-mini"),
    tools=[workflow_tools],
)

agent.print_response("Create a blog post on the topic: AI trends in 2024", stream=True)
```

See the [Workflow Tools](/concepts/tools/reasoning_tools/workflow-tools) documentation for more details.

---

## Local File System Tools

**URL:** llms-txt#local-file-system-tools

**Contents:**
- Code
- Usage

Source: https://docs.agno.com/examples/concepts/tools/local/local_file_system

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Set your API key">
    
  </Step>

<Step title="Install libraries">
    
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Set your API key">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Fetch the QA audio file and convert it to a base64 encoded string

**URL:** llms-txt#fetch-the-qa-audio-file-and-convert-it-to-a-base64-encoded-string

url = "https://agno-public.s3.us-east-1.amazonaws.com/demo_data/QA-01.mp3"
response = requests.get(url)
response.raise_for_status()
mp3_data = response.content

---

## Create workflow with direct execution

**URL:** llms-txt#create-workflow-with-direct-execution

workflow = Workflow(
    name="Content Creation Pipeline",
    steps=[
        Parallel(research_hn_step, research_web_step, name="Research Phase"),
        write_step,
        review_step,
    ],
)

workflow.print_response("Write about the latest AI developments")
```

This was a synchronous non-streaming example of this pattern. To checkout async and streaming versions, see the cookbooks-

* [Parallel Steps Workflow (sync streaming)](https://github.com/agno-agi/agno/blob/main/cookbook/workflows/_04_workflows_parallel_execution/sync/parallel_steps_workflow_stream.py)
* [Parallel Steps Workflow (async non-streaming)](https://github.com/agno-agi/agno/blob/main/cookbook/workflows/_04_workflows_parallel_execution/sync/parallel_steps_workflow.py)
* [Parallel Steps Workflow (async streaming)](https://github.com/agno-agi/agno/tree/main/cookbook/workflows/_04_workflows_parallel_execution/async/parallel_steps_workflow_stream.py)

---

## No event storage

**URL:** llms-txt#no-event-storage

**Contents:**
- Agno Telemetry
- Developer Resources

fast_workflow = Workflow(
    name="Fast Workflow",
    store_events=False,
    steps=[...]
)
bash  theme={null}
export AGNO_TELEMETRY=false
python  theme={null}
workflow = Workflow(..., telemetry=False)
```

See the [Workflow class reference](/reference/workflows/workflow) for more details.

## Developer Resources

* View the [Workflow reference](/reference/workflows/workflow)
* View the [WorkflowRunOutput schema](/reference/workflows/workflow_run_output)
* View [Cookbook](https://github.com/agno-agi/agno/tree/main/cookbook/workflows/README.md)

**Examples:**

Example 1 (unknown):
```unknown
<Tip>
  See this [example](/examples/concepts/workflows/06_workflows_advanced_concepts/store_events_and_events_to_skip_in_a_workflow) for more information.
</Tip>

## Agno Telemetry

Agno logs which model an workflow used so we can prioritize updates to the most popular providers. You can disable this by setting `AGNO_TELEMETRY=false` in your environment or by setting `telemetry=False` on the workflow.
```

Example 2 (unknown):
```unknown
or:
```

---

## Oxylabs

**URL:** llms-txt#oxylabs

**Contents:**
- Prerequisites
- Example
- Amazon Product Search
- Toolkit Params
- Toolkit Functions
- Developer Resources

Source: https://docs.agno.com/concepts/tools/toolkits/web_scrape/oxylabs

**OxylabsTools** provide Agents with access to Oxylabs' powerful web scraping capabilities, including SERP, Amazon product data, and universal web scraping endpoints.

The following examples require the `oxylabs-sdk` library:

Set your credentials as environment variables (recommended):

## Amazon Product Search

| Parameter  | Type  | Default | Description                                                                             |
| ---------- | ----- | ------- | --------------------------------------------------------------------------------------- |
| `username` | `str` | `None`  | Oxylabs dashboard username. If not provided, it defaults to `OXYLABS_USERNAME` env var. |
| `password` | `str` | `None`  | Oxylabs dashboard password. If not provided, it defaults to `OXYLABS_PASSWORD` env var. |

| Function                 | Description                                                                                            |
| ------------------------ | ------------------------------------------------------------------------------------------------------ |
| `search_google`          | Performs a Google SERP search. Accepts all the standard Oxylabs params (e.g. `query`, `geo_location`). |
| `get_amazon_product`     | Retrieves the details of Amazon product(s). Accepts ASIN code or full product URL.                     |
| `search_amazon_products` | Searches for Amazon product(s) using a search term.                                                    |
| `scrape_website`         | Scrapes a webpage URL.                                                                                 |

## Developer Resources

* View [Tools](https://github.com/agno-agi/agno/blob/main/libs/agno/agno/tools/oxylabs.py)
* View [Cookbook](https://github.com/agno-agi/agno/tree/main/cookbook/tools/oxylabs_tools.py)
* View [Oxylabs MCP Cookbook](https://github.com/agno-agi/agno/tree/main/cookbook/tools/mcp/oxylabs.py)

**Examples:**

Example 1 (unknown):
```unknown
Set your credentials as environment variables (recommended):
```

Example 2 (unknown):
```unknown
## Example
```

Example 3 (unknown):
```unknown
## Amazon Product Search
```

---

## AWS Credentials

**URL:** llms-txt#aws-credentials

AWS_ACCESS_KEY_ID = getenv("AWS_ACCESS_KEY_ID")
AWS_SECRET_ACCESS_KEY = getenv("AWS_SECRET_ACCESS_KEY")

db = DynamoDb(
    region_name="us-east-1",
    # aws_access_key_id: AWS access key id
    aws_access_key_id=AWS_ACCESS_KEY_ID,
    # aws_secret_access_key: AWS secret access key
    aws_secret_access_key=AWS_SECRET_ACCESS_KEY,
)

---

## StepOutput

**URL:** llms-txt#stepoutput

Source: https://docs.agno.com/reference/workflows/step_output

| Parameter       | Type                                                              | Default | Description                                                     |
| --------------- | ----------------------------------------------------------------- | ------- | --------------------------------------------------------------- |
| `step_name`     | `Optional[str]`                                                   | `None`  | Step identification name                                        |
| `step_id`       | `Optional[str]`                                                   | `None`  | Unique step identifier                                          |
| `step_type`     | `Optional[str]`                                                   | `None`  | Type of step (e.g., "Loop", "Condition", "Parallel")            |
| `executor_type` | `Optional[str]`                                                   | `None`  | Type of executor: "agent", "team", or "function"                |
| `executor_name` | `Optional[str]`                                                   | `None`  | Name of the executor                                            |
| `content`       | `Optional[Union[str, Dict[str, Any], List[Any], BaseModel, Any]]` | `None`  | Primary output (can be any format)                              |
| `step_run_id`   | `Optional[str]`                                                   | `None`  | Link to the run ID of the step execution                        |
| `images`        | `Optional[List[Image]]`                                           | `None`  | Media outputs - images (new or passed-through)                  |
| `videos`        | `Optional[List[Video]]`                                           | `None`  | Media outputs - videos (new or passed-through)                  |
| `audio`         | `Optional[List[Audio]]`                                           | `None`  | Media outputs - audio (new or passed-through)                   |
| `files`         | `Optional[List[File]]`                                            | `None`  | File outputs (new or passed-through)                            |
| `metrics`       | `Optional[Metrics]`                                               | `None`  | Execution metrics and metadata                                  |
| `success`       | `bool`                                                            | `True`  | Execution success status                                        |
| `error`         | `Optional[str]`                                                   | `None`  | Error message if execution failed                               |
| `stop`          | `bool`                                                            | `False` | Request early workflow termination                              |
| `steps`         | `Optional[List[StepOutput]]`                                      | `None`  | Nested step outputs for composite steps (Loop, Condition, etc.) |

---

## Brave Search

**URL:** llms-txt#brave-search

**Contents:**
- Prerequisites
- Example
- Toolkit Params
- Toolkit Functions
- Developer Resources

Source: https://docs.agno.com/concepts/tools/toolkits/search/bravesearch

**BraveSearch** enables an Agent to search the web for information using the Brave search engine.

The following examples requires the `brave-search` library.

| Parameter             | Type            | Default | Description                                                                    |
| --------------------- | --------------- | ------- | ------------------------------------------------------------------------------ |
| `api_key`             | `Optional[str]` | `None`  | Brave API key. If not provided, will use BRAVE\_API\_KEY environment variable. |
| `fixed_max_results`   | `Optional[int]` | `None`  | A fixed number of maximum results.                                             |
| `fixed_language`      | `Optional[str]` | `None`  | A fixed language for the search results.                                       |
| `enable_brave_search` | `bool`          | `True`  | Enable or disable the brave\_search function.                                  |
| `all`                 | `bool`          | `False` | Enable all available functions in the toolkit.                                 |

| Function       | Description                                                                                                                                                                                                                                                                                                                                                                       |
| -------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| `brave_search` | Searches Brave for a specified query. Parameters include `query` (str) for the search term, `max_results` (int, default=5) for the maximum number of results, `country` (str, default="US") for the country code for search results, and `search_lang` (str, default="en") for the language of the search results. Returns a JSON formatted string containing the search results. |

## Developer Resources

* View [Tools](https://github.com/agno-agi/agno/blob/main/libs/agno/agno/tools/bravesearch.py)
* View [Cookbook](https://github.com/agno-agi/agno/tree/main/cookbook/tools/bravesearch_tools.py)

**Examples:**

Example 1 (unknown):
```unknown

```

Example 2 (unknown):
```unknown
## Example
```

---

## Semiconductor market analysis example

**URL:** llms-txt#semiconductor-market-analysis-example

**Contents:**
- Usage

reasoning_agent.print_response(
    """\
    Analyze the semiconductor market performance focusing on:
    - NVIDIA (NVDA)
    - AMD (AMD)
    - Intel (INTC)
    - Taiwan Semiconductor (TSM)
    Compare their market positions, growth metrics, and future outlook.""",
    stream=True,
    show_full_reasoning=True,
    stream_events=True,
)
bash  theme={null}
    export ANTHROPIC_API_KEY=xxx
    bash  theme={null}
    pip install -U anthropic agno ddgs
    bash Mac theme={null}
      python cookbook/reasoning/tools/claude_reasoning_tools.py
      bash Windows theme={null}
      python cookbook/reasoning/tools/claude_reasoning_tools.py
      ```
    </CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Set your API key">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Example">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Mistral Embedder

**URL:** llms-txt#mistral-embedder

**Contents:**
- Code

Source: https://docs.agno.com/examples/concepts/knowledge/embedders/mistral-embedder

```python  theme={null}
import asyncio

from agno.knowledge.embedder.mistral import MistralEmbedder
from agno.knowledge.knowledge import Knowledge
from agno.vectordb.pgvector import PgVector

embeddings = MistralEmbedder().get_embedding(
    "The quick brown fox jumps over the lazy dog."
)

---

## Accuracy with Given Answer

**URL:** llms-txt#accuracy-with-given-answer

**Contents:**
- Code

Source: https://docs.agno.com/examples/concepts/evals/accuracy/accuracy_with_given_answer

Learn how to evaluate the accuracy of an Agno Agent's response with a given answer.

For this example an agent won't be executed, but the given result will be evaluated against the expected output for correctness.

---

## model=OpenAIChat(id="gpt-5-mini"),

**URL:** llms-txt#model=openaichat(id="gpt-5-mini"),

---

## Zendesk Tools

**URL:** llms-txt#zendesk-tools

**Contents:**
- Code
- Usage

Source: https://docs.agno.com/examples/concepts/tools/others/zendesk

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Set your API key">
    
  </Step>

<Step title="Set your Zendesk credentials">
    
  </Step>

<Step title="Install libraries">
    
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Set your API key">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Set your Zendesk credentials">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 4 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

---

## Memori Tools

**URL:** llms-txt#memori-tools

**Contents:**
- Code
- Usage

Source: https://docs.agno.com/examples/concepts/tools/others/memori

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Set your API key">
    
  </Step>

<Step title="Install libraries">
    
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Set your API key">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Get Run by ID

**URL:** llms-txt#get-run-by-id

Source: https://docs.agno.com/reference-api/schema/sessions/get-run-by-id

get /sessions/{session_id}/runs/{run_id}
Retrieve a specific run by its ID from a session. Response schema varies based on the run type (agent run, team run, or workflow run).

---

## Example usage showing stock analysis

**URL:** llms-txt#example-usage-showing-stock-analysis

agent.print_response(
    "Get me the current stock price and key information for Apple (AAPL)"
)

---

## LlamaCpp

**URL:** llms-txt#llamacpp

**Contents:**
  - Google Gemma Models
  - Meta Llama Models
  - Default Options
- Set up LlamaCpp
  - Install LlamaCpp

Source: https://docs.agno.com/concepts/models/llama_cpp

Learn how to use LlamaCpp with Agno.

Run Large Language Models locally with LLaMA CPP

[LlamaCpp](https://github.com/ggerganov/llama.cpp) is a powerful tool for running large language models locally with efficient inference. LlamaCpp supports multiple open-source models and provides an OpenAI-compatible API server.

LlamaCpp supports a wide variety of models in GGML format. You can find models on HuggingFace, including the default `ggml-org/gpt-oss-20b-GGUF` used in the examples below.

We recommend experimenting to find the best model for your use case. Here are some popular model recommendations:

### Google Gemma Models

* `google/gemma-2b-it-GGUF` - Lightweight 2B parameter model, great for resource-constrained environments
* `google/gemma-7b-it-GGUF` - Balanced 7B model with strong performance for general tasks
* `ggml-org/gemma-3-1b-it-GGUF` - Latest Gemma 3 series, efficient for everyday use

### Meta Llama Models

* `Meta-Llama-3-8B-Instruct` - Popular 8B parameter model with excellent instruction following
* `Meta-Llama-3.1-8B-Instruct` - Enhanced version with improved capabilities and 128K context
* `Meta-Llama-3.2-3B-Instruct` - Compact 3B model for faster inference

* `ggml-org/gpt-oss-20b-GGUF` - Default model for general use cases
* Models with different quantizations (Q4\_K\_M, Q8\_0, etc.) for different speed/quality tradeoffs
* Choose models based on your hardware constraints and performance requirements

First, install LlamaCpp following the [official installation guide](https://github.com/ggerganov/llama.cpp):

Or using package managers:

```bash brew install theme={null}

**Examples:**

Example 1 (unknown):
```unknown
Or using package managers:
```

---

## Set up ContentsDB - tracks content metadata

**URL:** llms-txt#set-up-contentsdb---tracks-content-metadata

contents_db = PostgresDb(
    db_url="postgresql+psycopg://user:pass@localhost:5432/db",
    knowledge_table="knowledge_contents"  # Optional: custom table name
)

---

## Create distributed PgVector RAG team

**URL:** llms-txt#create-distributed-pgvector-rag-team

**Contents:**
- Usage

distributed_pgvector_team = Team(
    name="Distributed PgVector RAG Team",
    model=OpenAIChat(id="gpt-5-mini"),
    members=[vector_retriever, hybrid_searcher, data_validator, response_composer],
    instructions=[
        "Work together to provide comprehensive RAG responses using PostgreSQL pgvector.",
        "Vector Retriever: First perform vector similarity search.",
        "Hybrid Searcher: Then perform hybrid search for comprehensive coverage.",
        "Data Validator: Validate and filter the retrieved information quality.",
        "Response Composer: Compose the final response with proper attribution.",
        "Leverage PostgreSQL's scalability and pgvector's performance.",
        "Ensure enterprise-grade reliability and accuracy.",
    ],
    show_members_responses=True,
    markdown=True,
)

async def async_pgvector_rag_demo():
    """Demonstrate async distributed PgVector RAG processing."""
    print("🐘 Async Distributed PgVector RAG Demo")
    print("=" * 40)

query = "How do I make chicken and galangal in coconut milk soup? What are the key ingredients and techniques?"

try:
        # Add content to knowledge bases
        await vector_knowledge.add_contents_async(
            url="https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf"
        )
        await hybrid_knowledge.add_contents_async(
            url="https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf"
        )
        # Run async distributed PgVector RAG
        await distributed_pgvector_team.aprint_response(input=query)
    except Exception as e:
        print(f"❌ Error: {e}")
        print("💡 Make sure PostgreSQL with pgvector is running!")
        print("   Run: ./cookbook/run_pgvector.sh")

def sync_pgvector_rag_demo():
    """Demonstrate sync distributed PgVector RAG processing."""
    print("🐘 Distributed PgVector RAG Demo")
    print("=" * 35)

query = "How do I make chicken and galangal in coconut milk soup? What are the key ingredients and techniques?"

try:
        # Add content to knowledge bases
        vector_knowledge.add_contents(
            url="https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf"
        )
        hybrid_knowledge.add_contents(
            url="https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf"
        )
        # Run distributed PgVector RAG
        distributed_pgvector_team.print_response(input=query)
    except Exception as e:
        print(f"❌ Error: {e}")
        print("💡 Make sure PostgreSQL with pgvector is running!")
        print("   Run: ./cookbook/run_pgvector.sh")

def complex_query_demo():
    """Demonstrate distributed RAG for complex culinary queries."""
    print("👨‍🍳 Complex Culinary Query with Distributed PgVector RAG")
    print("=" * 60)

query = """I'm planning a Thai dinner party for 8 people. Can you help me plan a complete menu?
    I need appetizers, main courses, and desserts. Please include:
    - Preparation timeline
    - Shopping list
    - Cooking techniques for each dish
    - Any dietary considerations or alternatives"""

try:
        # Add content to knowledge bases
        vector_knowledge.add_contents(
            url="https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf"
        )
        hybrid_knowledge.add_contents(
            url="https://agno-public.s3.amazonaws.com/recipes/ThaiRecipes.pdf"
        )

distributed_pgvector_team.print_response(input=query)
    except Exception as e:
        print(f"❌ Error: {e}")
        print("💡 Make sure PostgreSQL with pgvector is running!")
        print("   Run: ./cookbook/run_pgvector.sh")

if __name__ == "__main__":
    # Choose which demo to run

# asyncio.run(async_pgvector_rag_demo())

# complex_query_demo()

sync_pgvector_rag_demo()
bash  theme={null}
    ./cookbook/run_pgvector.sh
    bash  theme={null}
    pip install agno openai sqlalchemy 'psycopg[binary]' pgvector
    bash  theme={null}
    export OPENAI_API_KEY=****
    bash  theme={null}
    python cookbook/examples/teams/distributed_rag/01_distributed_rag_pgvector.py
    ```
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Set up PostgreSQL with pgvector">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Install required libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Set environment variables">
```

Example 4 (unknown):
```unknown
</Step>

  <Step title="Run the agent">
```

---

## Define tools to manage a shopping list in workflow session state

**URL:** llms-txt#define-tools-to-manage-a-shopping-list-in-workflow-session-state

def add_item(session_state, item: str) -> str:
    """Add an item to the shopping list in workflow session state.

Args:
        item (str): The item to add to the shopping list
    """
    # Check if item already exists (case-insensitive)
    existing_items = [
        existing_item.lower() for existing_item in session_state["shopping_list"]
    ]
    if item.lower() not in existing_items:
        session_state["shopping_list"].append(item)
        return f"Added '{item}' to the shopping list."
    else:
        return f"'{item}' is already in the shopping list."

def remove_item(session_state, item: str) -> str:
    """Remove an item from the shopping list in workflow session state.

Args:
        item (str): The item to remove from the shopping list
    """
    if len(session_state["shopping_list"]) == 0:
        return f"Shopping list is empty. Cannot remove '{item}'."

# Find and remove item (case-insensitive)
    shopping_list = session_state["shopping_list"]
    for i, existing_item in enumerate(shopping_list):
        if existing_item.lower() == item.lower():
            removed_item = shopping_list.pop(i)
            return f"Removed '{removed_item}' from the shopping list."

return f"'{item}' not found in the shopping list."

def remove_all_items(session_state) -> str:
    """Remove all items from the shopping list in workflow session state."""
    session_state["shopping_list"] = []
    return "Removed all items from the shopping list."

def list_items(session_state) -> str:
    """List all items in the shopping list from workflow session state."""
    if len(session_state["shopping_list"]) == 0:
        return "Shopping list is empty."

items = session_state["shopping_list"]
    items_str = "\n".join([f"- {item}" for item in items])
    return f"Shopping list:\n{items_str}"

---

## Set the local collector endpoint for Arize Phoenix

**URL:** llms-txt#set-the-local-collector-endpoint-for-arize-phoenix

os.environ["PHOENIX_COLLECTOR_ENDPOINT"] = "http://localhost:6006"

---

## Or for simple debug flow

**URL:** llms-txt#or-for-simple-debug-flow

---

## ag infra up

**URL:** llms-txt#ag-infra-up

**Contents:**
- Params

Source: https://docs.agno.com/reference/agno-infra/cli/ws/up

Create resources for the active workspace

<ResponseField name="resources_filter" type="str">
  Resource filter. Format - ENV:INFRA:GROUP:NAME:TYPE
</ResponseField>

<ResponseField name="env_filter" type="str">
  Filter the environment to deploy `--env` `-e`
</ResponseField>

<ResponseField name="infra_filter" type="str">
  Filter the infra to deploy. `--infra` `-i`
</ResponseField>

<ResponseField name="group_filter" type="str">
  Filter resources using group name. `--group` `-g`
</ResponseField>

<ResponseField name="name_filter" type="str">
  Filter resource using name. `--name` `-n`
</ResponseField>

<ResponseField name="type_filter" type="str">
  Filter resource using type `--type` `-t`
</ResponseField>

<ResponseField name="dry_run" type="bool">
  Print resources and exit. `--dry-run` `-dr`
</ResponseField>

<ResponseField name="auto_confirm" type="bool">
  Skip the confirmation before deploying resources. `--yes` `-y`
</ResponseField>

<ResponseField name="print_debug_log" type="bool">
  Print debug logs. `--debug` `-d`
</ResponseField>

<ResponseField name="force" type="bool">
  Force `--force` `-f`
</ResponseField>

<ResponseField name="pull" type="bool">
  Pull `--pull` `-p`
</ResponseField>

---

## Web Browser Tools

**URL:** llms-txt#web-browser-tools

**Contents:**
- Example
- Toolkit Params
- Toolkit Functions

Source: https://docs.agno.com/concepts/tools/toolkits/others/web-browser

WebBrowser Tools enable an Agent to open a URL in a web browser.

| Parameter          | Type   | Default | Description                                   |
| ------------------ | ------ | ------- | --------------------------------------------- |
| `enable_open_page` | `bool` | `True`  | Enables functionality to open URLs in browser |
| `all`              | `bool` | `False` | Enables all functionality when set to True    |

| Function    | Description                  |
| ----------- | ---------------------------- |
| `open_page` | Opens a URL in a web browser |

---

## Create meal planning subteam

**URL:** llms-txt#create-meal-planning-subteam

meal_planning_team = Team(
    name="Meal Planning Team",
    role="Plan meals based on shopping list items",
    id="meal_planning",
    model=OpenAIChat(id="gpt-5-mini"),
    members=[recipe_agent],
    instructions=[
        "You are a meal planning team that suggests recipes based on shopping list items.",
        "IMPORTANT: When users ask 'What can I make with these ingredients?' or any recipe-related questions, IMMEDIATELY forward the EXACT SAME request to the recipe_agent WITHOUT asking for further information.",
        "DO NOT ask the user for ingredients - the recipe_agent will work with what's already in the shopping list.",
        "Your primary job is to forward recipe requests directly to the recipe_agent without modification.",
    ],
)

def add_chore(session_state, chore: str, priority: str = "medium") -> str:
    """Add a chore to the list with priority level.

Args:
        chore (str): The chore to add to the list
        priority (str): Priority level of the chore (low, medium, high)

Returns:
        str: Confirmation message
    """
    # Initialize chores list if it doesn't exist
    if "chores" not in session_state:
        session_state["chores"] = []

# Validate priority
    valid_priorities = ["low", "medium", "high"]
    if priority.lower() not in valid_priorities:
        priority = "medium"  # Default to medium if invalid

# Add the chore with timestamp and priority
    from datetime import datetime

chore_entry = {
        "description": chore,
        "priority": priority.lower(),
        "added_at": datetime.now().strftime("%Y-%m-%d %H:%M"),
    }

session_state["chores"].append(chore_entry)

return f"Added chore: '{chore}' with {priority} priority"

---

## SurrealDB

**URL:** llms-txt#surrealdb

Source: https://docs.agno.com/reference/vector_db/surrealdb

<Snippet file="vector_db_surrealdb_params.mdx" />

---

## Spider

**URL:** llms-txt#spider

**Contents:**
- Prerequisites
- Example
- Toolkit Params
- Toolkit Functions
- Developer Resources

Source: https://docs.agno.com/concepts/tools/toolkits/web_scrape/spider

**SpiderTools** is an open source web Scraper & Crawler that returns LLM-ready data. To start using Spider, you need an API key from the [Spider dashboard](https://spider.cloud).

The following example requires the `spider-client` library.

The following agent will run a search query to get the latest news in USA and scrape the first search result. The agent will return the scraped data in markdown format.

| Parameter         | Type             | Default | Description                                             |
| ----------------- | ---------------- | ------- | ------------------------------------------------------- |
| `max_results`     | `Optional[int]`  | `None`  | Default maximum number of results.                      |
| `url`             | `Optional[str]`  | `None`  | Default URL for operations.                             |
| `optional_params` | `Optional[dict]` | `None`  | Additional parameters for operations.                   |
| `enable_search`   | `bool`           | `True`  | Enable web search functionality.                        |
| `enable_scrape`   | `bool`           | `True`  | Enable web scraping functionality.                      |
| `enable_crawl`    | `bool`           | `True`  | Enable web crawling functionality.                      |
| `all`             | `bool`           | `False` | Enable all tools. Overrides individual flags when True. |

| Function | Description                                                                                                                                                                                        |
| -------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| `search` | Searches the web for the given query. Parameters include `query` (str) for the search query and `max_results` (int, default=5) for maximum results. Returns search results in JSON format.         |
| `scrape` | Scrapes the content of a webpage. Parameters include `url` (str) for the URL of the webpage to scrape. Returns markdown of the webpage.                                                            |
| `crawl`  | Crawls the web starting from a URL. Parameters include `url` (str) for the URL to crawl and `limit` (Optional\[int], default=10) for maximum pages to crawl. Returns crawl results in JSON format. |

## Developer Resources

* View [Tools](https://github.com/agno-agi/agno/blob/main/libs/agno/agno/tools/spider.py)
* View [Cookbook](https://github.com/agno-agi/agno/tree/main/cookbook/tools/spider_tools.py)

**Examples:**

Example 1 (unknown):
```unknown
## Example

The following agent will run a search query to get the latest news in USA and scrape the first search result. The agent will return the scraped data in markdown format.
```

---

## Run the evaluation calling the arun method.

**URL:** llms-txt#run-the-evaluation-calling-the-arun-method.

result: Optional[AccuracyResult] = asyncio.run(evaluation.arun(print_results=True))
assert result is not None and result.avg_score >= 8
```

---

## OpenWeather

**URL:** llms-txt#openweather

**Contents:**
- Prerequisites
- Example

Source: https://docs.agno.com/concepts/tools/toolkits/others/openweather

**OpenWeatherTools** enable an Agent to access weather data from the OpenWeatherMap API.

The following example requires the `requests` library and an API key which can be obtained from [OpenWeatherMap](https://openweathermap.org/api). Once you sign up the mentioned api key will be activated in a few hours so please be patient.

The following agent will use OpenWeatherMap to get current weather information for Tokyo.

```python cookbook/tools/openweather_tools.py theme={null}
from agno.agent import Agent
from agno.tools.openweather import OpenWeatherTools

**Examples:**

Example 1 (unknown):
```unknown
## Example

The following agent will use OpenWeatherMap to get current weather information for Tokyo.
```

---

## -*- Make tool call

**URL:** llms-txt#-*--make-tool-call

agent.print_response("What is the weather in nyc?", stream=True)

---

## Strategic decision-making example

**URL:** llms-txt#strategic-decision-making-example

---

## - "What are the top 3 HN stories right now?"

**URL:** llms-txt#--"what-are-the-top-3-hn-stories-right-now?"

---

## Get your Supabase project and password

**URL:** llms-txt#get-your-supabase-project-and-password

SUPABASE_PROJECT = getenv("SUPABASE_PROJECT")
SUPABASE_PASSWORD = getenv("SUPABASE_PASSWORD")

SUPABASE_DB_URL = (
    f"postgresql://postgres:{SUPABASE_PASSWORD}@db.{SUPABASE_PROJECT}:5432/postgres"
)

---

## Example prompts:

**URL:** llms-txt#example-prompts:

"""
Customer Management:
- "Create a customer. Name: ACME Corp, Email: billing@acme.example.com"
- "List my customers."
- "Find customer by email 'jane.doe@example.com'" # Note: Requires 'customers.retrieve' or search capability

Product and Price Management:
- "Create a new product called 'Basic Plan'."
- "Create a recurring monthly price of $10 USD for product 'Basic Plan'."
- "Create a product 'Ebook Download' and a one-time price of $19.95 USD."
- "List all products." # Note: Requires 'products.list' capability
- "List all prices." # Note: Requires 'prices.list' capability

Payment Links:
- "Create a payment link for the $10 USD monthly 'Basic Plan' price."
- "Generate a payment link for the '$19.95 Ebook Download'."

Combined Tasks:
- "Create a product 'Pro Service', add a price $150 USD (one-time), and give me the payment link."
- "Register a new customer 'support@example.com' named 'Support Team'."
"""

---

## Create team with streaming capabilities

**URL:** llms-txt#create-team-with-streaming-capabilities

team = Team(
    name="Stock Research Team",
    model=OpenAIChat(id="gpt-5-mini"),
    members=[stock_searcher, company_info_agent],
    markdown=True,
    show_members_responses=True,
)

---

## Check reasoning_content

**URL:** llms-txt#check-reasoning_content

if hasattr(response, "reasoning_content") and response.reasoning_content:
    print("✅ reasoning_content FOUND in non-streaming response")
    print(f"   Length: {len(response.reasoning_content)} characters")
    print("\n=== reasoning_content preview (non-streaming) ===")
    preview = response.reasoning_content[:1000]
    if len(response.reasoning_content) > 1000:
        preview += "..."
    print(preview)
else:
    print("❌ reasoning_content NOT FOUND in non-streaming response")

---

## 4. EDUCATIONAL TUTORING WORKFLOW

**URL:** llms-txt#4.-educational-tutoring-workflow

---

## BaiduSearch

**URL:** llms-txt#baidusearch

**Contents:**
- Prerequisites
- Example
- Toolkit Params
- Toolkit Functions
- Developer Resources

Source: https://docs.agno.com/concepts/tools/toolkits/search/baidusearch

**BaiduSearch** enables an Agent to search the web for information using the Baidu search engine.

The following example requires the `baidusearch` library. To install BaiduSearch, run the following command:

| Parameter             | Type   | Default | Description                                                                                          |
| --------------------- | ------ | ------- | ---------------------------------------------------------------------------------------------------- |
| `fixed_max_results`   | `int`  | -       | Sets a fixed number of maximum results to return. No default is provided, must be specified if used. |
| `fixed_language`      | `str`  | -       | Set the fixed language for the results.                                                              |
| `headers`             | `Any`  | -       | Headers to be used in the search request.                                                            |
| `proxy`               | `str`  | -       | Specifies a single proxy address as a string to be used for the HTTP requests.                       |
| `timeout`             | `int`  | `10`    | Sets the timeout for HTTP requests, in seconds.                                                      |
| `enable_baidu_search` | `bool` | `True`  | Enable the baidu\_search functionality.                                                              |
| `all`                 | `bool` | `False` | Enable all functionality.                                                                            |

| Function       | Description                                    |
| -------------- | ---------------------------------------------- |
| `baidu_search` | Use this function to search Baidu for a query. |

## Developer Resources

* View [Tools](https://github.com/agno-agi/agno/blob/main/libs/agno/agno/tools/baidusearch.py)
* View [Cookbook](https://github.com/agno-agi/agno/tree/main/cookbook/tools/baidusearch_tools.py)

**Examples:**

Example 1 (unknown):
```unknown
## Example
```

---

## Nexus

**URL:** llms-txt#nexus

**Contents:**
- Authentication
- Example
- Params

Source: https://docs.agno.com/concepts/models/nexus

Learn how to use Nexus models in Agno.

Nexus is a routing platform that provides endpoints for various Large Language Models through a unified API interface.

Explore Nexus's capabilities and documentation [here](https://nexusrouter.com/).

Nexus requires API keys for the underlying model providers. Set the appropriate environment variables for the models you plan to use:

Use `Nexus` with your `Agent`:

<CodeGroup>
  
</CodeGroup>

<Note> View more examples [here](/examples/models/nexus/basic). </Note>

| Parameter  | Type            | Default                         | Description                                                 |
| ---------- | --------------- | ------------------------------- | ----------------------------------------------------------- |
| `id`       | `str`           | `"gpt-4o-mini"`                 | The id of the model to use through Nexus                    |
| `name`     | `str`           | `"Nexus"`                       | The name of the model                                       |
| `provider` | `str`           | `"Nexus"`                       | The provider of the model                                   |
| `api_key`  | `Optional[str]` | `None`                          | The API key for Nexus (defaults to NEXUS\_API\_KEY env var) |
| `base_url` | `str`           | `"https://api.nexusflow.ai/v1"` | The base URL for the Nexus API                              |

`Nexus` also supports the params of [OpenAI](/reference/models/openai).

**Examples:**

Example 1 (unknown):
```unknown

```

Example 2 (unknown):
```unknown
</CodeGroup>

## Example

Use `Nexus` with your `Agent`:

<CodeGroup>
```

---

## SingleStore

**URL:** llms-txt#singlestore

Source: https://docs.agno.com/reference/vector_db/singlestore

<Snippet file="vector-db-singlestore-reference.mdx" />

---

## Discord

**URL:** llms-txt#discord

**Contents:**
- Prerequisites
- Example
- Toolkit Params
- Toolkit Functions
- Developer Resources

Source: https://docs.agno.com/concepts/tools/toolkits/social/discord

**DiscordTools** enable an agent to send messages, read message history, manage channels, and delete messages in Discord.

The following example requires a Discord bot token which can be obtained from [here](https://discord.com/developers/applications).

| Parameter                   | Type   | Default | Description                                                   |
| --------------------------- | ------ | ------- | ------------------------------------------------------------- |
| `bot_token`                 | `str`  | -       | Discord bot token for authentication.                         |
| `enable_messaging`          | `bool` | `True`  | Whether to enable sending messages to channels.               |
| `enable_history`            | `bool` | `True`  | Whether to enable retrieving message history from channels.   |
| `enable_channel_management` | `bool` | `True`  | Whether to enable fetching channel info and listing channels. |
| `enable_message_management` | `bool` | `True`  | Whether to enable deleting messages from channels.            |

| Function               | Description                                                                                   |
| ---------------------- | --------------------------------------------------------------------------------------------- |
| `send_message`         | Send a message to a specified channel. Returns a success or error message.                    |
| `get_channel_info`     | Retrieve information about a specified channel. Returns the channel info as a JSON string.    |
| `list_channels`        | List all channels in a specified server (guild). Returns the list of channels as JSON.        |
| `get_channel_messages` | Retrieve message history from a specified channel. Returns messages as a JSON string.         |
| `delete_message`       | Delete a specific message by ID from a specified channel. Returns a success or error message. |

## Developer Resources

* View [Tools](https://github.com/agno-agi/agno/blob/main/libs/agno/agno/tools/discord.py)
* View [Cookbook](https://github.com/agno-agi/agno/tree/main/cookbook/tools/discord.py)

**Examples:**

Example 1 (unknown):
```unknown
## Example
```

---

## Accessing Multiple Previous Steps

**URL:** llms-txt#accessing-multiple-previous-steps

**Contents:**
- Example

Source: https://docs.agno.com/concepts/workflows/access-previous-steps

How to access multiple previous steps

Advanced workflows often require data from multiple previous steps beyond just the immediate predecessor. The `StepInput` object provides powerful methods to access any previous step's output by name or retrieve all accumulated content.

```python  theme={null}
def create_comprehensive_report(step_input: StepInput) -> StepOutput:
    """
    Custom function that creates a report using data from multiple previous steps.
    This function has access to ALL previous step outputs and the original workflow message.
    """

# Access original workflow input
    original_topic = step_input.workflow_message or ""

# Access specific step outputs by name
    hackernews_data = step_input.get_step_content("research_hackernews") or ""
    web_data = step_input.get_step_content("research_web") or ""

# Or access ALL previous content
    all_research = step_input.get_all_previous_content()

# Create a comprehensive report combining all sources
    report = f"""
        # Comprehensive Research Report: {original_topic}

## Executive Summary
        Based on research from HackerNews and web sources, here's a comprehensive analysis of {original_topic}.

## HackerNews Insights
        {hackernews_data[:500]}...

## Web Research Findings  
        {web_data[:500]}...
    """

return StepOutput(
        step_name="comprehensive_report", 
        content=report.strip(), 
        success=True
    )

---

## Overview

**URL:** llms-txt#overview

**Contents:**
- Guides

Source: https://docs.agno.com/tutorials/overview

Agno is a platform for building AI agents. It provides a set of tools and libraries to help you build and deploy AI agents.

---

## OpenAI o1 pro

**URL:** llms-txt#openai-o1-pro

**Contents:**
- Code
- Usage

Source: https://docs.agno.com/examples/concepts/reasoning/models/openai/o1-pro

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Set your API key">
    
  </Step>

<Step title="Install libraries">
    
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Set your API key">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Custom Functions in Workflows

**URL:** llms-txt#custom-functions-in-workflows

**Contents:**
- Example
- Class-based executor

Source: https://docs.agno.com/concepts/workflows/workflow-patterns/custom-function-step-workflow

How to use custom functions in workflows

Custom functions provide maximum flexibility by allowing you to define specific logic for step execution. Use them to preprocess inputs, orchestrate agents and teams, and postprocess outputs with complete programmatic control.

* **Custom Logic**: Implement complex business rules and data transformations
* **Agent Integration**: Call agents and teams within your custom processing logic
* **Data Flow Control**: Transform outputs between steps for optimal data handling

**Implementation Pattern**
Define a `Step` with a custom function as the `executor`. The function must accept a `StepInput` object and return a `StepOutput` object, ensuring seamless integration with the workflow system.

<img className="block dark:hidden" src="https://mintcdn.com/agno-v2/jBP_3mGLN1rT3Ezh/images/custom-function-steps-light.png?fit=max&auto=format&n=jBP_3mGLN1rT3Ezh&q=85&s=d9c94fbc2094b100df2fde1e4767f358" alt="Custom function step workflow diagram" data-og-width="2001" width="2001" data-og-height="756" height="756" data-path="images/custom-function-steps-light.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/agno-v2/jBP_3mGLN1rT3Ezh/images/custom-function-steps-light.png?w=280&fit=max&auto=format&n=jBP_3mGLN1rT3Ezh&q=85&s=cc6fdbdbcd274ffd8eeaf936653e9487 280w, https://mintcdn.com/agno-v2/jBP_3mGLN1rT3Ezh/images/custom-function-steps-light.png?w=560&fit=max&auto=format&n=jBP_3mGLN1rT3Ezh&q=85&s=7135a981cbf2afbbfc9e8a772843ca90 560w, https://mintcdn.com/agno-v2/jBP_3mGLN1rT3Ezh/images/custom-function-steps-light.png?w=840&fit=max&auto=format&n=jBP_3mGLN1rT3Ezh&q=85&s=084fd07440b87980f0899dea2b0b5ba1 840w, https://mintcdn.com/agno-v2/jBP_3mGLN1rT3Ezh/images/custom-function-steps-light.png?w=1100&fit=max&auto=format&n=jBP_3mGLN1rT3Ezh&q=85&s=9d57ad4d5f051dc65185bf29ad759118 1100w, https://mintcdn.com/agno-v2/jBP_3mGLN1rT3Ezh/images/custom-function-steps-light.png?w=1650&fit=max&auto=format&n=jBP_3mGLN1rT3Ezh&q=85&s=b1ba9db7305207a7867efa4ef47912dc 1650w, https://mintcdn.com/agno-v2/jBP_3mGLN1rT3Ezh/images/custom-function-steps-light.png?w=2500&fit=max&auto=format&n=jBP_3mGLN1rT3Ezh&q=85&s=a4e94cf04d1a1bb212f971ce60cefe5b 2500w" />

<img className="hidden dark:block" src="https://mintcdn.com/agno-v2/jBP_3mGLN1rT3Ezh/images/custom-function-steps-dark.png?fit=max&auto=format&n=jBP_3mGLN1rT3Ezh&q=85&s=537393db1d76a7d4c38d43e40c622300" alt="Custom function step workflow diagram" data-og-width="2001" width="2001" data-og-height="756" height="756" data-path="images/custom-function-steps-dark.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/agno-v2/jBP_3mGLN1rT3Ezh/images/custom-function-steps-dark.png?w=280&fit=max&auto=format&n=jBP_3mGLN1rT3Ezh&q=85&s=40450040ca47de4fa286b95de2e285ed 280w, https://mintcdn.com/agno-v2/jBP_3mGLN1rT3Ezh/images/custom-function-steps-dark.png?w=560&fit=max&auto=format&n=jBP_3mGLN1rT3Ezh&q=85&s=ef0a1060f07688a21b866766dc10526b 560w, https://mintcdn.com/agno-v2/jBP_3mGLN1rT3Ezh/images/custom-function-steps-dark.png?w=840&fit=max&auto=format&n=jBP_3mGLN1rT3Ezh&q=85&s=9c5572c23d25046db363ecaeeb65e3d6 840w, https://mintcdn.com/agno-v2/jBP_3mGLN1rT3Ezh/images/custom-function-steps-dark.png?w=1100&fit=max&auto=format&n=jBP_3mGLN1rT3Ezh&q=85&s=991e880aee26c31a9484d8603e0be424 1100w, https://mintcdn.com/agno-v2/jBP_3mGLN1rT3Ezh/images/custom-function-steps-dark.png?w=1650&fit=max&auto=format&n=jBP_3mGLN1rT3Ezh&q=85&s=0a8c70a163f2ba4a5cd5dfdc487ff14d 1650w, https://mintcdn.com/agno-v2/jBP_3mGLN1rT3Ezh/images/custom-function-steps-dark.png?w=2500&fit=max&auto=format&n=jBP_3mGLN1rT3Ezh&q=85&s=aec1041004efd198455fcb6a740f008d 2500w" />

**Standard Pattern**
All custom functions follow this consistent structure:

## Class-based executor

You can also use a class-based executor by defining a class that implements the `__call__` method.

**When is this useful?:**

* **Configuration at initialization**: Pass in settings, API keys, or behavior flags when creating the executor
* **Stateful execution**: Maintain counters, caches, or track information across multiple workflow runs
* **Reusable components**: Create configured executor instances that can be shared across multiple workflows

```python  theme={null}
class CustomExecutor:
    def __init__(self, max_retries: int = 3, use_cache: bool = True):
        # Configuration passed during instantiation
        self.max_retries = max_retries
        self.use_cache = use_cache
        self.call_count = 0  # Stateful tracking

def __call__(self, step_input: StepInput) -> StepOutput:
        self.call_count += 1

# Access instance configuration and state
        if self.use_cache and self.call_count > 1:
            return StepOutput(content="Using cached result")

# Your custom logic with access to self.max_retries, etc.
        return StepOutput(content=enhanced_content)

**Examples:**

Example 1 (unknown):
```unknown
**Standard Pattern**
All custom functions follow this consistent structure:
```

Example 2 (unknown):
```unknown
## Class-based executor

You can also use a class-based executor by defining a class that implements the `__call__` method.
```

Example 3 (unknown):
```unknown
**When is this useful?:**

* **Configuration at initialization**: Pass in settings, API keys, or behavior flags when creating the executor
* **Stateful execution**: Maintain counters, caches, or track information across multiple workflow runs
* **Reusable components**: Create configured executor instances that can be shared across multiple workflows
```

---

## Basic Streaming

**URL:** llms-txt#basic-streaming

**Contents:**
- Code

Source: https://docs.agno.com/examples/models/azure/openai/basic_stream

```python cookbook/models/azure/openai/basic_stream.py theme={null}
from typing import Iterator  # noqa

from agno.agent import Agent, RunOutputEvent  # noqa
from agno.models.azure import AzureOpenAI

agent = Agent(model=AzureOpenAI(id="gpt-5-mini"), markdown=True)

---

## Capture Reasoning Content with Reasoning Tools

**URL:** llms-txt#capture-reasoning-content-with-reasoning-tools

**Contents:**
- Code

Source: https://docs.agno.com/examples/concepts/reasoning/tools/capture-reasoning-content-reasoning-tools

```python cookbook/reasoning/tools/capture_reasoning_content_reasoning_tools.py theme={null}
from textwrap import dedent

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.reasoning import ReasoningTools

"""Test function to verify reasoning_content is populated in RunOutput."""
print("\n=== Testing reasoning_content generation ===\n")

---

## Replace with your own connection string, and notice the `async_` prefix

**URL:** llms-txt#replace-with-your-own-connection-string,-and-notice-the-`async_`-prefix

db_url = "postgresql+async_psycopg://ai:ai@localhost:5532/ai"

---

## Team Reliability with Stock Tools

**URL:** llms-txt#team-reliability-with-stock-tools

**Contents:**
- Code

Source: https://docs.agno.com/examples/concepts/evals/reliability/reliability_team_advanced

Learn how to evaluate team reliability with real-world tools like stock price lookup.

This example shows how to evaluate the reliability of a team using real-world tools.

---

## Our custom event, extending the CustomEvent class

**URL:** llms-txt#our-custom-event,-extending-the-customevent-class

@dataclass
class CustomerProfileEvent(CustomEvent):
    """CustomEvent for customer profile."""

customer_name: Optional[str] = None
    customer_email: Optional[str] = None
    customer_phone: Optional[str] = None

---

## Status

**URL:** llms-txt#status

Source: https://docs.agno.com/reference-api/schema/whatsapp/status

---

## Nvidia

**URL:** llms-txt#nvidia

**Contents:**
- Parameters

Source: https://docs.agno.com/reference/models/nvidia

The Nvidia model provides access to Nvidia's language models.

| Parameter  | Type            | Default                                    | Description                                                   |
| ---------- | --------------- | ------------------------------------------ | ------------------------------------------------------------- |
| `id`       | `str`           | `"nvidia/llama-3.1-nemotron-70b-instruct"` | The id of the NVIDIA model to use                             |
| `name`     | `str`           | `"NVIDIA"`                                 | The name of the model                                         |
| `provider` | `str`           | `"NVIDIA"`                                 | The provider of the model                                     |
| `api_key`  | `Optional[str]` | `None`                                     | The API key for NVIDIA (defaults to NVIDIA\_API\_KEY env var) |
| `base_url` | `str`           | `"https://integrate.api.nvidia.com/v1"`    | The base URL for the NVIDIA API                               |

NVIDIA extends the OpenAI-compatible interface and supports most parameters from OpenAI.

---

## Groq DeepSeek R1

**URL:** llms-txt#groq-deepseek-r1

**Contents:**
- Code
- Usage

Source: https://docs.agno.com/examples/concepts/reasoning/models/groq/groq-basic

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Set your API key">
    
  </Step>

<Step title="Install libraries">
    
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Set your API key">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Production Best Practices

**URL:** llms-txt#production-best-practices

**Contents:**
- Quick Reference
- The Agentic Memory Token Trap

Source: https://docs.agno.com/concepts/memory/production-best-practices

Avoid common pitfalls, optimize costs, and ensure reliable memory behavior in production.

Memory is powerful, but without careful configuration, it can lead to unexpected token consumption, behavioral issues, and high costs. This guide shows you what to watch out for and how to optimize your memory usage for production.

* **Default to automatic memory** (`enable_user_memories=True`) unless you have a specific reason for agentic control
* **Always provide user\_id**, don't rely on the default "default" user
* **Use cheaper models** for memory operations when using agentic memory
* **Implement pruning** for long-running applications
* **Monitor token usage** in production to catch memory-related cost spikes
* **Test with realistic data**: 100+ memories behave very differently than 5 memories

## The Agentic Memory Token Trap

**The Problem:** When you use `enable_agentic_memory=True`, every memory operation triggers a **separate, nested LLM call**. This architecture can cause token usage to explode, especially as memories accumulate.

Here's what happens under the hood:

1. User sends a message → Main LLM call processes it
2. Agent decides to update memory → Calls `update_user_memory` tool
3. **Nested LLM call fires** with:
   * Detailed system prompt (\~50 lines)
   * ALL existing user memories loaded into context
   * Memory management instructions and tools
4. Memory LLM makes tool calls (add, update, delete)
5. Control returns to main conversation

**Real-world impact:**

```python  theme={null}

---

## Python Tools

**URL:** llms-txt#python-tools

**Contents:**
- Code
- Usage

Source: https://docs.agno.com/examples/concepts/tools/local/python

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Set your API key">
    
  </Step>

<Step title="Install libraries">
    
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Set your API key">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Debug search quality

**URL:** llms-txt#debug-search-quality

**Contents:**
  - Issue: Content Loading is Slow

results = knowledge.search("your query", max_results=10)
if not results:
    content_list, count = knowledge.get_content()
    print(f"Total content items: {count}")
    
    # Check for failed content
    for content in content_list[:5]:
        status, message = knowledge.get_content_status(content.id)
        print(f"{content.name}: {status}")
python  theme={null}

**Examples:**

Example 1 (unknown):
```unknown
### Issue: Content Loading is Slow

**What's happening:** Processing large files without batching, or using semantic chunking on huge datasets.

**Quick fixes:**

1. Use `skip_if_exists=True` to avoid reprocessing
2. Switch to fixed-size chunking for faster processing
3. Process in batches instead of all at once
4. Use file filters to only process what you need
```

---

## ...

**URL:** llms-txt#...

**Contents:**
- Developer Resources

workflow_tools = WorkflowTools(
    workflow=blog_post_workflow,
)

agent = Agent(
    model=OpenAIChat(id="gpt-5-mini"),
    tools=[workflow_tools],
    markdown=True,
)

agent.print_response("Create a blog post on the topic: AI trends in 2024", stream=True)
```

See the [Workflow Tools](/concepts/tools/reasoning_tools/workflow-tools) documentation for more details.

## Developer Resources

* View [Agents with Reasoning Tools Examples](/examples/concepts/reasoning/tools)
* View [Agents with Reasoning Tools Cookbook](https://github.com/agno-agi/agno/tree/main/cookbook/reasoning/tools)
* View [Teams with Reasoning Tools Examples](/examples/concepts/reasoning/teams/reasoning-finance-team)
* View [Teams with Reasoning Tools Cookbook](https://github.com/agno-agi/agno/tree/main/cookbook/reasoning/teams)

---

## Postgres for Workflows

**URL:** llms-txt#postgres-for-workflows

**Contents:**
- Usage
  - Run PgVector

Source: https://docs.agno.com/examples/concepts/db/postgres/postgres_for_workflow

Agno supports using PostgreSQL as a storage backend for Workflows using the `PostgresDb` class.

Install [docker desktop](https://docs.docker.com/desktop/install/mac-install/) and run **PgVector** on port **5532** using:

```python postgres_for_workflow.py theme={null}
from agno.agent import Agent
from agno.db.postgres import PostgresDb
from agno.models.openai import OpenAIChat
from agno.team import Team
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.hackernews import HackerNewsTools
from agno.workflow.step import Step
from agno.workflow.workflow import Workflow

db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"

**Examples:**

Example 1 (unknown):
```unknown

```

---

## Run workflow with image input

**URL:** llms-txt#run-workflow-with-image-input

**Contents:**
  - Developer Resources

if __name__ == "__main__":
    media_workflow.print_response(
        input="Please analyze this image and find related news",
        images=[
            Image(url="https://upload.wikimedia.org/wikipedia/commons/0/0c/GoldenGateBridge-001.jpg")
        ],
        markdown=True,
    )
python  theme={null}
  from agno.run.workflow import WorkflowRunOutput

response: WorkflowRunOutput = media_workflow.run(
      input="Please analyze this image and find related news",
      images=[
          Image(url="https://upload.wikimedia.org/wikipedia/commons/0/0c/GoldenGateBridge-001.jpg")
      ],
      markdown=True,
  )

print(response.images)
  ```
</Note>

Similarly, you can pass `Video` and `Audio` as input.

### Developer Resources

* [Image/Video Selection Sequence](/examples/concepts/workflows/05_workflows_conditional_branching/selector_for_image_video_generation_pipelines)

**Examples:**

Example 1 (unknown):
```unknown
<Note>
  If you are using `Workflow.run()`, you need to use `WorkflowRunOutput` to access the images, videos, and audio.
```

---

## Returns: [Document(chunk1), Document(chunk2), Document(chunk3), ...]

**URL:** llms-txt#returns:-[document(chunk1),-document(chunk2),-document(chunk3),-...]

**Contents:**
  - Chunking Strategy Support
- Reader Factory and Auto-Selection

python  theme={null}
@classmethod
def get_supported_chunking_strategies(cls) -> List[ChunkingStrategyType]:
    return [
        ChunkingStrategyType.DOCUMENT_CHUNKING,  # Respect document structure
        ChunkingStrategyType.FIXED_SIZE_CHUNKING, # Fixed character/token limits
        ChunkingStrategyType.SEMANTIC_CHUNKING,   # Semantic boundaries
        ChunkingStrategyType.AGENTIC_CHUNKING,    # AI-powered chunking
    ]
python  theme={null}

**Examples:**

Example 1 (unknown):
```unknown
### Chunking Strategy Support

Different readers support different chunking strategies based on their content type:
```

Example 2 (unknown):
```unknown
## Reader Factory and Auto-Selection

Agno provides intelligent reader selection through the `ReaderFactory`:
```

---

## Save the audio response if available

**URL:** llms-txt#save-the-audio-response-if-available

if run_response.response_audio is not None:
    write_audio_to_file(
        audio=run_response.response_audio.content, filename="tmp/response.wav"
    )

---

## OpenAI Like

**URL:** llms-txt#openai-like

**Contents:**
- Parameters

Source: https://docs.agno.com/reference/models/openai_like

The OpenAI Like model works as a wrapper for the OpenAILike models.

| Parameter                            | Type                              | Default                       | Description                                                           |
| ------------------------------------ | --------------------------------- | ----------------------------- | --------------------------------------------------------------------- |
| `id`                                 | `str`                             | `"gpt-4o"`                    | The id of the model to use                                            |
| `name`                               | `str`                             | `"OpenAILike"`                | The name of the model                                                 |
| `provider`                           | `str`                             | `"OpenAILike"`                | The provider of the model                                             |
| `api_key`                            | `Optional[str]`                   | `None`                        | The API key for authentication (defaults to OPENAI\_API\_KEY env var) |
| `base_url`                           | `str`                             | `"https://api.openai.com/v1"` | The base URL for the API                                              |
| `supports_native_structured_outputs` | `Optional[bool]`                  | `None`                        | Whether the model supports native structured outputs                  |
| `response_format`                    | `Optional[str]`                   | `None`                        | The format of the response                                            |
| `seed`                               | `Optional[int]`                   | `None`                        | Random seed for deterministic sampling                                |
| `stop`                               | `Optional[Union[str, List[str]]]` | `None`                        | Up to 4 sequences where the API will stop generating further tokens   |
| `stream`                             | `bool`                            | `True`                        | Whether to stream the response                                        |
| `temperature`                        | `Optional[float]`                 | `None`                        | Controls randomness in the model's output                             |
| `top_p`                              | `Optional[float]`                 | `None`                        | Controls diversity via nucleus sampling                               |
| `request_params`                     | `Optional[Dict[str, Any]]`        | `None`                        | Additional parameters to include in the request                       |
| `client_params`                      | `Optional[Dict[str, Any]]`        | `None`                        | Additional parameters for client configuration                        |

---

## MongoDb

**URL:** llms-txt#mongodb

Source: https://docs.agno.com/reference/vector_db/mongodb

<Snippet file="vector-db-mongodb-reference.mdx" />

---

## Create Postgres-backed vector store

**URL:** llms-txt#create-postgres-backed-vector-store

vector_db = PgVector(
    db_url=db_url,
    table_name="agno_docs",
)
knowledge = Knowledge(
    name="Agno Docs",
    contents_db=db,
    vector_db=vector_db,
)

---

## Schedule a meeting

**URL:** llms-txt#schedule-a-meeting

**Contents:**
- Toolkit Params
- Toolkit Functions
- Rate Limits
- Developer Resources

response = agent.print_response("""
Schedule a team meeting with the following details:
- Topic: Weekly Team Sync
- Time: Tomorrow at 2 PM UTC
- Duration: 45 minutes
""", markdown=True)
```

| Parameter       | Type            | Default | Description                                                                          |
| --------------- | --------------- | ------- | ------------------------------------------------------------------------------------ |
| `account_id`    | `Optional[str]` | `None`  | Zoom account ID. If not provided, uses ZOOM\_ACCOUNT\_ID environment variable.       |
| `client_id`     | `Optional[str]` | `None`  | Zoom client ID. If not provided, uses ZOOM\_CLIENT\_ID environment variable.         |
| `client_secret` | `Optional[str]` | `None`  | Zoom client secret. If not provided, uses ZOOM\_CLIENT\_SECRET environment variable. |

| Function                 | Description                                       |
| ------------------------ | ------------------------------------------------- |
| `schedule_meeting`       | Schedule a new Zoom meeting                       |
| `get_upcoming_meetings`  | Get a list of upcoming meetings                   |
| `list_meetings`          | List all meetings based on type                   |
| `get_meeting_recordings` | Get recordings for a specific meeting             |
| `delete_meeting`         | Delete a scheduled meeting                        |
| `get_meeting`            | Get detailed information about a specific meeting |

You can use `include_tools` or `exclude_tools` to modify the list of tools the agent has access to. Learn more about [selecting tools](/concepts/tools/selecting-tools).

The Zoom API has rate limits that vary by endpoint and account type:

* Server-to-Server OAuth apps: 100 requests/second
* Meeting endpoints: Specific limits apply based on account type
* Recording endpoints: Lower rate limits, check Zoom documentation

For detailed rate limits, refer to [Zoom API Rate Limits](https://developers.zoom.us/docs/api/#rate-limits).

## Developer Resources

* View [Tools](https://github.com/agno-agi/agno/blob/main/libs/agno/agno/tools/zoom.py)
* View [Cookbook](https://github.com/agno-agi/agno/tree/main/cookbook/tools/zoom_tools.py)

---

## OpenBB

**URL:** llms-txt#openbb

Source: https://docs.agno.com/concepts/tools/toolkits/others/openbb

**OpenBBTools** enable an Agent to provide information about stocks and companies.

```python cookbook/tools/openbb_tools.py theme={null}
from agno.agent import Agent
from agno.tools.openbb import OpenBBTools

agent = Agent(tools=[OpenBBTools()], debug_mode=True)

---

## Initialize GCSJsonDb with explicit credentials, unique bucket name, and project.

**URL:** llms-txt#initialize-gcsjsondb-with-explicit-credentials,-unique-bucket-name,-and-project.

db = GcsJsonDb(
    bucket_name=unique_bucket_name,
    prefix="agent/",
    project=project_id,
    credentials=credentials,
)

---

## Production: Scalable, battle-tested

**URL:** llms-txt#production:-scalable,-battle-tested

**Contents:**
  - 2. Skip Already-Processed Files

prod_db = PgVector(
    table_name="prod_knowledge",
    db_url="postgresql+psycopg://user:pass@db:5432/knowledge"
)
python  theme={null}

**Examples:**

Example 1 (unknown):
```unknown
**Guidelines:**

* **LanceDB** for development and testing (no setup required)
* **PgVector** for production (up to 1M documents, need SQL features)
* **Pinecone** for managed services (no ops overhead, auto-scaling)

### 2. Skip Already-Processed Files

The single biggest speed-up for re-running your ingestion:
```

---

## Define a tool that uses dependencies claims

**URL:** llms-txt#define-a-tool-that-uses-dependencies-claims

def get_user_details(dependencies: dict):
    """
    Get the current user's details.
    """
    return {
        "name": dependencies.get("name"),
        "email": dependencies.get("email"),
        "roles": dependencies.get("roles"),
    }

---

## Multi-User, Multi-Session Chat Concurrently

**URL:** llms-txt#multi-user,-multi-session-chat-concurrently

**Contents:**
- Code
- Usage

Source: https://docs.agno.com/examples/concepts/memory/06-multi-user-multi-session-chat-concurrent

This example shows how to run a multi-user, multi-session chat concurrently.

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Set your API key">
    
  </Step>

<Step title="Install libraries">
    
  </Step>

<Step title="Run Example">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Set your API key">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Example">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Ollama DeepSeek R1

**URL:** llms-txt#ollama-deepseek-r1

**Contents:**
- Code
- Usage

Source: https://docs.agno.com/examples/concepts/reasoning/models/ollama/ollama-basic

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Install Ollama">
    Follow the [installation guide](https://github.com/ollama/ollama?tab=readme-ov-file#macos) and run:

<Step title="Install libraries">
    
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Install Ollama">
    Follow the [installation guide](https://github.com/ollama/ollama?tab=readme-ov-file#macos) and run:
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Print the aggregated metrics for the whole session

**URL:** llms-txt#print-the-aggregated-metrics-for-the-whole-session

**Contents:**
- Developer Resources

print("---" * 5, "Session Metrics", "---" * 5)
pprint(agent.get_session_metrics().to_dict())
```

You'll see the outputs with following information:

* `input_tokens`: The number of tokens sent to the model.
* `output_tokens`: The number of tokens received from the model.
* `total_tokens`: The sum of `input_tokens` and `output_tokens`.
* `audio_input_tokens`: The number of tokens sent to the model for audio input.
* `audio_output_tokens`: The number of tokens received from the model for audio output.
* `audio_total_tokens`: The sum of `audio_input_tokens` and `audio_output_tokens`.
* `cache_read_tokens`: The number of tokens read from the cache.
* `cache_write_tokens`: The number of tokens written to the cache.
* `reasoning_tokens`: The number of tokens used for reasoning.
* `duration`: The duration of the run in seconds.
* `time_to_first_token`: The time taken until the first token was generated.
* `provider_metrics`: Any provider-specific metrics.

## Developer Resources

* View the [RunOutput schema](/reference/agents/run-response)
* View the [Metrics schema](/reference/agents/metrics)
* View [Cookbook](https://github.com/agno-agi/agno/tree/main/cookbook/agents/other/agent_metrics.py)

---

## Run the team with a task

**URL:** llms-txt#run-the-team-with-a-task

**Contents:**
- Usage

content_team.print_response("Create a short article about quantum computing")
bash  theme={null}
    pip install agno ddgs
    bash  theme={null}
    export OPENAI_API_KEY=****
    bash  theme={null}
    python cookbook/examples/teams/coordinate_mode/content_team.py
    ```
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Install required libraries">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Set environment variables">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run the agent">
```

---

## Example 3: Address Validation and Geocoding

**URL:** llms-txt#example-3:-address-validation-and-geocoding

print("\n=== Address Validation and Geocoding Example ===")
agent.print_response(
    """Please validate and geocode this address: 
    '1600 Amphitheatre Parkway, Mountain View, CA'""",
    markdown=True,
    stream=True,
)

---

## print(async_response.content)

**URL:** llms-txt#print(async_response.content)

---

## Create a Steps sequence that chains these above steps together

**URL:** llms-txt#create-a-steps-sequence-that-chains-these-above-steps-together

article_creation_sequence = Steps(
    name="article_creation",
    description="Complete article creation workflow from research to final edit",
    steps=[research_step, writing_step, editing_step],
)

---

## Pre-hooks

**URL:** llms-txt#pre-hooks

**Contents:**
- Parameters

Source: https://docs.agno.com/reference/hooks/pre-hooks

Running a pre-hook is handled automatically during the Agent or Team run. These are the parameters that will be injected:

| Parameter       | Type                       | Default  | Description                                                                  |
| --------------- | -------------------------- | -------- | ---------------------------------------------------------------------------- |
| `agent`         | `Agent`                    | Required | The Agent that is running the pre-hook. Only present in Agent runs.          |
| `team`          | `Team`                     | Required | The Team that is running the pre-hook. Only present in Team runs.            |
| `run_input`     | `RunInput`                 | Required | The input provided to the Agent or Team when invoking the run.               |
| `session`       | `AgentSession`             | Required | The `AgentSession` or `TeamSession` object representing the current session. |
| `session_state` | `Optional[Dict[str, Any]]` | `None`   | The session state of the current session.                                    |
| `dependencies`  | `Optional[Dict[str, Any]]` | `None`   | The dependencies of the current run.                                         |
| `metadata`      | `Optional[Dict[str, Any]]` | `None`   | The metadata of the current run.                                             |
| `user_id`       | `Optional[str]`            | `None`   | The contextual user ID, if any.                                              |
| `debug_mode`    | `Optional[bool]`           | `None`   | Whether the debug mode is enabled.                                           |

---

## Initialize LanceDB

**URL:** llms-txt#initialize-lancedb

---

## Step-level: override for specific steps

**URL:** llms-txt#step-level:-override-for-specific-steps

**Contents:**
- Developer Resources

Step("Analysis", agent=analysis_agent, 
     add_workflow_history=True,
     num_history_runs=3  # Only last 3 runs for this step
)
```

## Developer Resources

Explore the different examples in [Workflow History Examples](/examples/concepts/workflows/06_workflows_advanced_concepts/workflow_history/01_single_step_continuous_execution_workflow) for more details.

---

## DeepInfra

**URL:** llms-txt#deepinfra

**Contents:**
- Parameters

Source: https://docs.agno.com/reference/models/deepinfra

The DeepInfra model provides access to DeepInfra's hosted language models.

| Parameter  | Type            | Default                                 | Description                                                         |
| ---------- | --------------- | --------------------------------------- | ------------------------------------------------------------------- |
| `id`       | `str`           | `"meta-llama/Llama-2-70b-chat-hf"`      | The id of the DeepInfra model to use                                |
| `name`     | `str`           | `"DeepInfra"`                           | The name of the model                                               |
| `provider` | `str`           | `"DeepInfra"`                           | The provider of the model                                           |
| `api_key`  | `Optional[str]` | `None`                                  | The API key for DeepInfra (defaults to DEEPINFRA\_API\_KEY env var) |
| `base_url` | `str`           | `"https://api.deepinfra.com/v1/openai"` | The base URL for the DeepInfra API                                  |

DeepInfra extends the OpenAI-compatible interface and supports most parameters from OpenAI.

---

## Skip If Exists

**URL:** llms-txt#skip-if-exists

**Contents:**
- Code

Source: https://docs.agno.com/examples/concepts/knowledge/basic-operations/skip-if-exists

```python 11_skip_if_exists.py theme={null}
import asyncio
from agno.knowledge.knowledge import Knowledge
from agno.vectordb.pgvector import PgVector

knowledge = Knowledge(
    name="Basic SDK Knowledge Base",
    description="Agno 2.0 Knowledge Implementation",
    vector_db=PgVector(
        table_name="vectors", db_url="postgresql+psycopg://ai:ai@localhost:5532/ai"
    ),
)

---

## Bitbucket

**URL:** llms-txt#bitbucket

**Contents:**
- Example
- Toolkit Params
- Toolkit Functions
- Developer Resources

Source: https://docs.agno.com/concepts/tools/toolkits/others/bitbucket

BitbucketTools enables agents to interact with Bitbucket repositories for managing code, pull requests, and issues.

The following agent can manage Bitbucket repositories:

| Parameter     | Type            | Default               | Description                                                  |
| ------------- | --------------- | --------------------- | ------------------------------------------------------------ |
| `server_url`  | `str`           | `"api.bitbucket.org"` | Bitbucket server URL (for Bitbucket Server instances).       |
| `username`    | `Optional[str]` | `None`                | Bitbucket username. Uses BITBUCKET\_USERNAME if not set.     |
| `password`    | `Optional[str]` | `None`                | Bitbucket app password. Uses BITBUCKET\_PASSWORD if not set. |
| `token`       | `Optional[str]` | `None`                | Access token. Uses BITBUCKET\_TOKEN if not set.              |
| `workspace`   | `Optional[str]` | `None`                | Bitbucket workspace name (required).                         |
| `repo_slug`   | `Optional[str]` | `None`                | Repository slug name (required).                             |
| `api_version` | `str`           | `"2.0"`               | Bitbucket API version to use.                                |

| Function                   | Description                                             |
| -------------------------- | ------------------------------------------------------- |
| `get_issue`                | Get details of a specific issue by ID.                  |
| `get_issues`               | List all issues in the repository.                      |
| `create_issue`             | Create a new issue in the repository.                   |
| `update_issue`             | Update an existing issue.                               |
| `get_pull_request`         | Get details of a specific pull request.                 |
| `get_pull_requests`        | List all pull requests in the repository.               |
| `create_pull_request`      | Create a new pull request.                              |
| `update_pull_request`      | Update an existing pull request.                        |
| `get_pull_request_diff`    | Get the diff/changes of a pull request.                 |
| `get_pull_request_commits` | Get commits associated with a pull request.             |
| `get_repository_info`      | Get detailed information about the repository.          |
| `get_branches`             | List all branches in the repository.                    |
| `get_commits`              | List commits in the repository.                         |
| `get_file_content`         | Get the content of a specific file from the repository. |

You can use `include_tools` or `exclude_tools` to modify the list of tools the agent has access to. Learn more about [selecting tools](/concepts/tools/selecting-tools).

## Developer Resources

* View [Tools Source](https://github.com/agno-agi/agno/blob/main/libs/agno/agno/tools/bitbucket.py)
* [Bitbucket API Documentation](https://developer.atlassian.com/bitbucket/api/2/reference/)

---

## File

**URL:** llms-txt#file

**Contents:**
- Example
- Toolkit Params
- Toolkit Functions
- Developer Resources

Source: https://docs.agno.com/concepts/tools/toolkits/local/file

**FileTools** enable an Agent to read and write files on the local file system.

The following agent will generate an answer and save it in a file.

| Parameter             | Type   | Default | Description                                           |
| --------------------- | ------ | ------- | ----------------------------------------------------- |
| `base_dir`            | `Path` | `None`  | Specifies the base directory path for file operations |
| `enable_save_file`    | `bool` | `True`  | Enables functionality to save files                   |
| `enable_read_file`    | `bool` | `True`  | Enables functionality to read files                   |
| `enable_list_files`   | `bool` | `True`  | Enables functionality to list files in directories    |
| `enable_search_files` | `bool` | `True`  | Enables functionality to search for files             |
| `all`                 | `bool` | `False` | Enables all functionality when set to True            |

| Name         | Description                                                                              |
| ------------ | ---------------------------------------------------------------------------------------- |
| `save_file`  | Saves the contents to a file called `file_name` and returns the file name if successful. |
| `read_file`  | Reads the contents of the file `file_name` and returns the contents if successful.       |
| `list_files` | Returns a list of files in the base directory                                            |

## Developer Resources

* View [Tools](https://github.com/agno-agi/agno/blob/main/libs/agno/agno/tools/file.py)
* View [Cookbook](https://github.com/agno-agi/agno/tree/main/cookbook/tools/file_tools.py)

---

## ClickUp Tools

**URL:** llms-txt#clickup-tools

**Contents:**
- Code
- Usage

Source: https://docs.agno.com/examples/concepts/tools/others/clickup

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Set your API keys">
    
  </Step>

<Step title="Install libraries">
    
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Set your API keys">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## OpenAI with Reasoning Tools

**URL:** llms-txt#openai-with-reasoning-tools

**Contents:**
- Code
- Usage

Source: https://docs.agno.com/examples/concepts/reasoning/tools/openai-reasoning-tools

This example shows how to use `ReasoningTools` with an OpenAI model.

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Set your API key">
    
  </Step>

<Step title="Install libraries">
    
  </Step>

<Step title="Run Example">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Set your API key">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Example">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Test with vulnerable code - workflow stops at security gate

**URL:** llms-txt#test-with-vulnerable-code---workflow-stops-at-security-gate

**Contents:**
- Developer Resources

workflow.print_response("Scan this code: exec(input('Enter command: '))")
```

## Developer Resources

* [Early Stop Workflow](/examples/concepts/workflows/06_workflows_advanced_concepts/early_stop_workflow)

---

## Verbose debug information

**URL:** llms-txt#verbose-debug-information

**Contents:**
- Usage

agent = Agent(
    model=Claude(id="claude-3-5-sonnet-20240620"),
    tools=[DuckDuckGoTools()],
    debug_mode=True,
    debug_level=2,
)

agent.print_response("What is the current price of Apple?")
bash  theme={null}
    pip install -U agno anthropic ddgs
    bash Mac/Linux theme={null}
        export ANTHROPIC_API_KEY="your-anthropic-api-key"
      bash Windows theme={null}
        $Env:export ANTHROPIC_API_KEY="your-anthropic-api-key"
      bash  theme={null}
    touch debug_level.py
    bash Mac theme={null}
      python debug_level.py
      bash Windows   theme={null}
      python debug_level.py
      ```
    </CodeGroup>
  </Step>

<Step title="Find All Cookbooks">
    Explore all the available cookbooks in the Agno repository. Click the link below to view the code on GitHub:

<Link href="https://github.com/agno-agi/agno/tree/main/cookbook/agents/other" target="_blank">
      Agno Cookbooks on GitHub
    </Link>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Install libraries">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Export your ANTHROPIC API key">
    <CodeGroup>
```

Example 3 (unknown):
```unknown

```

Example 4 (unknown):
```unknown
</CodeGroup>
  </Step>

  <Step title="Create a Python file">
    Create a Python file and add the above code.
```

---

## Run the async test

**URL:** llms-txt#run-the-async-test

---

## Mistral Small

**URL:** llms-txt#mistral-small

**Contents:**
- Code
- Usage

Source: https://docs.agno.com/examples/models/mistral/mistral_small

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Set your API key">
    
  </Step>

<Step title="Install libraries">
    
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Set your API key">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Custom API Tools

**URL:** llms-txt#custom-api-tools

**Contents:**
- Code
- Usage

Source: https://docs.agno.com/examples/concepts/tools/others/custom_api

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Set your API key">
    
  </Step>

<Step title="Install libraries">
    
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Set your API key">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Test 1: Non-streaming mode

**URL:** llms-txt#test-1:-non-streaming-mode

print("Running with stream=False...")
response = agent.run("What is the sum of the first 10 natural numbers?", stream=False)

---

## Brave Search Tools

**URL:** llms-txt#brave-search-tools

**Contents:**
- Code
- Usage

Source: https://docs.agno.com/examples/concepts/tools/search/bravesearch

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Set your API credentials">
    
  </Step>

<Step title="Install libraries">
    
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Set your API credentials">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Delete operations

**URL:** llms-txt#delete-operations

vector_db.delete_by_name("Recipes")

---

## Continue the session with user 2

**URL:** llms-txt#continue-the-session-with-user-2

team.print_response("What is the speed of light?", user_id=user_2_id, session_id=user_2_session_id)

---

## Search for information and process the results

**URL:** llms-txt#search-for-information-and-process-the-results

**Contents:**
  - 2. Website Content Crawler

agent.print_response("What are the latest developments in large language models?", markdown=True)
python  theme={null}
from agno.agent import Agent
from agno.tools.apify import ApifyTools

agent = Agent(
    tools=[
        ApifyTools(actors=["apify/website-content-crawler"])
    ],
    markdown=True
)

**Examples:**

Example 1 (unknown):
```unknown
### 2. Website Content Crawler

This tool uses Apify's [Website Content Crawler](https://apify.com/apify/website-content-crawler) Actor to extract text content from websites, making it perfect for RAG applications.
```

---

## Conditional fact-checking step

**URL:** llms-txt#conditional-fact-checking-step

fact_check_step = Step(
    name="fact_check",
    description="Verify facts and claims",
    agent=fact_checker,
)

write_article = Step(
    name="write_article",
    description="Write final article",
    agent=writer,
)

---

## Read the image file content as bytes

**URL:** llms-txt#read-the-image-file-content-as-bytes

**Contents:**
- Usage

image_bytes = image_path.read_bytes()

agent.print_response(
    "Tell me about this image and give me the latest news about it.",
    images=[
        Image(content=image_bytes),
    ],
    stream=True,
)

bash  theme={null}
    export XAI_API_KEY=xxx
    bash  theme={null}
    pip install -U xai ddgs agno
    bash Mac theme={null}
      python cookbook/models/xai/image_agent_bytes.py
      bash Windows theme={null}
      python cookbook/models/xai/image_agent_bytes.py
      ```
    </CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Set your API key">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## More example prompts to try:

**URL:** llms-txt#more-example-prompts-to-try:

**Contents:**
- Usage

"""
Try these research topics:
1. "Analyze the current state of solid-state batteries"
2. "Research recent breakthroughs in CRISPR gene editing"
3. "Investigate the development of autonomous vehicles"
4. "Explore advances in quantum machine learning"
5. "Study the impact of artificial intelligence on healthcare"
"""
bash  theme={null}
    pip install openai exa-py agno
    bash  theme={null}
    python research_agent.py
    ```
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Install libraries">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Run the agent">
```

---

## Claude

**URL:** llms-txt#claude

**Contents:**
- Parameters

Source: https://docs.agno.com/reference/models/anthropic

The Claude model provides access to Anthropic's Claude models.

| Parameter             | Type                                     | Default                        | Description                                                     |
| --------------------- | ---------------------------------------- | ------------------------------ | --------------------------------------------------------------- |
| `id`                  | `str`                                    | `"claude-3-5-sonnet-20241022"` | The id of the Anthropic Claude model to use                     |
| `name`                | `str`                                    | `"Claude"`                     | The name of the model                                           |
| `provider`            | `str`                                    | `"Anthropic"`                  | The provider of the model                                       |
| `max_tokens`          | `Optional[int]`                          | `4096`                         | Maximum number of tokens to generate in the chat completion     |
| `thinking`            | `Optional[Dict[str, Any]]`               | `None`                         | Configuration for the thinking (reasoning) process              |
| `temperature`         | `Optional[float]`                        | `None`                         | Controls randomness in the model's output                       |
| `stop_sequences`      | `Optional[List[str]]`                    | `None`                         | A list of strings that the model should stop generating text at |
| `top_p`               | `Optional[float]`                        | `None`                         | Controls diversity via nucleus sampling                         |
| `top_k`               | `Optional[int]`                          | `None`                         | Controls diversity via top-k sampling                           |
| `cache_system_prompt` | `Optional[bool]`                         | `False`                        | Whether to cache the system prompt for improved performance     |
| `extended_cache_time` | `Optional[bool]`                         | `False`                        | Whether to use extended cache time (1 hour instead of default)  |
| `request_params`      | `Optional[Dict[str, Any]]`               | `None`                         | Additional parameters to include in the request                 |
| `mcp_servers`         | `Optional[List[MCPServerConfiguration]]` | `None`                         | List of MCP (Model Context Protocol) server configurations      |
| `api_key`             | `Optional[str]`                          | `None`                         | The API key for authenticating with Anthropic                   |
| `default_headers`     | `Optional[Dict[str, Any]]`               | `None`                         | Default headers to include in all requests                      |
| `client_params`       | `Optional[Dict[str, Any]]`               | `None`                         | Additional parameters for client configuration                  |
| `client`              | `Optional[AnthropicClient]`              | `None`                         | A pre-configured instance of the Anthropic client               |
| `async_client`        | `Optional[AsyncAnthropicClient]`         | `None`                         | A pre-configured instance of the async Anthropic client         |

---

## 2. MEDICAL CONSULTATION WORKFLOW

**URL:** llms-txt#2.-medical-consultation-workflow

---

## Configure Agno to use our custom logger. It will be used for all logging.

**URL:** llms-txt#configure-agno-to-use-our-custom-logger.-it-will-be-used-for-all-logging.

configure_agno_logging(custom_default_logger=custom_logger)

---

## Define steps for video pipeline

**URL:** llms-txt#define-steps-for-video-pipeline

generate_video_step = Step(
    name="generate_video",
    agent=video_generator,
    description="Create a comprehensive video production plan and storyboard",
)

describe_video_step = Step(
    name="describe_video",
    agent=video_describer,
    description="Analyze and critique the video production plan with professional insights",
)

---

## End condition function

**URL:** llms-txt#end-condition-function

def research_evaluator(outputs: List[StepOutput]) -> bool:
    """
    Evaluate if research results are sufficient
    Returns True to break the loop, False to continue
    """
    # Check if we have good research results
    if not outputs:
        return False

# Calculate total content length from all outputs
    total_content_length = sum(len(output.content or "") for output in outputs)

# Check if we have substantial content (more than 500 chars total)
    if total_content_length > 500:
        print(
            f"✅ Research evaluation passed - found substantial content ({total_content_length} chars total)"
        )
        return True

print(
        f"❌ Research evaluation failed - need more substantial research (current: {total_content_length} chars)"
    )
    return False

---

## Workflow History & Continuous Execution

**URL:** llms-txt#workflow-history-&-continuous-execution

**Contents:**
- How It Works
- Control Levels
  - Workflow-Level History
  - Step-Level History
- Precedence Logic
  - History Length Control

Source: https://docs.agno.com/concepts/workflows/workflow_with_history

Build conversational workflows that maintain context across multiple executions, creating truly intelligent and natural interactions.

Workflow History enables your Agno workflows to remember and reference previous conversations, transforming isolated executions into continuous, context-aware interactions.

Instead of starting fresh each time, with Workflow History you can:

* **Build on previous interactions** - Reference the context of past interactions
* **Avoid repetitive questions** - Avoid requesting previously provided information
* **Maintain context continuity** - Create a conversational experience
* **Learn from patterns** - Analyze historical data to make better decisions

<Tip>
  Note that this feature is different from `add_history_to_context`.
  This does not add the history of a particular agent or team, but the full workflow history to either all or some steps.
</Tip>

When workflow history is enabled, previous messages are automatically injected into agent/team inputs as structured context:

Along with this, in using Steps with custom functions, you can access this history in the following ways:

1. As a formatted context string as shown above
2. In a structured format as well for more control

<Note>
  You can use these helper functions to access the history:

* `step_input.get_workflow_history(num_runs=3)`
  * `step_input.get_workflow_history_context(num_runs=3)`
</Note>

Refer to [StepInput](/reference/workflows/step_input) reference for more details.

You can be specific about which Steps to add the history to:

### Workflow-Level History

Add workflow history to **all steps** in the workflow:

### Step-Level History

Add workflow history to **specific steps** only:

<Note>
  You can also put `add_workflow_history=False` to disable history for a specific step.
</Note>

**Step-level settings always take precedence over workflow-level settings**:

### History Length Control

**By default, all available history is included** (no limit). It is recommended to use a fixed history run limit to avoid bloating the LLM context window.

You can control this at both levels:

```python  theme={null}

**Examples:**

Example 1 (unknown):
```unknown
Along with this, in using Steps with custom functions, you can access this history in the following ways:

1. As a formatted context string as shown above
2. In a structured format as well for more control
```

Example 2 (unknown):
```unknown
Example-
```

Example 3 (unknown):
```unknown
<Note>
  You can use these helper functions to access the history:

  * `step_input.get_workflow_history(num_runs=3)`
  * `step_input.get_workflow_history_context(num_runs=3)`
</Note>

Refer to [StepInput](/reference/workflows/step_input) reference for more details.

## Control Levels

You can be specific about which Steps to add the history to:

### Workflow-Level History

Add workflow history to **all steps** in the workflow:
```

Example 4 (unknown):
```unknown
### Step-Level History

Add workflow history to **specific steps** only:
```

---

## Retrieve and display generated images using get_last_run_output

**URL:** llms-txt#retrieve-and-display-generated-images-using-get_last_run_output

**Contents:**
- Usage

run_response = agent.get_last_run_output()
if run_response and isinstance(run_response, RunOutput) and run_response.images:
    for image_response in run_response.images:
        image_bytes = image_response.content
        if image_bytes:
            image = PILImage.open(BytesIO(image_bytes))
            image.show()
            # Save the image to a file
            # image.save("generated_image.png")
else:
    print("No images found in run response")
bash  theme={null}
    export GOOGLE_API_KEY=xxx
    bash  theme={null}
    pip install -U google-genai pillow agno
    bash Mac theme={null}
      python cookbook/models/google/gemini/image_editing.py
      bash Windows theme={null}
      python cookbook/models/google/gemini/image_editing.py
      ```
    </CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Prepare your image">
    Place an image file at `tmp/test_photo.png` or update the filepath in the code to point to your image.
  </Step>

  <Step title="Set your API key">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Team Sessions

**URL:** llms-txt#team-sessions

**Contents:**
- Multi-user, multi-session Teams

Source: https://docs.agno.com/concepts/teams/sessions

Learn about Team sessions and managing conversation history.

When we call `Team.run()`, it creates a stateless, singular Team run.

But what if we want to continue this conversation i.e. have a multi-turn conversation? That's where "Sessions" come in. A session is collection of consecutive runs.

In practice, a session in the context of a Team is a multi-turn conversation between a user and the Team. Using a `session_id`, we can connect the conversation history and state across multiple runs.

See more details in the [Agent Sessions](/concepts/agents/sessions) documentation.

## Multi-user, multi-session Teams

Each user that is interacting with a Team gets a unique set of sessions and you can have multiple users interacting with the same Team at the same time.

Set a `user_id` to connect a user to their sessions with the Team.

In the example below, we set a `session_id` to demo how to have multi-turn conversations with multiple users at the same time.

```python  theme={null}
from agno.team import Team
from agno.models.openai import OpenAIChat
from agno.db.sqlite import SqliteDb

db = SqliteDb(db_file="tmp/data.db")

team = Team(
    model=OpenAIChat(id="gpt-5-mini"),
    members=[
        Agent(name="Agent 1", role="You answer questions in English"),
        Agent(name="Agent 2", role="You answer questions in Chinese"),
        Agent(name="Agent 3", role="You answer questions in French"),
    ],
    db=db,
    respond_directly=True,
)

user_1_id = "user_101"
user_2_id = "user_102"

user_1_session_id = "session_101"
user_2_session_id = "session_102"

---

## Create steps for routing

**URL:** llms-txt#create-steps-for-routing

onboarding_step = Step(
    name="Onboard User",
    description="Onboard new user and set preferences",
    agent=onboarding_agent,
)

technical_step = Step(
    name="Technical Response",
    description="Provide technical assistance",
    agent=technical_agent,
)

friendly_step = Step(
    name="Friendly Response",
    description="Provide friendly assistance",
    agent=friendly_agent,
)

general_step = Step(
    name="General Response",
    description="Provide general assistance",
    agent=general_agent,
)

---

## MongoDB Async

**URL:** llms-txt#mongodb-async

**Contents:**
- Code
- Usage

Source: https://docs.agno.com/examples/concepts/vectordb/mongo-db/async-mongo-db

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Install libraries">
    
  </Step>

<Step title="Run MongoDB">
    
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Install libraries">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Run MongoDB">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Shared State

**URL:** llms-txt#shared-state

**Contents:**
- How to use Shared State
  - Example

Source: https://docs.agno.com/concepts/teams/state

Learn about the shared state of Agent Teams.

Team Session State enables sophisticated state management across teams of agents. Teams often need to coordinate on shared information.

<Check>
  Shared state propagates through nested team structures as well
</Check>

## How to use Shared State

You can set the `session_state` parameter on `Team` to share state between the team leader and team members.
This state is available to all team members and is synchronized between them.

Members can access the shared state using the `session_state` attribute in tools.

<Note>
  The `session_state` variable is automatically passed to the tool as an argument.  Any updates to it is automatically reflected in the shared state.
</Note>

Here's a simple example of a team managing a shared shopping list:

```python team_session_state.py theme={null}
from agno.models.openai import OpenAIChat
from agno.agent import Agent
from agno.team import Team

**Examples:**

Example 1 (unknown):
```unknown
Members can access the shared state using the `session_state` attribute in tools.

For example:
```

Example 2 (unknown):
```unknown
<Note>
  The `session_state` variable is automatically passed to the tool as an argument.  Any updates to it is automatically reflected in the shared state.
</Note>

### Example

Here's a simple example of a team managing a shared shopping list:
```

---

## Combine all instruction components

**URL:** llms-txt#combine-all-instruction-components

**Contents:**
- Step 4: Create the Complete Agent
  - 4a. Create the Agent

complete_instructions = f"""
You are a Senior Social Media Intelligence Analyst specializing in cross-platform
brand monitoring and strategic analysis.

{data_collection_strategy}

{intelligence_synthesis}

{analysis_principles}
"""

print(f"Complete instructions created: {len(complete_instructions)} characters")
python  theme={null}
from agno.agent import Agent

**Examples:**

Example 1 (unknown):
```unknown
## Step 4: Create the Complete Agent

Now let's put all the pieces together - model, tools, and instructions - to create your complete social media intelligence agent.

### 4a. Create the Agent
```

---

## Please download "GreatRedSpot.mp4" using

**URL:** llms-txt#please-download-"greatredspot.mp4"-using

---

## Creating your own tools

**URL:** llms-txt#creating-your-own-tools

**Contents:**
- Python Functions as Tools
- Magic of the @tool decorator
  - @tool Parameters Reference
- Writing your own Toolkit

Source: https://docs.agno.com/concepts/tools/custom-tools

Learn how to write your own tools and how to use the `@tool` decorator to modify the behavior of a tool.

In most production cases, you will need to write your own tools. Which is why we're focused on provide the best tool-use experience in Agno.

* Any Python function can be used as a tool by an Agent.
* Use the `@tool` decorator to modify what happens before and after this tool is called.

## Python Functions as Tools

For example, here's how to use a `get_top_hackernews_stories` function as a tool:

## Magic of the @tool decorator

To modify the behavior of a tool, use the `@tool` decorator. Some notable features:

* `requires_confirmation=True`: Requires user confirmation before execution.
* `requires_user_input=True`: Requires user input before execution. Use `user_input_fields` to specify which fields require user input.
* `external_execution=True`: The tool will be executed outside of the agent's control.
* `show_result=True`: Show the output of the tool call in the Agent's response, `True` by default. Without this flag, the result of the tool call is sent to the model for further processing.
* `stop_after_tool_call=True`: Stop the agent run after the tool call.
* `tool_hooks`: Run custom logic before and after this tool call.
* `cache_results=True`: Cache the tool result to avoid repeating the same call. Use `cache_dir` and `cache_ttl` to configure the cache.

Here's an example that uses many possible parameters on the `@tool` decorator.

### @tool Parameters Reference

| Parameter               | Type             | Description                                                       |
| ----------------------- | ---------------- | ----------------------------------------------------------------- |
| `name`                  | `str`            | Override for the function name                                    |
| `description`           | `str`            | Override for the function description                             |
| `stop_after_tool_call`  | `bool`           | If True, the agent will stop after the function call              |
| `tool_hooks`            | `list[Callable]` | List of hooks that wrap the function execution                    |
| `pre_hook`              | `Callable`       | Hook to run before the function is executed                       |
| `post_hook`             | `Callable`       | Hook to run after the function is executed                        |
| `requires_confirmation` | `bool`           | If True, requires user confirmation before execution              |
| `requires_user_input`   | `bool`           | If True, requires user input before execution                     |
| `user_input_fields`     | `list[str]`      | List of fields that require user input                            |
| `external_execution`    | `bool`           | If True, the tool will be executed outside of the agent's control |
| `cache_results`         | `bool`           | If True, enable caching of function results                       |
| `cache_dir`             | `str`            | Directory to store cache files                                    |
| `cache_ttl`             | `int`            | Time-to-live for cached results in seconds (default: 3600)        |

## Writing your own Toolkit

Many advanced use-cases will require writing custom Toolkits. Here's the general flow:

1. Create a class inheriting the `agno.tools.Toolkit` class.
2. Add your functions to the class.
3. **Important:** Include all the functions in the `tools` argument to the `Toolkit` constructor.

Now your Toolkit is ready to use with an Agent. For example:

**Examples:**

Example 1 (unknown):
```unknown
## Magic of the @tool decorator

To modify the behavior of a tool, use the `@tool` decorator. Some notable features:

* `requires_confirmation=True`: Requires user confirmation before execution.
* `requires_user_input=True`: Requires user input before execution. Use `user_input_fields` to specify which fields require user input.
* `external_execution=True`: The tool will be executed outside of the agent's control.
* `show_result=True`: Show the output of the tool call in the Agent's response, `True` by default. Without this flag, the result of the tool call is sent to the model for further processing.
* `stop_after_tool_call=True`: Stop the agent run after the tool call.
* `tool_hooks`: Run custom logic before and after this tool call.
* `cache_results=True`: Cache the tool result to avoid repeating the same call. Use `cache_dir` and `cache_ttl` to configure the cache.

Here's an example that uses many possible parameters on the `@tool` decorator.
```

Example 2 (unknown):
```unknown
### @tool Parameters Reference

| Parameter               | Type             | Description                                                       |
| ----------------------- | ---------------- | ----------------------------------------------------------------- |
| `name`                  | `str`            | Override for the function name                                    |
| `description`           | `str`            | Override for the function description                             |
| `stop_after_tool_call`  | `bool`           | If True, the agent will stop after the function call              |
| `tool_hooks`            | `list[Callable]` | List of hooks that wrap the function execution                    |
| `pre_hook`              | `Callable`       | Hook to run before the function is executed                       |
| `post_hook`             | `Callable`       | Hook to run after the function is executed                        |
| `requires_confirmation` | `bool`           | If True, requires user confirmation before execution              |
| `requires_user_input`   | `bool`           | If True, requires user input before execution                     |
| `user_input_fields`     | `list[str]`      | List of fields that require user input                            |
| `external_execution`    | `bool`           | If True, the tool will be executed outside of the agent's control |
| `cache_results`         | `bool`           | If True, enable caching of function results                       |
| `cache_dir`             | `str`            | Directory to store cache files                                    |
| `cache_ttl`             | `int`            | Time-to-live for cached results in seconds (default: 3600)        |

## Writing your own Toolkit

Many advanced use-cases will require writing custom Toolkits. Here's the general flow:

1. Create a class inheriting the `agno.tools.Toolkit` class.
2. Add your functions to the class.
3. **Important:** Include all the functions in the `tools` argument to the `Toolkit` constructor.

Now your Toolkit is ready to use with an Agent. For example:
```

---

## Example usage with different types of travel queries

**URL:** llms-txt#example-usage-with-different-types-of-travel-queries

if __name__ == "__main__":
    travel_agent.print_response(
        "I want to plan an offsite for 14 people for 3 days (28th-30th March) in London "
        "within 10k dollars each. Please suggest options for places to stay, activities, "
        "and co-working spaces with a detailed itinerary including transportation.",
        stream=True,
    )

---

## Create individual steps

**URL:** llms-txt#create-individual-steps

research_hn_step = Step(name="Research HackerNews", agent=researcher)
research_web_step = Step(name="Research Web", agent=researcher)
write_step = Step(name="Write Article", agent=writer)
review_step = Step(name="Review Article", agent=reviewer)

---

## Linear Tools

**URL:** llms-txt#linear-tools

**Contents:**
- Code
- Usage

Source: https://docs.agno.com/examples/concepts/tools/others/linear

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Set your Linear API key">
    
  </Step>

<Step title="Install libraries">
    
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Set your Linear API key">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Setup your Session Summary Manager, to adjust how summaries are created

**URL:** llms-txt#setup-your-session-summary-manager,-to-adjust-how-summaries-are-created

session_summary_manager = SessionSummaryManager(
    # Select the model used for session summary creation and updates. If not specified, the agent's model is used by default.
    model=OpenAIChat(id="gpt-5-mini"),
    # You can also overwrite the prompt used for session summary creation
    session_summary_prompt="Create a very succinct summary of the following conversation:",
)

---

## Session Summary Management

**URL:** llms-txt#session-summary-management

**Contents:**
- Code

Source: https://docs.agno.com/examples/concepts/teams/session/session_summary

This example shows how to use session summary to store and maintain conversation summaries for better context management over long conversations.

```python 03_session_summary.py theme={null}
"""
This example shows how to use the session summary to store the conversation summary.
"""

from agno.agent.agent import Agent
from agno.db.postgres import PostgresDb
from agno.models.openai import OpenAIChat
from agno.session.summary import SessionSummaryManager  # noqa: F401
from agno.team import Team

db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"

db = PostgresDb(db_url=db_url, session_table="sessions")

---

## Context Engineering

**URL:** llms-txt#context-engineering

**Contents:**
- System message context
  - System message Parameters
  - How the system message is built
  - Set the system message directly
- User message context
  - Additional user message context
- Chat history
- Managing Tool Calls

Source: https://docs.agno.com/concepts/teams/context

Learn how to write prompts and other context engineering techniques for your teams.

Context engineering is the process of designing and controlling the information (context) that is sent to language models to guide their behavior and outputs.
In practice, building context comes down to one question: "Which information is most likely to achieve the desired outcome?"

In Agno, this means carefully crafting the system message, which includes the team's description, instructions, member information, and other relevant settings. By thoughtfully constructing this context, you can:

* Steer the team toward specific behaviors or roles.
* Constrain or expand the team's capabilities.
* Ensure outputs are consistent, relevant, and aligned with your application's needs.
* Enable advanced use cases such as multi-step reasoning, member delegation, tool use, or structured output.
* Coordinate team members effectively for collaborative tasks.

Effective context engineering is an iterative process: refining the system message, trying out different descriptions and instructions, and using features such as schemas, delegation, and tool integrations.

The context of an Agno team consists of the following:

* **System message**: The system message is the main context that is sent to the team, including all additional context
* **User message**: The user message is the message that is sent to the team.
* **Chat history**: The chat history is the history of the conversation between the team and the user.
* **Additional input**: Any few-shot examples or other additional input that is added to the context.

## System message context

The following are some key parameters that are used to create the system message:

1. **Description**: A description that guides the overall behaviour of the team.
2. **Instructions**: A list of precise, task-specific instructions on how to achieve its goal.
3. **Expected Output**: A description of the expected output from the Team.
4. **Members**: Information about team members, their roles, and capabilities.

The system message is built from the team’s description, instructions, member details, and other settings. A team leader’s system message additionally includes delegation rules and coordination guidelines. For example:

Will produce the following system message:

### System message Parameters

The Team creates a default system message that can be customized using the following parameters:

| Parameter                          | Type        | Default | Description                                                                                                                                                                |
| ---------------------------------- | ----------- | ------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| `description`                      | `str`       | `None`  | A description of the Team that is added to the start of the system message.                                                                                                |
| `instructions`                     | `List[str]` | `None`  | List of instructions added to the system prompt in `<instructions>` tags. Default instructions are also created depending on values for `markdown`, `expected_output` etc. |
| `additional_context`               | `str`       | `None`  | Additional context added to the end of the system message.                                                                                                                 |
| `expected_output`                  | `str`       | `None`  | Provide the expected output from the Team. This is added to the end of the system message.                                                                                 |
| `markdown`                         | `bool`      | `False` | Add an instruction to format the output using markdown.                                                                                                                    |
| `add_datetime_to_context`          | `bool`      | `False` | If True, add the current datetime to the prompt to give the team a sense of time. This allows for relative times like "tomorrow" to be used in the prompt                  |
| `add_name_to_context`              | `bool`      | `False` | If True, add the name of the team to the context.                                                                                                                          |
| `add_location_to_context`          | `bool`      | `False` | If True, add the location of the team to the context. This allows for location-aware responses and local context.                                                          |
| `timezone_identifier`              | `str`       | `None`  | Allows for custom timezone for datetime instructions following the TZ Database format (e.g. "Etc/UTC")                                                                     |
| `add_member_tools_to_context`      | `bool`      | `True`  | If True, add the tools available to team members to the context.                                                                                                           |
| `add_session_summary_to_context`   | `bool`      | `False` | If True, add the session summary to the context. See [sessions](/concepts/teams/sessions) for more information.                                                            |
| `add_memories_to_context`          | `bool`      | `False` | If True, add the user memories to the context. See [memory](/concepts/teams/memory) for more information.                                                                  |
| `add_dependencies_to_context`      | `bool`      | `False` | If True, add the dependencies to the context. See [dependencies](/concepts/teams/dependencies) for more information.                                                       |
| `add_session_state_to_context`     | `bool`      | `False` | If True, add the session state to the context. See [state](/concepts/teams/state) for more information.                                                                    |
| `add_knowledge_to_context`         | `bool`      | `False` | If True, add retrieved knowledge to the context, to enable RAG. See [knowledge](/concepts/teams/knowledge) for more information.                                           |
| `enable_agentic_knowledge_filters` | `bool`      | `False` | If True, let the team choose the knowledge filters. See [knowledge](/concepts/knowledge/filters/overview) for more information.                                            |
| `system_message`                   | `str`       | `None`  | Override the default system message.                                                                                                                                       |
| `respond_directly`                 | `bool`      | `False` | If True, the team leader won't process responses from members and instead will return them directly.                                                                       |
| `delegate_task_to_all_members`     | `bool`      | `False` | If True, the team leader will delegate the task to all members, instead of deciding for a subset.                                                                          |
| `determine_input_for_members`      | `bool`      | `True`  | Set to false if you want to send the run input directly to the member agents.                                                                                              |
| `share_member_interactions`        | `bool`      | `False` | If True, send all previous member interactions to members.                                                                                                                 |
| `get_member_information_tool`      | `bool`      | `False` | If True, add a tool to get information about the team members.                                                                                                             |

See the full [Team reference](/reference/teams/team) for more information.

### How the system message is built

Lets take the following example team:

Below is the system message that will be built:

<Tip>
  This example is exhaustive and illustrates what is possible with the system message, however in practice you would only use some of these settings.
</Tip>

#### Additional Context

You can add additional context to the end of the system message using the `additional_context` parameter.

Here, `additional_context` adds a note to the system message indicating that the team can access specific database tables.

#### Team Member Information

The member information is automatically injected into the system message. This includes the member ID, name, role, and tools.
You can optionally minimize this by setting `add_member_tools_to_context` to False, which removes the member tools from the system message.

You can also give the team leader a tool to get information about the team members.

#### Tool Instructions

If you are using a [Toolkit](/concepts/tools/toolkits/toolkits) on your team, you can add tool instructions to the system message using the `instructions` parameter:

These instructions are injected into the system message after the `<additional_information>` tags.

#### Agentic Memories

If you have `enable_agentic_memory` set to `True` on your team, the team gets the ability to create/update user memories using tools.

This adds the following to the system message:

#### Agentic Knowledge Filters

If you have knowledge enabled on your team, you can let the team choose the knowledge filters using the `enable_agentic_knowledge_filters` parameter.

This will add the following to the system message:

Learn about agentic knowledge filters in more detail in the [knowledge filters](/concepts/knowledge/filters/overview) section.

### Set the system message directly

You can manually set the system message using the `system_message` parameter. This will ignore all other settings and use the system message you provide.

## User message context

The `input` sent to the `Team.run()` or `Team.print_response()` is used as the user message.

See [dependencies](/concepts/teams/dependencies) for how to do dependency injection for your user message.

### Additional user message context

By default, the user message is built using the `input` sent to the `Team.run()` or `Team.print_response()` functions.

The following team parameters configure how the user message is built:

* `add_knowledge_to_context`
* `add_dependencies_to_context`

The user message that is sent to the model will look like this:

If you have database storage enabled on your team, session history is automatically stored (see [sessions](/concepts/teams/sessions)).

You can now add the history of the conversation to the context using `add_history_to_context`.

This will add the history of the conversation to the context, which can be used to provide context for the next message.

See more details on [sessions](/concepts/teams/sessions#session-history).

<Note>All team member runs are added to the team session history.</Note>

## Managing Tool Calls

The `max_tool_calls_from_history` parameter can be used to add only the `n` most recent tool calls from history to the context.

This helps manage context size and reduce token costs during team runs.

```python  theme={null}
from agno.agent import Agent
from agno.db.sqlite import SqliteDb
from agno.models.openai import OpenAIChat
from agno.team import Team
from agno.tools.duckduckgo import DuckDuckGoTools

web_agent = Agent(
    name="Web Researcher",
    model=OpenAIChat(id="gpt-4o-mini"),
    instructions="You are a web researcher. Search the web for comprehensive information on given topics.",
    tools=[DuckDuckGoTools()],
)

team = Team(
    members=[web_agent],
    model=OpenAIChat(id="gpt-4o"),
    db=SqliteDb(db_file="tmp/filter_history_tool_calls_team.db"),
    add_history_to_context=True,
    max_tool_calls_from_history=5,  # Keep only last 5 tool calls in context
    show_members_responses=True,
)

team.print_response("Search for AI news")
team.print_response("Search for crypto trends")
team.print_response("Search for tech stocks")

**Examples:**

Example 1 (unknown):
```unknown
Will produce the following system message:
```

Example 2 (unknown):
```unknown
### System message Parameters

The Team creates a default system message that can be customized using the following parameters:

| Parameter                          | Type        | Default | Description                                                                                                                                                                |
| ---------------------------------- | ----------- | ------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| `description`                      | `str`       | `None`  | A description of the Team that is added to the start of the system message.                                                                                                |
| `instructions`                     | `List[str]` | `None`  | List of instructions added to the system prompt in `<instructions>` tags. Default instructions are also created depending on values for `markdown`, `expected_output` etc. |
| `additional_context`               | `str`       | `None`  | Additional context added to the end of the system message.                                                                                                                 |
| `expected_output`                  | `str`       | `None`  | Provide the expected output from the Team. This is added to the end of the system message.                                                                                 |
| `markdown`                         | `bool`      | `False` | Add an instruction to format the output using markdown.                                                                                                                    |
| `add_datetime_to_context`          | `bool`      | `False` | If True, add the current datetime to the prompt to give the team a sense of time. This allows for relative times like "tomorrow" to be used in the prompt                  |
| `add_name_to_context`              | `bool`      | `False` | If True, add the name of the team to the context.                                                                                                                          |
| `add_location_to_context`          | `bool`      | `False` | If True, add the location of the team to the context. This allows for location-aware responses and local context.                                                          |
| `timezone_identifier`              | `str`       | `None`  | Allows for custom timezone for datetime instructions following the TZ Database format (e.g. "Etc/UTC")                                                                     |
| `add_member_tools_to_context`      | `bool`      | `True`  | If True, add the tools available to team members to the context.                                                                                                           |
| `add_session_summary_to_context`   | `bool`      | `False` | If True, add the session summary to the context. See [sessions](/concepts/teams/sessions) for more information.                                                            |
| `add_memories_to_context`          | `bool`      | `False` | If True, add the user memories to the context. See [memory](/concepts/teams/memory) for more information.                                                                  |
| `add_dependencies_to_context`      | `bool`      | `False` | If True, add the dependencies to the context. See [dependencies](/concepts/teams/dependencies) for more information.                                                       |
| `add_session_state_to_context`     | `bool`      | `False` | If True, add the session state to the context. See [state](/concepts/teams/state) for more information.                                                                    |
| `add_knowledge_to_context`         | `bool`      | `False` | If True, add retrieved knowledge to the context, to enable RAG. See [knowledge](/concepts/teams/knowledge) for more information.                                           |
| `enable_agentic_knowledge_filters` | `bool`      | `False` | If True, let the team choose the knowledge filters. See [knowledge](/concepts/knowledge/filters/overview) for more information.                                            |
| `system_message`                   | `str`       | `None`  | Override the default system message.                                                                                                                                       |
| `respond_directly`                 | `bool`      | `False` | If True, the team leader won't process responses from members and instead will return them directly.                                                                       |
| `delegate_task_to_all_members`     | `bool`      | `False` | If True, the team leader will delegate the task to all members, instead of deciding for a subset.                                                                          |
| `determine_input_for_members`      | `bool`      | `True`  | Set to false if you want to send the run input directly to the member agents.                                                                                              |
| `share_member_interactions`        | `bool`      | `False` | If True, send all previous member interactions to members.                                                                                                                 |
| `get_member_information_tool`      | `bool`      | `False` | If True, add a tool to get information about the team members.                                                                                                             |

See the full [Team reference](/reference/teams/team) for more information.

### How the system message is built

Lets take the following example team:
```

Example 3 (unknown):
```unknown
Below is the system message that will be built:
```

Example 4 (unknown):
```unknown
<Tip>
  This example is exhaustive and illustrates what is possible with the system message, however in practice you would only use some of these settings.
</Tip>

#### Additional Context

You can add additional context to the end of the system message using the `additional_context` parameter.

Here, `additional_context` adds a note to the system message indicating that the team can access specific database tables.
```

---

## Autogenerate session name

**URL:** llms-txt#autogenerate-session-name

**Contents:**
- Usage

team.set_session_name(autogenerate=True)
print(team.get_session_name())
bash  theme={null}
    pip install agno psycopg2-binary
    bash  theme={null}
    export OPENAI_API_KEY=****
    bash  theme={null}
    cookbook/run_pgvector.sh
    bash  theme={null}
    python cookbook/examples/teams/session/06_rename_session.py
    ```
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Install required libraries">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Set environment variables">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Start PostgreSQL database">
```

Example 4 (unknown):
```unknown
</Step>

  <Step title="Run the agent">
```

---

## YouTube Reader

**URL:** llms-txt#youtube-reader

Source: https://docs.agno.com/reference/knowledge/reader/youtube

YouTubeReader is a reader class that allows you to read transcript from YouTube videos.

<Snippet file="youtube-reader-reference.mdx" />

---

## File Upload

**URL:** llms-txt#file-upload

**Contents:**
- Usage
- Working example

Source: https://docs.agno.com/examples/models/anthropic/file_upload

Learn how to use Anthropic's Files API with Agno.

With Anthropic's [Files API](https://docs.anthropic.com/en/docs/build-with-claude/files), you can upload files and later reference them in other API calls.
This is handy when a file is referenced multiple times in the same flow.

<Steps>
  <Step title="Upload a file">
    Initialize the Anthropic client and use `client.beta.files.upload`:

<Step title="Initialize the Claude model">
    When initializing the `Claude` model, pass the necessary beta header:

<Step title="Reference the file">
    You can now reference the uploaded file when interacting with your Agno agent:

Notice there are some storage limits attached to this feature. You can read more about that on Anthropic's [docs](https://docs.anthropic.com/en/docs/build-with-claude/files#file-storage-and-limits).

```python cookbook/models/anthropic/pdf_input_file_upload.py theme={null}
from pathlib import Path

from agno.agent import Agent
from agno.media import File
from agno.models.anthropic import Claude
from agno.utils.media import download_file
from anthropic import Anthropic

pdf_path = Path(__file__).parent.joinpath("ThaiRecipes.pdf")

**Examples:**

Example 1 (unknown):
```unknown
</Step>

  <Step title="Initialize the Claude model">
    When initializing the `Claude` model, pass the necessary beta header:
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Reference the file">
    You can now reference the uploaded file when interacting with your Agno agent:
```

Example 3 (unknown):
```unknown
</Step>
</Steps>

Notice there are some storage limits attached to this feature. You can read more about that on Anthropic's [docs](https://docs.anthropic.com/en/docs/build-with-claude/files#file-storage-and-limits).

## Working example
```

---

## Remove Content

**URL:** llms-txt#remove-content

**Contents:**
- Code

Source: https://docs.agno.com/examples/concepts/knowledge/basic-operations/remove-content

```python 09_remove_content.py theme={null}
import asyncio
from agno.db.postgres import PostgresDb
from agno.knowledge.knowledge import Knowledge
from agno.vectordb.pgvector import PgVector

---

## List All Workflows

**URL:** llms-txt#list-all-workflows

Source: https://docs.agno.com/reference-api/schema/workflows/list-all-workflows

get /workflows
Retrieve a comprehensive list of all workflows configured in this OS instance.

**Return Information:**
- Workflow metadata (ID, name, description)
- Input schema requirements
- Step sequence and execution flow
- Associated agents and teams

---

## Conditional Steps

**URL:** llms-txt#conditional-steps

Source: https://docs.agno.com/reference/workflows/conditional-steps

| Parameter     | Type                                                                               | Default  | Description                                   |
| ------------- | ---------------------------------------------------------------------------------- | -------- | --------------------------------------------- |
| `evaluator`   | `Union[Callable[[StepInput], bool], Callable[[StepInput], Awaitable[bool]], bool]` | Required | Function or boolean to evaluate the condition |
| `steps`       | `WorkflowSteps`                                                                    | Required | Steps to execute if the condition is met      |
| `name`        | `Optional[str]`                                                                    | `None`   | Name of the condition step                    |
| `description` | `Optional[str]`                                                                    | `None`   | Description of the condition step             |

---

## 1. Send a request that requires confirmation

**URL:** llms-txt#1.-send-a-request-that-requires-confirmation

curl -X POST http://localhost:7777/agents/data_manager/runs \
  -F "message=Delete 50 old records from the users table" \
  -F "user_id=test_user" \
  -F "session_id=test_session"

---

## Pipedream Auth

**URL:** llms-txt#pipedream-auth

**Contents:**
- Code

Source: https://docs.agno.com/examples/concepts/tools/mcp/pipedream_auth

This example shows how to add authorization when integrating Pipedream MCP servers with Agno Agents.

---

## Example 1: Basic research with simple string

**URL:** llms-txt#example-1:-basic-research-with-simple-string

**Contents:**
- Usage

agent.print_response(
    "Perform a comprehensive research on the current flagship GPUs from NVIDIA, AMD and Intel. Return a table of model name, MSRP USD, TDP watts, and launch date. Include citations for each cell."
)

bash  theme={null}
    export OPENAI_API_KEY=xxx
    export EXA_API_KEY=xxx
    bash  theme={null}
    pip install -U agno openai exa_py
    bash Mac theme={null}
      python cookbook/examples/agents/deep_research_agent_exa.py
      bash Windows theme={null}
      python cookbook/examples/agents/deep_research_agent_exa.py
      ```
    </CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Set your API key">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Get Session by ID

**URL:** llms-txt#get-session-by-id

Source: https://docs.agno.com/reference-api/schema/sessions/get-session-by-id

get /sessions/{session_id}
Retrieve detailed information about a specific session including metadata, configuration, and run history. Response schema varies based on session type (agent, team, or workflow).

---

## Reasoning

**URL:** llms-txt#reasoning

**Contents:**
- Example
- Toolkit Params
- Toolkit Functions
- Developer Resources

Source: https://docs.agno.com/concepts/tools/toolkits/others/reasoning

ReasoningTools provides step-by-step reasoning capabilities for agents to think through complex problems systematically.

The following agent can use structured reasoning to solve complex problems:

| Parameter           | Type            | Default | Description                                 |
| ------------------- | --------------- | ------- | ------------------------------------------- |
| `enable_think`      | `bool`          | `True`  | Enable the think reasoning function.        |
| `enable_analyze`    | `bool`          | `True`  | Enable the analyze reasoning function.      |
| `instructions`      | `Optional[str]` | `None`  | Custom instructions for reasoning behavior. |
| `add_instructions`  | `bool`          | `False` | Whether to add instructions to the agent.   |
| `add_few_shot`      | `bool`          | `False` | Whether to include few-shot examples.       |
| `few_shot_examples` | `Optional[str]` | `None`  | Custom few-shot examples for reasoning.     |

| Function  | Description                                                  |
| --------- | ------------------------------------------------------------ |
| `think`   | Perform step-by-step reasoning about a problem or situation. |
| `analyze` | Conduct detailed analysis with structured reasoning steps.   |

## Developer Resources

* View [Tools Source](https://github.com/agno-agi/agno/blob/main/libs/agno/agno/tools/reasoning.py)
* [Agno Reasoning Framework](https://docs.agno.com/reasoning)

---

## Install required packages

**URL:** llms-txt#install-required-packages

**Contents:**
- SDK Integration
  - Basic Usage

pip install agno litellm
shell  theme={null}
export LITELLM_API_KEY=your_api_key_here
python  theme={null}
from agno.agent import Agent
from agno.models.litellm import LiteLLM

**Examples:**

Example 1 (unknown):
```unknown
Set up your API key:
Regardless of the model used(OpenAI, Hugging Face, or XAI) the API key is referenced as `LITELLM_API_KEY`.
```

Example 2 (unknown):
```unknown
## SDK Integration

The `LiteLLM` class provides direct integration with the LiteLLM Python SDK.

### Basic Usage
```

---

## OpenAI Key Request While Using Other Models

**URL:** llms-txt#openai-key-request-while-using-other-models

**Contents:**
- Quick fix: Configure a Different Model

Source: https://docs.agno.com/faq/openai-key-request-for-other-models

If you see a request for an OpenAI API key but haven't explicitly configured OpenAI, it's because Agno uses OpenAI models by default in several places, including:

* The default model when unspecified in `Agent`
* The default embedder is OpenAIEmbedder with VectorDBs, unless specified

## Quick fix: Configure a Different Model

It is best to specify the model for the agent explicitly, otherwise it would default to `OpenAIChat`.

For example, to use Google's Gemini instead of OpenAI:

```python  theme={null}
from agno.agent import Agent, RunOutput
from agno.models.google import Gemini

agent = Agent(
    model=Gemini(id="gemini-1.5-flash"),
    markdown=True,
)

---

## Connecting to Tableplus

**URL:** llms-txt#connecting-to-tableplus

**Contents:**
- Step 1: Start Your `pgvector` Container
- Step 2: Configure TablePlus

Source: https://docs.agno.com/faq/connecting-to-tableplus

If you want to inspect your pgvector container to explore your storage or knowledge base, you can use TablePlus. Follow these steps:

## Step 1: Start Your `pgvector` Container

Run the following command to start a `pgvector` container locally:

* `POSTGRES_DB=ai` sets the default database name.
* `POSTGRES_USER=ai` and `POSTGRES_PASSWORD=ai` define the database credentials.
* The container exposes port `5432` (mapped to `5532` on your local machine).

## Step 2: Configure TablePlus

1. **Open TablePlus**: Launch the TablePlus application.
2. **Create a New Connection**: Click on the `+` icon to add a new connection.
3. **Select `PostgreSQL`**: Choose PostgreSQL as the database type.

Fill in the following connection details:

* **Host**: `localhost`
* **Port**: `5532`
* **Database**: `ai`
* **User**: `ai`
* **Password**: `ai`

<img src="https://mintcdn.com/agno-v2/yeT29TzCG5roT0hQ/images/tableplus.png?fit=max&auto=format&n=yeT29TzCG5roT0hQ&q=85&s=65f0ec170fdef92080bdc0e72feaacc4" data-og-width="492" width="492" data-og-height="386" height="386" data-path="images/tableplus.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/agno-v2/yeT29TzCG5roT0hQ/images/tableplus.png?w=280&fit=max&auto=format&n=yeT29TzCG5roT0hQ&q=85&s=4c693a789381ec09ee8112a29a19a23f 280w, https://mintcdn.com/agno-v2/yeT29TzCG5roT0hQ/images/tableplus.png?w=560&fit=max&auto=format&n=yeT29TzCG5roT0hQ&q=85&s=83b7b78de5bb7e3e04337380be4a846a 560w, https://mintcdn.com/agno-v2/yeT29TzCG5roT0hQ/images/tableplus.png?w=840&fit=max&auto=format&n=yeT29TzCG5roT0hQ&q=85&s=a933eddefc5117323116e34eba956055 840w, https://mintcdn.com/agno-v2/yeT29TzCG5roT0hQ/images/tableplus.png?w=1100&fit=max&auto=format&n=yeT29TzCG5roT0hQ&q=85&s=84a19124e98f325caa51ff1ee0032652 1100w, https://mintcdn.com/agno-v2/yeT29TzCG5roT0hQ/images/tableplus.png?w=1650&fit=max&auto=format&n=yeT29TzCG5roT0hQ&q=85&s=0a60723cf13d5771fcc8d9f89bb921f2 1650w, https://mintcdn.com/agno-v2/yeT29TzCG5roT0hQ/images/tableplus.png?w=2500&fit=max&auto=format&n=yeT29TzCG5roT0hQ&q=85&s=24d6892323098fdd654dcf4fcf579419 2500w" />

---

## Firestore for Team

**URL:** llms-txt#firestore-for-team

**Contents:**
- Usage

Source: https://docs.agno.com/examples/concepts/db/firestore/firestore_for_team

Agno supports using Firestore as a storage backend for Teams using the `FirestoreDb` class.

You need to provide a `project_id` parameter to the `FirestoreDb` class. Firestore will connect automatically using your Google Cloud credentials.

```python firestore_for_team.py theme={null}
"""
Run: `pip install openai ddgs newspaper4k lxml_html_clean agno` to install the dependencies
"""

from typing import List

from agno.agent import Agent
from agno.db.firestore import FirestoreDb
from agno.models.openai import OpenAIChat
from agno.team import Team
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.hackernews import HackerNewsTools
from pydantic import BaseModel

---

## Evaluate the accuracy of the Team's responses

**URL:** llms-txt#evaluate-the-accuracy-of-the-team's-responses

evaluation = AccuracyEval(
    name="Multi Language Team",
    model=OpenAIChat(id="o4-mini"),
    team=multi_language_team,
    input="Comment allez-vous?",
    expected_output="I can only answer in the following languages: English and Spanish.",
    num_iterations=1,
)

result: Optional[AccuracyResult] = evaluation.run(print_results=True)
assert result is not None and result.avg_score >= 8
```

---

## Example 1: Get stock price analysis as a variable

**URL:** llms-txt#example-1:-get-stock-price-analysis-as-a-variable

print("=" * 50)
print("STOCK PRICE ANALYSIS")
print("=" * 50)

stock_response = team.run("What is the current stock price of NVDA?")
assert isinstance(stock_response.content, StockAnalysis)
print(f"Response type: {type(stock_response.content)}")
print(f"Symbol: {stock_response.content.symbol}")
print(f"Company: {stock_response.content.company_name}")
print(f"Analysis: {stock_response.content.analysis}")
pprint_run_response(stock_response)

---

## Couchbase

**URL:** llms-txt#couchbase

Source: https://docs.agno.com/reference/vector_db/couchbase

<Snippet file="vector-db-couchbase-reference.mdx" />

---

## Todoist

**URL:** llms-txt#todoist

**Contents:**
- Prerequisites
- Example

Source: https://docs.agno.com/concepts/tools/toolkits/others/todoist

**TodoistTools** enables an Agent to interact with [Todoist](https://www.todoist.com/).

The following example requires the `todoist-api-python` library. and a Todoist API token which can be obtained from the [Todoist Developer Portal](https://app.todoist.com/app/settings/integrations/developer).

The following agent will create a new task in Todoist.

```python cookbook/tools/todoist.py theme={null}
"""
Example showing how to use the Todoist Tools with Agno

Requirements:
- Sign up/login to Todoist and get a Todoist API Token (get from https://app.todoist.com/app/settings/integrations/developer)
- pip install todoist-api-python

Usage:
- Set the following environment variables:
    export TODOIST_API_TOKEN="your_api_token"

- Or provide them when creating the TodoistTools instance
"""

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.todoist import TodoistTools

todoist_agent = Agent(
    name="Todoist Agent",
    role="Manage your todoist tasks",
    instructions=[
        "When given a task, create a todoist task for it.",
        "When given a list of tasks, create a todoist task for each one.",
        "When given a task to update, update the todoist task.",
        "When given a task to delete, delete the todoist task.",
        "When given a task to get, get the todoist task.",
    ],
    id="todoist-agent",
    model=OpenAIChat(id="gpt-5-mini"),
    tools=[TodoistTools()],
    markdown=True,
    debug_mode=True,
    )

**Examples:**

Example 1 (unknown):
```unknown

```

Example 2 (unknown):
```unknown
## Example

The following agent will create a new task in Todoist.
```

---

## Trello

**URL:** llms-txt#trello

**Contents:**
- Prerequisites
- Example
- Toolkit Functions
- Toolkit Params
  - Board Filter Options for `list_boards`
- Developer Resources

Source: https://docs.agno.com/concepts/tools/toolkits/others/trello

Agno TrelloTools helps to integrate Trello functionalities into your agents, enabling management of boards, lists, and cards.

The following examples require the `trello` library and Trello API credentials which can be obtained by following Trello's developer documentation.

Set the following environment variables:

The following agent will create a board called `ai-agent` and inside it create list called `todo` and `doing` and inside each of them create card called `create agent`.

| Function          | Description                                                   |
| ----------------- | ------------------------------------------------------------- |
| `create_card`     | Creates a new card in a specified board and list.             |
| `get_board_lists` | Retrieves all lists on a specified Trello board.              |
| `move_card`       | Moves a card to a different list.                             |
| `get_cards`       | Retrieves all cards from a specified list.                    |
| `create_board`    | Creates a new Trello board.                                   |
| `create_list`     | Creates a new list on a specified board.                      |
| `list_boards`     | Lists all Trello boards accessible by the authenticated user. |

| Parameter    | Type            | Default | Description                                                                        |
| ------------ | --------------- | ------- | ---------------------------------------------------------------------------------- |
| `api_key`    | `Optional[str]` | `None`  | Trello API key. If not provided, uses TRELLO\_API\_KEY environment variable.       |
| `api_secret` | `Optional[str]` | `None`  | Trello API secret. If not provided, uses TRELLO\_API\_SECRET environment variable. |
| `token`      | `Optional[str]` | `None`  | Trello token. If not provided, uses TRELLO\_TOKEN environment variable.            |

### Board Filter Options for `list_boards`

The `list_boards` function accepts a `board_filter` argument with the following options:

* `all` (default)
* `open`
* `closed`
* `organization`
* `public`
* `starred`

You can use `include_tools` or `exclude_tools` to modify the list of tools the agent has access to. Learn more about [selecting tools](/concepts/tools/selecting-tools).

## Developer Resources

* View [Tools Source](https://github.com/agno-agi/agno/blob/main/libs/agno/agno/tools/trello.py)
* View [Cookbook Example](https://github.com/agno-agi/agno/tree/main/cookbook/tools/trello_tools.py)

**Examples:**

Example 1 (unknown):
```unknown
Set the following environment variables:
```

Example 2 (unknown):
```unknown
## Example

The following agent will create a board called `ai-agent` and inside it create list called `todo` and `doing` and inside each of them create card called `create agent`.
```

---

## Retrieve and display generated videos

**URL:** llms-txt#retrieve-and-display-generated-videos

run_response = video_agent.get_last_run_output()
if run_response and run_response.videos:
    for video in run_response.videos:
        print(f"Generated video URL: {video.url}")

---

## Retrieve our freshly created session

**URL:** llms-txt#retrieve-our-freshly-created-session

**Contents:**
- Benefits of using session storage
- Storage for Teams and Workflows

session_history = agent.get_session(session_id="123")
python  theme={null}
from agno.team import Team
from agno.workflow import Workflow
from agno.db.sqlite import SQLiteDb

**Examples:**

Example 1 (unknown):
```unknown
## Benefits of using session storage

When building production-ready agentic applications, storage will often be a very important feature. This is because it enables to:

* Continue a session: retrieve previous messages and enable users to pick up a conversation where they left off.
* Keep a record of sessions: enable users to inspect their past conversations.
* Data ownership: keeping sessions in your own database gives you full control over the data.

<Warning>
  Storage is a critical part of your Agentic infrastructure. We recommend to
  never offload it to a third-party service. You should almost always use your
  own storage layer for your Agents.
</Warning>

## Storage for Teams and Workflows

Storage also works with Teams and Workflows, providing persistent memory for your more complex agentic applications.

Similarly to Agents, you simply need to provide your Team or Workflow with a database for sessions to be persisted:
```

---

## - "What's the most upvoted story today?"

**URL:** llms-txt#--"what's-the-most-upvoted-story-today?"

**Contents:**
- Usage

agent.print_response("Summarize the top 5 stories on hackernews?", stream=True)
bash  theme={null}
    pip install openai httpx agno
    bash  theme={null}
    python custom_tools.py
    ```
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Install libraries">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Run the agent">
```

---

## Models Labs Tools

**URL:** llms-txt#models-labs-tools

**Contents:**
- Code
- Usage

Source: https://docs.agno.com/examples/concepts/tools/others/models_labs

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Set your API key">
    
  </Step>

<Step title="Install libraries">
    
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Set your API key">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Basic

**URL:** llms-txt#basic

**Contents:**
- Code

Source: https://docs.agno.com/examples/models/xai/basic

```python cookbook/models/xai/basic.py theme={null}
from agno.agent import Agent, RunOutput  # noqa
from agno.models.xai import xAI

agent = Agent(model=xAI(id="grok-2"), markdown=True)

---

## CSV Row Chunking

**URL:** llms-txt#csv-row-chunking

Source: https://docs.agno.com/reference/knowledge/chunking/csv-row

CSV row chunking is a method of splitting CSV files into smaller chunks based on the number of rows, rather than character count. This approach is particularly useful for structured data where you want to process CSV files in manageable row-based chunks while preserving the integrity of individual records.

<Snippet file="chunking-csv-row.mdx" />

---

## Sets the global default tracer provider

**URL:** llms-txt#sets-the-global-default-tracer-provider

from opentelemetry import trace  # noqa: E402

trace.set_tracer_provider(trace_provider)

---

## Async Postgres for Team

**URL:** llms-txt#async-postgres-for-team

**Contents:**
- Usage
  - Run PgVector
- Params
- Developer Resources

Source: https://docs.agno.com/examples/concepts/db/async_postgres/async_postgres_for_team

Agno supports using [PostgreSQL](https://www.postgresql.org/) asynchronously, with the `AsyncPostgresDb` class.

Install [docker desktop](https://docs.docker.com/desktop/install/mac-install/) and run **PgVector** on port **5532** using:

<Snippet file="db-async-postgres-params.mdx" />

## Developer Resources

* View [Cookbook](https://github.com/agno-agi/agno/tree/main/cookbook/db/async_postgres/async_postgres_for_team.py)

**Examples:**

Example 1 (unknown):
```unknown

```

---

## "Remove all existing memories of me.",

**URL:** llms-txt#"remove-all-existing-memories-of-me.",

---

## Please download the image using

**URL:** llms-txt#please-download-the-image-using

---

## Update Evaluation Run

**URL:** llms-txt#update-evaluation-run

Source: https://docs.agno.com/reference-api/schema/evals/update-evaluation-run

patch /eval-runs/{eval_run_id}
Update the name or other properties of an existing evaluation run.

---

## Initialize LangWatch and instrument Agno

**URL:** llms-txt#initialize-langwatch-and-instrument-agno

langwatch.setup(instrumentors=[AgnoInstrumentor()])

---

## Use Pydantic model as structured input

**URL:** llms-txt#use-pydantic-model-as-structured-input

research_request = ResearchTopic(
    topic="AI Agent Frameworks",
    focus_areas=["AI Agents", "Framework Design", "Developer Tools", "Open Source"],
    target_audience="Software Developers and AI Engineers",
    sources_required=7,
)

---

## Advanced research topics to explore:

**URL:** llms-txt#advanced-research-topics-to-explore:

**Contents:**
- Usage

"""
Quantum Science & Computing:
1. "Investigate recent breakthroughs in quantum error correction"
2. "Analyze the development of topological quantum computing"
3. "Research quantum machine learning algorithms and applications"
4. "Explore advances in quantum sensing technologies"

Biotechnology & Medicine:
1. "Examine recent developments in mRNA vaccine technology"
2. "Analyze breakthroughs in organoid research"
3. "Investigate advances in precision medicine"
4. "Research developments in neurotechnology"

Materials Science:
1. "Explore recent advances in metamaterials"
2. "Analyze developments in 2D materials beyond graphene"
3. "Research progress in self-healing materials"
4. "Investigate new battery technologies"

Artificial Intelligence:
1. "Examine recent advances in foundation models"
2. "Analyze developments in AI safety research"
3. "Research progress in neuromorphic computing"
4. "Investigate advances in explainable AI"
"""
bash  theme={null}
    pip install openai exa_py agno
    bash  theme={null}
    export OPENAI_API_KEY=****
    export EXA_API_KEY=****
    bash  theme={null}
    python research_agent_exa.py
    ```
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Install libraries">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Set environment variables">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run the agent">
```

---

## Validate your filters before searching (catches typos!)

**URL:** llms-txt#validate-your-filters-before-searching-(catches-typos!)

**Contents:**
- Automatic vs Manual Search

valid_filters, invalid_keys = knowledge.validate_filters({
    "department": "hr",
    "invalid_key": "value"  # This will be flagged as invalid
})
python  theme={null}

**Examples:**

Example 1 (unknown):
```unknown
<Tip>
  Use `knowledge.get_content_status()` to debug when content doesn't appear in search results. It'll tell you if processing failed or is still in progress.
</Tip>

## Automatic vs Manual Search

Agno gives you two ways to use knowledge with agents:

**Agentic Search** (`search_knowledge=True`):
The agent automatically decides when to search and what to look for. This is the recommended approach for most use cases - it's smarter and more dynamic.

**Traditional RAG** (`add_knowledge_to_context=True`):
Relevant knowledge is always added to the agent's context. Simpler but less flexible. Use this when you want predictable, consistent behavior.
```

---

## Fetch the audio file and convert it to a base64 encoded string

**URL:** llms-txt#fetch-the-audio-file-and-convert-it-to-a-base64-encoded-string

url = "https://openaiassets.blob.core.windows.net/$web/API/docs/audio/alloy.wav"
response = requests.get(url)
response.raise_for_status()
wav_data = response.content

---

## Create output directory

**URL:** llms-txt#create-output-directory

output_dir = Path(output_dir)
output_dir.mkdir(parents=True, exist_ok=True)

---

## Human-in-the-Loop Example

**URL:** llms-txt#human-in-the-loop-example

**Contents:**
- Prerequisites
- Code

Source: https://docs.agno.com/examples/agent-os/hitl

AgentOS with tools requiring user confirmation

This example shows how to implement Human-in-the-Loop in AgentOS. When an agent needs to execute a tool that requires confirmation, the run pauses and waits for user approval before proceeding.

* Python 3.10 or higher
* PostgreSQL with pgvector (setup instructions below)
* OpenAI API key

```python hitl_confirmation.py theme={null}
from agno.agent import Agent
from agno.db.postgres import PostgresDb
from agno.models.openai import OpenAIChat
from agno.os import AgentOS
from agno.tools import tool

---

## Upload Content

**URL:** llms-txt#upload-content

Source: https://docs.agno.com/reference-api/schema/knowledge/upload-content

post /knowledge/content
Upload content to the knowledge base. Supports file uploads, text content, or URLs. Content is processed asynchronously in the background. Supports custom readers and chunking strategies.

---

## Get embedding with usage information

**URL:** llms-txt#get-embedding-with-usage-information

embedding, usage = custom_embedder.get_embedding_and_usage(
    "Advanced text processing with Jina embeddings and late chunking."
)
print(f"Embedding dimensions: {len(embedding)}")
if usage:
    print(f"Usage info: {usage}")

---

## Install infinity

**URL:** llms-txt#install-infinity

pip install "infinity-emb[all]"

---

## Access metrics from the response

**URL:** llms-txt#access-metrics-from-the-response

---

## Redis

**URL:** llms-txt#redis

**Contents:**
- Usage
  - Run Redis

Source: https://docs.agno.com/concepts/db/redis

Learn to use Redis as a database for your Agents

Agno supports using [Redis](https://redis.io/) as a database with the `RedisDb` class.

Install [docker desktop](https://docs.docker.com/desktop/install/mac-install/) and run **Redis** on port **6379** using:

```python redis_for_agent.py theme={null}
from agno.agent import Agent
from agno.db.redis import RedisDb

**Examples:**

Example 1 (unknown):
```unknown

```

---

## Add Python Libraries

**URL:** llms-txt#add-python-libraries

**Contents:**
- Update pyproject.toml
- Generate requirements
- Rebuild Images
  - Rebuild dev images
  - Rebuild production images
- Recreate Resources
  - Recreate dev containers
  - Update ECS services

Source: https://docs.agno.com/templates/infra-management/python-packages

Agno templates are setup to manage dependencies using a [pyproject.toml](https://packaging.python.org/en/latest/specifications/declaring-project-metadata/#declaring-project-metadata) file, **which is used to generate the `requirements.txt` file using [uv](https://github.com/astral-sh/uv) or [pip-tools](https://pip-tools.readthedocs.io/en/latest/).**

Adding or Updating a python library is a 2 step process:

1. Add library to the `pyproject.toml` file
2. Auto-Generate the `requirements.txt` file

<Warning>
  We highly recommend auto-generating the `requirements.txt` file using this process.
</Warning>

## Update pyproject.toml

* Open the `pyproject.toml` file
* Add new libraries to the dependencies section.

## Generate requirements

After updating the `dependencies` in the `pyproject.toml` file, auto-generate the `requirements.txt` file using a helper script or running `pip-compile` directly.

If you'd like to upgrade all python libraries to their latest version, run:

After updating the `requirements.txt` file, rebuild your images.

### Rebuild dev images

### Rebuild production images

<Note>
  Remember to [authenticate with ECR](/templates/infra-management/production-app#ecr-images) if needed.
</Note>

## Recreate Resources

After rebuilding images, recreate the resources.

### Recreate dev containers

### Update ECS services

**Examples:**

Example 1 (unknown):
```unknown

```

Example 2 (unknown):
```unknown
</CodeGroup>

If you'd like to upgrade all python libraries to their latest version, run:

<CodeGroup>
```

Example 3 (unknown):
```unknown

```

Example 4 (unknown):
```unknown
</CodeGroup>

## Rebuild Images

After updating the `requirements.txt` file, rebuild your images.

### Rebuild dev images

<CodeGroup>
```

---

## Print team member message metrics

**URL:** llms-txt#print-team-member-message-metrics

**Contents:**
- Developer Resources

print("---" * 5, "Team Member Message Metrics", "---" * 5)
if run_response.member_responses:
    for member_response in run_response.member_responses:
        if member_response.messages:
            for message in member_response.messages:
                if message.role == "assistant":
                    if message.content:
                        print(f"Member Message: {message.content}")
                    elif message.tool_calls:
                        print(f"Member Tool calls: {message.tool_calls}")
                    print("---" * 5, "Member Metrics", "---" * 5)
                    pprint(message.metrics)
                    print("---" * 20)
```

You'll see the outputs with following information:

* `input_tokens`: The number of tokens sent to the model.
* `output_tokens`: The number of tokens received from the model.
* `total_tokens`: The sum of `input_tokens` and `output_tokens`.
* `audio_input_tokens`: The number of tokens sent to the model for audio input.
* `audio_output_tokens`: The number of tokens received from the model for audio output.
* `audio_total_tokens`: The sum of `audio_input_tokens` and `audio_output_tokens`.
* `cache_read_tokens`: The number of tokens read from the cache.
* `cache_write_tokens`: The number of tokens written to the cache.
* `reasoning_tokens`: The number of tokens used for reasoning.
* `duration`: The duration of the run in seconds.
* `time_to_first_token`: The time taken until the first token was generated.
* `provider_metrics`: Any provider-specific metrics.

## Developer Resources

* View the [TeamRunOutput schema](/reference/teams/team-response)
* View the [Metrics schema](/reference/agents/metrics)
* View [Cookbook](https://github.com/agno-agi/agno/tree/main/cookbook/teams/metrics/01_team_metrics.py)

---

## Web Fetch

**URL:** llms-txt#web-fetch

**Contents:**
- Code
- Usage

Source: https://docs.agno.com/examples/models/anthropic/web_fetch

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Set your API key">
    
  </Step>

<Step title="Install libraries">
    
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Set your API key">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Configure Exa for broader web intelligence

**URL:** llms-txt#configure-exa-for-broader-web-intelligence

**Contents:**
- Step 3: Define Intelligence Strategy
  - 3a. Define the Data Collection Strategy

exa_tools = ExaTools(
    num_results=10,               # Comprehensive but not overwhelming
    include_domains=["reddit.com", "news.ycombinator.com", "medium.com"],
)

print("ExaTools configured for web search")
python  theme={null}
from textwrap import dedent

**Examples:**

Example 1 (unknown):
```unknown
**Why these specific domains?**

* **Reddit**: Early discussion indicators, community sentiment
* **HackerNews**: Tech industry insights, developer opinions
* **Medium**: Thought leadership, analysis articles

## Step 3: Define Intelligence Strategy

**Why do we need instructions?** We need to describe the strategy that the agent should take to collect and analyze content. Without clear instructions, the agent won't know how to use the tools effectively or what kind of analysis to provide.

### 3a. Define the Data Collection Strategy
```

---

## BaseGuardrail

**URL:** llms-txt#baseguardrail

**Contents:**
- Methods
  - `check`
  - `async_check`

Source: https://docs.agno.com/reference/hooks/base-guardrail

Perform the guardrail checks synchronously.

* `run_input` (RunInput | TeamRunInput): The input provided to the Agent or Team when invoking the run.

Perform the guardrail checks asynchronously.

* `run_input` (RunInput | TeamRunInput): The input provided to the Agent or Team when invoking the run.

---

## Usage

**URL:** llms-txt#usage

```python aws_bedrock_embedder.py theme={null}
import asyncio
from agno.knowledge.embedder.aws_bedrock import AwsBedrockEmbedder
from agno.knowledge.knowledge import Knowledge
from agno.knowledge.reader.pdf_reader import PDFReader
from agno.vectordb.pgvector import PgVector

embeddings = AwsBedrockEmbedder().get_embedding(
    "The quick brown fox jumps over the lazy dog."
)

---

## Azure OpenAI with Reasoning Tools

**URL:** llms-txt#azure-openai-with-reasoning-tools

**Contents:**
- Code
- Usage

Source: https://docs.agno.com/examples/concepts/reasoning/tools/azure-openai-reasoning-tools

This example shows how to use `ReasoningTools` with an Azure OpenAI model.

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Set your API key">
    
  </Step>

<Step title="Install libraries">
    
  </Step>

<Step title="Run Example">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Set your API key">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Example">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Milvus Async

**URL:** llms-txt#milvus-async

**Contents:**
- Code
- Usage

Source: https://docs.agno.com/examples/concepts/vectordb/milvus-db/async-milvus-db

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Install libraries">
    
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Install libraries">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

Example 3 (unknown):
```unknown

```

---

## Large PDF gets broken into multiple documents

**URL:** llms-txt#large-pdf-gets-broken-into-multiple-documents

pdf_reader = PDFReader(chunk=True, chunk_size=1000)
documents = pdf_reader.read("large_document.pdf")

---

## Imagen Tool with OpenAI

**URL:** llms-txt#imagen-tool-with-openai

**Contents:**
- Code
- Usage

Source: https://docs.agno.com/examples/models/gemini/imagen_tool

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Set your API keys">
    
  </Step>

<Step title="Install libraries">
    
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Set your API keys">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Webex

**URL:** llms-txt#webex

**Contents:**
- Prerequisites
- Example

Source: https://docs.agno.com/concepts/tools/toolkits/social/webex

**WebexTools** enable an Agent to interact with Cisco Webex, allowing it to send messages and list rooms.

The following example requires the `webexpythonsdk` library and a Webex access token which can be obtained from [Webex Developer Portal](https://developer.webex.com/docs/bots).

To get started with Webex:

1. **Create a Webex Bot:**
   * Go to the [Developer Portal](https://developer.webex.com/)
   * Navigate to My Webex Apps → Create a Bot
   * Fill in the bot details and click Add Bot

2. **Get your access token:**
   * Copy the token shown after bot creation
   * Or regenerate via My Webex Apps → Edit Bot
   * Set as WEBEX\_ACCESS\_TOKEN environment variable

3. **Add the bot to Webex:**
   * Launch Webex and add the bot to a space
   * Use the bot's email (e.g. [test@webex.bot](mailto:test@webex.bot))

The following agent will list all spaces and send a message using Webex:

```python cookbook/tools/webex_tool.py theme={null}
from agno.agent import Agent
from agno.tools.webex import WebexTools

agent = Agent(tools=[WebexTools()])

**Examples:**

Example 1 (unknown):
```unknown

```

Example 2 (unknown):
```unknown
## Example

The following agent will list all spaces and send a message using Webex:
```

---

## ContentsDB automatically stores all the fields from the schema above

**URL:** llms-txt#contentsdb-automatically-stores-all-the-fields-from-the-schema-above

---

## Structured Output With Tool Use

**URL:** llms-txt#structured-output-with-tool-use

**Contents:**
- Code
- Usage

Source: https://docs.agno.com/examples/models/mistral/structured_output_with_tool_use

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Set your API key">
    
  </Step>

<Step title="Install libraries">
    
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Set your API key">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Qdrant

**URL:** llms-txt#qdrant

Source: https://docs.agno.com/reference/vector_db/qdrant

<Snippet file="vector-db-qdrant-reference.mdx" />

---

## Our custom tool

**URL:** llms-txt#our-custom-tool

@tool()
async def get_customer_profile():
    """
    Get the customer profile for the customer with ID 123.
    """
    yield CustomerProfileEvent(
        customer_name="John Doe",
        customer_email="john.doe@example.com",
        customer_phone="1234567890",
    )

agent = Agent(
    name="Customer Support Agent",
    role="Support agent that handles customer requests.",
    model=OpenAIChat(id="gpt-4o"),
)

---

## Example: Data visualization

**URL:** llms-txt#example:-data-visualization

agent.print_response(
    "Write a Python script that creates a sample dataset of sales by region and visualize it with matplotlib"
)

---

## For batch loading

**URL:** llms-txt#for-batch-loading

**Contents:**
  - 3. Use Metadata Filters

knowledge.add_contents(
    paths=["docs/", "policies/"],
    skip_if_exists=True,
    include=["*.pdf", "*.md"],
    exclude=["*temp*", "*draft*"]
)
python  theme={null}

**Examples:**

Example 1 (unknown):
```unknown
### 3. Use Metadata Filters

Narrow searches before vector comparison for faster, more accurate results:
```

---

## Files Generation Tools

**URL:** llms-txt#files-generation-tools

**Contents:**
- Developer Resources

Source: https://docs.agno.com/concepts/multimodal/files/file_generation

Learn how to use files generation tools with Agno agents.

Agno provides the [`FileGenerationTools`](/concepts/tools/toolkits/file-generation/file-generation) to generate files in different formats.

## Developer Resources

* See the [File Generation Tools](/concepts/tools/toolkits/file-generation/file-generation) documentation.
* See the [File Generation Tools](/examples/concepts/tools/file-generation) example.

---

## Redis for Workflows

**URL:** llms-txt#redis-for-workflows

**Contents:**
- Usage
  - Run Redis

Source: https://docs.agno.com/examples/concepts/db/redis/redis_for_workflow

Agno supports using Redis as a storage backend for Workflows using the `RedisDb` class.

Install [docker desktop](https://docs.docker.com/desktop/install/mac-install/) and run **Redis** on port **6379** using:

```python redis_for_workflow.py theme={null}
"""
Run: `pip install openai httpx newspaper4k redis agno` to install the dependencies
"""
from agno.agent import Agent
from agno.db.redis import RedisDb
from agno.models.openai import OpenAIChat
from agno.team import Team
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.hackernews import HackerNewsTools
from agno.workflow.step import Step
from agno.workflow.workflow import Workflow

**Examples:**

Example 1 (unknown):
```unknown

```

---

## Async Basic Stream.Py

**URL:** llms-txt#async-basic-stream.py

**Contents:**
- Code
- Usage

Source: https://docs.agno.com/examples/models/huggingface/async_basic_stream

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Set your API key">
    
  </Step>

<Step title="Install libraries">
    
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Set your API key">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Create workflow with condition

**URL:** llms-txt#create-workflow-with-condition

workflow = Workflow(
    name="Conditional Greeting Workflow",
    steps=[
        # First, check if user has been greeted before
        Condition(
            name="Check If New User",
            description="Check if this is a new user who needs greeting",
            # Condition returns True if user has context, so we negate it
            evaluator=lambda step_input, session_state: not check_user_has_context(
                step_input, session_state
            ),
            steps=[
                # Only execute these steps for new users
                Step(
                    name="Greet User",
                    description="Greet the new user",
                    agent=greeter_agent,
                ),
                Step(
                    name="Mark as Greeted",
                    description="Mark user as greeted in session",
                    executor=mark_user_as_greeted,
                ),
            ],
        ),
        # This step always executes
        Step(
            name="Handle Query",
            description="Handle the user's query with or without greeting",
            agent=contextual_agent,
        ),
    ],
    session_state={
        "has_been_greeted": False,
        "greeting_count": 0,
    },
)

def run_example():
    """Run the example workflow multiple times to see conditional behavior."""

print("=" * 80)
    print("First Run - New User (Condition will be True, greeting will happen)")
    print("=" * 80)

workflow.print_response(
        input="Hi, can you help me with something?",
        session_id="user-123",
        user_id="user-123",
        stream=True,
        stream_events=True,
    )

print("\n" + "=" * 80)
    print("Second Run - Same Session (Skips greeting)")
    print("=" * 80)

workflow.print_response(
        input="Tell me a joke",
        session_id="user-123",
        user_id="user-123",
        stream=True,
        stream_events=True,
    )

if __name__ == "__main__":
    run_example()
```

---

## macOS with Homebrew

**URL:** llms-txt#macos-with-homebrew

**Contents:**
  - Download a Model
  - Start the Server
- Example
- Configuration
- Params
- Server Configuration
  - Common Server Options
  - Model Options
- Performance Optimization
  - Hardware Acceleration

brew install llama.cpp
bash start server theme={null}
llama-server -hf ggml-org/gpt-oss-20b-GGUF --ctx-size 0 --jinja -ub 2048 -b 2048
python agent.py theme={null}
  from agno.agent import Agent
  from agno.models.llama_cpp import LlamaCpp

agent = Agent(
      model=LlamaCpp(id="ggml-org/gpt-oss-20b-GGUF"),
      markdown=True
  )

# Print the response in the terminal
  agent.print_response("Share a 2 sentence horror story.")
  python custom_config.py theme={null}
  from agno.agent import Agent
  from agno.models.llama_cpp import LlamaCpp

# Custom server configuration
  agent = Agent(
      model=LlamaCpp(
          id="your-custom-model",
          base_url="http://localhost:8080/v1",  # Custom server URL
      ),
      markdown=True
  )
  bash gpu acceleration theme={null}

**Examples:**

Example 1 (unknown):
```unknown
### Download a Model

Download a model in GGUF format following the [llama.cpp model download guide](https://github.com/ggerganov/llama.cpp#obtaining-and-using-the-facebook-llama-2-model). For the examples below, we use `ggml-org/gpt-oss-20b-GGUF`.

### Start the Server

Start the LlamaCpp server with your model:
```

Example 2 (unknown):
```unknown
This starts the server at `http://127.0.0.1:8080` with an OpenAI Chat compatible endpoints

## Example

After starting the LlamaCpp server, use the `LlamaCpp` model class to access it:

<CodeGroup>
```

Example 3 (unknown):
```unknown
</CodeGroup>

## Configuration

The `LlamaCpp` model supports customizing the server URL and model ID:

<CodeGroup>
```

Example 4 (unknown):
```unknown
</CodeGroup>

<Note> View more examples [here](/examples/models/llama_cpp/basic). </Note>

## Params

| Parameter     | Type              | Default                   | Description                                                  |
| ------------- | ----------------- | ------------------------- | ------------------------------------------------------------ |
| `id`          | `str`             | `"llama-cpp"`             | The identifier for the Llama.cpp model                       |
| `name`        | `str`             | `"LlamaCpp"`              | The name of the model                                        |
| `provider`    | `str`             | `"LlamaCpp"`              | The provider of the model                                    |
| `base_url`    | `str`             | `"http://localhost:8080"` | The base URL for the Llama.cpp server                        |
| `api_key`     | `Optional[str]`   | `None`                    | The API key (usually not needed for local Llama.cpp)         |
| `chat_format` | `Optional[str]`   | `None`                    | The chat format to use (e.g., "chatml", "llama-2", "alpaca") |
| `n_ctx`       | `Optional[int]`   | `None`                    | The context window size                                      |
| `temperature` | `Optional[float]` | `None`                    | Sampling temperature (0.0 to 2.0)                            |
| `top_p`       | `Optional[float]` | `None`                    | Top-p sampling parameter                                     |
| `top_k`       | `Optional[int]`   | `None`                    | Top-k sampling parameter                                     |

`LlamaCpp` is a subclass of the [OpenAILike](/concepts/models/openai-like) class and has access to the same params.

## Server Configuration

The LlamaCpp server supports many configuration options:

### Common Server Options

* `--ctx-size`: Context size (0 for unlimited)
* `--batch-size`, `-b`: Batch size for prompt processing
* `--ubatch-size`, `-ub`: Physical batch size for prompt processing
* `--threads`, `-t`: Number of threads to use
* `--host`: IP address to listen on (default: 127.0.0.1)
* `--port`: Port to listen on (default: 8080)

### Model Options

* `--model`, `-m`: Model file path
* `--hf-repo`: HuggingFace model repository
* `--jinja`: Use Jinja templating for chat formatting

For a complete list of server options, run `llama-server --help`.

## Performance Optimization

### Hardware Acceleration

LlamaCpp supports various acceleration backends:
```

---

## Example 2: Sleep for a longer duration

**URL:** llms-txt#example-2:-sleep-for-a-longer-duration

**Contents:**
- Toolkit Params
- Toolkit Functions
- Developer Resources

agent.print_response("Sleep for 5 seconds")
```

| Parameter      | Type   | Default | Description                                |
| -------------- | ------ | ------- | ------------------------------------------ |
| `enable_sleep` | `bool` | `True`  | Enables sleep functionality                |
| `all`          | `bool` | `False` | Enables all functionality when set to True |

| Function | Description                                        |
| -------- | -------------------------------------------------- |
| `sleep`  | Pauses execution for a specified number of seconds |

## Developer Resources

* View [Tools](https://github.com/agno-agi/agno/blob/main/libs/agno/agno/tools/sleep.py)
* View [Cookbook](https://github.com/agno-agi/agno/tree/main/cookbook/tools/sleep_tools.py)

---

## Example 2: Invoke a specific Lambda function

**URL:** llms-txt#example-2:-invoke-a-specific-lambda-function

**Contents:**
- Toolkit Params
- Toolkit Functions
- Developer Resources

agent.print_response("Invoke the 'hello-world' Lambda function with an empty payload", markdown=True)
```

| Parameter                | Type   | Default       | Description                                         |
| ------------------------ | ------ | ------------- | --------------------------------------------------- |
| `region_name`            | `str`  | `"us-east-1"` | AWS region name where Lambda functions are located. |
| `enable_list_functions`  | `bool` | `True`        | Enable the list\_functions functionality.           |
| `enable_invoke_function` | `bool` | `True`        | Enable the invoke\_function functionality.          |
| `all`                    | `bool` | `False`       | Enable all functionality.                           |

| Function          | Description                                                                                                           |
| ----------------- | --------------------------------------------------------------------------------------------------------------------- |
| `list_functions`  | Lists all Lambda functions available in the AWS account.                                                              |
| `invoke_function` | Invokes a specific Lambda function with an optional payload. Takes `function_name` and optional `payload` parameters. |

## Developer Resources

* View [Tools](https://github.com/agno-agi/agno/blob/main/libs/agno/agno/tools/aws_lambda.py)
* View [Cookbook](https://github.com/agno-agi/agno/tree/main/cookbook/tools/aws_lambda_tools.py)

---

## Create our News Reporter with a fun personality

**URL:** llms-txt#create-our-news-reporter-with-a-fun-personality

agent = Agent(
    model=OpenAIChat(id="gpt-5-mini"),
    instructions=dedent("""\
        You are an enthusiastic news reporter with a flair for storytelling! 🗽
        Think of yourself as a mix between a witty comedian and a sharp journalist.

Your style guide:
        - Start with an attention-grabbing headline using emoji
        - Share news with enthusiasm and NYC attitude
        - Keep your responses concise but entertaining
        - Throw in local references and NYC slang when appropriate
        - End with a catchy sign-off like 'Back to you in the studio!' or 'Reporting live from the Big Apple!'

Remember to verify all facts while keeping that NYC energy high!\
    """),
    markdown=True,
)

---

## This may work but with unpredictable results due to message format differences

**URL:** llms-txt#this-may-work-but-with-unpredictable-results-due-to-message-format-differences

gemini_agent.print_response(
    "Continue our discussion about machine learning", session_id=session_id, user_id=user_id
)

---

## Use Custom Domain and HTTPS

**URL:** llms-txt#use-custom-domain-and-https

**Contents:**
- Overview
- Use a custom domain
  - Custom domain for your AgentOS App
- Add HTTPS

Source: https://docs.agno.com/templates/infra-management/domain-https

To add a live AgentOS instance to os.agno.com, the endpoint must be HTTPS. Here is how you can add a custom domain and HTTPS to your AWS loadbalancer.

## Use a custom domain

1. Register your domain with [Route 53](https://us-east-1.console.aws.amazon.com/route53/).
2. Point the domain to the loadbalancer DNS.

### Custom domain for your AgentOS App

Create a record in the Route53 console to point `app.[YOUR_DOMAIN]` to the AgentOS endpoint.

<img src="https://mintcdn.com/agno-v2/yeT29TzCG5roT0hQ/images/llm-app-aidev-run.png?fit=max&auto=format&n=yeT29TzCG5roT0hQ&q=85&s=2387492f4fa89cab98e2a603da83535b" alt="llm-app-aidev-run" data-og-width="1081" width="1081" data-og-height="569" height="569" data-path="images/llm-app-aidev-run.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/agno-v2/yeT29TzCG5roT0hQ/images/llm-app-aidev-run.png?w=280&fit=max&auto=format&n=yeT29TzCG5roT0hQ&q=85&s=1d9004362958e21665a9deb78811a4dd 280w, https://mintcdn.com/agno-v2/yeT29TzCG5roT0hQ/images/llm-app-aidev-run.png?w=560&fit=max&auto=format&n=yeT29TzCG5roT0hQ&q=85&s=16e4a3f99f355b484c3bfc0ff98ec7c9 560w, https://mintcdn.com/agno-v2/yeT29TzCG5roT0hQ/images/llm-app-aidev-run.png?w=840&fit=max&auto=format&n=yeT29TzCG5roT0hQ&q=85&s=d2659746f0b009a8e84c7fe1528cac65 840w, https://mintcdn.com/agno-v2/yeT29TzCG5roT0hQ/images/llm-app-aidev-run.png?w=1100&fit=max&auto=format&n=yeT29TzCG5roT0hQ&q=85&s=1b3b6f7c9a098578bcc4cf88f5802588 1100w, https://mintcdn.com/agno-v2/yeT29TzCG5roT0hQ/images/llm-app-aidev-run.png?w=1650&fit=max&auto=format&n=yeT29TzCG5roT0hQ&q=85&s=d1e8a2a0a23ce16cbaf50ebe0fce0fba 1650w, https://mintcdn.com/agno-v2/yeT29TzCG5roT0hQ/images/llm-app-aidev-run.png?w=2500&fit=max&auto=format&n=yeT29TzCG5roT0hQ&q=85&s=4c1c3fbd924321ecfe7aabedc3d9ad1e 2500w" />

You can visit the app at `[http://app.[YOUR_DOMAIN]`

<Note>Note the `http` in the domain name.</Note>

1. Create a certificate using [AWS ACM](https://us-east-1.console.aws.amazon.com/acm). Request a certificat for `*.[YOUR_DOMAIN]`

<img src="https://mintcdn.com/agno-v2/yeT29TzCG5roT0hQ/images/llm-app-request-cert.png?fit=max&auto=format&n=yeT29TzCG5roT0hQ&q=85&s=15b580029369ef5c8039bddfad4be52d" alt="llm-app-request-cert" data-og-width="1105" width="1105" data-og-height="581" height="581" data-path="images/llm-app-request-cert.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/agno-v2/yeT29TzCG5roT0hQ/images/llm-app-request-cert.png?w=280&fit=max&auto=format&n=yeT29TzCG5roT0hQ&q=85&s=05442b5b3ac14b98d42e885488be4ede 280w, https://mintcdn.com/agno-v2/yeT29TzCG5roT0hQ/images/llm-app-request-cert.png?w=560&fit=max&auto=format&n=yeT29TzCG5roT0hQ&q=85&s=9ce2e4e86b46e10e984aa1b1ab9939a7 560w, https://mintcdn.com/agno-v2/yeT29TzCG5roT0hQ/images/llm-app-request-cert.png?w=840&fit=max&auto=format&n=yeT29TzCG5roT0hQ&q=85&s=85149f2d24287108d22bd604f7014dc3 840w, https://mintcdn.com/agno-v2/yeT29TzCG5roT0hQ/images/llm-app-request-cert.png?w=1100&fit=max&auto=format&n=yeT29TzCG5roT0hQ&q=85&s=dbb81c8a8e8df3c85eaed02a097a89b1 1100w, https://mintcdn.com/agno-v2/yeT29TzCG5roT0hQ/images/llm-app-request-cert.png?w=1650&fit=max&auto=format&n=yeT29TzCG5roT0hQ&q=85&s=6c804f2e99c3881f52cabd5566b54802 1650w, https://mintcdn.com/agno-v2/yeT29TzCG5roT0hQ/images/llm-app-request-cert.png?w=2500&fit=max&auto=format&n=yeT29TzCG5roT0hQ&q=85&s=b9534df74f2df9325cdeae0448e25bfd 2500w" />

2. Creating records in Route 53.

<img src="https://mintcdn.com/agno-v2/yeT29TzCG5roT0hQ/images/llm-app-validate-cert.png?fit=max&auto=format&n=yeT29TzCG5roT0hQ&q=85&s=4291826e3abd20126daf4e1bbd42c0a3" alt="llm-app-validate-cert" data-og-width="1322" width="1322" data-og-height="566" height="566" data-path="images/llm-app-validate-cert.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/agno-v2/yeT29TzCG5roT0hQ/images/llm-app-validate-cert.png?w=280&fit=max&auto=format&n=yeT29TzCG5roT0hQ&q=85&s=c7813e664032c848f1b2c5a51953f8eb 280w, https://mintcdn.com/agno-v2/yeT29TzCG5roT0hQ/images/llm-app-validate-cert.png?w=560&fit=max&auto=format&n=yeT29TzCG5roT0hQ&q=85&s=b98c005abf74fc4a7b7c75678cf8b0f6 560w, https://mintcdn.com/agno-v2/yeT29TzCG5roT0hQ/images/llm-app-validate-cert.png?w=840&fit=max&auto=format&n=yeT29TzCG5roT0hQ&q=85&s=b25347bff02d1684a44906d860b51a98 840w, https://mintcdn.com/agno-v2/yeT29TzCG5roT0hQ/images/llm-app-validate-cert.png?w=1100&fit=max&auto=format&n=yeT29TzCG5roT0hQ&q=85&s=f7a1bafd43f582f40f13513ea64daa32 1100w, https://mintcdn.com/agno-v2/yeT29TzCG5roT0hQ/images/llm-app-validate-cert.png?w=1650&fit=max&auto=format&n=yeT29TzCG5roT0hQ&q=85&s=179c892f6141685a69e521865a7c4d28 1650w, https://mintcdn.com/agno-v2/yeT29TzCG5roT0hQ/images/llm-app-validate-cert.png?w=2500&fit=max&auto=format&n=yeT29TzCG5roT0hQ&q=85&s=051a273f486a1f576ee0cedd97c24c69 2500w" />

3. Add the certificate ARN to Apps

<Note>Make sure the certificate is `Issued` before adding it to your Apps</Note>

Update the `infra/prd_resources.py` file and add the `load_balancer_certificate_arn` to the `FastAPI` app.

```python infra/prd_resources.py theme={null}

---

## pprint(response.content)

**URL:** llms-txt#pprint(response.content)

**Contents:**
- Usage

agent.print_response("New York")
bash  theme={null}
    export DEEPINFRA_API_KEY=xxx
    bash  theme={null}
    pip install -U openai agno
    bash Mac theme={null}
      python cookbook/models/deepinfra/json_output.py
      bash Windows theme={null}
      python cookbook/models/deepinfra/json_output.py
      ```
    </CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Set your API key">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Fast: Filter first, then search

**URL:** llms-txt#fast:-filter-first,-then-search

results = knowledge.search(
    query="deployment process",
    max_results=10,
    filters={"department": "engineering", "type": "procedure"}
)

---

## Team using our URLGuardrail

**URL:** llms-txt#team-using-our-urlguardrail

team = Team(
    name="URL-Protected Team",
    model=OpenAIChat(id="gpt-5-mini"),
    # Provide the Guardrails to be used with the pre_hooks parameter
    pre_hooks=[URLGuardrail()],
)

---

## Google Sheets

**URL:** llms-txt#google-sheets

**Contents:**
- Prerequisites
- How to Get Credentials
- Example
- Toolkit Params
- Toolkit Functions
- Developer Resources

Source: https://docs.agno.com/concepts/tools/toolkits/others/google_sheets

**GoogleSheetsTools** enable an Agent to interact with Google Sheets API for reading, creating, updating, and duplicating spreadsheets.

You need to install the required Google API client libraries:

Set up the following environment variables:

## How to Get Credentials

1. Go to Google Cloud Console ([https://console.cloud.google.com](https://console.cloud.google.com))

2. Create a new project or select an existing one

3. Enable the Google Sheets API:
   * Go to "APIs & Services" > "Enable APIs and Services"
   * Search for "Google Sheets API"
   * Click "Enable"

4. Create OAuth 2.0 credentials:
   * Go to "APIs & Services" > "Credentials"
   * Click "Create Credentials" > "OAuth client ID"
   * Go through the OAuth consent screen setup
   * Give it a name and click "Create"
   * You'll receive:
     * Client ID (GOOGLE\_CLIENT\_ID)
     * Client Secret (GOOGLE\_CLIENT\_SECRET)
   * The Project ID (GOOGLE\_PROJECT\_ID) is visible in the project dropdown at the top of the page

The following agent will use Google Sheets to read and update spreadsheet data.

| Parameter                       | Type                    | Default | Description                                                |
| ------------------------------- | ----------------------- | ------- | ---------------------------------------------------------- |
| `scopes`                        | `Optional[List[str]]`   | `None`  | Custom OAuth scopes. If None, uses write scope by default. |
| `spreadsheet_id`                | `Optional[str]`         | `None`  | ID of the target spreadsheet.                              |
| `spreadsheet_range`             | `Optional[str]`         | `None`  | Range within the spreadsheet.                              |
| `creds`                         | `Optional[Credentials]` | `None`  | Pre-existing credentials.                                  |
| `creds_path`                    | `Optional[str]`         | `None`  | Path to credentials file.                                  |
| `token_path`                    | `Optional[str]`         | `None`  | Path to token file.                                        |
| `oauth_port`                    | `int`                   | `0`     | Port to use for OAuth authentication.                      |
| `enable_read_sheet`             | `bool`                  | `True`  | Enable reading from a sheet.                               |
| `enable_create_sheet`           | `bool`                  | `False` | Enable creating a sheet.                                   |
| `enable_update_sheet`           | `bool`                  | `False` | Enable updating a sheet.                                   |
| `enable_create_duplicate_sheet` | `bool`                  | `False` | Enable creating a duplicate sheet.                         |
| `all`                           | `bool`                  | `False` | Enable all tools.                                          |

| Function                 | Description                                                                                                                                                                                                                                                                                 |
| ------------------------ | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| `read_sheet`             | Read values from a Google Sheet. Parameters include `spreadsheet_id` (Optional\[str]) for fallback spreadsheet ID and `spreadsheet_range` (Optional\[str]) for fallback range. Returns JSON of list of rows.                                                                                |
| `create_sheet`           | Create a new Google Sheet. Parameters include `title` (str) for the title of the Google Sheet. Returns the ID of the created Google Sheet.                                                                                                                                                  |
| `update_sheet`           | Update data in a Google Sheet. Parameters include `data` (List\[List\[Any]]) for the data to update, `spreadsheet_id` (Optional\[str]) for the ID of the Google Sheet, and `range_name` (Optional\[str]) for the range to update. Returns success or failure message.                       |
| `create_duplicate_sheet` | Create a duplicate of an existing Google Sheet. Parameters include `source_id` (str) for the ID of the source spreadsheet, `new_title` (Optional\[str]) for new title, and `copy_permissions` (bool, default=True) for whether to copy permissions. Returns link to duplicated spreadsheet. |

## Developer Resources

* View [Tools](https://github.com/agno-agi/agno/blob/main/libs/agno/agno/tools/googlesheets.py)
* View [Cookbook](https://github.com/agno-agi/agno/tree/main/cookbook/tools/googlesheets_tools.py)

**Examples:**

Example 1 (unknown):
```unknown
Set up the following environment variables:
```

Example 2 (unknown):
```unknown
## How to Get Credentials

1. Go to Google Cloud Console ([https://console.cloud.google.com](https://console.cloud.google.com))

2. Create a new project or select an existing one

3. Enable the Google Sheets API:
   * Go to "APIs & Services" > "Enable APIs and Services"
   * Search for "Google Sheets API"
   * Click "Enable"

4. Create OAuth 2.0 credentials:
   * Go to "APIs & Services" > "Credentials"
   * Click "Create Credentials" > "OAuth client ID"
   * Go through the OAuth consent screen setup
   * Give it a name and click "Create"
   * You'll receive:
     * Client ID (GOOGLE\_CLIENT\_ID)
     * Client Secret (GOOGLE\_CLIENT\_SECRET)
   * The Project ID (GOOGLE\_PROJECT\_ID) is visible in the project dropdown at the top of the page

## Example

The following agent will use Google Sheets to read and update spreadsheet data.
```

---

## Define workflow with steps

**URL:** llms-txt#define-workflow-with-steps

workflow = Workflow(
    name="Content Creation Workflow",
    steps=[
        Step(name="Research Step", team=research_team),
        Step(name="Content Planning Step", executor=custom_content_planning_function),
    ]
)

---

## The response will have status: "paused" with tools awaiting confirmation

**URL:** llms-txt#the-response-will-have-status:-"paused"-with-tools-awaiting-confirmation

---

## DynamoDB for Team

**URL:** llms-txt#dynamodb-for-team

**Contents:**
- Usage

Source: https://docs.agno.com/examples/concepts/db/dynamodb/dynamodb_for_team

Agno supports using DynamoDB as a storage backend for Teams using the `DynamoDb` class.

You need to provide `aws_access_key_id` and `aws_secret_access_key` parameters to the `DynamoDb` class.

```python dynamo_for_team.py theme={null}
"""
Run: `pip install openai ddgs newspaper4k lxml_html_clean agno` to install the dependencies
"""

from typing import List

from agno.agent import Agent
from agno.db.dynamo import DynamoDb
from agno.models.openai import OpenAIChat
from agno.team import Team
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.hackernews import HackerNewsTools
from pydantic import BaseModel

---

## === BASIC LINEAR WORKFLOW ===

**URL:** llms-txt#===-basic-linear-workflow-===

basic_workflow = Workflow(
    name="Basic Linear Workflow",
    description="Research -> Summarize -> Condition(Fact Check) -> Write Article",
    steps=[
        research_step,
        summarize_step,
        Condition(
            name="fact_check_condition",
            description="Check if fact-checking is needed",
            evaluator=needs_fact_checking,
            steps=[fact_check_step],
        ),
        write_article,
    ],
)

if __name__ == "__main__":
    print("🚀 Running Basic Linear Workflow Example")
    print("=" * 50)

try:
        basic_workflow.print_response(
            input="Recent breakthroughs in quantum computing",
            stream=True,
            stream_events=True,
        )
    except Exception as e:
        print(f"❌ Error: {e}")
        import traceback

traceback.print_exc()
```

To see the async example, see the cookbook-

* [Condition steps workflow (async streaming)](https://github.com/agno-agi/agno/blob/main/cookbook/workflows/_02_workflows_conditional_execution/sync/condition_steps_workflow_stream.py)

---

## Analyze individual member metrics

**URL:** llms-txt#analyze-individual-member-metrics

**Contents:**
- Usage

print("=" * 50)
print("TEAM MEMBER MESSAGE METRICS")
print("=" * 50)

if run_output.member_responses:
    for member_response in run_output.member_responses:
        if member_response.messages:
            for message in member_response.messages:
                if message.role == "assistant":
                    if message.content:
                        print(f"📝 Member Message: {message.content[:100]}...")
                    elif message.tool_calls:
                        print(f"🔧 Member Tool calls: {message.tool_calls}")

print("-" * 20, "Member Metrics", "-" * 20)
                    pprint(message.metrics)
                    print("-" * 60)
bash  theme={null}
    pip install agno exa_py rich
    bash  theme={null}
    export OPENAI_API_KEY=****
    export EXA_API_KEY=****
    bash  theme={null}
    cookbook/run_pgvector.sh
    bash  theme={null}
    python cookbook/examples/teams/metrics/01_team_metrics.py
    ```
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Install required libraries">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Set environment variables">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Start PostgreSQL database">
```

Example 4 (unknown):
```unknown
</Step>

  <Step title="Run the agent">
```

---

## Gemini Embedder

**URL:** llms-txt#gemini-embedder

**Contents:**
- Code

Source: https://docs.agno.com/examples/concepts/knowledge/embedders/gemini-embedder

```python  theme={null}
from agno.knowledge.embedder.google import GeminiEmbedder
from agno.knowledge.knowledge import Knowledge
from agno.vectordb.pgvector import PgVector

embeddings = GeminiEmbedder().get_embedding(
    "The quick brown fox jumps over the lazy dog."
)

---

## LanceDB Async

**URL:** llms-txt#lancedb-async

**Contents:**
- Code
- Usage

Source: https://docs.agno.com/examples/concepts/vectordb/lance-db/async-lance-db

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Install libraries">
    
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Install libraries">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

Example 3 (unknown):
```unknown

```

---

## --- Response models ---

**URL:** llms-txt#----response-models----

class IdeaClarification(BaseModel):
    originality: str = Field(..., description="Originality of the idea.")
    mission: str = Field(..., description="Mission of the company.")
    objectives: str = Field(..., description="Objectives of the company.")

class MarketResearch(BaseModel):
    total_addressable_market: str = Field(
        ..., description="Total addressable market (TAM)."
    )
    serviceable_available_market: str = Field(
        ..., description="Serviceable available market (SAM)."
    )
    serviceable_obtainable_market: str = Field(
        ..., description="Serviceable obtainable market (SOM)."
    )
    target_customer_segments: str = Field(..., description="Target customer segments.")

class CompetitorAnalysis(BaseModel):
    competitors: str = Field(..., description="List of identified competitors.")
    swot_analysis: str = Field(..., description="SWOT analysis for each competitor.")
    positioning: str = Field(
        ..., description="Startup's potential positioning relative to competitors."
    )

class ValidationReport(BaseModel):
    executive_summary: str = Field(
        ..., description="Executive summary of the validation."
    )
    idea_assessment: str = Field(..., description="Assessment of the startup idea.")
    market_opportunity: str = Field(..., description="Market opportunity analysis.")
    competitive_landscape: str = Field(
        ..., description="Competitive landscape overview."
    )
    recommendations: str = Field(..., description="Strategic recommendations.")
    next_steps: str = Field(..., description="Recommended next steps.")

---

## Firecrawl Tools

**URL:** llms-txt#firecrawl-tools

**Contents:**
- Code
- Usage

Source: https://docs.agno.com/examples/concepts/tools/web_scrape/firecrawl

Use Firecrawl with Agno to scrape and crawl the web.

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Set your API key">
    
  </Step>

<Step title="Install libraries">
    
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Set your API key">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Luma Labs Tools

**URL:** llms-txt#luma-labs-tools

**Contents:**
- Code
- Usage

Source: https://docs.agno.com/examples/concepts/tools/others/lumalabs

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Set your API key">
    
  </Step>

<Step title="Install libraries">
    
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Set your API key">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Example 1: List all Lambda functions

**URL:** llms-txt#example-1:-list-all-lambda-functions

agent.print_response("List all Lambda functions in our AWS account", markdown=True)

---

## Send a message to a Space in Webex

**URL:** llms-txt#send-a-message-to-a-space-in-webex

**Contents:**
- Usage

agent.print_response(
    "Send a funny ice-breaking message to the webex Welcome space", markdown=True
)
bash  theme={null}
    export WEBEX_ACCESS_TOKEN=xxx
    export OPENAI_API_KEY=xxx
    bash  theme={null}
    pip install -U webexpythonsdk openai agno
    bash Mac theme={null}
      python cookbook/tools/webex_tools.py
      bash Windows theme={null}
      python cookbook/tools/webex_tools.py
      ```
    </CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Set up Webex Bot">
    1. Go to [Webex Developer Portal](https://developer.webex.com/)
    2. Create a Bot:
       * Navigate to My Webex Apps → Create a Bot
       * Fill in the bot details and click Add Bot
    3. Get your access token:
       * Copy the token shown after bot creation
       * Or regenerate via My Webex Apps → Edit Bot
  </Step>

  <Step title="Set your API keys">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Custom routes that use JWT

**URL:** llms-txt#custom-routes-that-use-jwt

@app.post("/auth/login")
async def login(username: str = Form(...), password: str = Form(...)):
    """Login endpoint that returns JWT token"""
    if username == "demo" and password == "password":
        payload = {
            "sub": "user_123",
            "username": username,
            "exp": datetime.now(UTC) + timedelta(hours=24),
            "iat": datetime.now(UTC),
        }
        token = jwt.encode(payload, JWT_SECRET, algorithm="HS256")
        return {"access_token": token, "token_type": "bearer"}

raise HTTPException(status_code=401, detail="Invalid credentials")

---

## Remove the tmp db file before running the script

**URL:** llms-txt#remove-the-tmp-db-file-before-running-the-script

**Contents:**
- Usage

os.remove("tmp/data.db")

agent = Agent(
    model=OpenAIChat(id="gpt-5-mini"),
    user_id="user_1",
    db=SqliteDb(db_file="tmp/data.db"),
    add_history_to_context=True,
    num_history_runs=3,
    search_session_history=True,  # allow searching previous sessions
    num_history_sessions=2,  # only include the last 2 sessions in the search to avoid context length issues
)

session_1_id = "session_1_id"
session_2_id = "session_2_id"
session_3_id = "session_3_id"
session_4_id = "session_4_id"
session_5_id = "session_5_id"

agent.print_response("What is the capital of South Africa?", session_id=session_1_id)
agent.print_response("What is the capital of China?", session_id=session_2_id)
agent.print_response("What is the capital of France?", session_id=session_3_id)
agent.print_response("What is the capital of Japan?", session_id=session_4_id)
agent.print_response(
    "What did I discuss in my previous conversations?", session_id=session_5_id
)  # It should only include the last 2 sessions
bash  theme={null}
    pip install -U agno openai
    bash Mac/Linux theme={null}
        export OPENAI_API_KEY="your_openai_api_key_here"
      bash Windows theme={null}
        $Env:OPENAI_API_KEY="your_openai_api_key_here"
      bash  theme={null}
    touch last_n_session_messages.py
    bash Mac theme={null}
      python last_n_session_messages.py
      bash Windows   theme={null}
      python last_n_session_messages.py
      ```
    </CodeGroup>
  </Step>

<Step title="Find All Cookbooks">
    Explore all the available cookbooks in the Agno repository. Click the link below to view the code on GitHub:

<Link href="https://github.com/agno-agi/agno/tree/main/cookbook/agents/state" target="_blank">
      Agno Cookbooks on GitHub
    </Link>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Install libraries">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Export your OpenAI API key">
    <CodeGroup>
```

Example 3 (unknown):
```unknown

```

Example 4 (unknown):
```unknown
</CodeGroup>
  </Step>

  <Step title="Create a Python file">
    Create a Python file and add the above code.
```

---

## Together Embedder

**URL:** llms-txt#together-embedder

**Contents:**
- Code

Source: https://docs.agno.com/examples/concepts/knowledge/embedders/together-embedder

```python  theme={null}
from agno.knowledge.embedder.together import TogetherEmbedder
from agno.knowledge.knowledge import Knowledge
from agno.vectordb.pgvector import PgVector

embeddings = TogetherEmbedder().get_embedding(
    "The quick brown fox jumps over the lazy dog."
)

---

## Process segments

**URL:** llms-txt#process-segments

shorts = extract_segments(response.content)

---

## Showing input audio, output audio and total audio tokens metrics

**URL:** llms-txt#showing-input-audio,-output-audio-and-total-audio-tokens-metrics

print(f"Input audio tokens: {run_response.metrics.audio_input_tokens}")
print(f"Output audio tokens: {run_response.metrics.audio_output_tokens}")
print(f"Audio tokens: {run_response.metrics.audio_total_tokens}")

agent = Agent(
    model=OpenAIChat(id="gpt-5-mini"),
    markdown=True,
    telemetry=False,
)
run_response = agent.run(
    "Solve the trolley problem. Evaluate multiple ethical frameworks. Include an ASCII diagram of your solution.",
    stream=False,
)
pprint_run_response(run_response)

---

## SingleStore Async

**URL:** llms-txt#singlestore-async

**Contents:**
- Code
- Usage

Source: https://docs.agno.com/examples/concepts/vectordb/singlestore-db/async-singlestore-db

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Install libraries">
    
  </Step>

<Step title="Run SingleStore">
    
  </Step>

<Step title="Set environment variables">
    
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Install libraries">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Run SingleStore">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Set environment variables">
```

Example 4 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

---

## Define steps using custom streaming functions for parallel execution

**URL:** llms-txt#define-steps-using-custom-streaming-functions-for-parallel-execution

hackernews_step = Step(
    name="HackerNews Research",
    executor=hackernews_research_function,
)

web_search_step = Step(
    name="Web Search Research",
    executor=web_search_research_function,
)

content_planning_step = Step(
    name="Content Planning Step",
    executor=custom_content_planning_function,
)

streaming_content_workflow = Workflow(
    name="Streaming Content Creation Workflow",
    description="Automated content creation with parallel custom streaming functions",
    db=SqliteDb(
        session_table="streaming_workflow_session",
        db_file="tmp/workflow.db",
    ),
    # Define the sequence with parallel research steps followed by planning
    steps=[
        Parallel(hackernews_step, web_search_step, name="Parallel Research Phase"),
        content_planning_step,
    ],
)

---

## Add your custom routes

**URL:** llms-txt#add-your-custom-routes

@app.get("/status")
async def status_check():
    return {"status": "healthy"}

---

## Get Evaluation Run

**URL:** llms-txt#get-evaluation-run

Source: https://docs.agno.com/reference-api/schema/evals/get-evaluation-run

get /eval-runs/{eval_run_id}
Retrieve detailed results and metrics for a specific evaluation run.

---

## LM Studio

**URL:** llms-txt#lm-studio

**Contents:**
- Set up a model
- Example
- Params

Source: https://docs.agno.com/concepts/models/lmstudio

Learn how to use LM Studio with Agno.

Run Large Language Models locally with LM Studio

[LM Studio](https://lmstudio.ai) is a fantastic tool for running models locally.

LM Studio supports multiple open-source models. See the library [here](https://lmstudio.ai/models).

We recommend experimenting to find the best-suited model for your use-case. Here are some general recommendations:

* `llama3.3` models are good for most basic use-cases.
* `qwen` models perform specifically well with tool use.
* `deepseek-r1` models have strong reasoning capabilities.
* `phi4` models are powerful, while being really small in size.

Install [LM Studio](https://lmstudio.ai), download the model you want to use, and run it.

After you have the model locally, use the `LM Studio` model class to access it

<CodeGroup>
  
</CodeGroup>

<Note> View more examples [here](/examples/models/lmstudio/basic). </Note>

| Parameter  | Type            | Default                                              | Description                                             |
| ---------- | --------------- | ---------------------------------------------------- | ------------------------------------------------------- |
| `id`       | `str`           | `"lmstudio-community/Meta-Llama-3-8B-Instruct-GGUF"` | The id of the LMStudio model to use                     |
| `name`     | `str`           | `"LMStudio"`                                         | The name of the model                                   |
| `provider` | `str`           | `"LMStudio"`                                         | The provider of the model                               |
| `api_key`  | `Optional[str]` | `None`                                               | The API key for LMStudio (usually not needed for local) |
| `base_url` | `str`           | `"http://localhost:1234/v1"`                         | The base URL for the local LMStudio server              |

`LM Studio` also supports the params of [OpenAI](/reference/models/openai).

---

## Cartesia

**URL:** llms-txt#cartesia

**Contents:**
- Prerequisites
- Example

Source: https://docs.agno.com/concepts/tools/toolkits/others/cartesia

Tools for interacting with Cartesia Voice AI services including text-to-speech and voice localization

**CartesiaTools** enable an Agent to perform text-to-speech, list available voices, and localize voices using [Cartesia](https://docs.cartesia.ai/).

The following example requires the `cartesia` library and an API key.

```python  theme={null}
from agno.agent import Agent
from agno.tools.cartesia import CartesiaTools
from agno.utils.audio import write_audio_to_file

**Examples:**

Example 1 (unknown):
```unknown

```

Example 2 (unknown):
```unknown
## Example
```

---

## Streamable HTTP Transport

**URL:** llms-txt#streamable-http-transport

Source: https://docs.agno.com/concepts/tools/mcp/transports/streamable_http

The new [Streamable HTTP transport](https://modelcontextprotocol.io/specification/draft/basic/transports#streamable-http) replaces the HTTP+SSE transport from protocol version `2024-11-05`.

This transport enables the MCP server to handle multiple client connections, and can also use SSE for server-to-client streaming.

To use it, initialize the `MCPTools` passing the URL of the MCP server and setting the transport to `streamable-http`:

```python  theme={null}
from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.mcp import MCPTools

---

## )

**URL:** llms-txt#)

**Contents:**
- Usage

bash  theme={null}
    pip install agno ddgs slack_sdk exa_py pgvector psycopg
    bash  theme={null}
    export OPENAI_API_KEY=****
    export SLACK_TOKEN=****
    export EXA_API_KEY=****
    bash  theme={null}
    python cookbook/examples/teams/route_mode/ai_customer_support_team.py
    ```
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Install required libraries">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Set environment variables">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run the agent">
```

---

## Add content - gets embedded automatically

**URL:** llms-txt#add-content---gets-embedded-automatically

knowledge.add_content(
    text_content="The sky is blue during the day and dark at night."
)

---

## Reliability Evals

**URL:** llms-txt#reliability-evals

**Contents:**
- Basic Tool Call Reliability
- Multiple Tool Calls Reliability
- Team Reliability
- Usage
- Track Evals in AgnoOS platform

Source: https://docs.agno.com/concepts/evals/reliability

Learn how to evaluate your Agno Agents and Teams for reliability by testing tool calls and error handling.

What makes an Agent or Team reliable?

* Does it make the expected tool calls?
* Does it handle errors gracefully?
* Does it respect the rate limits of the model API?

## Basic Tool Call Reliability

The first check is to ensure the Agent makes the expected tool calls. Here's an example:

<Frame>
  <img height="200" src="https://mintcdn.com/agno-v2/3rn2Dg1ZNvoQRtu4/images/evals/reliability_basic.png?fit=max&auto=format&n=3rn2Dg1ZNvoQRtu4&q=85&s=816d4832aa2d3d19ae007f85e9573c13" data-og-width="1148" data-og-height="488" data-path="images/evals/reliability_basic.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/agno-v2/3rn2Dg1ZNvoQRtu4/images/evals/reliability_basic.png?w=280&fit=max&auto=format&n=3rn2Dg1ZNvoQRtu4&q=85&s=2cc17d3702c17b96b1a4828793dcad08 280w, https://mintcdn.com/agno-v2/3rn2Dg1ZNvoQRtu4/images/evals/reliability_basic.png?w=560&fit=max&auto=format&n=3rn2Dg1ZNvoQRtu4&q=85&s=6cb3b00f1c366ee9ae87db978ba96c14 560w, https://mintcdn.com/agno-v2/3rn2Dg1ZNvoQRtu4/images/evals/reliability_basic.png?w=840&fit=max&auto=format&n=3rn2Dg1ZNvoQRtu4&q=85&s=445cd7213631a55a500414dc1fe20152 840w, https://mintcdn.com/agno-v2/3rn2Dg1ZNvoQRtu4/images/evals/reliability_basic.png?w=1100&fit=max&auto=format&n=3rn2Dg1ZNvoQRtu4&q=85&s=c8a131d365b92fd94d67b6cd67b8eea5 1100w, https://mintcdn.com/agno-v2/3rn2Dg1ZNvoQRtu4/images/evals/reliability_basic.png?w=1650&fit=max&auto=format&n=3rn2Dg1ZNvoQRtu4&q=85&s=83f280c6bd6929bfe56870283599826a 1650w, https://mintcdn.com/agno-v2/3rn2Dg1ZNvoQRtu4/images/evals/reliability_basic.png?w=2500&fit=max&auto=format&n=3rn2Dg1ZNvoQRtu4&q=85&s=d47181b3504263137665022f59073978 2500w" />
</Frame>

## Multiple Tool Calls Reliability

Test that agents make multiple tool calls:

Test how teams handle various error conditions:

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Install libraries">
    
  </Step>

<Step title="Run Basic Tool Call Reliability Test">
    <CodeGroup>

</CodeGroup>
  </Step>

<Step title="Test Multiple Tool Calls">
    <CodeGroup>

</CodeGroup>
  </Step>

<Step title="Test Team Reliability">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

## Track Evals in AgnoOS platform

<video autoPlay muted controls className="w-full aspect-video" src="https://mintcdn.com/agno-v2/hzelS2cST9lEqMuM/videos/eval_platform.mp4?fit=max&auto=format&n=hzelS2cST9lEqMuM&q=85&s=9329eaac5cd0f551081e51656cc0227c" data-path="videos/eval_platform.mp4" />

```python evals_demo.py theme={null}

"""Simple example creating a evals and using the AgentOS."""

from agno.agent import Agent
from agno.db.postgres.postgres import PostgresDb
from agno.eval.accuracy import AccuracyEval
from agno.models.openai import OpenAIChat
from agno.os import AgentOS
from agno.tools.calculator import CalculatorTools

**Examples:**

Example 1 (unknown):
```unknown
<Frame>
  <img height="200" src="https://mintcdn.com/agno-v2/3rn2Dg1ZNvoQRtu4/images/evals/reliability_basic.png?fit=max&auto=format&n=3rn2Dg1ZNvoQRtu4&q=85&s=816d4832aa2d3d19ae007f85e9573c13" data-og-width="1148" data-og-height="488" data-path="images/evals/reliability_basic.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/agno-v2/3rn2Dg1ZNvoQRtu4/images/evals/reliability_basic.png?w=280&fit=max&auto=format&n=3rn2Dg1ZNvoQRtu4&q=85&s=2cc17d3702c17b96b1a4828793dcad08 280w, https://mintcdn.com/agno-v2/3rn2Dg1ZNvoQRtu4/images/evals/reliability_basic.png?w=560&fit=max&auto=format&n=3rn2Dg1ZNvoQRtu4&q=85&s=6cb3b00f1c366ee9ae87db978ba96c14 560w, https://mintcdn.com/agno-v2/3rn2Dg1ZNvoQRtu4/images/evals/reliability_basic.png?w=840&fit=max&auto=format&n=3rn2Dg1ZNvoQRtu4&q=85&s=445cd7213631a55a500414dc1fe20152 840w, https://mintcdn.com/agno-v2/3rn2Dg1ZNvoQRtu4/images/evals/reliability_basic.png?w=1100&fit=max&auto=format&n=3rn2Dg1ZNvoQRtu4&q=85&s=c8a131d365b92fd94d67b6cd67b8eea5 1100w, https://mintcdn.com/agno-v2/3rn2Dg1ZNvoQRtu4/images/evals/reliability_basic.png?w=1650&fit=max&auto=format&n=3rn2Dg1ZNvoQRtu4&q=85&s=83f280c6bd6929bfe56870283599826a 1650w, https://mintcdn.com/agno-v2/3rn2Dg1ZNvoQRtu4/images/evals/reliability_basic.png?w=2500&fit=max&auto=format&n=3rn2Dg1ZNvoQRtu4&q=85&s=d47181b3504263137665022f59073978 2500w" />
</Frame>

## Multiple Tool Calls Reliability

Test that agents make multiple tool calls:
```

Example 2 (unknown):
```unknown
## Team Reliability

Test how teams handle various error conditions:
```

Example 3 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Install libraries">
```

Example 4 (unknown):
```unknown
</Step>

  <Step title="Run Basic Tool Call Reliability Test">
    <CodeGroup>
```

---

## debug_mode=True,

**URL:** llms-txt#debug_mode=true,

---

## BrightData Tools

**URL:** llms-txt#brightdata-tools

**Contents:**
- Code

Source: https://docs.agno.com/examples/concepts/tools/web_scrape/brightdata

```python cookbook/tools/brightdata_tools.py theme={null}
from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.brightdata import BrightDataTools
from agno.utils.media import save_base64_data

agent = Agent(
    model=OpenAIChat(id="gpt-5-mini"),
    tools=[
        BrightDataTools(
            enable_get_screenshot=True,
        )
    ],
    markdown=True,
    )

---

## Session Tracking

**URL:** llms-txt#session-tracking

**Contents:**
- Overview
- Accessing Sessions
- Troubleshooting
- Useful Links

Source: https://docs.agno.com/agent-os/features/session-tracking

Monitor, analyze, and manage agent sessions through the AgentOS interface

Sessions are durable conversation timelines that bind inputs, model outputs, tools, files, metrics, and summaries under a single `session_id`. AgentOS persists sessions for Agents, Teams, and Workflows so you can resume work, audit behavior, and analyze quality over time.

* A session collects ordered runs (each run contains messages, tool calls, and metrics).
* Summaries and metadata help you search, group, and reason about long histories.
* Token usage can be monitored per session via the metrics tab.
* Inspect details about the agent and models tied to each session.

<Frame>
  <video autoPlay muted loop playsInline style={{ borderRadius: "0.5rem", width: "100%", height: "auto" }}>
    <source src="https://mintcdn.com/agno-v2/xm93WWN8gg4nzCGE/videos/agentos-session-management.mp4?fit=max&auto=format&n=xm93WWN8gg4nzCGE&q=85&s=70dda8ee349f38e48272eff8cdd4719a" type="video/mp4" data-path="videos/agentos-session-management.mp4" />
  </video>
</Frame>

## Accessing Sessions

* Open the `Sessions` section in the sidebar.
* If multiple session databases are configured, pick one from the database selector in the header.
* Switch between `Agents` and `Teams` using the header tabs.
* Click `Reload page` (Refresh) to sync the list and statuses.

* Sessions not loading: Ensure your OS is connected and active, select a session database, then click `Reload page`.
* No sessions yet: Start a conversation to generate sessions.
* Wrong list: Check the `Agents` vs `Teams` tab and sorting.
* Configuration errors: If you see endpoint or database errors, verify your OS endpoint and database settings.

<CardGroup cols={3}>
  <Card title="Agent Sessions" icon="user" href="/concepts/agents/sessions">
    Learn about Agent sessions and multi-turn conversations
  </Card>

<Card title="Team Sessions" icon="users" href="/concepts/teams/sessions">
    Understand Team sessions and collaborative workflows
  </Card>

<Card title="Workflow Session State" icon="sitemap" href="/concepts/workflows/workflow_session_state">
    Master shared state between workflow components
  </Card>
</CardGroup>

---

## This middleware will automatically inject JWT values into request.state and is used in the relevant endpoints.

**URL:** llms-txt#this-middleware-will-automatically-inject-jwt-values-into-request.state-and-is-used-in-the-relevant-endpoints.

**Contents:**
- Usage
- How It Works
- Next Steps

app.add_middleware(
    JWTMiddleware,
    secret_key=JWT_SECRET, # or use JWT_SECRET_KEY environment variable
    algorithm="HS256",
    user_id_claim="sub",  # Extract user_id from 'sub' claim
    session_id_claim="session_id",  # Extract session_id from 'session_id' claim
    dependencies_claims=["name", "email", "roles"],
    # In this example, we want this middleware to demonstrate parameter injection, not token validation.
    # In production scenarios, you will probably also want token validation. Be careful setting this to False.
    validate=False,
)

if __name__ == "__main__":
    """
    Run your AgentOS with JWT parameter injection.
    
    Test by calling /agents/user-agent/runs with a message: "What do you know about me?"
    """
    # Test token with user_id and session_id:
    payload = {
        "sub": "user_123",  # This will be injected as user_id parameter
        "session_id": "demo_session_456",  # This will be injected as session_id parameter
        "exp": datetime.now(UTC) + timedelta(hours=24),
        "iat": datetime.now(UTC),
        # Dependency claims
        "name": "John Doe",
        "email": "john.doe@example.com",
        "roles": ["admin", "user"],
    }
    token = jwt.encode(payload, JWT_SECRET, algorithm="HS256")
    print("Test token:")
    print(token)
    agent_os.serve(app="jwt_middleware:app", port=7777, reload=True)
bash  theme={null}
    export OPENAI_API_KEY=your_openai_api_key
    bash  theme={null}
    pip install -U agno openai pyjwt fastapi["standard"] uvicorn sqlalchemy pgvector psycopg
    bash  theme={null}
    # Using Docker
    docker run -d \
      --name agno-postgres \
      -e POSTGRES_DB=ai \
      -e POSTGRES_USER=ai \
      -e POSTGRES_PASSWORD=ai \
      -p 5532:5432 \
      pgvector/pgvector:pg17
    bash  theme={null}
    python jwt_middleware.py
    bash  theme={null}
    # Use the token printed in the console
    export TOKEN="eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9..."

curl --location 'http://localhost:7777/agents/user-agent/runs' \
        --header 'Content-Type: application/x-www-form-urlencoded' \
        --header 'Authorization: Bearer $TOKEN' \
        --data-urlencode 'message=What do you know about me?'
    bash  theme={null}
    curl --location 'http://localhost:7777/agents/user-agent/runs' \
        --header 'Content-Type: application/x-www-form-urlencoded' \
        --data-urlencode 'message=What do you know about me?'
    ```

**Check the AgentOS API docs:**
    Visit [http://localhost:7777/docs](http://localhost:7777/docs) to see all available endpoints.
  </Step>
</Steps>

1. **JWT Generation**: The example creates a test JWT token with user claims
2. **Middleware Setup**: JWT middleware extracts claims from the `Authorization: Bearer <token>` header
3. **Parameter Injection**: The middleware automatically injects:
   * `user_id` from the `sub` claim
   * `session_id` from the `session_id` claim
   * `dependencies` dict with name, email, and roles
4. **Agent Tools**: The agent can access user details through the injected dependencies

* [JWT Middleware with Cookies](/examples/agent-os/middleware/jwt-cookies)
* [Custom FastAPI with JWT](/examples/agent-os/middleware/custom-fastapi-jwt)
* [JWT Middleware Documentation](/agent-os/customize/middleware/jwt)

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Set Environment Variables">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Setup PostgreSQL Database">
```

Example 4 (unknown):
```unknown
</Step>

  <Step title="Run Example">
```

---

## session_summary_manager=session_summary_manager,

**URL:** llms-txt#session_summary_manager=session_summary_manager,

---

## Desi Vocal Tools

**URL:** llms-txt#desi-vocal-tools

**Contents:**
- Code
- Usage

Source: https://docs.agno.com/examples/concepts/tools/others/desi_vocal

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Set your API key">
    
  </Step>

<Step title="Install libraries">
    
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Set your API key">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## OpenCV Tools

**URL:** llms-txt#opencv-tools

**Contents:**
- Code
- Usage

Source: https://docs.agno.com/examples/concepts/tools/others/opencv

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Set your API key">
    
  </Step>

<Step title="Install libraries">
    
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Set your API key">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Should raise an error - missing required field

**URL:** llms-txt#should-raise-an-error---missing-required-field

try:
    hackernews_agent.print_response(
        input={
            "topic": "AI",
            # Missing required fields: focus_areas, target_audience, sources_required
        }
    )
except ValueError as e:
    print("\n=== Expected Error for Missing Fields ===")
    print(f"Error: {e}")

---

## Alternatively, you can create a new session summary without adding the session summary to context.

**URL:** llms-txt#alternatively,-you-can-create-a-new-session-summary-without-adding-the-session-summary-to-context.

---

## PgVector

**URL:** llms-txt#pgvector

Source: https://docs.agno.com/reference/vector_db/pgvector

<Snippet file="vector-db-pgvector-reference.mdx" />

---

## SQLite for Workflow

**URL:** llms-txt#sqlite-for-workflow

**Contents:**
- Usage

Source: https://docs.agno.com/examples/concepts/db/sqlite/sqlite_for_workflow

Agno supports using SQLite as a storage backend for Workflows using the `SqliteDb` class.

```python sqlite_for_workflow.py theme={null}
from agno.agent import Agent
from agno.db.sqlite import SqliteDb
from agno.models.openai import OpenAIChat
from agno.team import Team
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.hackernews import HackerNewsTools
from agno.workflow.step import Step
from agno.workflow.workflow import Workflow

db = SqliteDb(db_file="tmp/workflow.db")

---

## Wikipedia Tools

**URL:** llms-txt#wikipedia-tools

**Contents:**
- Code
- Usage

Source: https://docs.agno.com/examples/concepts/tools/search/wikipedia

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Set your API key">
    
  </Step>

<Step title="Install libraries">
    
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Set your API key">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Image Generation Tools

**URL:** llms-txt#image-generation-tools

**Contents:**
- Image Generation using a tool
- Developer Resources

Source: https://docs.agno.com/concepts/multimodal/images/image_generation

Learn how to use image generation tools with Agno agents.

Similar to providing multimodal inputs, you can also get multimodal outputs from an agent.

## Image Generation using a tool

The following example demonstrates how to generate an image using an OpenAI tool with an agent.

<Check>
  The output of the tool generating a media also goes to the model's input as a
  message so it has access to the media (image, audio, video) and can use it in
  the response. For example, if you say "Generate an image of a dog and tell me
  its color." the model will have access to the image and can use it to describe
  the dog's color in the response in the same run.

That also means you can ask follow-up questions about the image, since it would be available in the history of the agent.
</Check>

## Developer Resources

* View more [Examples](/examples/concepts/multimodal/generate-image)

---

## Define tools that work with shared team state

**URL:** llms-txt#define-tools-that-work-with-shared-team-state

def add_item(session_state, item: str) -> str:
    """Add an item to the shopping list."""
    if item.lower() not in [
        i.lower() for i in session_state["shopping_list"]
    ]:
        session_state["shopping_list"].append(item)
        return f"Added '{item}' to the shopping list"
    else:
        return f"'{item}' is already in the shopping list"

def remove_item(session_state, item: str) -> str:
    """Remove an item from the shopping list."""
    for i, list_item in enumerate(session_state["shopping_list"]):
        if list_item.lower() == item.lower():
            session_state["shopping_list"].pop(i)
            return f"Removed '{list_item}' from the shopping list"

return f"'{item}' was not found in the shopping list"

---

## PgVector Async

**URL:** llms-txt#pgvector-async

**Contents:**
- Code
- Usage

Source: https://docs.agno.com/examples/concepts/vectordb/pgvector/async-pgvector-db

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Install libraries">
    
  </Step>

<Snippet file="run-pgvector.mdx" />

<Step title="Set environment variables">
    
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Install libraries">
```

Example 2 (unknown):
```unknown
</Step>

  <Snippet file="run-pgvector.mdx" />

  <Step title="Set environment variables">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Azure OpenAI o1

**URL:** llms-txt#azure-openai-o1

**Contents:**
- Code
- Usage

Source: https://docs.agno.com/examples/concepts/reasoning/models/azure-openai/o1

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Set your API key">
    
  </Step>

<Step title="Install libraries">
    
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Set your API key">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Session Summary with Context References

**URL:** llms-txt#session-summary-with-context-references

**Contents:**
- Code

Source: https://docs.agno.com/examples/concepts/teams/session/session_summary_references

This example shows how to use the `add_session_summary_to_context` parameter to add session summaries to team context for maintaining conversation continuity.

```python cookbook/examples/teams/session/04_session_summary_references.py theme={null}
"""
This example shows how to use the `add_session_summary_to_context` parameter in the Team config to
add session summaries to the Team context.

Start the postgres db locally on Docker by running: cookbook/scripts/run_pgvector.sh
"""

from agno.agent.agent import Agent
from agno.db.postgres import PostgresDb
from agno.models.openai import OpenAIChat
from agno.team import Team

db_url = "postgresql+psycopg://ai:ai@localhost:5532/ai"

db = PostgresDb(db_url=db_url, session_table="sessions")

agent = Agent(
    model=OpenAIChat(id="gpt-5-mini"),
)

team = Team(
    model=OpenAIChat(id="gpt-5-mini"),
    members=[agent],
    db=db,
    enable_session_summaries=True,
)

---

## Cost increase: 8x more expensive!

**URL:** llms-txt#cost-increase:-8x-more-expensive!

**Contents:**
- Mitigation Strategy #1: Use Automatic Memory

**Examples:**

Example 1 (unknown):
```unknown
As memories accumulate, each memory operation gets more expensive. With 200 memories, a single memory update could consume 10,000+ tokens just loading context.

## Mitigation Strategy #1: Use Automatic Memory

For most use cases, automatic memory is your best bet—it's significantly more efficient:
```

---

## Video Generation

**URL:** llms-txt#video-generation

**Contents:**
- Code

Source: https://docs.agno.com/examples/getting-started/15-video-generation

This example shows how to create an AI agent that generates videos using ModelsLabs.
You can use this agent to create various types of short videos, from animated scenes
to creative visual stories.

Example prompts to try:

* "Create a serene video of waves crashing on a beach at sunset"
* "Generate a magical video of butterflies flying in a enchanted forest"
* "Create a timelapse of a blooming flower in a garden"
* "Generate a video of northern lights dancing in the night sky"

```python video_generation.py theme={null}
from textwrap import dedent

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.models_labs import ModelsLabTools

---

## CSV

**URL:** llms-txt#csv

**Contents:**
- Example
- Toolkit Params
- Toolkit Functions
- Developer Resources

Source: https://docs.agno.com/concepts/tools/toolkits/database/csv

**CsvTools** enable an Agent to read and write CSV files.

The following agent will download the IMDB csv file and allow the user to query it using a CLI app.

| Parameter               | Type                     | Default | Description                                                            |
| ----------------------- | ------------------------ | ------- | ---------------------------------------------------------------------- |
| `csvs`                  | `List[Union[str, Path]]` | `None`  | A list of CSV files or paths to be processed or read.                  |
| `row_limit`             | `int`                    | `None`  | The maximum number of rows to process from each CSV file.              |
| `duckdb_connection`     | `Any`                    | `None`  | Specifies a connection instance for DuckDB database operations.        |
| `duckdb_kwargs`         | `Dict[str, Any]`         | `None`  | A dictionary of keyword arguments for configuring DuckDB operations.   |
| `enable_read_csv_file`  | `bool`                   | `True`  | Enables the functionality to read data from specified CSV files.       |
| `enable_list_csv_files` | `bool`                   | `True`  | Enables the functionality to list all available CSV files.             |
| `enable_get_columns`    | `bool`                   | `True`  | Enables the functionality to read the column names from CSV files.     |
| `enable_query_csv_file` | `bool`                   | `True`  | Enables the functionality to execute queries on data within CSV files. |
| `all`                   | `bool`                   | `False` | Enables all functionality when set to True.                            |

| Function         | Description                                      |
| ---------------- | ------------------------------------------------ |
| `list_csv_files` | Lists all available CSV files.                   |
| `read_csv_file`  | This function reads the contents of a csv file   |
| `get_columns`    | This function returns the columns of a csv file  |
| `query_csv_file` | This function queries the contents of a csv file |

## Developer Resources

* View [Tools](https://github.com/agno-agi/agno/blob/main/libs/agno/agno/tools/csv.py)
* View [Cookbook](https://github.com/agno-agi/agno/tree/main/cookbook/tools/csv_tools.py)

---

## Financial Datasets Tools

**URL:** llms-txt#financial-datasets-tools

**Contents:**
- Code

Source: https://docs.agno.com/examples/concepts/tools/others/financial_datasets

```python cookbook/tools/financial_datasets_tools.py theme={null}
from agno.agent import Agent
from agno.tools.financial_datasets import FinancialDatasetsTools

agent = Agent(
    name="Financial Data Agent",
    tools=[
        FinancialDatasetsTools(),  # For accessing financial data
    ],
    description="You are a financial data specialist that helps analyze financial information for stocks and cryptocurrencies.",
    instructions=[
        "When given a financial query:",
        "1. Use appropriate Financial Datasets methods based on the query type",
        "2. Format financial data clearly and highlight key metrics",
        "3. For financial statements, compare important metrics with previous periods when relevant",
        "4. Calculate growth rates and trends when appropriate",
        "5. Handle errors gracefully and provide meaningful feedback",
    ],
    markdown=True,
    )

---

## Generate Video using Models Lab

**URL:** llms-txt#generate-video-using-models-lab

**Contents:**
- Code
- Usage

Source: https://docs.agno.com/examples/concepts/multimodal/generate-video-models-lab

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Set your API key">
    
  </Step>

<Step title="Install libraries">
    
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Set your API key">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Nebius Embedder

**URL:** llms-txt#nebius-embedder

**Contents:**
- Code

Source: https://docs.agno.com/examples/concepts/knowledge/embedders/nebius-embedder

```python  theme={null}
from agno.knowledge.embedder.nebius import NebiusEmbedder
from agno.knowledge.knowledge import Knowledge
from agno.vectordb.pgvector import PgVector

embeddings = NebiusEmbedder().get_embedding(
    "The quick brown fox jumps over the lazy dog."
)

---

## Example usage with different types of movie queries

**URL:** llms-txt#example-usage-with-different-types-of-movie-queries

**Contents:**
- More example prompts to explore:
- Usage

movie_recommendation_agent.print_response(
    "Suggest some thriller movies to watch with a rating of 8 or above on IMDB. "
    "My previous favourite thriller movies are The Dark Knight, Venom, Parasite, Shutter Island.",
    stream=True,
)
bash  theme={null}
    pip install openai exa_py agno
    bash  theme={null}
    export OPENAI_API_KEY=****
    export EXA_API_KEY=****
    bash  theme={null}
    python movie_recommender.py
    ```
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## More example prompts to explore:

**Genre-specific queries:**

1. "Find me psychological thrillers similar to Black Swan and Gone Girl"
2. "What are the best animated movies from Studio Ghibli?"
3. "Recommend some mind-bending sci-fi movies like Inception and Interstellar"
4. "What are the highest-rated crime documentaries from the last 5 years?"

**International Cinema:**

1. "Suggest Korean movies similar to Parasite and Train to Busan"
2. "What are the must-watch French films from the last decade?"
3. "Recommend Japanese animated movies for adults"
4. "Find me award-winning European drama films"

**Family & Group Watching:**

1. "What are good family movies for kids aged 8-12?"
2. "Suggest comedy movies perfect for a group movie night"
3. "Find educational documentaries suitable for teenagers"
4. "Recommend adventure movies that both adults and children would enjoy"

**Upcoming Releases:**

1. "What are the most anticipated movies coming out next month?"
2. "Show me upcoming superhero movie releases"
3. "What horror movies are releasing this Halloween season?"
4. "List upcoming book-to-movie adaptations"

## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Install libraries">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Set environment variables">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run the agent">
```

---

## What are Models?

**URL:** llms-txt#what-are-models?

**Contents:**
- Error handling
- Supported Models
  - Native Model Providers
  - Local Model Providers
  - Cloud Model Providers
  - Model Gateways & Aggregators

Source: https://docs.agno.com/concepts/models/overview

Language Models are machine-learning programs that are trained to understand natural language and code.

When we discuss Models, we are normally referring to Large Language Models (LLMs).

These models act as the **brain** of your Agents - enabling them to reason, act, and respond to the user. The better the model, the smarter the Agent.

You can set `exponential_backoff` to `True` on the `Agent` to automatically retry requests that fail due to third-party model provider errors.

Agno supports the following model providers organized by category:

### Native Model Providers

<CardGroup cols={3}>
  <Card title="Anthropic" icon="brain" iconType="duotone" href="/concepts/models/anthropic">
    Anthropic Claude models integration.
  </Card>

<Card title="Cohere" icon="robot" iconType="duotone" href="/concepts/models/cohere">
    Cohere language models integration.
  </Card>

<Card title="DashScope" icon="robot" iconType="duotone" href="/concepts/models/dashscope">
    Alibaba Cloud DashScope models.
  </Card>

<Card title="DeepSeek" icon="robot" iconType="duotone" href="/concepts/models/deepseek">
    DeepSeek AI models integration.
  </Card>

<Card title="Google Gemini" icon="google" iconType="duotone" href="/concepts/models/google">
    Google Gemini models integration.
  </Card>

<Card title="Meta" icon="meta" iconType="duotone" href="/concepts/models/meta">
    Meta AI models integration.
  </Card>

<Card title="Mistral" icon="wind" iconType="duotone" href="/concepts/models/mistral">
    Mistral AI models integration.
  </Card>

<Card title="OpenAI" icon="robot" iconType="duotone" href="/concepts/models/openai">
    OpenAI models integration.
  </Card>

<Card title="OpenAI Responses" icon="robot" iconType="duotone" href="/concepts/models/openai-responses">
    OpenAI response format handling.
  </Card>

<Card title="Perplexity" icon="question" iconType="duotone" href="/concepts/models/perplexity">
    Perplexity AI models integration.
  </Card>

<Card title="Vercel" icon="robot" iconType="duotone" href="/concepts/models/vercel">
    Vercel AI models integration.
  </Card>

<Card title="xAI" icon="x" iconType="duotone" href="/concepts/models/xai">
    xAI models integration.
  </Card>
</CardGroup>

### Local Model Providers

<CardGroup cols={3}>
  <Card title="LlamaCpp" icon="robot" iconType="duotone" href="/concepts/models/llama_cpp">
    LlamaCpp local model inference.
  </Card>

<Card title="LM Studio" icon="laptop" iconType="duotone" href="/concepts/models/lmstudio">
    LM Studio local model integration.
  </Card>

<Card title="Ollama" icon="robot" iconType="duotone" href="/concepts/models/ollama">
    Ollama local model integration.
  </Card>

<Card title="VLLM" icon="server" iconType="duotone" href="/concepts/models/vllm">
    VLLM high-throughput inference.
  </Card>
</CardGroup>

### Cloud Model Providers

<CardGroup cols={3}>
  <Card title="AWS Bedrock" icon="cloud" iconType="duotone" href="/concepts/models/aws-bedrock">
    Amazon Web Services Bedrock models.
  </Card>

<Card title="Claude via AWS Bedrock" icon="brain" iconType="duotone" href="/concepts/models/aws-claude">
    Anthropic Claude models via AWS Bedrock.
  </Card>

<Card title="Azure AI Foundry" icon="microsoft" iconType="duotone" href="/concepts/models/azure-ai-foundry">
    Microsoft Azure AI Foundry models.
  </Card>

<Card title="Azure OpenAI" icon="microsoft" iconType="duotone" href="/concepts/models/azure-openai">
    Microsoft Azure OpenAI models.
  </Card>

<Card title="Vertex AI Claude" icon="brain" iconType="duotone" href="/concepts/models/vertexai-claude">
    Anthropic Claude models via Google Vertex AI.
  </Card>

<Card title="IBM WatsonX" icon="robot" iconType="duotone" href="/concepts/models/ibm-watsonx">
    IBM WatsonX models integration.
  </Card>
</CardGroup>

### Model Gateways & Aggregators

<CardGroup cols={3}>
  <Card title="AI/ML API" icon="robot" iconType="duotone" href="/concepts/models/aimlapi">
    AI/ML API model provider integration.
  </Card>

<Card title="Cerebras" icon="robot" iconType="duotone" href="/concepts/models/cerebras">
    Cerebras AI models integration.
  </Card>

<Card title="Cerebras OpenAI" icon="robot" iconType="duotone" href="/concepts/models/cerebras_openai">
    Cerebras OpenAI-compatible models.
  </Card>

<Card title="CometAPI" icon="comet" iconType="duotone" href="/concepts/models/cometapi">
    CometAPI model provider integration.
  </Card>

<Card title="DeepInfra" icon="infinity" iconType="duotone" href="/concepts/models/deepinfra">
    DeepInfra model provider integration.
  </Card>

<Card title="Fireworks" icon="fire" iconType="duotone" href="/concepts/models/fireworks">
    Fireworks AI models integration.
  </Card>

<Card title="Groq" icon="bolt" iconType="duotone" href="/concepts/models/groq">
    Groq fast inference models.
  </Card>

<Card title="Hugging Face" icon="robot" iconType="duotone" href="/concepts/models/huggingface">
    Hugging Face models integration.
  </Card>

<Card title="LangDB" icon="database" iconType="duotone" href="/concepts/models/langdb">
    LangDB model provider integration.
  </Card>

<Card title="LiteLLM" icon="lightbulb" iconType="duotone" href="/concepts/models/litellm">
    LiteLLM unified model interface.
  </Card>

<Card title="LiteLLM OpenAI" icon="lightbulb" iconType="duotone" href="/concepts/models/litellm_openai">
    LiteLLM OpenAI-compatible models.
  </Card>

<Card title="Nebius AI Studio" icon="star" iconType="duotone" href="/concepts/models/nebius">
    Nebius AI Studio models.
  </Card>

<Card title="Nexus" icon="link" iconType="duotone" href="/concepts/models/nexus">
    Nexus model provider integration.
  </Card>

<Card title="NVIDIA" icon="robot" iconType="duotone" href="/concepts/models/nvidia">
    NVIDIA AI models integration.
  </Card>

<Card title="OpenRouter" icon="route" iconType="duotone" href="/concepts/models/openrouter">
    OpenRouter model aggregation.
  </Card>

<Card title="Portkey" icon="key" iconType="duotone" href="/concepts/models/portkey">
    Portkey model gateway integration.
  </Card>

<Card title="Requesty" icon="robot" iconType="duotone" href="/concepts/models/requesty">
    Requesty model provider integration.
  </Card>

<Card title="Sambanova" icon="server" iconType="duotone" href="/concepts/models/sambanova">
    SambaNova AI models integration.
  </Card>

<Card title="SiliconFlow" icon="robot" iconType="duotone" href="/concepts/models/siliconflow">
    SiliconFlow model provider.
  </Card>

<Card title="Together" icon="users" iconType="duotone" href="/concepts/models/together">
    Together AI models integration.
  </Card>
</CardGroup>

Each provider offers a different set of models, with different capabilities and features. By default, Agno supports all models provided by the mentioned providers.

**Examples:**

Example 1 (unknown):
```unknown
## Error handling

You can set `exponential_backoff` to `True` on the `Agent` to automatically retry requests that fail due to third-party model provider errors.
```

---

## wget https://upload.wikimedia.org/wikipedia/commons/b/bf/Krakow_-_Kosciol_Mariacki.jpg

**URL:** llms-txt#wget-https://upload.wikimedia.org/wikipedia/commons/b/bf/krakow_-_kosciol_mariacki.jpg

**Contents:**
- Usage

image_path = Path(__file__).parent.joinpath("Krakow_-_Kosciol_Mariacki.jpg")
image_file: file_types.File = upload_file(image_path)
print(f"Uploaded image: {image_file}")

agent.print_response(
    "Tell me about this image and give me the latest news about it.",
    images=[Image(content=image_file)],
    stream=True,
)
bash  theme={null}
    wget https://upload.wikimedia.org/wikipedia/commons/b/bf/Krakow_-_Kosciol_Mariacki.jpg
    bash  theme={null}
    export GOOGLE_API_KEY=xxx
    bash  theme={null}
    pip install -U google-genai ddgs agno
    bash Mac theme={null}
      python cookbook/models/google/gemini/image_input_file_upload.py
      bash Windows theme={null}
      python cookbook/models/google/gemini/image_input_file_upload.py
      ```
    </CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Download the image">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Set your API key">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 4 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

---

## Python

**URL:** llms-txt#python

**Contents:**
- Example
- Toolkit Params
- Toolkit Functions
- Developer Resources

Source: https://docs.agno.com/concepts/tools/toolkits/local/python

**PythonTools** enable an Agent to write and run python code.

The following agent will write a python script that creates the fibonacci series, save it to a file, run it and return the result.

| Parameter      | Type   | Default | Description                                                                                            |
| -------------- | ------ | ------- | ------------------------------------------------------------------------------------------------------ |
| `base_dir`     | `Path` | `None`  | Specifies the base directory for operations. Default is None, indicating the current working directory |
| `safe_globals` | `dict` | `None`  | Dictionary of global variables that are considered safe to use during execution                        |
| `safe_locals`  | `dict` | `None`  | Dictionary of local variables that are considered safe to use during execution                         |

| Function                          | Description                                                                                                                                                                                                                                                            |
| --------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| `save_to_file_and_run`            | This function saves Python code to a file called `file_name` and then runs it. If successful, returns the value of `variable_to_return` if provided otherwise returns a success message. If failed, returns an error message. Make sure the file\_name ends with `.py` |
| `run_python_file_return_variable` | This function runs code in a Python file. If successful, returns the value of `variable_to_return` if provided otherwise returns a success message. If failed, returns an error message.                                                                               |
| `read_file`                       | Reads the contents of the file `file_name` and returns the contents if successful.                                                                                                                                                                                     |
| `list_files`                      | Returns a list of files in the base directory                                                                                                                                                                                                                          |
| `run_python_code`                 | This function runs Python code in the current environment. If successful, returns the value of `variable_to_return` if provided otherwise returns a success message. If failed, returns an error message.                                                              |
| `pip_install_package`             | This function installs a package using pip in the current environment. If successful, returns a success message. If failed, returns an error message.                                                                                                                  |

You can use `include_tools` or `exclude_tools` to modify the list of tools the agent has access to. Learn more about [selecting tools](/concepts/tools/selecting-tools).

## Developer Resources

* View [Tools](https://github.com/agno-agi/agno/blob/main/libs/agno/agno/tools/python.py)
* View [Cookbook](https://github.com/agno-agi/agno/tree/main/cookbook/tools/python_tools.py)

---

## Create routing team

**URL:** llms-txt#create-routing-team

team = Team(
    name="Stock Research Team",
    model=OpenAIChat("gpt-5-mini"),
    members=[stock_searcher, company_info_agent],
    respond_directly=True,
    markdown=True,
    show_members_responses=True,
    instructions=[
        "Route stock price questions to the Stock Searcher",
        "Route company news and info questions to the Company Info Searcher",
    ],
)

---

## Print metrics per message

**URL:** llms-txt#print-metrics-per-message

if run_output.messages:
    for message in run_output.messages:  # type: ignore
        if message.role == "assistant":
            if message.content:
                print(f"Message: {message.content}")
            elif message.tool_calls:
                print(f"Tool calls: {message.tool_calls}")
            print("---" * 5, "Metrics", "---" * 5)
            pprint(message.metrics)
            print("---" * 20)

---

## run_events: Iterator[RunOutputEvent] = national_park_expert.run(f"What is the best season to visit {national_parks[random.randint(0, len(national_parks) - 1)]}? Please provide a detailed one week itinerary for a visit to the park.", stream=True)

**URL:** llms-txt#run_events:-iterator[runoutputevent]-=-national_park_expert.run(f"what-is-the-best-season-to-visit-{national_parks[random.randint(0,-len(national_parks)---1)]}?-please-provide-a-detailed-one-week-itinerary-for-a-visit-to-the-park.",-stream=true)

---

## -*- Only include tables that are in the target_metadata

**URL:** llms-txt#-*--only-include-tables-that-are-in-the-target_metadata

def include_name(name, type_, parent_names):
    if type_ == "table":
        return name in target_metadata.tables
    else:
        return True
...
```

---

## Navigate to the stagehand directory

**URL:** llms-txt#navigate-to-the-stagehand-directory

cd mcp-server-browserbase/stagehand

---

## Sets the session state for the session with the id "user_1_session_1"

**URL:** llms-txt#sets-the-session-state-for-the-session-with-the-id-"user_1_session_1"

team.print_response(
    "What is my name?",
    session_id="user_1_session_1",
    user_id="user_1",
    session_state={"user_name": "John", "age": 30},
)

---

## Infra Settings

**URL:** llms-txt#infra-settings

**Contents:**
  - Infra Name
- Image Repository
- Build Images
- Push Images
- AWS Settings

Source: https://docs.agno.com/templates/infra-management/infra-settings

The `InfraSettings` object in the `infra/settings.py` file defines common settings used by your apps and resources. Here are the settings we recommend updating:

<Note>
  `InfraSettings` can also be updated using environment variables or the `.env` file.

Checkout the `example.env` file for an example.
</Note>

The `infra_name` is used to name your apps and resources. Change it to your project or team name, for example:

* `infra_name="booking-ai"`
* `infra_name="reddit-ai"`
* `infra_name="vantage-ai"`

The `infra_name` is used to name:

* The image for your application
* Apps like db, streamlit app and FastAPI server
* Resources like buckets, secrets and loadbalancers

Checkout the `infra/dev_resources.py` and `infra/prd_resources.py` file to see how its used.

The `image_repo` defines the repo for your image.

* If using dockerhub it would be something like `agno`.
* If using ECR it would be something like `[ACCOUNT_ID].dkr.ecr.us-east-1.amazonaws.com`

Checkout the `dev_image` in `infra/dev_resources.py` and `prd_image` in `infra/prd_resources.py` to see how its used.

Setting `build_images=True` will build images locally when running `ag infra up dev:docker` or `ag infra up prd:docker`.

Checkout the `dev_image` in `infra/dev_resources.py` and `prd_image` in `infra/prd_resources.py` to see how its used.

* [Building your development image](/templates/infra-management/development-app#build-your-development-image)
* [Building your production image](/templates/infra-management/production-app#build-your-production-image)

Setting `push_images=True` will push images after building when running `ag infra up dev:docker` or `ag infra up prd:docker`.

Checkout the `dev_image` in `infra/dev_resources.py` and `prd_image` in `infra/prd_resources.py` to see how its used.

* [Building your development image](/templates/infra-management/development-app#build-your-development-image)
* [Building your production image](/templates/infra-management/production-app#build-your-production-image)

The `aws_region` and `subnet_ids` provide values used for creating production resources. Checkout the `infra/prd_resources.py` file to see how its used.

---

## GCS Content

**URL:** llms-txt#gcs-content

Source: https://docs.agno.com/reference/knowledge/remote-content/gcs-content

GCSContent is a class that allows you to add content from a GCS bucket to the knowledge base.

<Snippet file="gcs-remote-content-params.mdx" />

---

## Create supplier profile request

**URL:** llms-txt#create-supplier-profile-request

supplier_request = SupplierProfile(
    supplier_name="Your Company Name",
    supplier_homepage_url="https://yourcompany.com",
    user_email="your.email@company.com",
)

---

## Find business information in a specific location

**URL:** llms-txt#find-business-information-in-a-specific-location

**Contents:**
- Example Scenarios
  - RAG Web Browser + Google Places Crawler

agent.print_response("What are the top-rated restaurants in San Francisco?", markdown=True)
agent.print_response("Find coffee shops in Prague", markdown=True)
python  theme={null}
from agno.agent import Agent
from agno.tools.apify import ApifyTools

agent = Agent(
    tools=[
        ApifyTools(actors=[
            "apify/rag-web-browser",
            "compass/crawler-google-places"
        ])
    ]
)

**Examples:**

Example 1 (unknown):
```unknown
## Example Scenarios

### RAG Web Browser + Google Places Crawler

This example combines web search with local business data to provide comprehensive information about a topic:
```

---

## Second run - this will use the cached system prompt

**URL:** llms-txt#second-run---this-will-use-the-cached-system-prompt

response = agent.run(
    "What are the key principles of clean code and how do I apply them in Python?"
)
if response and response.metrics:
    print(f"Second run cache read tokens = {response.metrics.cache_read_tokens}")
```

---

## Ollama Tools

**URL:** llms-txt#ollama-tools

**Contents:**
- Parameters

Source: https://docs.agno.com/reference/models/ollama_tools

The Ollama Tools model provides access to the Ollama models and passes tools in XML format to the model.

| Parameter    | Type                          | Default                    | Description                                                  |
| ------------ | ----------------------------- | -------------------------- | ------------------------------------------------------------ |
| `id`         | `str`                         | `"llama3.2"`               | The name of the Ollama model to use                          |
| `name`       | `str`                         | `"OllamaTools"`            | The name of the model                                        |
| `provider`   | `str`                         | `"Ollama"`                 | The provider of the model                                    |
| `host`       | `str`                         | `"http://localhost:11434"` | The host URL for the Ollama server                           |
| `timeout`    | `Optional[int]`               | `None`                     | Request timeout in seconds                                   |
| `format`     | `Optional[str]`               | `None`                     | The format to return the response in (e.g., "json")          |
| `options`    | `Optional[Dict[str, Any]]`    | `None`                     | Additional model options (temperature, top\_p, etc.)         |
| `keep_alive` | `Optional[Union[float, str]]` | `None`                     | How long to keep the model loaded (e.g., "5m", 3600 seconds) |
| `template`   | `Optional[str]`               | `None`                     | The prompt template to use                                   |
| `system`     | `Optional[str]`               | `None`                     | System message to use                                        |
| `raw`        | `Optional[bool]`              | `None`                     | Whether to return raw response without formatting            |
| `stream`     | `bool`                        | `True`                     | Whether to stream the response                               |

This model passes tools in XML format instead of JSON for better compatibility with certain models.

---

## Create steps with shared history

**URL:** llms-txt#create-steps-with-shared-history

tech_support_step = Step(
    name="Technical Support",
    agent=tech_support_agent,
    add_workflow_history=True,
)

billing_support_step = Step(
    name="Billing Support",
    agent=billing_agent,
    add_workflow_history=True,
)

general_support_step = Step(
    name="General Support",
    agent=general_support_agent,
    add_workflow_history=True,
)

def simple_intent_router(step_input: StepInput) -> List[Step]:
    """
    Simple intent-based router with basic keyword detection.
    The focus is on shared history, not complex routing logic.
    """
    current_message = step_input.input or ""
    current_message_lower = current_message.lower()

# Simple keyword matching for intent detection
    tech_keywords = [
        "api",
        "error",
        "bug",
        "technical",
        "login",
        "not working",
        "broken",
        "crash",
    ]
    billing_keywords = [
        "billing",
        "payment",
        "refund",
        "charge",
        "subscription",
        "invoice",
        "plan",
    ]

# Simple routing logic
    if any(keyword in current_message_lower for keyword in tech_keywords):
        print("🔧 Routing to Technical Support")
        return [tech_support_step]
    elif any(keyword in current_message_lower for keyword in billing_keywords):
        print("💳 Routing to Billing Support")
        return [billing_support_step]
    else:
        print("🎧 Routing to General Support")
        return [general_support_step]

def create_smart_customer_service_workflow():
    """Customer service workflow with simple routing and shared history"""

return Workflow(
        name="Smart Customer Service",
        description="Simple routing to specialists with shared conversation history",
        db=SqliteDb(db_file="tmp/smart_customer_service.db"),
        steps=[
            Router(
                name="Customer Service Router",
                selector=simple_intent_router,
                choices=[tech_support_step, billing_support_step, general_support_step],
                description="Routes to appropriate specialist based on simple intent detection",
            )
        ],
        add_workflow_history_to_steps=True,  # Enable history for the workflow
    )

def demo_smart_customer_service_cli():
    """Demo the smart customer service workflow with CLI"""
    workflow = create_smart_customer_service_workflow()

print("🎧 Smart Customer Service Demo")
    print("=" * 60)
    print("")
    print("This workflow demonstrates:")
    print("• 🤖 Simple routing between Technical, Billing, and General support")
    print("• 📚 Shared conversation history across ALL agents")
    print("• 💬 Context continuity - agents remember your entire conversation")
    print("")
    print("🎯 TRY THESE CONVERSATIONS:")
    print("")
    print("🔧 TECHNICAL SUPPORT:")
    print("   • 'My API is not working'")
    print("   • 'I'm getting an error message'")
    print("   • 'There's a technical bug'")
    print("")
    print("💳 BILLING SUPPORT:")
    print("   • 'I need help with billing'")
    print("   • 'Can I get a refund?'")
    print("   • 'My payment was charged twice'")
    print("")
    print("🎧 GENERAL SUPPORT:")
    print("   • 'Hello, I have a question'")
    print("   • 'What features do you offer?'")
    print("   • 'I need general help'")
    print("")
    print("Type 'exit' to quit")
    print("-" * 60)

workflow.cli_app(
        session_id="smart_customer_service_demo",
        user="Customer",
        emoji="🎧",
        stream=True,
        stream_events=True,
        show_step_details=True,
    )

if __name__ == "__main__":
    demo_smart_customer_service_cli()
```

---

## Create team with event monitoring

**URL:** llms-txt#create-team-with-event-monitoring

**Contents:**
- Usage

company_info_team = Team(
    name="Company Info Team",
    id=id,
    model=Claude(id="claude-3-7-sonnet-latest"),
    members=[
        hacker_news_agent,
        website_agent,
    ],
    markdown=True,
    instructions=[
        "You are a team that finds information about a company.",
        "First search the web and Hacker News for information about the company.",
        "If you can find the company's website URL, then scrape the homepage and the about page.",
    ],
    show_members_responses=True,
)

async def run_team_with_events(prompt: str):
    """
    Run the team and capture all events for monitoring and debugging.

This function demonstrates how to handle different types of events:
    - Team-level events (run start/completion, tool calls)
    - Member-level events (agent tool calls)
    - Content generation events
    """
    content_started = False

async for run_response_event in company_info_team.arun(
        prompt,
        stream=True,
        stream_events=True,
    ):
        # Handle team-level events
        if run_response_event.event in [
            TeamRunEvent.run_started,
            TeamRunEvent.run_completed,
        ]:
            print(f"\n🎯 TEAM EVENT: {run_response_event.event}")

# Handle team tool call events
        if run_response_event.event in [TeamRunEvent.tool_call_started]:
            print(f"\n🔧 TEAM TOOL STARTED: {run_response_event.tool.tool_name}")
            print(f"   Args: {run_response_event.tool.tool_args}")

if run_response_event.event in [TeamRunEvent.tool_call_completed]:
            print(f"\n✅ TEAM TOOL COMPLETED: {run_response_event.tool.tool_name}")
            print(f"   Result: {run_response_event.tool.result}")

# Handle member-level events
        if run_response_event.event in [RunEvent.tool_call_started]:
            print(f"\n🤖 MEMBER TOOL STARTED: {run_response_event.agent_id}")
            print(f"   Tool: {run_response_event.tool.tool_name}")
            print(f"   Args: {run_response_event.tool.tool_args}")

if run_response_event.event in [RunEvent.tool_call_completed]:
            print(f"\n✅ MEMBER TOOL COMPLETED: {run_response_event.agent_id}")
            print(f"   Tool: {run_response_event.tool.tool_name}")
            print(
                f"   Result: {run_response_event.tool.result[:100]}..."
            )  # Truncate for readability

# Handle content generation
        if run_response_event.event in [TeamRunEvent.run_content]:
            if not content_started:
                print("\n📝 CONTENT:")
                content_started = True
            else:
                print(run_response_event.content, end="")

if __name__ == "__main__":
    asyncio.run(
        run_team_with_events(
            "Write me a full report on everything you can find about Agno, the company building AI agent infrastructure.",
        )
    )
bash  theme={null}
    pip install agno ddgs
    bash  theme={null}
    export ANTHROPIC_API_KEY=****
    export OPENAI_API_KEY=****
    bash  theme={null}
    python cookbook/examples/teams/streaming/05_async_team_events.py
    ```
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Install required libraries">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Set environment variables">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run the agent">
```

---

## Example 4: Listing current items

**URL:** llms-txt#example-4:-listing-current-items

print("Example 4: Viewing Current Shopping List")
print("-" * 50)
shopping_team.print_response("What's on my shopping list right now?", stream=True)
print(f"Session state: {shopping_team.get_session_state()}")
print()

---

## Create a team for collaborative video caption generation

**URL:** llms-txt#create-a-team-for-collaborative-video-caption-generation

**Contents:**
- Usage

caption_team = Team(
    name="Video Caption Team",
    members=[video_processor, caption_generator],
    model=OpenAIChat(id="gpt-5-mini"),
    description="Team that generates and embeds captions for videos",
    instructions=[
        "Process videos to generate captions in this sequence:",
        "1. Extract audio from the video using extract_audio",
        "2. Transcribe the audio using transcribe_audio",
        "3. Generate SRT captions using create_srt",
        "4. Embed captions into the video using embed_captions",
    ],
    markdown=True,
)

caption_team.print_response(
    "Generate captions for {video with location} and embed them in the video"
)
bash  theme={null}
    pip install agno moviepy ffmpeg-python
    bash  theme={null}
    export OPENAI_API_KEY=****
    bash  theme={null}
    python cookbook/examples/teams/multimodal/video_caption_generation.py
    ```
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Install required libraries">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Set environment variables">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run the agent">
```

---

## Image Model Output

**URL:** llms-txt#image-model-output

**Contents:**
- Image Output Modality

Source: https://docs.agno.com/concepts/multimodal/images/image_output

Learn how to use images from models as output with Agno agents.

Similar to providing image inputs, you can also get image outputs from an agent. Take a look at the [compatibility matrix](/concepts/models/compatibility#multimodal-support) to see which models support image as output.

## Image Output Modality

The following example demonstrates how some models can directly generate images as part of their response.

```python image_agent.py theme={null}
from io import BytesIO

from agno.agent import Agent, RunOutput  # noqa
from agno.models.google import Gemini
from PIL import Image

---

## Analyze team leader message metrics

**URL:** llms-txt#analyze-team-leader-message-metrics

print("=" * 50)
print("TEAM LEADER MESSAGE METRICS")
print("=" * 50)

if run_output.messages:
    for message in run_output.messages:
        if message.role == "assistant":
            if message.content:
                print(f"📝 Message: {message.content[:100]}...")
            elif message.tool_calls:
                print(f"🔧 Tool calls: {message.tool_calls}")

print("-" * 30, "Metrics", "-" * 30)
            pprint(message.metrics)
            print("-" * 70)

---

## Audio Input (Bytes Content)

**URL:** llms-txt#audio-input-(bytes-content)

**Contents:**
- Code

Source: https://docs.agno.com/examples/models/gemini/audio_input_bytes_content

```python cookbook/models/google/gemini/audio_input_bytes_content.py theme={null}
import requests
from agno.agent import Agent
from agno.media import Audio
from agno.models.google import Gemini

agent = Agent(
    model=Gemini(id="gemini-2.0-flash-exp"),
    markdown=True,
)

url = "https://openaiassets.blob.core.windows.net/$web/API/docs/audio/alloy.wav"

---

## You can either specify the user_input_fields or leave empty for all fields to be provided by the user

**URL:** llms-txt#you-can-either-specify-the-user_input_fields-or-leave-empty-for-all-fields-to-be-provided-by-the-user

**Contents:**
- Dynamic User Input

@tool(requires_user_input=True, user_input_fields=["to_address"])
def send_email(subject: str, body: str, to_address: str) -> str:
    """
    Send an email.

Args:
        subject (str): The subject of the email.
        body (str): The body of the email.
        to_address (str): The address to send the email to.
    """
    return f"Sent email to {to_address} with subject {subject} and body {body}"

agent = Agent(
    model=OpenAIChat(id="gpt-5-mini"),
    tools=[send_email],
)

run_response = agent.run("Send an email with the subject 'Hello' and the body 'Hello, world!'")
if run_response.is_paused:
    for tool in run_response.tools_requiring_user_input:
        input_schema: List[UserInputField] = tool.user_input_schema

for field in input_schema:
            # Display field information to the user
            print(f"\nField: {field.name} ({field.field_type.__name__}) -> {field.description}")

# Get user input (if the value is not set, it means the user needs to provide the value)
            if field.value is None:
                user_value = input(f"Please enter a value for {field.name}: ")
                field.value = user_value
            else:
                print(f"Value provided by the agent: {field.value}")

run_response = (
        agent.continue_run(run_response=run_response)
    )
python  theme={null}
from typing import List

from agno.agent import Agent
from agno.tools.function import UserInputField
from agno.models.openai import OpenAIChat
from agno.tools import tool
from agno.tools.toolkit import Toolkit
from agno.tools.user_control_flow import UserControlFlowTools
from agno.utils import pprint

**Examples:**

Example 1 (unknown):
```unknown
## Dynamic User Input

This pattern provides the agent with tools to indicate when it needs user input. It's ideal for cases:

* Where it is unknown how the user will interact with the agent
* When you want a form-like interaction with the user

In the following example, we use a specialized tool to allow the agent to collect user feedback when it needs it.
```

---

## Create workflow with router

**URL:** llms-txt#create-workflow-with-router

workflow = Workflow(
    name="Adaptive Assistant Workflow",
    steps=[
        # Router that selects agent based on session state
        Router(
            name="Route to Appropriate Agent",
            description="Route to the appropriate agent based on user preferences",
            selector=route_based_on_user_preference,
            choices=[
                onboarding_step,
                technical_step,
                friendly_step,
                general_step,
            ],
        ),
        # After first interaction, update preferences
        Step(
            name="Update Preferences",
            description="Update user preferences based on interaction",
            executor=set_user_preference,
        ),
    ],
    session_state={
        "agent_preference": "general",
        "interaction_count": 0,
    },
)

def run_example():
    """Run the example workflow multiple times to see dynamic routing."""

queries = [
        "Hello! I'm new here.",
        "How do I implement a binary search tree in Python?",
        "What's the best pizza topping?",
        "Explain quantum computing",
    ]

for i, query in enumerate(queries, 1):
        print("\n" + "=" * 80)
        print(f"Interaction {i}: {query}")
        print("=" * 80)

workflow.print_response(
            input=query,
            session_id="user-456",
            user_id="user-456",
            stream=True,
            stream_events=True,
        )

if __name__ == "__main__":
    run_example()
```

---

## SQL

**URL:** llms-txt#sql

**Contents:**
- Prerequisites
  - PostgreSQL
  - MySQL
- Example
- Toolkit Params
- Toolkit Functions
- Developer Resources

Source: https://docs.agno.com/concepts/tools/toolkits/database/sql

**SQLTools** enable an Agent to run SQL queries and interact with databases.

The following example requires the `sqlalchemy` library and a database URL.

You will also need to install the appropriate Python adapter for the specific database you intend to use.

For PostgreSQL, you can install the `psycopg2-binary` adapter:

For MySQL, you can install the `mysqlclient` adapter:

The `mysqlclient` adapter may have additional system-level dependencies. Please consult the [official installation guide](https://github.com/PyMySQL/mysqlclient/blob/main/README.md#install) for more details.

You will also need a database. The following example uses a Postgres database running in a Docker container.

The following agent will run a SQL query to list all tables in the database and describe the contents of one of the tables.

| Parameter               | Type             | Default | Description                                                                 |
| ----------------------- | ---------------- | ------- | --------------------------------------------------------------------------- |
| `db_url`                | `str`            | `None`  | The URL for connecting to the database.                                     |
| `db_engine`             | `Engine`         | `None`  | The database engine used for connections and operations.                    |
| `user`                  | `str`            | `None`  | The username for database authentication.                                   |
| `password`              | `str`            | `None`  | The password for database authentication.                                   |
| `host`                  | `str`            | `None`  | The hostname or IP address of the database server.                          |
| `port`                  | `int`            | `None`  | The port number on which the database server is listening.                  |
| `schema`                | `str`            | `None`  | The specific schema within the database to use.                             |
| `dialect`               | `str`            | `None`  | The SQL dialect used by the database.                                       |
| `tables`                | `Dict[str, Any]` | `None`  | A dictionary mapping table names to their respective metadata or structure. |
| `enable_list_tables`    | `bool`           | `True`  | Enables the functionality to list all tables in the database.               |
| `enable_describe_table` | `bool`           | `True`  | Enables the functionality to describe the schema of a specific table.       |
| `enable_run_sql_query`  | `bool`           | `True`  | Enables the functionality to execute SQL queries directly.                  |
| `all`                   | `bool`           | `False` | Enables all functionality when set to True.                                 |

| Function         | Description                               |
| ---------------- | ----------------------------------------- |
| `list_tables`    | Lists all tables in the database.         |
| `describe_table` | Describes the schema of a specific table. |
| `run_sql_query`  | Executes SQL queries directly.            |

## Developer Resources

* View [Tools](https://github.com/agno-agi/agno/blob/main/libs/agno/agno/tools/sql.py)
* View [Cookbook](https://github.com/agno-agi/agno/tree/main/cookbook/tools/sql_tools.py)

**Examples:**

Example 1 (unknown):
```unknown
You will also need to install the appropriate Python adapter for the specific database you intend to use.

### PostgreSQL

For PostgreSQL, you can install the `psycopg2-binary` adapter:
```

Example 2 (unknown):
```unknown
### MySQL

For MySQL, you can install the `mysqlclient` adapter:
```

Example 3 (unknown):
```unknown
The `mysqlclient` adapter may have additional system-level dependencies. Please consult the [official installation guide](https://github.com/PyMySQL/mysqlclient/blob/main/README.md#install) for more details.

You will also need a database. The following example uses a Postgres database running in a Docker container.
```

Example 4 (unknown):
```unknown
## Example

The following agent will run a SQL query to list all tables in the database and describe the contents of one of the tables.
```

---

## Download the video file from the URL as bytes

**URL:** llms-txt#download-the-video-file-from-the-url-as-bytes

**Contents:**
- Usage

response = requests.get(url)
video_content = response.content

agent.print_response(
    "Tell me about this video",
    videos=[Video(content=video_content)],
)
bash  theme={null}
    export GOOGLE_API_KEY=xxx
    bash  theme={null}
    pip install -U google-genai agno
    bash Mac theme={null}
      python cookbook/models/google/gemini/video_input_bytes_content.py
      bash Windows theme={null}
      python cookbook/models/google/gemini/video_input_bytes_content.py
      ```
    </CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Set your API key">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## MoviePy Video Tools

**URL:** llms-txt#moviepy-video-tools

**Contents:**
- Prerequisites
- Example
- Toolkit Functions
- Toolkit Params
- Developer Resources

Source: https://docs.agno.com/concepts/tools/toolkits/others/moviepy

Agno MoviePyVideoTools enable an Agent to process videos, extract audio, generate SRT caption files, and embed rich, word-highlighted captions.

To use `MoviePyVideoTools`, you need to install `moviepy` and its dependency `ffmpeg`:

**Important for Captioning Workflow:**
The `create_srt` and `embed_captions` tools require a transcription of the video's audio. `MoviePyVideoTools` itself does not perform speech-to-text. You'll typically use another tool, such as `OpenAITools` with its `transcribe_audio` function, to generate the transcription (often in SRT format) which is then used by these tools.

The following example demonstrates a complete workflow where an agent uses `MoviePyVideoTools` in conjunction with `OpenAITools` to:

1. Extract audio from a video file
2. Transcribe the audio using OpenAI's speech-to-text
3. Generate an SRT caption file from the transcription
4. Embed the captions into the video with word-level highlighting

These are the functions exposed by `MoviePyVideoTools`:

| Function                | Description                                                                                            |
| ----------------------- | ------------------------------------------------------------------------------------------------------ |
| `enable_extract_audio`  | Extracts the audio track from a video file and saves it to a specified output path.                    |
| `enable_create_srt`     | Saves a given transcription (expected in SRT format) to a `.srt` file at the specified output path.    |
| `enable_embed_captions` | Embeds captions from an SRT file into a video, creating a new video file with word-level highlighting. |

These parameters are passed to the `MoviePyVideoTools` constructor:

| Parameter           | Type   | Default | Description                        |
| ------------------- | ------ | ------- | ---------------------------------- |
| `process_video`     | `bool` | `True`  | Enables the `extract_audio` tool.  |
| `generate_captions` | `bool` | `True`  | Enables the `create_srt` tool.     |
| `embed_captions`    | `bool` | `True`  | Enables the `embed_captions` tool. |

## Developer Resources

* View [Tools](https://github.com/agno-agi/agno/blob/main/libs/agno/agno/tools/moviepy_video.py)
* View [Cookbook](https://github.com/agno-agi/agno/tree/main/cookbook/tools/moviepy_video_tools.py)

**Examples:**

Example 1 (unknown):
```unknown
**Important for Captioning Workflow:**
The `create_srt` and `embed_captions` tools require a transcription of the video's audio. `MoviePyVideoTools` itself does not perform speech-to-text. You'll typically use another tool, such as `OpenAITools` with its `transcribe_audio` function, to generate the transcription (often in SRT format) which is then used by these tools.

## Example

The following example demonstrates a complete workflow where an agent uses `MoviePyVideoTools` in conjunction with `OpenAITools` to:

1. Extract audio from a video file
2. Transcribe the audio using OpenAI's speech-to-text
3. Generate an SRT caption file from the transcription
4. Embed the captions into the video with word-level highlighting
```

---

## E2B

**URL:** llms-txt#e2b

**Contents:**
- Prerequisites
- Example

Source: https://docs.agno.com/concepts/tools/toolkits/others/e2b

Enable your Agents to run code in a remote, secure sandbox.

**E2BTools** enable an Agent to execute code in a secure sandboxed environment with support for Python, file operations, and web server capabilities.

The E2B tools require the `e2b_code_interpreter` Python package and an E2B API key.

The following example demonstrates how to create an agent that can run Python code in a secure sandbox:

```python cookbook/tools/e2b_tools.py theme={null}
from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.e2b import E2BTools

e2b_tools = E2BTools(
    timeout=600,  # 10 minutes timeout (in seconds)
)

agent = Agent(
    name="Code Execution Sandbox",
    id="e2b-sandbox",
    model=OpenAIChat(id="gpt-5-mini"),
    tools=[e2b_tools],
    markdown=True,
        instructions=[
        "You are an expert at writing and validating Python code using a secure E2B sandbox environment.",
        "Your primary purpose is to:",
        "1. Write clear, efficient Python code based on user requests",
        "2. Execute and verify the code in the E2B sandbox",
        "3. Share the complete code with the user, as this is the main use case",
        "4. Provide thorough explanations of how the code works",
        "",
        "You can use these tools:",
        "1. Run Python code (run_python_code)",
        "2. Upload files to the sandbox (upload_file)",
        "3. Download files from the sandbox (download_file_from_sandbox)",
        "4. Generate and add visualizations as image artifacts (download_png_result)",
        "5. List files in the sandbox (list_files)",
        "6. Read and write file content (read_file_content, write_file_content)",
        "7. Start web servers and get public URLs (run_server, get_public_url)",
        "8. Manage the sandbox lifecycle (set_sandbox_timeout, get_sandbox_status, shutdown_sandbox)",
    ],
)

**Examples:**

Example 1 (unknown):
```unknown

```

Example 2 (unknown):
```unknown
## Example

The following example demonstrates how to create an agent that can run Python code in a secure sandbox:
```

---

## Jira

**URL:** llms-txt#jira

**Contents:**
- Prerequisites
- Example
- Toolkit Params
- Toolkit Functions
- Developer Resources

Source: https://docs.agno.com/concepts/tools/toolkits/others/jira

**JiraTools** enable an Agent to perform Jira tasks.

The following example requires the `jira` library and auth credentials.

The following agent will use Jira API to search for issues in a project.

| Parameter              | Type   | Default | Description                                                                                                                   |
| ---------------------- | ------ | ------- | ----------------------------------------------------------------------------------------------------------------------------- |
| `server_url`           | `str`  | `""`    | The URL of the JIRA server, retrieved from the environment variable `JIRA_SERVER_URL`. Default is an empty string if not set. |
| `username`             | `str`  | `None`  | The JIRA username for authentication, retrieved from the environment variable `JIRA_USERNAME`. Default is None if not set.    |
| `password`             | `str`  | `None`  | The JIRA password for authentication, retrieved from the environment variable `JIRA_PASSWORD`. Default is None if not set.    |
| `token`                | `str`  | `None`  | The JIRA API token for authentication, retrieved from the environment variable `JIRA_TOKEN`. Default is None if not set.      |
| `enable_get_issue`     | `bool` | `True`  | Enable the get\_issue functionality.                                                                                          |
| `enable_create_issue`  | `bool` | `True`  | Enable the create\_issue functionality.                                                                                       |
| `enable_search_issues` | `bool` | `True`  | Enable the search\_issues functionality.                                                                                      |
| `enable_add_comment`   | `bool` | `True`  | Enable the add\_comment functionality.                                                                                        |
| `all`                  | `bool` | `False` | Enable all functionality.                                                                                                     |

| Function        | Description                                                                                                                                                                                                                                                                                                                                |
| --------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |
| `get_issue`     | Retrieves issue details from JIRA. Parameters include:<br />- `issue_key`: the key of the issue to retrieve<br />Returns a JSON string containing issue details or an error message.                                                                                                                                                       |
| `create_issue`  | Creates a new issue in JIRA. Parameters include:<br />- `project_key`: the project in which to create the issue<br />- `summary`: the issue summary<br />- `description`: the issue description<br />- `issuetype`: the type of issue (default is "Task")<br />Returns a JSON string with the new issue's key and URL or an error message. |
| `search_issues` | Searches for issues using a JQL query in JIRA. Parameters include:<br />- `jql_str`: the JQL query string<br />- `max_results`: the maximum number of results to return (default is 50)<br />Returns a JSON string containing a list of dictionaries with issue details or an error message.                                               |
| `add_comment`   | Adds a comment to an issue in JIRA. Parameters include:<br />- `issue_key`: the key of the issue<br />- `comment`: the comment text<br />Returns a JSON string indicating success or an error message.                                                                                                                                     |

## Developer Resources

* View [Tools](https://github.com/agno-agi/agno/blob/main/libs/agno/agno/tools/jira.py)
* View [Cookbook](https://github.com/agno-agi/agno/tree/main/cookbook/tools/jira_tools.py)

**Examples:**

Example 1 (unknown):
```unknown

```

Example 2 (unknown):
```unknown
## Example

The following agent will use Jira API to search for issues in a project.
```

---

## HuggingFace Embedder

**URL:** llms-txt#huggingface-embedder

**Contents:**
- Usage

Source: https://docs.agno.com/concepts/knowledge/embedder/huggingface

The `HuggingfaceCustomEmbedder` class is used to embed text data into vectors using the Hugging Face API. You can get one from [here](https://huggingface.co/settings/tokens).

```python huggingface_embedder.py theme={null}
from agno.knowledge.knowledge import Knowledge
from agno.vectordb.pgvector import PgVector
from agno.knowledge.embedder.huggingface import HuggingfaceCustomEmbedder

---

## Pandas

**URL:** llms-txt#pandas

Source: https://docs.agno.com/concepts/tools/toolkits/database/pandas

**PandasTools** enable an Agent to perform data manipulation tasks using the Pandas library.

```python cookbook/tools/pandas_tool.py theme={null}
from agno.agent import Agent
from agno.tools.pandas import PandasTools

---

## Dalle

**URL:** llms-txt#dalle

**Contents:**
- Prerequisites
- Example

Source: https://docs.agno.com/concepts/tools/toolkits/others/dalle

You need to install the `openai` library.

Set the `OPENAI_API_KEY` environment variable.

The following agent will use DALL-E to generate an image based on a text prompt.

```python cookbook/tools/dalle_tools.py theme={null}
from agno.agent import Agent
from agno.tools.dalle import DalleTools

**Examples:**

Example 1 (unknown):
```unknown
Set the `OPENAI_API_KEY` environment variable.
```

Example 2 (unknown):
```unknown
## Example

The following agent will use DALL-E to generate an image based on a text prompt.
```

---

## Define steps

**URL:** llms-txt#define-steps

research_step = Step(
    name="Research Step",
    team=research_team,
)

content_planning_step = Step(
    name="Content Planning Step",
    agent=content_planner,
)

---

## GCS for Team

**URL:** llms-txt#gcs-for-team

**Contents:**
- Usage

Source: https://docs.agno.com/examples/concepts/db/gcs/gcs_for_team

Agno supports using Google Cloud Storage (GCS) as a storage backend for Teams using the `GcsJsonDb` class. This storage backend stores session data as JSON blobs in a GCS bucket.

Configure your team with GCS storage to enable cloud-based session persistence.

```python gcs_for_team.py theme={null}
"""
Run: `pip install openai ddgs newspaper4k lxml_html_clean agno` to install the dependencies
"""

import uuid
import google.auth
from typing import List

from agno.agent import Agent
from agno.db.gcs_json import GcsJsonDb
from agno.models.openai import OpenAIChat
from agno.team import Team
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.hackernews import HackerNewsTools
from pydantic import BaseModel

---

## Accuracy with Tools

**URL:** llms-txt#accuracy-with-tools

**Contents:**
- Code

Source: https://docs.agno.com/examples/concepts/evals/accuracy/accuracy_with_tools

Learn how to evaluate the accuracy of an Agent that is using tools.

This example shows an evaluation that runs the provided agent with the provided input and then evaluates the answer that the agent gives.

---

## Singlestore for Workflow

**URL:** llms-txt#singlestore-for-workflow

**Contents:**
- Usage

Source: https://docs.agno.com/examples/concepts/db/singlestore/singlestore_for_workflow

Agno supports using Singlestore as a storage backend for Workflows using the `SingleStoreDb` class.

Obtain the credentials for Singlestore from [here](https://portal.singlestore.com/)

```python singlestore_for_workflow.py theme={null}
from agno.agent import Agent
from agno.db.singlestore import SingleStoreDb
from agno.models.openai import OpenAIChat
from agno.team import Team
from agno.tools.duckduckgo import DuckDuckGoTools
from agno.tools.hackernews import HackerNewsTools
from agno.workflow.step import Step
from agno.workflow.workflow import Workflow

---

## Tool Use

**URL:** llms-txt#tool-use

**Contents:**
- Code
- Usage

Source: https://docs.agno.com/examples/models/xai/tool_use

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Set your API key">
    
  </Step>

<Step title="Install libraries">
    
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Set your API key">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## "user_profile": get_user_profile,

**URL:** llms-txt#"user_profile":-get_user_profile,

---

## Print the response on the terminal

**URL:** llms-txt#print-the-response-on-the-terminal

**Contents:**
- Usage

agent.print_response("Share a 2 sentence horror story", stream=True)
bash  theme={null}
    export GROQ_API_KEY=xxx
    bash  theme={null}
    pip install -U groq agno
    bash Mac theme={null}
      python cookbook/models/groq/basic_stream.py
      bash Windows theme={null}
      python cookbook/models/groq/basic_stream.py
      ```
    </CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Set your API key">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## - `deepseek-r1-distill-llama-70b` as the reasoning model

**URL:** llms-txt#--`deepseek-r1-distill-llama-70b`-as-the-reasoning-model

---

## Qdrant Hybrid Search

**URL:** llms-txt#qdrant-hybrid-search

**Contents:**
- Code
- Usage

Source: https://docs.agno.com/examples/concepts/vectordb/qdrant-db/qdrant-db-hybrid-search

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Install libraries">
    
  </Step>

<Step title="Run Qdrant">
    
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Install libraries">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Run Qdrant">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Change this if your Postgres container is running elsewhere

**URL:** llms-txt#change-this-if-your-postgres-container-is-running-elsewhere

DB_URL = "postgresql+psycopg://ai:ai@localhost:5532/ai"

agent = Agent(
    model=VLLM(id="microsoft/Phi-3-mini-128k-instruct"),
    db=PostgresDb(db_url=DB_URL),
    enable_user_memories=True,
    enable_session_summaries=True,
)

---

## Agno Infra

**URL:** llms-txt#agno-infra

**Contents:**
- Create a new Agno Infra project
- Start infra resources
- Stop infra resources
- Patch infra resources
- Restart infra
- Command Options
  - Environment (`--env`)
  - Infra (`--infra`)
  - Group (`--group`)
  - Name (`--name`)

Source: https://docs.agno.com/deploy/agno-infra

The Agno Infra library provides standardized codebases to help you take your AgentOS to production. It lets you define, deploy, and manage your entire Agentic System as Python code.

## Create a new Agno Infra project

Run `ag infra create` to create a new infra project, the command will ask your for a starter template and infra project name.

<CodeGroup>
  
</CodeGroup>

And select a template from the list of available templates.

## Start infra resources

Run `ag infra up` to start i.e. create infra resources. This will start your AgentOS instance and PostgreSQL database locally using docker.

## Stop infra resources

Run `ag infra down` to stop i.e. delete infra resources

## Patch infra resources

Run `ag infra patch` to patch i.e. update infra resources

<Note>
  The `patch` command in under development for some resources. Use `restart` if needed
</Note>

Run `ag infra restart` to stop resources and start them again

<Note>Run `ag infra up --help` to view all options</Note>

### Environment (`--env`)

Use the `--env` or `-e` flag to filter the environment (dev/prd)

### Infra (`--infra`)

Use the `--infra` or `-i` flag to filter the infra (docker/aws/k8s)

### Group (`--group`)

Use the `--group` or `-g` flag to filter by resource group.

Use the `--name` or `-n` flag to filter by resource name

Use the `--type` or `-t` flag to filter by resource type.

### Dry Run (`--dry-run`)

The `--dry-run` or `-dr` flag can be used to **dry-run** the command. `ag infra up -dr` will only print resources, not create them.

### Show Debug logs (`--debug`)

Use the `--debug` or `-d` flag to show debug logs.

### Force recreate images & containers (`-f`)

Use the `--force` or `-f` flag to force recreate images & containers

**Examples:**

Example 1 (unknown):
```unknown
</CodeGroup>

And select a template from the list of available templates.

## Start infra resources

Run `ag infra up` to start i.e. create infra resources. This will start your AgentOS instance and PostgreSQL database locally using docker.

<CodeGroup>
```

Example 2 (unknown):
```unknown

```

Example 3 (unknown):
```unknown

```

Example 4 (unknown):
```unknown

```

---

## PromptInjectionGuardrail

**URL:** llms-txt#promptinjectionguardrail

**Contents:**
- Parameters
- Injection patterns

Source: https://docs.agno.com/reference/hooks/prompt-injection-guardrail

| Parameter            | Type                  | Default | Description                                                                              |
| -------------------- | --------------------- | ------- | ---------------------------------------------------------------------------------------- |
| `injection_patterns` | `Optional[List[str]]` | `None`  | A list of patterns to check for. Defaults to a list of common prompt injection patterns. |

## Injection patterns

The default list of injection patterns handled by the guardrail are:

* "ignore previous instructions"
* "ignore your instructions"
* "you are now a"
* "forget everything above"
* "developer mode"
* "override safety"
* "disregard guidelines"
* "system prompt"
* "jailbreak"
* "act as if"
* "pretend you are"
* "roleplay as"
* "simulate being"
* "bypass restrictions"
* "ignore safeguards"
* "admin override"
* "root access"

---

## Define parallel research steps

**URL:** llms-txt#define-parallel-research-steps

tech_research_step = Step(
    name="TechResearch",
    agent=tech_researcher,
    description="Research tech developments from Hacker News",
)

news_research_step = Step(
    name="NewsResearch",
    agent=news_researcher,
    description="Research current news and trends",
)

---

## Analyze session-level metrics

**URL:** llms-txt#analyze-session-level-metrics

print("=" * 50)
print("SESSION METRICS")
print("=" * 50)
pprint(team.get_session_metrics(session_id="team_metrics_demo"))

---

## Reasoning Models

**URL:** llms-txt#reasoning-models

**Contents:**
- Examples
  - gpt-5-mini

Source: https://docs.agno.com/concepts/reasoning/reasoning-models

Reasoning models are a new class of large language models pre-trained to think before they answer. They produce a long internal chain of thought before responding. Examples of reasoning models include:

* OpenAI o1-pro and gpt-5-mini
* Claude 3.7 sonnet in extended-thinking mode
* Gemini 2.0 flash thinking
* DeepSeek-R1

Reasoning models deeply consider and think through a plan before taking action. Its all about what the model does **before it starts generating a response**. Reasoning models excel at single-shot use-cases. They're perfect for solving hard problems (coding, math, physics) that don't require multiple turns, or calling tools sequentially.

```python o3_mini.py theme={null}
from agno.agent import Agent
from agno.models.openai import OpenAIChat

---

## Continue conversation - history shared via session_id

**URL:** llms-txt#continue-conversation---history-shared-via-session_id

**Contents:**
- Cross-Provider Switching

budget_agent.print_response(
    "Can you summarize our discussion so far?", session_id=session_id, user_id=user_id
)
python  theme={null}
from uuid import uuid4
from agno.agent import Agent
from agno.db.sqlite import SqliteDb
from agno.models.openai.chat import OpenAIChat
from agno.models.google import Gemini

db = SqliteDb(db_file="tmp/cross_provider_demo.db")

session_id = str(uuid4())
user_id = "user@example.com"

**Examples:**

Example 1 (unknown):
```unknown
## Cross-Provider Switching

Switching between providers can cause compatibility issues and is not recommended for production use without thorough testing:
```

---

## Postgres Tools

**URL:** llms-txt#postgres-tools

**Contents:**
- Code
- Usage

Source: https://docs.agno.com/examples/concepts/tools/database/postgres

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Set your API key">
    
  </Step>

<Step title="Set your database URL">
    
  </Step>

<Step title="Install libraries">
    
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Set your API key">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Set your database URL">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 4 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

---

## ag infra patch

**URL:** llms-txt#ag-infra-patch

**Contents:**
- Params

Source: https://docs.agno.com/reference/agno-infra/cli/ws/patch

Update resources for active infra

<ResponseField name="resources_filter" type="str">
  Resource filter. Format - ENV:INFRA:GROUP:NAME:TYPE
</ResponseField>

<ResponseField name="env_filter" type="str">
  Filter the environment to deploy `--env` `-e`
</ResponseField>

<ResponseField name="infra_filter" type="str">
  Filter the infra to deploy. `--infra` `-i`
</ResponseField>

<ResponseField name="group_filter" type="str">
  Filter resources using group name. `--group` `-g`
</ResponseField>

<ResponseField name="name_filter" type="str">
  Filter resource using name. `--name` `-n`
</ResponseField>

<ResponseField name="type_filter" type="str">
  Filter resource using type `--type` `-t`
</ResponseField>

<ResponseField name="dry_run" type="bool">
  Print resources and exit. `--dry-run` `-dr`
</ResponseField>

<ResponseField name="auto_confirm" type="bool">
  Skip the confirmation before deploying resources. `--yes` `-y`
</ResponseField>

<ResponseField name="print_debug_log" type="bool">
  Print debug logs. `--debug` `-d`
</ResponseField>

<ResponseField name="force" type="bool">
  Force `--force` `-f`
</ResponseField>

<ResponseField name="pull" type="bool">
  Pull `--pull` `-p`
</ResponseField>

---

## What are Embedders?

**URL:** llms-txt#what-are-embedders?

**Contents:**
  - Why use embedders?
  - When to use embedders
  - How it works in Agno

Source: https://docs.agno.com/concepts/knowledge/embedder/overview

Learn how to use embedders with Agno to convert complex information into vector representations.

Embedders turn text, images, and other data into vectors (lists of numbers) that capture meaning. Those vectors make it easy to store and search information semantically—so you find content by intent and context, not just exact keywords.

If you're building features like retrieval-augmented generation (RAG), semantic search, question answering over docs, or long-term memory for agents, embedders are the foundation that makes it all work.

### Why use embedders?

* **Better recall than keywords**: They understand meaning, so "How do I reset my passcode?" finds docs mentioning "change PIN".
* **Ground LLMs in your data**: Provide the model with trusted, domain-specific context at answer time.
* **Scale to large knowledge bases**: Vectors enable fast similarity search across thousands or millions of chunks.
* **Multilingual retrieval**: Many embedders map different languages to the same semantic space.

### When to use embedders

Use embedders when you need any of the following:

* **RAG and context injection**: Supply relevant snippets to your agent before responding.
* **Semantic search**: Let users query by meaning across product docs, wikis, tickets, or chats.
* **Deduplication and clustering**: Group similar content or avoid repeating the same info.
* **Personal and team memory**: Store summaries and facts for later recall by agents.

You probably don’t need embedders when your dataset is tiny (a handful of pages) and simple keyword search already works well.

### How it works in Agno

Agno uses `OpenAIEmbedder` as the default, but you can swap in any supported embedder. When you add content to a knowledge base, the embedder converts each chunk into a vector and stores it in your vector database. Later, when an agent searches, it embeds the query and finds the most similar vectors.

Here's a basic setup:

```python  theme={null}
from agno.agent import Agent
from agno.knowledge.knowledge import Knowledge
from agno.vectordb.pgvector import PgVector
from agno.knowledge.embedder.openai import OpenAIEmbedder

---

## Reliability with Multiple Tools

**URL:** llms-txt#reliability-with-multiple-tools

**Contents:**
- Code

Source: https://docs.agno.com/examples/concepts/evals/reliability/reliability_with_multiple_tools

Learn how to assert an Agno Agent is making multiple expected tool calls.

---

## Giphy Tools

**URL:** llms-txt#giphy-tools

**Contents:**
- Code
- Usage

Source: https://docs.agno.com/examples/concepts/tools/others/giphy

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Set your API key">
    
  </Step>

<Step title="Install libraries">
    
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Set your API key">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Define individual steps

**URL:** llms-txt#define-individual-steps

initial_research_step = Step(
    name="InitialResearch",
    agent=researcher,
    description="Initial research on the topic",
)

---

## Example 6: Multi-mode Transit Comparison

**URL:** llms-txt#example-6:-multi-mode-transit-comparison

**Contents:**
- Usage

print("\n=== Transit Options Example ===")
agent.print_response(
    """Compare different travel modes from 'Phoenix Convention Center' to 'Phoenix Art Museum':
    1. Driving
    2. Walking
    3. Transit (if available)
    Include estimated time and distance for each option.""",
    markdown=True,
    stream=True,
)
bash  theme={null}
    export GOOGLE_MAPS_API_KEY=xxx
    export OPENAI_API_KEY=xxx
    bash  theme={null}
    pip install -U openai googlemaps agno
    bash Mac theme={null}
      python cookbook/tools/google_maps_tools.py
      bash Windows theme={null}
      python cookbook/tools/google_maps_tools.py
      ```
    </CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Set your API keys">
```

Example 2 (unknown):
```unknown
Get your API key from the [Google Cloud Console](https://console.cloud.google.com/projectselector2/google/maps-apis/credentials)
  </Step>

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Together

**URL:** llms-txt#together

**Contents:**
- Parameters

Source: https://docs.agno.com/reference/models/together

The Together model provides access to Together's language models.

| Parameter  | Type            | Default                                         | Description                                                       |
| ---------- | --------------- | ----------------------------------------------- | ----------------------------------------------------------------- |
| `id`       | `str`           | `"meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo"` | The id of the Together model to use                               |
| `name`     | `str`           | `"Together"`                                    | The name of the model                                             |
| `provider` | `str`           | `"Together"`                                    | The provider of the model                                         |
| `api_key`  | `Optional[str]` | `None`                                          | The API key for Together (defaults to TOGETHER\_API\_KEY env var) |
| `base_url` | `str`           | `"https://api.together.xyz/v1"`                 | The base URL for the Together API                                 |

Together extends the OpenAI-compatible interface and supports most parameters from OpenAI.

---

## Shopping Partner

**URL:** llms-txt#shopping-partner

**Contents:**
- Code
- Usage

Source: https://docs.agno.com/examples/use-cases/agents/shopping_partner

The Shopping Partner agent is an AI-powered product recommendation system that helps users find the perfect products based on their specific preferences and requirements. This agent specializes in:

* **Smart Product Matching**: Analyzes user preferences and finds products that best match their criteria, ensuring a minimum 50% match rate
* **Trusted Sources**: Searches only authentic e-commerce platforms like Amazon, Flipkart, Myntra, Meesho, Google Shopping, Nike, and other reputable websites
* **Real-time Availability**: Verifies that recommended products are in stock and available for purchase
* **Quality Assurance**: Avoids counterfeit or unverified products to ensure user safety
* **Detailed Information**: Provides comprehensive product details including price, brand, features, and key attributes
* **User-Friendly Formatting**: Presents recommendations in a clear, organized manner for easy understanding

This agent is particularly useful for:

* Finding specific products within budget constraints
* Discovering alternatives when preferred items are unavailable
* Getting personalized recommendations based on multiple criteria
* Ensuring purchases from trusted, legitimate sources
* Saving time in product research and comparison

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Set your API key">
    
  </Step>

<Step title="Install libraries">
    
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Set your API key">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Method 2: Set session_summary_manager

**URL:** llms-txt#method-2:-set-session_summary_manager

---

## Or clone and copy

**URL:** llms-txt#or-clone-and-copy

**Contents:**
- IDE Support
- Learn More

git clone https://github.com/agno-agi/agno.git
cp agno/.cursorrules /path/to/your/project/
```

Your AI assistant (Cursor, Windsurf, etc.) will automatically detect and use it.

<Card title="View .cursorrules on GitHub" icon="github" href="https://github.com/agno-agi/agno/blob/main/.cursorrules">
  View the full .cursorrules file for building agents with Agno
</Card>

`.cursorrules` works with:

* **Cursor** - Automatic detection
* **Windsurf** - Native support
* **Other AI assistants** - Support may vary depending on integration

For detailed examples and patterns:

<CardGroup cols={2}>
  <Card title="Agent Examples" icon="code" href="/examples/concepts/agent">
    See complete agent examples
  </Card>

<Card title="Agent Concepts" icon="book" href="/concepts/agents/overview">
    Learn agent fundamentals
  </Card>

<Card title="Teams" icon="users" href="/concepts/teams/overview">
    Multi-agent coordination
  </Card>

<Card title="Workflows" icon="workflow" href="/concepts/workflows/overview">
    Deterministic agent orchestration
  </Card>
</CardGroup>

<Note>
  The `.cursorrules` file is focused on **building agents with Agno**. If you're contributing to the Agno framework itself, that will be covered separately.
</Note>

---

## Async Basic Stream

**URL:** llms-txt#async-basic-stream

**Contents:**
- Code

Source: https://docs.agno.com/examples/models/openai/responses/async_basic_stream

```python cookbook/models/openai/responses/async_basic_stream.py theme={null}
import asyncio
from typing import Iterator  # noqa

from agno.agent import Agent, RunOutputEvent  # noqa
from agno.models.openai import OpenAIResponses

agent = Agent(model=OpenAIResponses(id="gpt-5-mini"), markdown=True)

---

## ModelsLabs

**URL:** llms-txt#modelslabs

**Contents:**
- Prerequisites
- Example

Source: https://docs.agno.com/concepts/tools/toolkits/others/models_labs

You need to install the `requests` library.

Set the `MODELS_LAB_API_KEY` environment variable.

The following agent will use ModelsLabs to generate a video based on a text prompt.

```python cookbook/tools/models_labs_tools.py theme={null}
from agno.agent import Agent
from agno.tools.models_labs import ModelsLabsTools

**Examples:**

Example 1 (unknown):
```unknown
Set the `MODELS_LAB_API_KEY` environment variable.
```

Example 2 (unknown):
```unknown
## Example

The following agent will use ModelsLabs to generate a video based on a text prompt.
```

---

## Set the endpoint and headers for LangSmith

**URL:** llms-txt#set-the-endpoint-and-headers-for-langsmith

endpoint = "https://eu.api.smith.langchain.com/otel/v1/traces"
headers = {
    "x-api-key": os.getenv("LANGSMITH_API_KEY"),
    "Langsmith-Project": os.getenv("LANGSMITH_PROJECT"),
}

---

## Files As Input

**URL:** llms-txt#files-as-input

**Contents:**
- Developer Resources

Source: https://docs.agno.com/concepts/multimodal/files/file_input

Learn how to use files as input with Agno agents.

Agno supports files as input to agents and teams.  Take a look at the [compatibility matrix](/concepts/models/compatibility#multimodal-support) to see which models support files as input.

Let's create an agent that can understand files and make tool calls as needed.

## Developer Resources

* View more [Anthropic](/examples/models/anthropic/pdf_input_url) examples.
* View more [OpenAI](/examples/models/openai/responses/pdf_input_url) examples.
* View more [Gemini](/examples/models/gemini/pdf_input_url) examples.

---

## Could Not Connect To Docker

**URL:** llms-txt#could-not-connect-to-docker

**Contents:**
- Quick fix
- Full details
- More info

Source: https://docs.agno.com/faq/could-not-connect-to-docker

If you have Docker up and running and get the following error, please read on:

Create the `/var/run/docker.sock` symlink using:

In 99% of the cases, this should work. If it doesnt, try:

Agno uses [docker-py](https://github.com/docker/docker-py) to run containers, and if the `/var/run/docker.sock` is missing or has incorrect permissions, it cannot connect to docker.

**To fix, please create the `/var/run/docker.sock` file using:**

If that does not work, check the permissions using `ls -l /var/run/docker.sock`.

If the `/var/run/docker.sock` does not exist, check if the `$HOME/.docker/run/docker.sock` file is missing. If its missing, please reinstall Docker.

**If none of this works and the `/var/run/docker.sock` exists:**

* Give your user permissions to the `/var/run/docker.sock` file:

* Give your user permissions to the docker group:

* [Docker-py Issue](https://github.com/docker/docker-py/issues/3059#issuecomment-1294369344)
* [Stackoverflow answer](https://stackoverflow.com/questions/48568172/docker-sock-permission-denied/56592277#56592277)

**Examples:**

Example 1 (unknown):
```unknown
## Quick fix

Create the `/var/run/docker.sock` symlink using:
```

Example 2 (unknown):
```unknown
In 99% of the cases, this should work. If it doesnt, try:
```

Example 3 (unknown):
```unknown
## Full details

Agno uses [docker-py](https://github.com/docker/docker-py) to run containers, and if the `/var/run/docker.sock` is missing or has incorrect permissions, it cannot connect to docker.

**To fix, please create the `/var/run/docker.sock` file using:**
```

Example 4 (unknown):
```unknown
If that does not work, check the permissions using `ls -l /var/run/docker.sock`.

If the `/var/run/docker.sock` does not exist, check if the `$HOME/.docker/run/docker.sock` file is missing. If its missing, please reinstall Docker.

**If none of this works and the `/var/run/docker.sock` exists:**

* Give your user permissions to the `/var/run/docker.sock` file:
```

---

## Print team leader message metrics

**URL:** llms-txt#print-team-leader-message-metrics

print("---" * 5, "Team Leader Message Metrics", "---" * 5)
if run_response.messages:
    for message in run_response.messages:
        if message.role == "assistant":
            if message.content:
                print(f"Message: {message.content}")
            elif message.tool_calls:
                print(f"Tool calls: {message.tool_calls}")
            print("---" * 5, "Metrics", "---" * 5)
            pprint(message.metrics)
            print("---" * 20)

---

## Normal conversation: 10 × 500 tokens = 5,000 tokens

**URL:** llms-txt#normal-conversation:-10-×-500-tokens-=-5,000-tokens

---

## AWS Lambda

**URL:** llms-txt#aws-lambda

**Contents:**
- Prerequisites
- Example

Source: https://docs.agno.com/concepts/tools/toolkits/others/aws_lambda

The following example requires the `boto3` library.

The following agent will use AWS Lambda to list all Lambda functions in our AWS account and invoke a specific Lambda function.

```python cookbook/tools/aws_lambda_tools.py theme={null}

from agno.agent import Agent
from agno.tools.aws_lambda import AWSLambdaTools

**Examples:**

Example 1 (unknown):
```unknown
## Example

The following agent will use AWS Lambda to list all Lambda functions in our AWS account and invoke a specific Lambda function.
```

---

## Finance Team Chain of Thought

**URL:** llms-txt#finance-team-chain-of-thought

**Contents:**
- Code
- Usage

Source: https://docs.agno.com/examples/concepts/reasoning/teams/finance_team_chain_of_thought

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Set your API key">
    
  </Step>

<Step title="Install libraries">
    
  </Step>

<Step title="Run Example">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Set your API key">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Example">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## MySQL

**URL:** llms-txt#mysql

**Contents:**
- Usage

Source: https://docs.agno.com/concepts/db/mysql

Learn to use MySQL as a database for your Agents

Agno supports using [MySQL](https://www.mysql.com/) as a database with the `MySQLDb` class.

```python mysql_for_agent.py theme={null}
from agno.agent import Agent
from agno.db.mysql import MySQLDb

---

## Shell Tools

**URL:** llms-txt#shell-tools

**Contents:**
- Code
- Usage

Source: https://docs.agno.com/examples/concepts/tools/local/shell

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Set your API key">
    
  </Step>

<Step title="Install libraries">
    
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Set your API key">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Linear

**URL:** llms-txt#linear

**Contents:**
- Prerequisites
- Example
- Toolkit Params
- Toolkit Functions
- Developer Resources

Source: https://docs.agno.com/concepts/tools/toolkits/others/linear

**LinearTool** enable an Agent to perform [Linear](https://linear.app/) tasks.

The following examples require Linear API key, which can be obtained from [here](https://linear.app/settings/account/security).

The following agent will use Linear API to search for issues in a project for a specific user.

| Parameter | Type  | Default | Description         |
| --------- | ----- | ------- | ------------------- |
| `api_key` | `str` | `None`  | Add Linear API key. |

| Function                   | Description                                                      |
| -------------------------- | ---------------------------------------------------------------- |
| `get_user_details`         | Fetch authenticated user details.                                |
| `get_issue_details`        | Retrieve details of a specific issue by issue ID.                |
| `create_issue`             | Create a new issue within a specific project and team.           |
| `update_issue`             | Update the title or state of a specific issue by issue ID.       |
| `get_user_assigned_issues` | Retrieve issues assigned to a specific user by user ID.          |
| `get_workflow_issues`      | Retrieve issues within a specific workflow state by workflow ID. |
| `get_high_priority_issues` | Retrieve issues with a high priority (priority `<=` 2).          |

You can use `include_tools` or `exclude_tools` to modify the list of tools the agent has access to. Learn more about [selecting tools](/concepts/tools/selecting-tools).

## Developer Resources

* View [Tools](https://github.com/agno-agi/agno/blob/main/libs/agno/agno/tools/linear.py)
* View [Cookbook](https://github.com/agno-agi/agno/tree/main/cookbook/tools/linear_tools.py)

**Examples:**

Example 1 (unknown):
```unknown
## Example

The following agent will use Linear API to search for issues in a project for a specific user.
```

---

## Bitbucket Tools

**URL:** llms-txt#bitbucket-tools

**Contents:**
- Code
- Usage

Source: https://docs.agno.com/examples/concepts/tools/others/bitbucket

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Set your Bitbucket credentials">
    
  </Step>

<Step title="Install libraries">
    
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Set your Bitbucket credentials">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Organized by user ID and session ID

**URL:** llms-txt#organized-by-user-id-and-session-id

def add_item(session_state, item: str) -> str:
    """Add an item to the current user's shopping list."""

current_user_id = session_state["current_user_id"]
    current_session_id = session_state["current_session_id"]
    shopping_list.setdefault(current_user_id, {}).setdefault(
        current_session_id, []
    ).append(item)

return f"Item {item} added to the shopping list"

def remove_item(session_state, item: str) -> str:
    """Remove an item from the current user's shopping list."""

current_user_id = session_state["current_user_id"]
    current_session_id = session_state["current_session_id"]

if (
        current_user_id not in shopping_list
        or current_session_id not in shopping_list[current_user_id]
    ):
        return f"No shopping list found for user {current_user_id} and session {current_session_id}"

if item not in shopping_list[current_user_id][current_session_id]:
        return f"Item '{item}' not found in the shopping list for user {current_user_id} and session {current_session_id}"

shopping_list[current_user_id][current_session_id].remove(item)
    return f"Item {item} removed from the shopping list"

def get_shopping_list(session_state) -> str:
    """Get the current user's shopping list."""

current_user_id = session_state["current_user_id"]
    current_session_id = session_state["current_session_id"]
    return f"Shopping list for user {current_user_id} and session {current_session_id}: \n{json.dumps(shopping_list[current_user_id][current_session_id], indent=2)}"

---

## Delete Session

**URL:** llms-txt#delete-session

Source: https://docs.agno.com/reference-api/schema/sessions/delete-session

delete /sessions/{session_id}
Permanently delete a specific session and all its associated runs. This action cannot be undone and will remove all conversation history.

---

## Ask "How are you?" in all supported languages

**URL:** llms-txt#ask-"how-are-you?"-in-all-supported-languages

**Contents:**
- Send input directly to members

multi_language_team.print_response(
    "How are you?", stream=True  # English
)

multi_language_team.print_response(
    "お元気ですか?", stream=True  # Japanese
)
python  theme={null}
from typing import List

from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.team.team import Team
from agno.tools.hackernews import HackerNewsTools
from pydantic import BaseModel, Field

class ResearchTopic(BaseModel):
    """Structured research topic with specific requirements."""

topic: str = Field(description="The main research topic")
    focus_areas: List[str] = Field(description="Specific areas to focus on")
    target_audience: str = Field(description="Who this research is for")
    sources_required: int = Field(description="Number of sources needed", default=5)

**Examples:**

Example 1 (unknown):
```unknown
<Note>
  `respond_directly` is not compatible with `delegate_task_to_all_members`.
</Note>

<Note>
  When using `respond_directly` and the team leader decides to delegate the task to multiple members, the final content will be the results of all member responses concatenated together.
</Note>

## Send input directly to members

When a team is run, by default the team leader will determine the "task" to give a specific member. This then becomes the `input` when that member is run.

If you set `determine_input_for_members` to `False`, the team leader will send the user-provided input **directly** to the member agent(s).  The team leader still determines the appropriate member to delegate the task to.

<Tip>
  This feature is particularly useful when you have specialized agents with distinct expertise areas and want to automatically direct queries to the right specialist.
</Tip>

In the example below, we want to send stuctured pydantic input directly to the member agent.  We don't want the team leader to ingest this input and determine a task to give to the member agent.
```

---

## Code Execution Tool

**URL:** llms-txt#code-execution-tool

**Contents:**
- Working example

Source: https://docs.agno.com/examples/models/anthropic/code_execution

Learn how to use Anthropic's code execution tool with Agno.

With Anthropic's [code execution tool](https://docs.anthropic.com/en/docs/agents-and-tools/tool-use/code-execution-tool), your model can execute Python code in a secure, sandboxed environment.
This is useful for your model to perform tasks as analyzing data, creating visualizations, or performing complex calculations.

---

## Define the two distinct pipelines

**URL:** llms-txt#define-the-two-distinct-pipelines

image_sequence = Steps(
    name="image_generation",
    description="Complete image generation and analysis workflow",
    steps=[generate_image_step, describe_image_step],
)

video_sequence = Steps(
    name="video_generation",
    description="Complete video production and analysis workflow",
    steps=[generate_video_step, describe_video_step],
)

def media_sequence_selector(step_input: StepInput) -> List[Step]:
    """
    Simple pipeline selector based on keywords in the message.

Args:
        step_input: StepInput containing message

Returns:
        List of Steps to execute
    """

# Check if message exists and is a string
    if not step_input.input or not isinstance(step_input.input, str):
        return [image_sequence]  # Default to image sequence

# Convert message to lowercase for case-insensitive matching
    message_lower = step_input.input.lower()

# Check for video keywords
    if "video" in message_lower:
        return [video_sequence]
    # Check for image keywords
    elif "image" in message_lower:
        return [image_sequence]
    else:
        # Default to image for any other case
        return [image_sequence]

---

## What is on Mark Smith's shopping list?

**URL:** llms-txt#what-is-on-mark-smith's-shopping-list?

agent.print_response(
    "What is on Mark Smith's shopping list?",
    stream=True,
    user_id=user_id_2,
    session_id="user_2_session_1",
)

---

## Fal

**URL:** llms-txt#fal

**Contents:**
- Prerequisites
- Example
- Toolkit Params
- Toolkit Functions
- Developer Resources

Source: https://docs.agno.com/concepts/tools/toolkits/others/fal

**FalTools** enable an Agent to perform media generation tasks.

The following example requires the `fal_client` library and an API key which can be obtained from [Fal](https://fal.ai/).

The following agent will use FAL to generate any video requested by the user.

| Parameter               | Type   | Default | Description                                |
| ----------------------- | ------ | ------- | ------------------------------------------ |
| `api_key`               | `str`  | `None`  | API key for authentication purposes.       |
| `model`                 | `str`  | `None`  | The model to use for the media generation. |
| `enable_generate_media` | `bool` | `True`  | Enable the generate\_media functionality.  |
| `enable_image_to_image` | `bool` | `True`  | Enable the image\_to\_image functionality. |
| `all`                   | `bool` | `False` | Enable all functionality.                  |

| Function         | Description                                                    |
| ---------------- | -------------------------------------------------------------- |
| `generate_media` | Generate either images or videos depending on the user prompt. |
| `image_to_image` | Transform an input image based on a text prompt.               |

## Developer Resources

* View [Tools](https://github.com/agno-agi/agno/blob/main/libs/agno/agno/tools/fal.py)
* View [Cookbook](https://github.com/agno-agi/agno/tree/main/cookbook/tools/fal_tools.py)

**Examples:**

Example 1 (unknown):
```unknown

```

Example 2 (unknown):
```unknown
## Example

The following agent will use FAL to generate any video requested by the user.
```

---

## DeepSeek Reasoner

**URL:** llms-txt#deepseek-reasoner

**Contents:**
- Code
- Usage

Source: https://docs.agno.com/examples/concepts/reasoning/models/deepseek/trolley-problem

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Set your API key">
    
  </Step>

<Step title="Install libraries">
    
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Set your API key">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Stream the response

**URL:** llms-txt#stream-the-response

---

## === WORKFLOW STEPS ===

**URL:** llms-txt#===-workflow-steps-===

research_step = Step(
    name="research",
    description="Research the topic",
    agent=researcher,
)

summarize_step = Step(
    name="summarize",
    description="Summarize research findings",
    agent=summarizer,
)

---

## Fireworks Embedder

**URL:** llms-txt#fireworks-embedder

**Contents:**
- Code

Source: https://docs.agno.com/examples/concepts/knowledge/embedders/fireworks-embedder

```python  theme={null}
from agno.knowledge.embedder.fireworks import FireworksEmbedder
from agno.knowledge.knowledge import Knowledge
from agno.vectordb.pgvector import PgVector

embeddings = FireworksEmbedder().get_embedding(
    "The quick brown fox jumps over the lazy dog."
)

---

## print("\n=== Async Run Response ===")

**URL:** llms-txt#print("\n===-async-run-response-===")

---

## Creates a tracer from the global tracer provider

**URL:** llms-txt#creates-a-tracer-from-the-global-tracer-provider

tracer = trace.get_tracer(__name__)

import openlit  # noqa: E402

---

## Print the session metrics

**URL:** llms-txt#print-the-session-metrics

**Contents:**
- Usage

print("---" * 5, "Session Metrics", "---" * 5)
try:
    session_metrics = agent.get_session_metrics()
    if session_metrics:
        pprint(session_metrics)
    else:
        print("No session metrics available")
except Exception as e:
    print(f"Error getting session metrics: {e}")
bash  theme={null}
    pip install -U agno openai ddgs psycopg
    bash  theme={null}
    # Make sure PostgreSQL is running
    # Update connection string in the code as needed
    bash Mac/Linux theme={null}
        export OPENAI_API_KEY="your_openai_api_key_here"
      bash Windows theme={null}
        $Env:OPENAI_API_KEY="your_openai_api_key_here"
      bash  theme={null}
    touch agent_metrics.py
    bash Mac theme={null}
      python agent_metrics.py
      bash Windows theme={null}
      python agent_metrics.py
      ```
    </CodeGroup>
  </Step>

<Step title="Find All Cookbooks">
    Explore all the available cookbooks in the Agno repository. Click the link below to view the code on GitHub:

<Link href="https://github.com/agno-agi/agno/tree/main/cookbook/agents/other" target="_blank">
      Agno Cookbooks on GitHub
    </Link>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Install libraries">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Setup PostgreSQL">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Export your OpenAI API key">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## DALL-E Tools

**URL:** llms-txt#dall-e-tools

**Contents:**
- Code
- Usage

Source: https://docs.agno.com/examples/concepts/tools/others/dalle

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Set your API key">
    
  </Step>

<Step title="Install libraries">
    
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Set your API key">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Website Tools

**URL:** llms-txt#website-tools

**Contents:**
- Code
- Usage

Source: https://docs.agno.com/examples/concepts/tools/web_scrape/website

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Set your API key">
    
  </Step>

<Step title="Install libraries">
    
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Set your API key">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Define the structured message data

**URL:** llms-txt#define-the-structured-message-data

class MediaRequest(BaseModel):
    topic: str
    content_type: str  # "image" or "video"
    prompt: str
    style: Optional[str] = "realistic"
    duration: Optional[int] = None  # For video, duration in seconds
    resolution: Optional[str] = "1024x1024"  # For image resolution

---

## Markdown Chunking

**URL:** llms-txt#markdown-chunking

Source: https://docs.agno.com/reference/knowledge/chunking/markdown

Markdown chunking is a method of splitting markdown based on structure like headers, paragraphs and sections.
This is useful when you want to process large markdown documents in smaller, manageable pieces.

<Snippet file="chunking-markdown.mdx" />

---

## Sqlite for Team

**URL:** llms-txt#sqlite-for-team

**Contents:**
- Usage
- Params
- Developer Resources

Source: https://docs.agno.com/examples/concepts/db/sqlite/sqlite_for_team

Agno supports using Sqlite as a storage backend for Teams using the `SqliteDb` class.

You need to provide either `db_url`, `db_file` or `db_engine`. The following example uses `db_file`.

<Snippet file="db-sqlite-params.mdx" />

## Developer Resources

* View [Cookbook](https://github.com/agno-agi/agno/blob/main/cookbook/db/sqllite/sqlite_for_team.py)

---

## Pinecone Async

**URL:** llms-txt#pinecone-async

**Contents:**
- Code
- Usage

Source: https://docs.agno.com/examples/concepts/vectordb/pinecone-db/async-pinecone-db

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Install libraries">
    
  </Step>

<Step title="Set environment variables">
    
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Install libraries">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Set environment variables">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## FirestoreDb

**URL:** llms-txt#firestoredb

Source: https://docs.agno.com/reference/storage/firestore

`FirestoreDb` is a class that implements the Db interface using Google Firestore as the backend storage system. It provides high-performance, distributed storage for agent sessions with support for JSON data types and schema versioning.

<Snippet file="db-firestore-params.mdx" />

<Snippet file="db-new-bulk-methods.mdx" />

---

## Send a history of messages and add memories

**URL:** llms-txt#send-a-history-of-messages-and-add-memories

**Contents:**
- Usage

memory.create_user_memories(
    messages=[
        Message(role="user", content="Hi, how are you?"),
        Message(role="assistant", content="I'm good, thank you!"),
        Message(role="user", content="What are you capable of?"),
        Message(
            role="assistant",
            content="I can help you with your homework and answer questions about the universe.",
        ),
        Message(role="user", content="My name is Jane Doe"),
        Message(role="user", content="I like to play chess"),
        Message(
            role="user",
            content="Actually, forget that I like to play chess. I more enjoy playing table top games like dungeons and dragons",
        ),
        Message(
            role="user",
            content="I'm also interested in learning about the history of the universe and other astronomical topics.",
        ),
        Message(role="assistant", content="That is great!"),
        Message(
            role="user",
            content="I am really interested in physics. Tell me about quantum mechanics?",
        ),
    ],
    user_id=jane_doe_id,
)

memories = memory.get_user_memories(user_id=jane_doe_id)
print("Jane Doe's memories:")
pprint(memories)
bash  theme={null}
    pip install -U agno
    bash Mac theme={null}
      python cookbook/memory/memory_manager/03_custom_memory_instructions.py
      bash Windows theme={null}
      python cookbook/memory/memory_manager/03_custom_memory_instructions.py
      ```
    </CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Install libraries">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Run Example">
    <CodeGroup>
```

Example 3 (unknown):
```unknown

```

---

## Example usage with different types of videos

**URL:** llms-txt#example-usage-with-different-types-of-videos

youtube_agent.print_response(
    "Analyze this video: https://www.youtube.com/watch?v=zjkBMFhNj_g",
    stream=True,
)

---

## Audio As Input

**URL:** llms-txt#audio-as-input

Source: https://docs.agno.com/concepts/multimodal/audio/audio_input

Learn how to use audio as input with Agno agents.

Agno supports audio as input to agents and teams.  Take a look at the [compatibility matrix](/concepts/models/compatibility#multimodal-support) to see which models support audio as input.

Let's create an agent that can understand audio input.

```python audio_agent.py theme={null}
import base64

import requests
from agno.agent import Agent, RunOutput  # noqa
from agno.media import Audio
from agno.models.openai import OpenAIChat

---

## Tool Use Stream

**URL:** llms-txt#tool-use-stream

**Contents:**
- Code
- Usage

Source: https://docs.agno.com/examples/models/xai/tool_use_stream

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Set your API key">
    
  </Step>

<Step title="Install libraries">
    
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Set your API key">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## No system message should be provided (Gemini requires only the image)

**URL:** llms-txt#no-system-message-should-be-provided-(gemini-requires-only-the-image)

agent = Agent(
    model=Gemini(
        id="gemini-2.0-flash-exp-image-generation",
        response_modalities=["Text", "Image"],
    )
)

---

## Pinecone

**URL:** llms-txt#pinecone

Source: https://docs.agno.com/reference/vector_db/pinecone

<Snippet file="vector-db-pinecone-reference.mdx" />

---

## Setup your Team

**URL:** llms-txt#setup-your-team

team = Team(db=db, ...)

---

## Basic usage - automatically loads from JINA_API_KEY environment variable

**URL:** llms-txt#basic-usage---automatically-loads-from-jina_api_key-environment-variable

embeddings = JinaEmbedder().get_embedding(
    "The quick brown fox jumps over the lazy dog."
)

---

## Note: Replace '''hello_world''' with your actual template name

**URL:** llms-txt#note:-replace-'''hello_world'''-with-your-actual-template-name

**Contents:**
- Usage

agent.print_response(
    "Send a template message using the '''hello_world''' template in English to +91 1234567890"
)
bash  theme={null}
    export WHATSAPP_ACCESS_TOKEN=xxx
    export WHATSAPP_PHONE_NUMBER_ID=xxx
    export OPENAI_API_KEY=xxx # Or your preferred LLM API key
    bash  theme={null}
    pip install -U agno openai google-generativeai # Add any other necessary WhatsApp SDKs
    bash Mac theme={null}
      python cookbook/tools/whatsapp_tools.py
      bash Windows theme={null}
      python cookbook/tools/whatsapp_tools.py
      ```
    </CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Set up WhatsApp Business API">
    1. Go to [Meta for Developers](https://developers.facebook.com/docs/whatsapp/cloud-api/get-started)
    2. Create a Meta App and set up the WhatsApp Business API.
    3. Obtain your Phone Number ID and a permanent System User Access Token.
  </Step>

  <Step title="Set your API keys and identifiers">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## Print team leader session metrics

**URL:** llms-txt#print-team-leader-session-metrics

print("---" * 5, "Session Metrics", "---" * 5)
pprint(team.get_session_metrics().to_dict())

---

## Delete All Content

**URL:** llms-txt#delete-all-content

Source: https://docs.agno.com/reference-api/schema/knowledge/delete-all-content

delete /knowledge/content
Permanently remove all content from the knowledge base. This is a destructive operation that cannot be undone. Use with extreme caution.

---

## Print aggregated team leader metrics

**URL:** llms-txt#print-aggregated-team-leader-metrics

print("---" * 5, "Aggregated Metrics of Team", "---" * 5)
pprint(run_response.metrics)

---

## Create a Loop step for deep tech research

**URL:** llms-txt#create-a-loop-step-for-deep-tech-research

deep_tech_research_loop = Loop(
    name="Deep Tech Research Loop",
    steps=[research_hackernews],
    end_condition=research_quality_check,
    max_iterations=3,
    description="Perform iterative deep research on tech topics",
)

---

## Create workflow with conditional early termination

**URL:** llms-txt#create-workflow-with-conditional-early-termination

workflow = Workflow(
    name="Data Processing with Early Exit",
    description="Process data but stop early if validation fails",
    steps=[
        data_validator,  # Step 1: Validate data
        early_exit_validator,  # Step 2: Check validation and possibly stop early
        data_processor,  # Step 3: Process data (only if validation passed)
        report_generator,  # Step 4: Generate report (only if processing completed)
    ],
)

if __name__ == "__main__":
    print("\n=== Testing with INVALID data ===")
    workflow.print_response(
        input="Process this data: {'user_count': -50, 'revenue': 'invalid_amount', 'date': 'bad_date'}"
    )

print("=== Testing with VALID data ===")
    workflow.print_response(
        input="Process this data: {'user_count': 1000, 'revenue': 50000, 'date': '2024-01-15'}"
    )
```

To checkout async version, see the cookbook-

* [Early Stop Workflow with Loop](https://github.com/agno-agi/agno/blob/main/cookbook/workflows/_06_advanced_concepts/_02_early_stopping/early_stop_workflow_with_loop.py)
* [Early Stop Workflow with Parallel](https://github.com/agno-agi/agno/blob/main/cookbook/workflows/_06_advanced_concepts/_02_early_stopping/early_stop_workflow_with_parallel.py)
* [Early Stop Workflow with Router](https://github.com/agno-agi/agno/blob/main/cookbook/workflows/_06_advanced_concepts/_02_early_stopping/early_stop_workflow_with_router.py)
* [Early Stop Workflow with Step](https://github.com/agno-agi/agno/blob/main/cookbook/workflows/_06_advanced_concepts/_02_early_stopping/early_stop_workflow_with_step.py)
* [Early Stop Workflow with Steps](https://github.com/agno-agi/agno/blob/main/cookbook/workflows/_06_advanced_concepts/_02_early_stopping/early_stop_workflow_with_steps.py)

---

## "[Feature Request] Support json schemas in Gemini client in addition to pydantic base model",

**URL:** llms-txt#"[feature-request]-support-json-schemas-in-gemini-client-in-addition-to-pydantic-base-model",

---

## 1. Create a new bot with BotFather on Telegram. https://core.telegram.org/bots/features#creating-a-new-bot

**URL:** llms-txt#1.-create-a-new-bot-with-botfather-on-telegram.-https://core.telegram.org/bots/features#creating-a-new-bot

---

## Atla

**URL:** llms-txt#atla

**Contents:**
- Prerequisites
- Configuration
- Example

Source: https://docs.agno.com/integrations/observability/atla

Integrate `Atla` with Agno for real-time monitoring, automated evaluation, and performance analytics of your AI agents.

[Atla](https://www.atla-ai.com/) is an advanced observability platform designed specifically for AI agent monitoring and evaluation.
This integration provides comprehensive insights into agent performance, automated quality assessment, and detailed analytics for production AI systems.

* **API Key**: Obtain your API key from the [Atla dashboard](https://app.atla-ai.com)

Install the Atla Insights SDK with Agno support:

Configure your API key as an environment variable:

```python  theme={null}
from os import getenv
from agno.agent import Agent
from agno.models.openai import OpenAIChat
from agno.tools.duckduckgo import DuckDuckGoTools
from atla_insights import configure, instrument_agno

**Examples:**

Example 1 (unknown):
```unknown
## Configuration

Configure your API key as an environment variable:
```

Example 2 (unknown):
```unknown
## Example
```

---

## "current_context": get_current_context

**URL:** llms-txt#"current_context":-get_current_context

---

## Setup workflow

**URL:** llms-txt#setup-workflow

**Contents:**
- Developer Resources

long_running_workflow = Workflow(
    name="Long Running Analysis",
    steps=[
        Step(name="Research", agent=Agent(model=OpenAIChat(id="gpt-5-mini"), instructions="You are a helpful assistant that can research the web.")),
        Step(name="Deep Analysis", agent=Agent(model=OpenAIChat(id="gpt-5-mini"), instructions="You are a helpful assistant that can analyze the web.")),
        Step(name="Report Generation", agent=Agent(model=OpenAIChat(id="gpt-5-mini"), instructions="You are a helpful assistant that can generate a report.")),
    ]
)

async def main():
    # Start background workflow
    bg_response = await long_running_workflow.arun(
        input="Comprehensive market analysis for emerging technologies",
        background=True
    )
    
    print(f"Started workflow with run_id: {bg_response.run_id}")
    
    # Simulate some time passing
    await asyncio.sleep(5)
    
    # Cancel the workflow
    cancellation_result = long_running_workflow.cancel_run(bg_response.run_id)
    
    if cancellation_result:  # cancellation_result is a bool
        print(f"✅ Workflow {bg_response.run_id} cancelled successfully")
    else:
        print(f"❌ Failed to cancel workflow {bg_response.run_id}")

asyncio.run(main())
```

<Note>
  When a workflow in streaming mode is cancelled, a specific event is triggered: `WorkflowRunEvent.workflow_cancelled` or simply known as `WorkflowCancelledEvent`
</Note>

## Developer Resources

* [Workflow Cancellation](/examples/concepts/workflows/06_workflows_advanced_concepts/workflow_cancellation)

---

## LlamaIndex

**URL:** llms-txt#llamaindex

**Contents:**
- Code
- Usage

Source: https://docs.agno.com/examples/concepts/vectordb/llamaindex-db/llamaindex-db

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Install libraries">
    
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Install libraries">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

Example 3 (unknown):
```unknown

```

---

## Embed each chunk and load it into the vector store

**URL:** llms-txt#embed-each-chunk-and-load-it-into-the-vector-store

Chroma.from_documents(
    documents, OpenAIEmbeddings(), persist_directory=str(chroma_db_dir)
)

---

## Configure email settings

**URL:** llms-txt#configure-email-settings

sender_email = "verified-sender@example.com"  # Your verified SES email
sender_name = "AI Research Updates"
region_name = "us-east-1"

---

## OpenCV

**URL:** llms-txt#opencv

**Contents:**
- Example
- Toolkit Params
- Toolkit Functions
- Developer Resources

Source: https://docs.agno.com/concepts/tools/toolkits/others/opencv

OpenCVTools enables agents to capture images and videos from webcam using OpenCV computer vision library.

The following agent can capture images and videos from your webcam:

| Parameter              | Type   | Default | Description                                           |
| ---------------------- | ------ | ------- | ----------------------------------------------------- |
| `show_preview`         | `bool` | `False` | Whether to show camera preview window during capture. |
| `enable_capture_image` | `bool` | `True`  | Enable image capture functionality.                   |
| `enable_capture_video` | `bool` | `True`  | Enable video capture functionality.                   |

| Function        | Description                                              |
| --------------- | -------------------------------------------------------- |
| `capture_image` | Capture a single image from the webcam.                  |
| `capture_video` | Record a video from the webcam for a specified duration. |

## Developer Resources

* View [Tools Source](https://github.com/agno-agi/agno/blob/main/libs/agno/agno/tools/opencv.py)
* [OpenCV Documentation](https://docs.opencv.org/)

---

## Code Generation

**URL:** llms-txt#code-generation

**Contents:**
- Code
- Usage

Source: https://docs.agno.com/examples/models/vllm/code_generation

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Install Libraries">
    
  </Step>

<Step title="Start vLLM server">
    
  </Step>

<Step title="Run Agent">
    
  </Step>
</Steps>

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Install Libraries">
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Start vLLM server">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
```

---

## Ollama Cloud

**URL:** llms-txt#ollama-cloud

**Contents:**
- Code
- Usage
- Key Features

Source: https://docs.agno.com/examples/models/ollama/cloud

<Steps>
  <Snippet file="create-venv-step.mdx" />

<Step title="Set up Ollama Cloud API Key">
    Sign up at [ollama.com](https://ollama.com) and get your API key, then export it:

<Step title="Install libraries">
    
  </Step>

<Step title="Run Agent">
    <CodeGroup>

</CodeGroup>
  </Step>
</Steps>

* **No local setup required**: Access powerful models instantly without downloading or managing local installations
* **Production-ready**: Enterprise-grade infrastructure with reliable uptime and performance
* **Wide model selection**: Access to powerful models including GPT-OSS and other optimized cloud models
* **Automatic configuration**: When `api_key` is provided, the host automatically defaults to `https://ollama.com`

**Examples:**

Example 1 (unknown):
```unknown
## Usage

<Steps>
  <Snippet file="create-venv-step.mdx" />

  <Step title="Set up Ollama Cloud API Key">
    Sign up at [ollama.com](https://ollama.com) and get your API key, then export it:
```

Example 2 (unknown):
```unknown
</Step>

  <Step title="Install libraries">
```

Example 3 (unknown):
```unknown
</Step>

  <Step title="Run Agent">
    <CodeGroup>
```

Example 4 (unknown):
```unknown

```

---

## The first tool call will be performed. The second one will fail gracefully.

**URL:** llms-txt#the-first-tool-call-will-be-performed.-the-second-one-will-fail-gracefully.

**Contents:**
- To consider

agent.print_response(
    "Find me the current price of TSLA, then after that find me the latest news about Tesla.",
    stream=True,
)

* If the Agent tries to run a number of tool calls that exceeds the limit **all at once**, the limit will remain effective. Only as many tool calls as allowed will be performed.
* The limit is enforced **across a full run**, and not per individual requests triggered by the Agent.

---

## 2. Multimodal Query for Video Analysis

**URL:** llms-txt#2.-multimodal-query-for-video-analysis

You are an expert in video content creation, specializing in crafting engaging short-form content for platforms like YouTube Shorts and Instagram Reels. Your task is to analyze the provided video and identify segments that maximize viewer engagement.

For each video, you'll:

1. Identify key moments that will capture viewers' attention, focusing on:
   - High-energy sequences
   - Emotional peaks
   - Surprising or unexpected moments
   - Strong visual and audio elements
   - Clear narrative segments with compelling storytelling

2. Extract segments that work best for short-form content, considering:
   - Optimal length (strictly 15–60 seconds)
   - Natural start and end points that ensure smooth transitions
   - Engaging pacing that maintains viewer attention
   - Audio-visual harmony for an immersive experience
   - Vertical format compatibility and adjustments if necessary

3. Provide a detailed analysis of each segment, including:
   - Precise timestamps (Start Time | End Time in MM:SS format)
   - A clear description of why the segment would be engaging
   - Suggestions on how to enhance the segment for short-form content
   - An importance score (1-10) based on engagement potential

Your goal is to identify moments that are visually compelling, emotionally engaging, and perfectly optimized for short-form platforms.
"""

---

## Add JWT middleware to the app

**URL:** llms-txt#add-jwt-middleware-to-the-app

---

## Create model using Portkey

**URL:** llms-txt#create-model-using-portkey

model = Portkey(
    id="@first-integrati-707071/gpt-5-nano",
)

agent = Agent(model=model, markdown=True)

---
